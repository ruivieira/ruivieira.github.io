<!doctype html><html lang=en-uk><head><script data-goatcounter=https://ruivieira-dev.goatcounter.com/count async src=//gc.zgo.at/count.js></script>
<script type=module src=/js/deeplinks/deeplinks.js></script>
<link rel=preload href=/lib/fonts/fa-brands-400.woff2 as=font type=font/woff2 crossorigin=anonymous><link rel=preload href=/lib/fonts/fa-regular-400.woff2 as=font type=font/woff2 crossorigin=anonymous><link rel=preload href=/lib/fonts/fa-solid-900.woff2 as=font type=font/woff2 crossorigin=anonymous><link rel=preload href=/fonts/firacode/FiraCode-Regular.woff2 as=font type=font/woff2 crossorigin=anonymous><link rel=preload href=/fonts/prociono/Prociono-Regular.woff2 as=font type=font/woff2 crossorigin=anonymous><link rel=stylesheet href=/css/kbd.css type=text/css><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><title>A streaming ALS implementation Â· Rui Vieira</title><link rel=canonical href=/a-streaming-als-implementation.html><meta name=viewport content="width=device-width,initial-scale=1"><meta name=robots content="all,follow"><meta name=googlebot content="index,follow,snippet,archive"><meta property="og:title" content="A streaming ALS implementation"><meta property="og:description" content="In this blog post I would like to talk a little bit about recommendation engines in general and how to build a streaming recommendation engine on top of Apache Spark.
I will start by introducing the concept of collaborative filterig, and focus in two variants: batch and streaming Alternating Least Squares (ALS). I will look at the principles of a streaming distributed recommendation engine on Spark and finally, I&rsquo;ll talk about practical issues when using these methods."><meta property="og:type" content="article"><meta property="og:url" content="/a-streaming-als-implementation.html"><meta property="article:section" content="posts"><meta property="article:modified_time" content="2023-01-15T15:53:40+00:00"><meta name=twitter:card content="summary"><meta name=twitter:title content="A streaming ALS implementation"><meta name=twitter:description content="In this blog post I would like to talk a little bit about recommendation engines in general and how to build a streaming recommendation engine on top of Apache Spark.
I will start by introducing the concept of collaborative filterig, and focus in two variants: batch and streaming Alternating Least Squares (ALS). I will look at the principles of a streaming distributed recommendation engine on Spark and finally, I&rsquo;ll talk about practical issues when using these methods."><link rel=stylesheet href=/css/styles.css><!--[if lt IE 9]><script src=https://oss.maxcdn.com/html5shiv/3.7.2/html5shiv.min.js></script>
<script src=https://oss.maxcdn.com/respond/1.4.2/respond.min.js></script><![endif]--><link rel=icon type=image/png href=images/favicon.ico></head><body class="max-width mx-auto px3 ltr"><div class="content index py4"><div id=header-post><a id=menu-icon href=#><i class="fas fa-eye fa-lg"></i></a>
<a id=menu-icon-tablet href=#><i class="fas fa-eye fa-lg"></i></a>
<a id=top-icon-tablet href=# onclick='$("html, body").animate({scrollTop:0},"fast")' style=display:none aria-label="Top of Page"><i class="fas fa-chevron-up fa-lg"></i></a>
<span id=menu><span id=nav><ul><li><a href=/>Home</a></li><li><a href=/map/>All pages</a></li><li><a href=/search.html>Search</a></li></ul></span><br><div id=share style=display:none></div><div id=toc><h4>Contents</h4><nav id=TableOfContents><ul><li><a href=#batch-als>Batch ALS</a></li><li><a href=#streaming-als>Streaming ALS</a></li><li><a href=#apache-spark>Apache Spark</a></li><li><a href=#streaming-data>Streaming data</a></li><li><a href=#caveats>Caveats</a><ul><li><a href=#cold-start>Cold start</a></li><li><a href=#hyperparameter-estimation>Hyperparameter estimation</a></li><li><a href=#performance>Performance</a></li></ul></li></ul></nav></ul></nav></div></span></div><article class=post itemscope itemtype=http://schema.org/BlogPosting><header><h1 class=posttitle itemprop="name headline">A streaming ALS implementation</h1><div class=meta><div class=postdate>Updated <time datetime="2023-01-15 15:53:40 +0000 GMT" itemprop=datePublished>2023-01-15</time>
<span class=commit-hash>(<a href=/log/index.html#fe73f60>fe73f60</a>)</span></div></div></header><div class=content itemprop=articleBody><p>In this blog post I would like to talk a little bit about recommendation engines in general and how to build a streaming recommendation engine on top of <a href=https://spark.apache.org/>Apache Spark</a>.</p><p>I will start by introducing the concept of collaborative filterig, and focus in two variants: <a href=#batch_als>batch</a> and <a href=#streaming_als>streaming</a> Alternating Least Squares (ALS). I will look at the principles of a streaming distributed recommendation engine on Spark and finally, I&rsquo;ll talk about practical issues when using these methods.</p><h1 id=recommendation-engines>Recommendation engines</h1><p>So what <em>are</em> &ldquo;recommendation engines&rdquo;?</p><p>Recommendation engines are a popular method to match <em>users</em>, <em>products</em> and historical <em>data</em> on user behaviour.</p><h1 id=collaborative-filtering>Collaborative filtering</h1><p>In the majority of cases, we assume there&rsquo;s a unique mapping between a <em>user</em> $x$, a <em>product</em> $y$ and <em>rating</em> $\mathsf{R}_{x,y}$.</p><p>$$
\left(x,y\right) \mapsto \mathsf{R}_{x,y}
$$</p><p>The &ldquo;collaborative&rdquo; aspect refers to the fact that we are using collective information from a group of users and &ldquo;filtering&rdquo; is simply a synonym for &ldquo;prediction&rdquo;.</p><p>So, we use collaborative filtering quite frequently in our daily life and it really seems like common sense.</p><p>The main principle is that if a group of people tend to collectively have similar tastes, it is more likely that they agree on an unknown product.</p><p>Let&rsquo;s imagine that you have a number of friends with whom you share a very similar musical taste, let&rsquo;s call it <code>A</code> and another group, <code>B</code>, compared to which you have very different musical tastes.</p><p>If group <code>A</code> and group <code>B</code> both recommend you a new album which they regard highly, which one would you pick?</p><p>You will probably pick the album from group <code>A</code>, right? So that&rsquo;s collaborative filtering in a nutshell.</p><figure><img src=/site/images/als/groups.png alt=groups.png></figure><blockquote><p><strong>Bonus question</strong></p><p><em>What if an album is considered really bad by group B? Does it mean you&rsquo;ll like it?</em></p><p>It&rsquo;s difficult to tell. Because group <code>A</code> has relevance to you, it&rsquo;s easy to match.
Because <code>B</code> is too dissimilar, a low rating is not very informative.</p></blockquote><h1 id=alternating-least-squares-als>Alternating Least Squares (ALS)</h1><p>One of the most popular collaborative filtering methods is <a href=https://link.springer.com/chapter/10.1007/978-3-540-68880-8_32>Alternating Least Squares</a> (ALS).</p><p>In ALS we assume that the available rating data can be represented in a sparse matrix form, that is, we will assume a sequential ordering of both users and products. Each entry of the matrix will then represent the rating for a unique pair of user and products.</p><p>If we then consider ratings data as a matrix, let&rsquo;s call it $\mathsf{R}$, the <em>user</em> and <em>product</em> ids will represent coordinates in a ratings matrix and the actual rating will be the value for that particular entry. To keep the notation consistent with the above we simply call the entry $(x,y)$ as $\mathsf{R}_{x,y}$. This will look something like the matrix represented in the figure below.</p><figure><img src=/site/images/als/ratings_table.png alt=ratings_table.png></figure><p>The idea behind ALS is to factorise the ratings matrix $\mathsf{R}_{x,y}$ into two matrices $\mathsf{U}$ and $\mathsf{P}$, which in turn, when multiplied back, will return an approximation of the original ratings matrix, that is:</p><p>$$
\mathsf{R} \approx \hat{\mathsf{R}} = \mathsf{U}^T \mathsf{P}
$$</p><p>To &ldquo;predict&rdquo; a missing rating for a user $x$ and product $y$, we can simply multiply two vectors, namely the $x$ row from the user latent factors and the $y$ column from the product latent factors, $\hat{\mathsf{R}}_{x,y}$, that is:</p><p>$$
\hat{\mathsf{R}}_{x,y} = \mathsf{U}_x^T \mathsf{P}_y
$$</p><p>There are several ways to tackle this factorisation problem and we will cover two of them in here. We will first look at a <a href=#batch_als>batch method</a>, which aims at factorising using the whole of the ratings matrix and a <a href=#streaming_als>stochastic gradient descent method</a>, which uses a single observation at a time.</p><h2 id=batch-als>Batch ALS</h2><p>This factorisation is performed by first defining an (objective) loss function (here called $\ell$).</p><p>A general form is represented below where, as before, $\mathsf{R} _{x,y}$ is the <em>true rating</em> and $\hat{\mathsf{R}} _{x,y}$ is the <em>predicted rating</em>, calculated as seen previously. The remaining terms are simply regularisation terms to help prevent overfitting.</p><p>$$
\ell = \sum c _{x,y} \left(\mathsf{R} _{x,y} - \underbrace{\mathsf{U}_x^T \mathsf{P} _y} _{\hat{\mathsf{R}} _{x,t}}\right)^2 + \lambda\left(\left\lVert \mathsf{U} \right\rVert^2 + \left\lVert \mathsf{P} \right\rVert^2\right)
$$</p><p>The value of $(c_{x,y})$ constitutes a penalisation function and will depend on whether we are considering <em>explicit</em> or <em>implicit</em> feedback. If we consider the known ratings as our training dataset $\mathcal{T}$, then, in the case of explicit feedback we have</p><p>$$
c_{x,y} = \begin{cases}
0,\qquad\text{if}\ \left(x,y\right) \notin \mathcal{T} \\
1,\qquad\text{if}\ \left(x,y\right) \in \mathcal{T}
\end{cases}
$$</p><p>Constraining our loss function to only include known ratings. The implicit feedback case is different (and a possible future topic) and for the remainder of this post we will only consider the explicit feedback case. Given the above, we can then simplify our loss function, in the explicit feedback case, to</p><p>$$
\ell = \sum _{x,y \in \mathcal{T}} \left(\mathsf{R} _{x,y} -\hat{\mathsf{R}} _{x,y}\right)^2 + \lambda\left(\left\lVert \mathsf{U} \right\rVert^2 + \left\lVert \mathsf{P} \right\rVert^2\right)
$$</p><p>Minimizing $\ell$ is however an NP-hard problem, due to its non-convexity. However, if we treat $\mathsf{U}$ as constant, then $\ell$ is a convex in relation to $\mathsf{P}$ and if we treat $\mathsf{P}$ as constant, $\ell$ is convex in relation to $\mathsf{U}$. We can then alternate between fixing $\mathsf{U}$ and $\mathsf{P}$, changing the values such that the loss function $\ell$ (above) is minimized. This procedure is then repeated until we reach convergence.</p><p>The way that ALS works is, in simplified terms, to find the factors $\mathsf{U}$ and $\mathsf{P}$, which when multiplied together provide an approximation of our ratings matrix $\mathsf{R}$, as we&rsquo;ve seen previously.</p><p>Once we have the factors $\mathsf{U}$ and $\mathsf{P}$, we can then predict the missing values in $\mathsf{R}$ by using the approximation $\hat{\mathsf{R}}$.</p><p>It is clear that in a real world scenario we would have <em>many</em> missing ratings, simply due to the assumption that no user rates all products (if they did, the case for a recommendation engine will be significantly weaker). ALS is designed to deal with sparse matrices and to fill the blanks using <em>predicted values</em>. After factorization, our approximated ratings matrix will look something like this:</p><figure><img src=/site/images/als/ratings_table_filled.png alt=ratings_table_filled.png></figure><p>As mentioned previously, the first step is then to minimise the loss function. In this case we take the partial derivatives and set them to zero and fortunately this has a closed form solution. We get a system of linear equations which we can easily implement. The system will correspond to the solution of</p><p>$$
\frac{\partial \ell}{\partial \mathsf{U}_x}=0, \qquad \frac{\partial \ell}{\partial \mathsf{P}_y}=0.
$$</p><p>We start by solving the user latent factor minimisation using:</p><p>$$
\begin{aligned}
\frac{1}{2}\frac{\partial \ell}{\partial \mathsf{U} _x}&=0 \\</p><p>\frac{1}{2}\frac{\partial}{\partial \mathsf{U} _ x} \sum _ {x,y \in \mathcal{T}} \left(\mathsf{R} _ {x,y} - \mathsf{U} _ x^T \mathsf{P } _ y\right)^2 + \lambda\left(\left\lVert \mathsf{U} \right\rVert^2 + \left\lVert \mathsf{P} \right\rVert^2\right)&=0 \\</p><p>-\sum _{x,y \in \mathcal{T}} \left(\mathsf{R} _ {x,y} - \mathsf{U} _x^T \mathsf{P}_y\right)\mathsf{P} _y^T + \lambda \mathsf{U}\ _ x^T&=0\\</p><p>-\left(\mathsf{R} _ x -\mathsf{U} _ x^T \mathsf{P}^T\right)\mathsf{P} + \lambda \mathsf{U} _x^T&=0\\</p><p>\mathsf{U} _ x^T\left(\mathsf{P}^T \mathsf{P} + \lambda \boldsymbol{\mathsf{I}}\right) &= \mathsf{R} _ x \mathsf{P} \\</p><p>\mathsf{U} _ x^T &= \mathsf{R} _ x \mathsf{P} \left(\mathsf{P}^T \mathsf{P} + \lambda \boldsymbol{\mathsf{I}}\right)^{-1}.
\end{aligned}
$$</p><p>Similarly, we can solve for the product latent factor by using:</p><p>$$
\begin{aligned}
\frac{1}{2}\frac{\partial \ell}{\partial \mathsf{P} _y}&=0 \\
-\sum _{x,y \in \mathcal{T}} \left(\mathsf{R} _{x,y} - \mathsf{P} _y^T \mathsf{U} _x\right)\mathsf{U}_x^T + \lambda \mathsf{P} _y^T&=0\\
-\left(\mathsf{R}_y - \mathsf{P} _y^T \mathsf{U}^T\right)\mathsf{U} + \lambda \mathsf{P} _y^T&=0\\
\mathsf{P} _y^T\left(\mathsf{U}^T \mathsf{U} + \lambda \boldsymbol{\mathsf{I}}\right) &= \mathsf{R} _y \mathsf{U} \\
\mathsf{P} _y^T &= \mathsf{R} _y \mathsf{U} \left(\mathsf{U}^T \mathsf{U} + \lambda \boldsymbol{\mathsf{I}}\right)^{-1}.
\end{aligned}
$$</p><p>We can then calculate each factor iteratively, by fixing the other one and solving the estimator. While this process is alternated, an error measure (usually the <a href=/root-mean-squared-error.html>Root Mean Squared Error</a>), or $RMSE$ is calculated (as below) between the rating matrix approximation given by the latent factors and the ratings which we have, $\mathcal{T}$.
This method is guaranteed to converge and when we consider out approximation to be good enough, or after a set number of iterations we can then stop the refinement.</p><p>$$
RMSE = \sqrt{\frac{1}{n}\sum _{x,y \in \mathcal{T}}\lvert \hat{\mathsf{R}} _{x,y} - \mathsf{R} _{x,y}\rvert}
$$</p><p>After the latent factors are estimated, we can then use them to try to recreate the original ratings matrix with the approximation as we&rsquo;ve seen.
The missing ratings in the original matrix will now be filled by values which minimize the least squares recursion and these are taken as the ratings &ldquo;predictions&rdquo;.</p><p>To illustrate the working of ALS, let&rsquo;s assume we have a very quirky shop that only ever sells 300 products and has exactly 300 customers.
On top of that, users are allowed to use 8 bit number to rate the products. We will also assume in this unusual shop that every user has rated every product.</p><p>Now we&rsquo;re humans, and we visualise patterns in colour more easily than in numbers. We will assign a palette to the ratings, so that each rating corresponds to a colour.</p><figure><img src=/site/images/als/colour_matrix.png alt=colour_matrix.png></figure><p>I think you know where this is going &mldr; we make up this final ratings matrix so now we can visualise the ALS progress.</p><figure><img src=/site/images/als/mona_lisa_pixelated_viridis.png alt=mona_lisa_pixelated_viridis.png></figure><p>So how do we perform this factorisation? The initial step is to fill the latent factors $\mathsf{U}$ and $\mathsf{P}$) with random values. Since at this point, we assume we don&rsquo;t have any ratings, having random factors will lead to an initial random guess of the ratings matrix.</p><figure><img src=/site/images/als/initial.png alt=initial.png></figure><p>We then proceed to calculate each factor matrix, as we&rsquo;ve seen, by calculating one using the estimator while keeping the other one constant and then alternating. We can see by the movie below that at each iteration the approximation to the original ratings gets better, stabilising after a few steps.</p><p>This is to be expected, in this case, since this would be the simplest implementation of ALS: a batch ALS on a single machine where we know all the ratings.</p><p><a href>simple-factorization.mp4</a></p><p>So a fair question that arises is: why can&rsquo;t we update this model and perform recommendations in a streaming fashion using this method?</p><p>After all, if users add product ratings, we can simply update the predictions by recalculating the factors!</p><p>The problem is that when a new rating is added, or when new users and new products are added, we need to recalculate the entirety of the $\mathsf{U}$ and $\mathsf{P}$ matrices, and to do so, we need to have access to all of the data, $\mathsf{R}$.</p><h2 id=streaming-als>Streaming ALS</h2><p>Ideally, we want a method that would allow us to update $\mathsf{U}$ and $\mathsf{P}$ using one observation, $\mathsf{R}_{x,y}$ at a time</p><p>It turns out that the <em>Stochastic Gradient Descent</em> (or SGD)<sup id=fnref:1><a href=#fn:1 class=footnote-ref role=doc-noteref>1</a></sup> method allows us to do precisely that. We&rsquo;ll look at the specific variant of SGD we&rsquo;ve used which is called <em>Bias-Stochastic Gradient Descent</em> (B-SGD).</p><p>It is important to keep in mind, under a certain point of view, both methods aim at the same thing.</p><figure><img src=/assets/batch_streaming_comparison.png alt=batch_streaming_comparison.png></figure><p>They both try to factorise the ratings matrix as latent factors, which would then be used to perform predictions. The main differences are of course, how the data is used (batch or one observation at the time) and how the factorisation is calculated.</p><p>In the SGD case we use the concept of biases in both users and items. The bias is a measure of how consistently a product is rated by different users. The bias of rating $(x,y)$, that is the rating given by user $x$ to product $y$, can be calculated as the sum of $\mu$, an overall average rating and the observed deviations of user $x$, which we call $b_x$, and the observed deviations of product $y$, called $b_y$, that is:</p><p>$$
b_{x,y} = \mu + b_x + b_y
$$</p><p>This bias information is now incorporated in the rating prediction. We can see that the SGD prediction is simply the batch prediction plus the corresponding bias term</p><p>$$
\hat{\mathsf{R}} _{x,y} = b _{x,y} + \underbrace{\mathsf{U}^T _x \times \mathsf{P} _y} _{batch}
$$</p><p>If we take the loss function definition for the batch method (and still considering the explicit feedback case), we can then replace the predicted rating formulation with our new one. We have, as before, some regularisation terms, but now also include a new regularisation term for the bias components,</p><p>but we don&rsquo;t need to go into that.</p><p>$$
\ell _{SGD} = \sum _{x,y \in \mathcal{T}} \left(\mathsf{R} _{x,y} - b _{x,y} - \hat{\mathsf{R}} _{x,y}\right)^2 + \lambda\left(\left\lVert \mathsf{U} \right\rVert^2 + \left\lVert \mathsf{P} \right\rVert^2 + b _x^2 + b _y^2\right)
$$</p><p>Since calculating the full gradient is computationally very expensive, we calculate it for a single observation. As we can see, the SGD method allows us to update the user and product specific bias as well as a single user and product latent factor row given a single rating.</p><p>Provided we have a single rating, the rating of user $x$ for product $y$, we can update the biases as well as the latent vectors for user $x$ and for product $y$, that is, we no longer need to update the entire matrices $\mathsf{U}$ and $\mathsf{P}$, while still maintaining a convergence property.</p><p>Provided with a learning rate $\gamma$ and defining our <em>prediction error</em> as</p><p>$$
\epsilon _ {x,y}=\mathsf{R} _{x,y}-\hat{\mathsf{R}} _{x,y},
$$</p><p>the biases and latent factors can now be updated in the opposite direction of the calculated gradient, proportionally to the learning rate, such that</p><p>$$
\begin{aligned}
b _x &\leftarrow b _x + \gamma \left(\epsilon _{x,y}-\lambda _x b _x\right) \\
b _y &\leftarrow b _y + \gamma \left(\epsilon _{x,y}-\lambda _y b _y\right) \\
\mathsf{U} _x &\leftarrow \mathsf{U} _x + \gamma \left(\epsilon _{x,y}\mathsf{P} _y - \lambda^\prime _x \mathsf{U} _x\right) \\
\mathsf{P} _y &\leftarrow \mathsf{P} _y + \gamma \left(\epsilon _{x,y}\mathsf{U} _x - \lambda^\prime _y \mathsf{P} _y\right)
\end{aligned}
$$</p><p>So the practical difference, in terms of streaming data is evident now. Given that, in both methods, the objective is to estimate the latent factors, given the ratings: with batch ALS, whenever we get a new rating, we need to fully recalculate the factors iteratively until we reach convergence. Conversely, with an SGD based factorisation, whenever we have a new rating, we can simply estimate the relevant row and column in the latent factors, by calculating the gradients and adjusting its values.</p><figure><img src=/site/images/als/batch_streaming_comparison_2.png alt=batch_streaming_comparison_2.png></figure><p>Next we show the previous manufactured ratings matrix being factorised using B-SGD. We now simply recalculate the biases and a single latent factor vector, one observation at the time. We can see that, as expected, the convergence is slower (we <em>are</em> using a single observation at each step) but in the end, it produces a similar result.</p><p><a href>sgd-factorization.mp4</a></p><p>Now, this works fine for a single machine implementing streaming ALS. But we are interested in scaling this to something larger than this example so we will use a distributed implementation of ALS. And this is were it can start to get tricky. As it is the case with distributed algorithms, there are some pitfalls which we need to avoid in order to have a performant implementation. We will look at a few of these by looking at the Apache Spark&rsquo;s and its default ALS implementation.</p><h2 id=apache-spark>Apache Spark</h2><p>As probably most of you are familiar with, Spark is an Apache community project which aims at providing a modern platform for distributed computations. Spark provides several core data structures, such as <code>RDD</code>s (<a href=https://spark.apache.org/docs/latest/rdd-programming-guide.html#resilient-distributed-datasets-rdds>Resilient Distributed Datasets</a>), <code>Dataframes</code> and <code>Datasets</code>.</p><p>The <code>RDD</code> is an immutable, distributed typed collection of objects. The <code>RDD</code> is partitioned across the cluster. This allows the spark operations, such as function mapping, to be applied to each subset of the <code>RDD</code> in parallel at each partition.</p><figure><img src=/Excalidraw/ALS%20RDD.excalidraw.svg alt="ALS RDD.excalidraw.svg"></figure><p>For the streaming ALS application we will use <code>RDD</code>s to implement the algorithm. Spark&rsquo;s MLlib provides a collaborative filtering implementation based on the distributed batch ALS which we&rsquo;ve covered previously. The API is quite simple and to train a model we need:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-scala data-lang=scala><span style=display:flex><span><span style=font-weight:700>val</span> model <span style=font-weight:700>=</span> <span style=color:#458;font-weight:700>ALS</span><span style=font-weight:700>.</span>train<span style=font-weight:700>(</span>ratings<span style=font-weight:700>,</span> rank<span style=font-weight:700>,</span> iterations<span style=font-weight:700>,</span> lambda<span style=font-weight:700>)</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=font-weight:700>case</span> <span style=font-weight:700>class</span> <span style=color:#458;font-weight:700>Rating</span><span style=font-weight:700>(</span>int user<span style=font-weight:700>,</span> int product<span style=font-weight:700>,</span> double rating<span style=font-weight:700>)</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=font-weight:700>val</span> ratings<span style=font-weight:700>:</span> <span style=color:#458;font-weight:700>RDD</span><span style=font-weight:700>[</span><span style=color:#458;font-weight:700>Rating</span><span style=font-weight:700>]</span>
</span></span><span style=display:flex><span><span style=font-weight:700>val</span> rank<span style=font-weight:700>:</span> <span style=color:#458;font-weight:700>int</span>
</span></span><span style=display:flex><span><span style=font-weight:700>val</span> iterations<span style=font-weight:700>:</span> <span style=color:#458;font-weight:700>int</span>
</span></span><span style=display:flex><span><span style=font-weight:700>val</span> lambda<span style=font-weight:700>:</span> <span style=color:#458;font-weight:700>Double</span>
</span></span></code></pre></div><ul><li>An <code>RDD</code> containing the ratings. The <code>RDD</code> has elements of the class <code>Rating</code>, which is basically a wrapper around a tuple of user id, product id and rating. This <code>RDD</code> corresponds to all the entries in our ratings matrix used previously.</li><li>The rank which corresponds to the number of elements in our latent factor vectors (this would be the number of columns or rows in our $\mathsf{U}$ and $\mathsf{P}$ matrices).</li><li>A stopping criteria in terms of iterations for the ALS.</li><li>And finally, we set the <code>lambda</code> parameter, a regularisation parameter, which we&rsquo;ve shown to be a part of the loss function&rsquo;s regularisation.</li></ul><p>Since we have the data, the question is then how to choose the parameters. A typical method is to split the original ratings data into two random sets, one for training and one for validation. We then proceed to train the model to several different parameters, usually according to a grid search, and calculate some error measure between the predicted and validation ratings, choosing the parameters which minimise the error.</p><p>Once the model is trained, we get a <code>MatrixFactorizationModel</code> instance, which is basically a wrapper for the latent factors as <code>RDD</code>s.</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-scala data-lang=scala><span style=display:flex><span><span style=font-weight:700>val</span> model <span style=font-weight:700>=</span> <span style=color:#458;font-weight:700>ALS</span><span style=font-weight:700>.</span>train<span style=font-weight:700>(</span>ratings<span style=font-weight:700>,</span> rank<span style=font-weight:700>,</span> iterations<span style=font-weight:700>,</span> lambda<span style=font-weight:700>)</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>model<span style=font-weight:700>:</span> <span style=color:#458;font-weight:700>MatrixFactorizationModel</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=font-weight:700>class</span> <span style=color:#458;font-weight:700>MatrixFactorizationModel</span> <span style=font-weight:700>{</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>	<span style=font-weight:700>val</span> userFeatures<span style=font-weight:700>:</span> <span style=color:#458;font-weight:700>RDD</span><span style=font-weight:700>[(</span><span style=color:#458;font-weight:700>Int</span>, <span style=color:#458;font-weight:700>Array</span><span style=font-weight:700>[</span><span style=color:#458;font-weight:700>Double</span><span style=font-weight:700>])]</span>
</span></span><span style=display:flex><span>	<span style=font-weight:700>val</span> productFeatures<span style=font-weight:700>:</span> <span style=color:#458;font-weight:700>RDD</span><span style=font-weight:700>[(</span><span style=color:#458;font-weight:700>Int</span>, <span style=color:#458;font-weight:700>Array</span><span style=font-weight:700>[</span><span style=color:#458;font-weight:700>Double</span><span style=font-weight:700>])]</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=font-weight:700>}</span>
</span></span></code></pre></div><p>One we have the trained model, we can now perform predictions.</p><h2 id=streaming-data>Streaming data</h2><p>We now want to build a streaming recommender system. For this scenario, we will assume that the observations take the form of a Spark&rsquo;s <em>Discretised Stream</em> or <a href=https://spark.apache.org/docs/latest/streaming-programming-guide.html#discretized-streams-dstreams>DStream</a>. With DStreams we consume the stream as mini-batches of <code>RDD</code>s over a certain interval window.</p><figure><img src=/Excalidraw/ALS%20DStream.excalidraw.svg alt="ALS DStream.excalidraw.svg"></figure><p>We can for instance, use the first mini-batch to initialise the model and the following batches to continuously train the model.</p><p>One immediate advantage of using observations as a stream is that we no longer need to keep the entirety of the data in memory or read it from storage. If we consider the batch implementation with a very large dataset if we had a single new observation and wanted to retrain the model, we would have to, for instance, read several million ratings from a database. With a streaming variant we can use that single observation to update the latent factors.</p><p>We will try to recreate Spark&rsquo;s batch ALS API by allowing model training using a ratings <code>RDD</code>, however this time, we consume each <code>RDD</code> from the stream mini-batch and will incrementally train the model as observations trickle in.</p><figure><img src=/Excalidraw/ALS%20DStream%202.excalidraw.svg alt="ALS DStream 2.excalidraw.svg"></figure><p>First, we start by establishing the quantities and data structures needed to implement streaming ALS.</p><p>We&rsquo;ve seen in the previous slides that the recursions for the gradient calculation take the following form, here presented in pseudo-code:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-scala data-lang=scala><span style=display:flex><span>userBias <span style=font-weight:700>+=</span> gamma <span style=font-weight:700>*</span> <span style=font-weight:700>(</span>error <span style=font-weight:700>-</span> lambda <span style=font-weight:700>*</span> userBias<span style=font-weight:700>)</span> 
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>userFeature<span style=font-weight:700>(</span>i<span style=font-weight:700>)</span> <span style=font-weight:700>+=</span> gamma <span style=font-weight:700>*</span> <span style=font-weight:700>(</span>error   prodFeature<span style=font-weight:700>(</span>j<span style=font-weight:700>)</span> <span style=font-weight:700>-</span> lambda <span style=font-weight:700>*</span> userFeature<span style=font-weight:700>(</span>i<span style=font-weight:700>))</span>
</span></span></code></pre></div><p>We create a <code>Factor</code> class to encapsulate the features and the corresponding bias.</p><p>We recall that in the Spark ALS implementation, the features were stored in <code>RDD</code>s typed as a tuple of <code>(id, Array)</code>, where now we have an equivalent form of <code>(id, Factor)</code> which allows us to capture the bias. I&rsquo;ll now provide a quick overview of the steps required to go from the initial ratings stream to the trained model, in terms of Spark&rsquo;s <code>RDD</code> operations.</p><p>Similarly to Spark&rsquo;s ALS, we can assume that the model data will be in the form of <code>Rating</code>&rsquo;s <code>RDD</code>s. These, as we&rsquo;ve seen, will correspond to a mini-batch of ratings from our data stream. We first need to create the initial user and product latent factors for the observed data and we would start by creating two separate <code>RDD</code>s from the data, one keyed by user id, the other by product id.</p><figure><img src=/site/images/als/stream_als_step_1.png alt=stream_als_step_1.png></figure><p>For each entry of those new ~RDDs~ (the user and product indexed ones) we will now generate a random vector of features. This can be done by simply filling a vector of size ~rank~ with random uniform values, but as you will recall, we now also have a bias associated with each entry, which will initially also be set to a random value.</p><figure><img src=/site/images/als/stream_als_step_2.png alt=stream_als_step_2.png></figure><p>We now join the incoming ratings, with the generated user factors (using the user id as the key) getting a resulting ~RDD~ consisting of product ids, user ids, ratings and user factors (and the same thing for products and product factors).</p><figure><img src=/site/images/als/stream_als_step_3.png alt=stream_als_step_3.png></figure><p>Finally, we have these two joint ~RDDs~ which have all the necessary quantities needed to calculate the partial gradient in each element. Recalling how to calculate a predicted rating in streaming ALS, we need the global bias, the user and product bias and the corresponding user and product latent vectors.</p><p>This is straightforward to calculate for each element as we can see from the pseudo-code.</p><p>Here the ~dot~ function is simply a function to calculate the dot product treating the two factor arrays as vectors.</p><p>$$
\hat{\mathsf{R}}_{x,y}=\mu + b_x + b_y + \mathsf{U}_x^T \times \mathsf{P}_y
$$</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-scala data-lang=scala><span style=display:flex><span>prediction <span style=font-weight:700>=</span> dot<span style=font-weight:700>(</span>userFactors<span style=font-weight:700>.</span>features<span style=font-weight:700>,</span> itemFactors<span style=font-weight:700>.</span>features<span style=font-weight:700>)</span> <span style=font-weight:700>+</span> userFactors<span style=font-weight:700>.</span>bias <span style=font-weight:700>+</span> itemFactors<span style=font-weight:700>.</span>bias <span style=font-weight:700>+</span> bias
</span></span></code></pre></div><p>Given the prediction, the error is also straightforward to calculate, since the real rating is also included in this <code>RDD</code>.</p><p>$$
\epsilon _{x,y} = \mathsf{R} _{x,y} - \hat{\mathsf{R}} _{x,y}
$$</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-scala data-lang=scala><span style=display:flex><span>eps <span style=font-weight:700>=</span> rating <span style=font-weight:700>-</span> prediction
</span></span></code></pre></div><p>And now, since we have the error, we can also easily calculate the update term for the user and product features. As mentioned previously, ~gamma~ and ~lambda~ are known model parameters which we pick ourselves when instantiating the streaming ALS algorithm.</p><p>$$
\gamma \left(\epsilon_{x,y}\mathsf{U}_x-\lambda^{\prime}\mathsf{P}_y\right)
$$</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-scala data-lang=scala><span style=display:flex><span><span style=font-weight:700>(</span><span style=color:#099>0</span> until rank<span style=font-weight:700>).</span>map <span style=font-weight:700>{</span> i <span style=font-weight:700>=&gt;</span> 
</span></span><span style=display:flex><span>	gamma <span style=font-weight:700>*</span> <span style=font-weight:700>(</span>eps <span style=font-weight:700>*</span> userFactors<span style=font-weight:700>.</span>features<span style=font-weight:700>(</span>i<span style=font-weight:700>)</span> <span style=font-weight:700>-</span> lambda <span style=font-weight:700>*</span> itemFactors<span style=font-weight:700>.</span>features<span style=font-weight:700>(</span>i<span style=font-weight:700>))</span>
</span></span><span style=display:flex><span><span style=font-weight:700>}</span>
</span></span></code></pre></div><p>Finally, we update the user and product biases given the model parameters and the previous bias.</p><p>$$
\gamma \left(\epsilon_{x,y}-\lambda_y b_y\right)
$$</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-scala data-lang=scala><span style=display:flex><span>gamma <span style=font-weight:700>*</span> <span style=font-weight:700>(</span>eps <span style=font-weight:700>-</span> lambda <span style=font-weight:700>*</span> itemFactors<span style=font-weight:700>.</span>bias<span style=font-weight:700>)</span>
</span></span></code></pre></div><p>These calculated gradients can now be mapped to a new ~RDD~ which we will use to update the final biases and latent factors.</p><p>The last step is to split the gradients according to user and product, and finally, when in possession of all the individual gradients, we reduce them into latent factors by performing an aggregated sum for each user and product.</p><p>So these steps define the entirety of the streaming ALS operation. For each observation window, we calculate the latent factors ~RDD~, and on the following window we update these factors, given the current observations.</p><p>We&rsquo;ve covered the initialisation case, that is, we assumed the case where the model is not initialised and we received the first mini-batch of ratings. If, on the following window, we receive ratings for previously unseen user or products, the procedure is exactly the same, that is, we generate random factors and update as described.</p><figure><img src=/site/images/als/stream_als_overview.png alt=stream_als_overview.png></figure><p>Now I&rsquo;ll just quickly cover the case where we get some ratings from users or for some product we&rsquo;ve already seen. For this new set of observations, we proceed exactly as previously, that is, we split the data into separate <code>RDD</code>s, each one containing the ratings but keyed by user and product id. I&rsquo;ll assume that we get a mixture of completely new data, that is, unseen user and products and some ratings for previously seen users and products shown in red.</p><figure><img src=/site/images/als/stream_als_post_step_1.png alt=stream_als_post_step_1.png></figure><p>The difference now is that, instead of assigning random factors and biases to each entry of these ~RDDs~, we perform a full outer join between them and the current latent factors.</p><p>The strategy is then to keep the matching existing latent factor and create random features and biases just for the user and product entries we haven&rsquo;t seen before.</p><p>Now that we are in possession of this joint ~RDD~, we can apply exactly the same steps as previously to update the factors and repeat these steps for all future incoming observations, allowing us to continuously update the model.</p><p>It is now easy to see that, in the limit situation where we only have one new rating, we would now only have to update a single entry of the latent factors ~RDD~, in contrast with the batch method, where the entirety of the factors would be used in the ALS step.</p><figure><img src=/site/images/als/stream_als_post_step_2.png alt=stream_als_post_step_2.png></figure><p>Let&rsquo;s look at some results comparing the streaming implementation with the Spark&rsquo;s batch implementation.</p><p>The dataset we have chosen to use in these tests is one of the <a href=https://grouplens.org/datasets/movielens/latest/>MovieLens&rsquo; datasets</a>. These dataset are a widely used data in recommendation engine research. They are managed by the Lens corporation and are freely available for non-commercial applications.</p><p>These datasets come in several variants, namely a <a href=http://files.grouplens.org/datasets/movielens/ml-latest-small-README.html>small</a> variant, useful for a quick algorithm prototyping and testing, and a <a href=http://files.grouplens.org/datasets/movielens/ml-latest-README.html>full</a> variant, with approximately 26 million ratings (from 45,000 movies and 270,000 users), useful for a more comprehensive testing and benchmark.</p><p>The data is available as a set of Comma Separated Value files, each containing different variables, but we are mainly interested in the ~ratings~ file which contains four variables: a unique user and movie id, represented as integers, a rating represented by a value from 0 to 5 with steps of 0.5 and a timestamp for when the movie was rated by this user.</p><p>First, we&rsquo;ll start by training a batch ALS model using the MovieLens data. We assume that we already have the observations as an ~RDD~ of ratings and simply split the data into 80% for training and 20% for validation.</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-scala data-lang=scala><span style=display:flex><span><span style=font-weight:700>val</span> split<span style=font-weight:700>:</span> <span style=color:#458;font-weight:700>Array</span><span style=font-weight:700>[</span><span style=color:#458;font-weight:700>RDD</span><span style=font-weight:700>[</span><span style=color:#458;font-weight:700>Rating</span><span style=font-weight:700>]]</span> <span style=font-weight:700>=</span> ratings<span style=font-weight:700>.</span>randomSplit<span style=font-weight:700>(</span><span style=color:#099>0.8</span><span style=font-weight:700>,</span> <span style=color:#099>0.2</span><span style=font-weight:700>)</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=font-weight:700>val</span> model <span style=font-weight:700>=</span> <span style=color:#458;font-weight:700>ALS</span><span style=font-weight:700>.</span>train<span style=font-weight:700>(</span>split<span style=font-weight:700>(</span><span style=color:#099>0</span><span style=font-weight:700>),</span> rank<span style=font-weight:700>,</span> iter<span style=font-weight:700>,</span> lambda<span style=font-weight:700>)</span>
</span></span></code></pre></div><p>Here, we won&rsquo;t show the steps to determine the best parameters for this dataset, were we performed a simple parameter grid search over a number of possible candidates. The Spark ALS API is quite simple and to train the model we simply pass the training ~RDD~ and the parameters.</p><p>We can now use the remaining 20% of the observations to calculate the RMSE between the model predictions and the actual ratings.</p><p>We now can persist the validation ~RDD~, so we can use the exact same one for the streaming ALS run.</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-scala data-lang=scala><span style=display:flex><span><span style=font-weight:700>val</span> predictions<span style=font-weight:700>:</span> <span style=color:#458;font-weight:700>RDD</span><span style=font-weight:700>[</span><span style=color:#458;font-weight:700>Rating</span><span style=font-weight:700>]</span> <span style=font-weight:700>=</span> model
</span></span><span style=display:flex><span>	<span style=font-weight:700>.</span>predict<span style=font-weight:700>(</span>split<span style=font-weight:700>(</span><span style=color:#099>1</span><span style=font-weight:700>).</span>map <span style=font-weight:700>{</span> 
</span></span><span style=display:flex><span>		x <span style=font-weight:700>=&gt;(</span>x<span style=font-weight:700>.</span>user<span style=font-weight:700>,</span> x<span style=font-weight:700>.</span>product<span style=font-weight:700>))</span>
</span></span><span style=display:flex><span>	<span style=font-weight:700>}</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=font-weight:700>val</span> pairs <span style=font-weight:700>=</span> predictions
</span></span><span style=display:flex><span>	<span style=font-weight:700>.</span>map<span style=font-weight:700>(</span>x <span style=font-weight:700>=&gt;</span> <span style=font-weight:700>((</span>x<span style=font-weight:700>.</span>user<span style=font-weight:700>,</span> x<span style=font-weight:700>.</span>product<span style=font-weight:700>),</span> x<span style=font-weight:700>.</span>rating<span style=font-weight:700>))</span>
</span></span><span style=display:flex><span>	<span style=font-weight:700>.</span>join<span style=font-weight:700>(</span>
</span></span><span style=display:flex><span>		split<span style=font-weight:700>(</span><span style=color:#099>1</span><span style=font-weight:700>)</span>
</span></span><span style=display:flex><span>			<span style=font-weight:700>.</span>map<span style=font-weight:700>(</span>x <span style=font-weight:700>=&gt;</span> <span style=font-weight:700>((</span>x<span style=font-weight:700>.</span>user<span style=font-weight:700>,</span> x<span style=font-weight:700>.</span>product<span style=font-weight:700>),</span> x<span style=font-weight:700>.</span>rating<span style=font-weight:700>))</span>
</span></span><span style=display:flex><span>	<span style=font-weight:700>.</span>values
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=font-weight:700>val</span> <span style=color:#458;font-weight:700>RMSE</span> <span style=font-weight:700>=</span> math<span style=font-weight:700>.</span>sqrt<span style=font-weight:700>(</span>
</span></span><span style=display:flex><span>	pairs<span style=font-weight:700>.</span>map<span style=font-weight:700>(</span>x <span style=font-weight:700>=&gt;</span> math<span style=font-weight:700>.</span>pow<span style=font-weight:700>(</span>x<span style=font-weight:700>.</span>_1 <span style=font-weight:700>-</span> x<span style=font-weight:700>.</span>_2<span style=font-weight:700>,</span> <span style=color:#099>2</span><span style=font-weight:700>)).</span>mean<span style=font-weight:700>())</span>
</span></span></code></pre></div><p>In order to test the streaming version, we first need to define a data source. We start with the original MovieLens data and remove all the ratings from the validation observations.</p><p>We then create a simulated stream of observations using <a href=https://kafka.apache.org/>Kafka</a>, with an interval of 5 seconds and with 1000 observations in each mini-batch. These are arbitrary numbers, chosen just for practical reasons. We could have, for instance, a single observation in each mini-batch.</p><p>It is not guaranteed that the best parameters (namely ~rank~ and ~lambda~) chosen for the batch version are the best for the streaming implementation, however we&rsquo;ve decided to use the same ones. For each mini-batch we then incrementally train the model and calculate the RMSE up to that point. Given the actual ratings in the validation set and the model&rsquo;s prediction, the RMSE calculation is the same as in the batch version. And looking at the results, we can see that with each mini-batch (of 1000 observations), the RMSE from the streaming version (in blue) is edging towards the batch value (plotted as the horizontal dashed line).</p><figure><img src=/site/images/als/mse_comparison.png alt=mse_comparison.png></figure><h2 id=caveats>Caveats</h2><p>However, streaming ALS has pitfalls which we have to take into account.</p><h3 id=cold-start>Cold start</h3><p>An issue, which is shared with batch ALS, is usually called the <em>cold start</em> problem. This refers to initial point in a recommender engine where we have too few observations to make meaningful predictions. As we now know, when having a small number of ratings, since our latent factors are initialised to random numbers, most of our predicted ratings will also be random.</p><p>Although this is not an exclusive problem to the streaming implementation, we might be tempted, since the system is suited for realtime recommendations, to immediately start serving predictions. It might be wise to exercise caution and train the model offline with a larger dataset or at least perform some model diagnostics to check how sensible our predictions are.</p><h3 id=hyperparameter-estimation>Hyperparameter estimation</h3><p>Another challenge we encounter is hyperparameter estimation.</p><p>In the batch ALS case, we can perform a grid search for instance and estimate the hyperparameters. If, after some time, we find ourselves with a new ratings or even new product and users, we can simply repeat this procedure using the totality of the data. As an example, if in batch ALS at any point we wish to estimate the model with a different rank, this would be perfectly acceptable.</p><figure><img src=/site/images/als/hyperparameters.png alt=hyperparameters.png></figure><p>In the streaming case, we can&rsquo;t do that. When we have a new batch of observations, we assume that previous ones were discarded since they are already incorporated in the latent factors. If they weren&rsquo;t and we keep all the observations in the stream, we might as well use batch ALS.</p><figure><img src=/site/images/als/hyperparameters_2.png alt=hyperparameters_2.png></figure><p>A solution is to perform a grid search in parallel from the start and prune the least performant models as time progresses. This has the disadvantage of being expensive in terms of resources, since we have to keep several models simultaneously and again, we have the cold start problem surfacing.</p><p>This means that we have no guarantee that the best parameters for a initial batch with few observations will still be the best further on.</p><figure><img src=/site/images/als/hyperparameters_3.png alt=hyperparameters_3.png></figure><h3 id=performance>Performance</h3><p>Also, there are some performance considerations. As we&rsquo;ve seen, we implement some operations which can be costly in a Spark setting. We have several join operations which can lead to a considerable amount of data shuffling between partitions.</p><p>Care must be taken into choosing an appropriate partitioning strategy to minimise data shuffling.</p><p>Spark&rsquo;s implementation of batch ALS uses a specific method called blocked ALS, which computes outgoing and ingoing links between user and products vectors and then partitioning them in blocks in order to minimise data transfer between nodes.</p><figure><img src=/site/images/als/partitioning.png alt=partitioning.png></figure><p>Also, to make predictions we might have to try and perform random access to the latent factors ~RDDs~. This also can be quite inefficient since we are using ~lookup~ methods.</p><p>If you want to get straight setting up your own distributed recommendation engine, I highly suggest you start with Spark&rsquo;s builtin solution. I would highly recommended looking at the ~jiminy~ project (part of the <a href=https://radanalytics.io/>radanalytics.io</a> community), a micro-service oriented complete recommendation engine, ready to deploy on <a href=https://www.openshift.com/>OpenShift</a>.</p><p>The engine is split into services such as a <a href=https://github.com/radanalyticsio/jiminy-predictor>predictor</a> and a <a href=https://github.com/radanalyticsio/jiminy-modeler>modeler</a>, along with a <a href=https://github.com/radanalyticsio/jiminy-html-server>front-end</a> and <a href=https://github.com/radanalyticsio/jiminy-tools>tools</a> to simplify tasks (such as using the MovieLens data) and it&rsquo;s a great way to look at how to put a modern recommender engine together and also a great code read.</p><div class=footnotes role=doc-endnotes><hr><ol><li id=fn:1><p>Vinagre, J., Jorge, A. M., & Gama, J. (2014). Fast incremental matrix factorization for recommendation with positive-only feedback. In , International Conference on User Modeling, Adaptation, and Personalization (pp. 459â470).&#160;<a href=#fnref:1 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li></ol></div></div></article><div id=footer-post-container><div id=footer-post><div id=nav-footer style=display:none><ul><li><a href=/>Home</a></li><li><a href=/map/>All pages</a></li><li><a href=/search.html>Search</a></li></ul></div><div id=toc-footer style=display:none><nav id=TableOfContents><ul><li><a href=#batch-als>Batch ALS</a></li><li><a href=#streaming-als>Streaming ALS</a></li><li><a href=#apache-spark>Apache Spark</a></li><li><a href=#streaming-data>Streaming data</a></li><li><a href=#caveats>Caveats</a><ul><li><a href=#cold-start>Cold start</a></li><li><a href=#hyperparameter-estimation>Hyperparameter estimation</a></li><li><a href=#performance>Performance</a></li></ul></li></ul></nav></div><div id=share-footer style=display:none></div><div id=actions-footer><a id=menu-toggle class=icon href=# onclick='return $("#nav-footer").toggle(),!1' aria-label=Menu><i class="fas fa-bars fa-lg" aria-hidden=true></i> Menu</a>
<a id=toc-toggle class=icon href=# onclick='return $("#toc-footer").toggle(),!1' aria-label=TOC><i class="fas fa-list fa-lg" aria-hidden=true></i> TOC</a>
<a id=share-toggle class=icon href=# onclick='return $("#share-footer").toggle(),!1' aria-label=Share><i class="fas fa-share-alt fa-lg" aria-hidden=true></i> share</a>
<a id=top style=display:none class=icon href=# onclick='$("html, body").animate({scrollTop:0},"fast")' aria-label="Top of Page"><i class="fas fa-chevron-up fa-lg" aria-hidden=true></i> Top</a></div></div></div><footer id=footer><div class=footer-left>Copyright &copy; 2023 Rui Vieira</div><div class=footer-right><nav><ul><li><a href=/>Home</a></li><li><a href=/map/>All pages</a></li><li><a href=/search.html>Search</a></li></ul></nav></div></footer></div></body><link rel=stylesheet href=/css/fa.min.css><script src=/js/jquery-3.6.0.min.js></script>
<script src=/js/mark.min.js></script>
<script src=/js/main.js></script>
<script>MathJax={tex:{inlineMath:[["$","$"],["\\(","\\)"]]},svg:{fontCache:"global"}}</script><script type=text/javascript id=MathJax-script async src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js></script></html>