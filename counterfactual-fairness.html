<!DOCTYPE html>
<html lang="en">
<head>
    <link rel="icon" type="image/png" sizes="16x16" href="/favicons/favicon-16x16.png">
    <link rel="icon" type="image/png" sizes="32x32" href="/favicons/favicon-32x32.png">
    <link rel="icon" type="image/png" sizes="48x48" href="/favicons/favicon.ico">

    <meta http-equiv="Content-Type" content="text/html; charset=utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="description" content="Notes on Counterfactual Fairness">
    <meta name="robots" content="index">
    <link rel="canonical" href="https://ruivieira.dev/counterfactual-fairness.html">

    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css"
          integrity="sha384-AfEj0r4/OFrOo5t7NnNe46zW/tFgW6x/bCJG8FqQCEo3+Aro6EYUG4+cU+KJWu/X" crossorigin="anonymous">

    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.js"
            integrity="sha384-g7c+Jr9ZivxKLnZTDUhnkOnsh30B4H0rpLUpJ4jAIKs4fnJI+sEnkvrMWph2EDg4"
            crossorigin="anonymous"></script>

        <script defer src="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/contrib/auto-render.min.js"
            integrity="sha384-mll67QQFJfxn0IYznZYonOWZ644AWYC+Pt2cHqMaRhXVrursRwvLnLaebdGIlYNa" crossorigin="anonymous"></script>

    <script src="/assets/mark.min.js"></script>

    <link href="https://fonts.googleapis.com/css?family=Nunito:400,300i,800&display=swap" rel="stylesheet"/>
    <link href="/assets/style.css" rel="stylesheet">
    <style>
        /*Original style from softwaremaniacs.org (c) Ivan Sagalaev <Maniac@SoftwareManiacs.Org>*/

        .hljs {
            display: block;
            overflow-x: auto;
            padding: 0.5em;
            background: white;
            color: black;
            -webkit-text-size-adjust: none;
        }

        .hljs-string,
        .hljs-tag .hljs-value,
        .hljs-filter .hljs-argument,
        .hljs-addition,
        .hljs-change,
        .hljs-name,
        .apache .hljs-tag,
        .apache .hljs-cbracket,
        .nginx .hljs-built_in,
        .tex .hljs-formula {
            color: #888;
        }

        .hljs-comment,
        .hljs-shebang,
        .hljs-doctype,
        .hljs-pi,
        .hljs-javadoc,
        .hljs-deletion,
        .apache .hljs-sqbracket {
            color: #ccc;
        }

        .hljs-keyword,
        .hljs-tag .hljs-title,
        .ini .hljs-title,
        .lisp .hljs-title,
        .http .hljs-title,
        .nginx .hljs-title,
        .css .hljs-tag,
        .hljs-winutils,
        .hljs-flow,
        .apache .hljs-tag,
        .tex .hljs-command,
        .hljs-request,
        .hljs-status {
            font-weight: bold;
        }
    </style>
    <title>ruivieira.dev - Counterfactual Fairness</title>
    <script data-goatcounter="https://ruivieira-dev.goatcounter.com/count" async src="//gc.zgo.at/count.js"></script>
    <style>
        #search_terms {
            font-size: 1rem;
            font-family: Nunito;
            width: 40%;
        }
        #search_terms::placeholder {
            color: #bbb;
        }
        #search_button {
            background-color: #eee;
            border: none;
            color: black;
            padding: 0.25rem 0.25rem;
            font-size: 1rem;
            font-family: Nunito;
            text-decoration: none;
            cursor: pointer;
            border-radius: 5px;
            width: 3rem;
        }
    </style>
</head>
<body>
<div id="grid">

    <div id="content">
        <h1 id="counterfactual-fairness">Counterfactual fairness</h1>
<h2 id="building-counterfactually-fair-models">Building counterfactually fair models</h2>
<h3 id="data">Data</h3>
<p>To evaluate <em>counterfactual fairness</em> we will be using the &quot;law school&quot; dataset<sup class="footnote-ref"><a href="#fn1" id="fnref1">[1]</a></sup>.</p>
<p>The Law School Admission Council conducted a survey across 163 law schools in the United States.
It contains information on 21,790 law students such as their entrance exam scores (<code>LSAT</code>), their
grade-point average (<code>GPA</code>) collected prior to law school, and their first year average grade (<code>FYA</code>).
Given this data, a school may wish to predict if an applicant will have a high <code>FYA</code>. The school would
also like to make sure these predictions are not biased by an individual’s race and sex.
However, the <code>LSAT</code>, <code>GPA</code>, and <code>FYA</code> scores, may be biased due to social factors.</p>
<p>We start by importing the data into a <a href="pandas.html">Pandas</a> <code>DataFrame</code>.</p>
<pre><code class="language-python"><span class="hljs-keyword">import</span> warnings

warnings.filterwarnings(<span class="hljs-string">&quot;ignore&quot;</span>)
</code></pre>
<pre><code class="language-python"><span class="hljs-keyword">import</span> pandas <span class="hljs-keyword">as</span> pd

df = pd.read_csv(<span class="hljs-string">&quot;data/law_data.csv&quot;</span>, index_col=<span class="hljs-number">0</span>)
df.head()
</code></pre>
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
<pre><code>.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
</code></pre>
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>race</th>
      <th>sex</th>
      <th>LSAT</th>
      <th>UGPA</th>
      <th>region_first</th>
      <th>ZFYA</th>
      <th>sander_index</th>
      <th>first_pf</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>White</td>
      <td>1</td>
      <td>39.0</td>
      <td>3.1</td>
      <td>GL</td>
      <td>-0.98</td>
      <td>0.782738</td>
      <td>1.0</td>
    </tr>
    <tr>
      <th>1</th>
      <td>White</td>
      <td>1</td>
      <td>36.0</td>
      <td>3.0</td>
      <td>GL</td>
      <td>0.09</td>
      <td>0.735714</td>
      <td>1.0</td>
    </tr>
    <tr>
      <th>2</th>
      <td>White</td>
      <td>2</td>
      <td>30.0</td>
      <td>3.1</td>
      <td>MS</td>
      <td>-0.35</td>
      <td>0.670238</td>
      <td>1.0</td>
    </tr>
    <tr>
      <th>5</th>
      <td>Hispanic</td>
      <td>2</td>
      <td>39.0</td>
      <td>2.2</td>
      <td>NE</td>
      <td>0.58</td>
      <td>0.697024</td>
      <td>1.0</td>
    </tr>
    <tr>
      <th>6</th>
      <td>White</td>
      <td>1</td>
      <td>37.0</td>
      <td>3.4</td>
      <td>GL</td>
      <td>-1.26</td>
      <td>0.786310</td>
      <td>1.0</td>
    </tr>
  </tbody>
</table>
</div>
### Pre-processing
We now pre-process the data. We start by creating categorical "dummy" variables according to the `race` variable.
```python
df = pd.get_dummies(df, columns=["race"], prefix="", prefix_sep="")
df.iloc[:, : 7].head()
```
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
<pre><code>.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
</code></pre>
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>sex</th>
      <th>LSAT</th>
      <th>UGPA</th>
      <th>region_first</th>
      <th>ZFYA</th>
      <th>sander_index</th>
      <th>first_pf</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>1</td>
      <td>39.0</td>
      <td>3.1</td>
      <td>GL</td>
      <td>-0.98</td>
      <td>0.782738</td>
      <td>1.0</td>
    </tr>
    <tr>
      <th>1</th>
      <td>1</td>
      <td>36.0</td>
      <td>3.0</td>
      <td>GL</td>
      <td>0.09</td>
      <td>0.735714</td>
      <td>1.0</td>
    </tr>
    <tr>
      <th>2</th>
      <td>2</td>
      <td>30.0</td>
      <td>3.1</td>
      <td>MS</td>
      <td>-0.35</td>
      <td>0.670238</td>
      <td>1.0</td>
    </tr>
    <tr>
      <th>5</th>
      <td>2</td>
      <td>39.0</td>
      <td>2.2</td>
      <td>NE</td>
      <td>0.58</td>
      <td>0.697024</td>
      <td>1.0</td>
    </tr>
    <tr>
      <th>6</th>
      <td>1</td>
      <td>37.0</td>
      <td>3.4</td>
      <td>GL</td>
      <td>-1.26</td>
      <td>0.786310</td>
      <td>1.0</td>
    </tr>
  </tbody>
</table>
</div>
```python
df.iloc[:, 7 :].head()
```
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
<pre><code>.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
</code></pre>
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Amerindian</th>
      <th>Asian</th>
      <th>Black</th>
      <th>Hispanic</th>
      <th>Mexican</th>
      <th>Other</th>
      <th>Puertorican</th>
      <th>White</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
    </tr>
    <tr>
      <th>1</th>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
    </tr>
    <tr>
      <th>2</th>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
    </tr>
    <tr>
      <th>5</th>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>6</th>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
    </tr>
  </tbody>
</table>
</div>
We also want to expand the `sex` variable into `male`/`female` categorical variables and remove the original.
```python
df["male"] = df["sex"].map(lambda x: 1 if x == 2 else 0)
df["female"] = df["sex"].map(lambda x: 1 if x == 1 else 0)
df = df.drop(axis=1, columns=["sex"])
```
<pre><code class="language-python">df.iloc[:, <span class="hljs-number">0</span>:<span class="hljs-number">7</span>].head()
</code></pre>
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
<pre><code>.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
</code></pre>
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>LSAT</th>
      <th>UGPA</th>
      <th>region_first</th>
      <th>ZFYA</th>
      <th>sander_index</th>
      <th>first_pf</th>
      <th>Amerindian</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>39.0</td>
      <td>3.1</td>
      <td>GL</td>
      <td>-0.98</td>
      <td>0.782738</td>
      <td>1.0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>1</th>
      <td>36.0</td>
      <td>3.0</td>
      <td>GL</td>
      <td>0.09</td>
      <td>0.735714</td>
      <td>1.0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>2</th>
      <td>30.0</td>
      <td>3.1</td>
      <td>MS</td>
      <td>-0.35</td>
      <td>0.670238</td>
      <td>1.0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>5</th>
      <td>39.0</td>
      <td>2.2</td>
      <td>NE</td>
      <td>0.58</td>
      <td>0.697024</td>
      <td>1.0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>6</th>
      <td>37.0</td>
      <td>3.4</td>
      <td>GL</td>
      <td>-1.26</td>
      <td>0.786310</td>
      <td>1.0</td>
      <td>0</td>
    </tr>
  </tbody>
</table>
</div>
```python
df.iloc[:, 7:].head()
```
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
<pre><code>.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
</code></pre>
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Asian</th>
      <th>Black</th>
      <th>Hispanic</th>
      <th>Mexican</th>
      <th>Other</th>
      <th>Puertorican</th>
      <th>White</th>
      <th>male</th>
      <th>female</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>1</td>
    </tr>
    <tr>
      <th>1</th>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>1</td>
    </tr>
    <tr>
      <th>2</th>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>1</td>
      <td>0</td>
    </tr>
    <tr>
      <th>5</th>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
    </tr>
    <tr>
      <th>6</th>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>1</td>
    </tr>
  </tbody>
</table>
</div>
We will also convert the entrance exam scores (`LSAT`) to a discrete variable.
```python
df["LSAT"] = df["LSAT"].astype(int)
df.iloc[:, :6].head()
```
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
<pre><code>.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
</code></pre>
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>LSAT</th>
      <th>UGPA</th>
      <th>region_first</th>
      <th>ZFYA</th>
      <th>sander_index</th>
      <th>first_pf</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>39</td>
      <td>3.1</td>
      <td>GL</td>
      <td>-0.98</td>
      <td>0.782738</td>
      <td>1.0</td>
    </tr>
    <tr>
      <th>1</th>
      <td>36</td>
      <td>3.0</td>
      <td>GL</td>
      <td>0.09</td>
      <td>0.735714</td>
      <td>1.0</td>
    </tr>
    <tr>
      <th>2</th>
      <td>30</td>
      <td>3.1</td>
      <td>MS</td>
      <td>-0.35</td>
      <td>0.670238</td>
      <td>1.0</td>
    </tr>
    <tr>
      <th>5</th>
      <td>39</td>
      <td>2.2</td>
      <td>NE</td>
      <td>0.58</td>
      <td>0.697024</td>
      <td>1.0</td>
    </tr>
    <tr>
      <th>6</th>
      <td>37</td>
      <td>3.4</td>
      <td>GL</td>
      <td>-1.26</td>
      <td>0.786310</td>
      <td>1.0</td>
    </tr>
  </tbody>
</table>
</div>
```python
df.iloc[:, 6:].head()
```
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }
<pre><code>.dataframe tbody tr th {
    vertical-align: top;
}

.dataframe thead th {
    text-align: right;
}
</code></pre>
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Amerindian</th>
      <th>Asian</th>
      <th>Black</th>
      <th>Hispanic</th>
      <th>Mexican</th>
      <th>Other</th>
      <th>Puertorican</th>
      <th>White</th>
      <th>male</th>
      <th>female</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>1</td>
    </tr>
    <tr>
      <th>1</th>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>1</td>
    </tr>
    <tr>
      <th>2</th>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>1</td>
      <td>0</td>
    </tr>
    <tr>
      <th>5</th>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
    </tr>
    <tr>
      <th>6</th>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>1</td>
    </tr>
  </tbody>
</table>
</div>
### Protected attributes
_Counterfactual fairness_ enforces that a distribution over possible predictions for an individual should
remain unchanged in a world where an individual’s protected attributes $A$ had been different in a causal sense.
Let's start by defining the _protected attributes_. Obvious candidates are the different categorical variables for ethnicity (`Asian`, `White`, `Black`, _etc_) and gender (`male`, `female`).
```python
A = [
    "Amerindian",
    "Asian",
    "Black",
    "Hispanic",
    "Mexican",
    "Other",
    "Puertorican",
    "White",
    "male",
    "female",
]
```
<h3 id="training-and-testing-subsets">Training and testing subsets</h3>
<p>We will now divide the dataset into training and testing subsets.
We will use the same ratio as in <sup class="footnote-ref"><a href="#fn2" id="fnref2">[2]</a></sup>, that is 20%.</p>
<pre><code class="language-python"><span class="hljs-keyword">from</span> sklearn.model_selection <span class="hljs-keyword">import</span> train_test_split

df_train, df_test = train_test_split(df, random_state=<span class="hljs-number">23</span>, test_size=<span class="hljs-number">0.2</span>);
</code></pre>
<h2 id="models">Models</h2>
<h3 id="unfair-model">Unfair model</h3>
<p>As detailed in <sup class="footnote-ref"><a href="#fn2" id="fnref2:1">[2]</a></sup>, the concept of counterfactual fairness holds
under three levels of assumptions of increasing strength.</p>
<p>The first of such levels is <em>Level 1</em>, where $\hat{Y}$ is built using only the observable non-descendants of $A$.
This only requires <em>partial</em> causal ordering and no further causal assumptions, but in many problems there will be few, if any,
observables which are not descendants of protected demographic factors.</p>
<p>For this dataset, since <code>LSAT</code>, <code>GPA</code>, and <code>FYA</code> are all biased by ethnicity and gender, we cannot use any observed
features to construct a Level 1 counterfactually fair predictor as described in Level 1.</p>
<p>Instead (and in order to compare the performance with Level 2 and 3 models) we will build two <em>unfair baselines</em>.</p>
<ul>
<li>A <em>Full</em> model, which will be trained with the totality of the variables</li>
<li>An <em>Unaware</em> model (FTU), which will be trained will all the variables, except the protected attributes $A$.</li>
</ul>
<p>Let's proceed with calculating the <em>Full</em> model.</p>
<h3 id="full-model">Full model</h3>
<p>As mentioned previously, the full model will be a simple linear regression in order to predict <code>ZFYA</code> using all of the variables.</p>
<pre><code class="language-python"><span class="hljs-keyword">from</span> sklearn.linear_model <span class="hljs-keyword">import</span> LinearRegression

linreg_unfair = LinearRegression()
</code></pre>
<p>The inputs will then be the totality of the variabes (protected variables $A$, as well as <code>UGPA</code> and <code>LSAT</code>).</p>
<pre><code class="language-python"><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np

X = np.hstack(
    (
        df_train[A],
        np.array(df_train[<span class="hljs-string">&quot;UGPA&quot;</span>]).reshape(-<span class="hljs-number">1</span>, <span class="hljs-number">1</span>),
        np.array(df_train[<span class="hljs-string">&quot;LSAT&quot;</span>]).reshape(-<span class="hljs-number">1</span>, <span class="hljs-number">1</span>),
    )
)
print(X)
</code></pre>
<pre><code>[[ <span class="hljs-number">0.</span>   <span class="hljs-number">0.</span>   <span class="hljs-number">0.</span>  ...  <span class="hljs-number">1.</span>   <span class="hljs-number">3.1</span> <span class="hljs-number">39.</span> ]
 [ <span class="hljs-number">0.</span>   <span class="hljs-number">0.</span>   <span class="hljs-number">0.</span>  ...  <span class="hljs-number">1.</span>   <span class="hljs-number">3.5</span> <span class="hljs-number">36.</span> ]
 [ <span class="hljs-number">0.</span>   <span class="hljs-number">0.</span>   <span class="hljs-number">0.</span>  ...  <span class="hljs-number">1.</span>   <span class="hljs-number">3.9</span> <span class="hljs-number">46.</span> ]
 ...
 [ <span class="hljs-number">0.</span>   <span class="hljs-number">0.</span>   <span class="hljs-number">0.</span>  ...  <span class="hljs-number">1.</span>   <span class="hljs-number">2.9</span> <span class="hljs-number">33.</span> ]
 [ <span class="hljs-number">0.</span>   <span class="hljs-number">0.</span>   <span class="hljs-number">0.</span>  ...  <span class="hljs-number">0.</span>   <span class="hljs-number">2.9</span> <span class="hljs-number">31.</span> ]
 [ <span class="hljs-number">0.</span>   <span class="hljs-number">0.</span>   <span class="hljs-number">0.</span>  ...  <span class="hljs-number">0.</span>   <span class="hljs-number">3.6</span> <span class="hljs-number">39.</span> ]]

</code></pre>
<p>As for our target, we are trying to predict <code>ZFYA</code> (first year average grade).</p>
<pre><code class="language-python">y = df_train[<span class="hljs-string">&quot;ZFYA&quot;</span>]
y[:<span class="hljs-number">10</span>]
</code></pre>
<p>We fit the model:</p>
<pre><code class="language-python">linreg_unfair = linreg_unfair.fit(X, y)
</code></pre>
<p>And perform some predictions on the test subset.</p>
<pre><code class="language-python">X_test = np.hstack(
    (
        df_test[A],
        np.array(df_test[<span class="hljs-string">&quot;UGPA&quot;</span>]).reshape(-<span class="hljs-number">1</span>, <span class="hljs-number">1</span>),
        np.array(df_test[<span class="hljs-string">&quot;LSAT&quot;</span>]).reshape(-<span class="hljs-number">1</span>, <span class="hljs-number">1</span>),
    )
)
X_test
</code></pre>
<pre><code class="language-python">predictions_unfair = linreg_unfair.predict(X_test)
predictions_unfair
</code></pre>
<p>We will also calculate the <em>unfair model</em> score for future use.</p>
<pre><code class="language-python">score_unfair = linreg_unfair.score(X_test, df_test[<span class="hljs-string">&quot;ZFYA&quot;</span>])
print(score_unfair)
</code></pre>
<pre><code><span class="hljs-attribute">0</span>.<span class="hljs-number">12701634112845117</span>

</code></pre>
<pre><code class="language-python"><span class="hljs-keyword">from</span> sklearn.metrics <span class="hljs-keyword">import</span> mean_squared_error

RMSE_unfair = np.sqrt(mean_squared_error(df_test[<span class="hljs-string">&quot;ZFYA&quot;</span>], predictions_unfair))
print(RMSE_unfair)
</code></pre>
<pre><code><span class="hljs-attribute">0</span>.<span class="hljs-number">8666709890234552</span>

</code></pre>
<h3 id="fairness-through-unawareness-ftu">Fairness through unawareness (FTU)</h3>
<p>As also mentioned in <sup class="footnote-ref"><a href="#fn2" id="fnref2:2">[2]</a></sup>, the second baseline we will use is an <strong>Unaware</strong> model (FTU), which will be trained will all the variables, except the protected attributes $A$.</p>
<pre><code class="language-python">linreg_ftu = LinearRegression()
</code></pre>
<p>We will create the inputs as previously, but without using the protected attributes, $A$.</p>
<pre><code class="language-python">X_ftu = np.hstack(
    (
        np.array(df_train[<span class="hljs-string">&quot;UGPA&quot;</span>]).reshape(-<span class="hljs-number">1</span>, <span class="hljs-number">1</span>),
        np.array(df_train[<span class="hljs-string">&quot;LSAT&quot;</span>]).reshape(-<span class="hljs-number">1</span>, <span class="hljs-number">1</span>),
    )
)
X_ftu
</code></pre>
<p>And we fit the model:</p>
<pre><code class="language-python">linreg_ftu = linreg_ftu.fit(X_ftu, y)
</code></pre>
<p>Again, let's perform some predictions on the test subset.</p>
<pre><code class="language-python">X_ftu_test = np.hstack(
    (np.array(df_test[<span class="hljs-string">&quot;UGPA&quot;</span>]).reshape(-<span class="hljs-number">1</span>, <span class="hljs-number">1</span>), np.array(df_test[<span class="hljs-string">&quot;LSAT&quot;</span>]).reshape(-<span class="hljs-number">1</span>, <span class="hljs-number">1</span>))
)
X_ftu_test
</code></pre>
<pre><code class="language-python">predictions_ftu = linreg_ftu.predict(X_ftu_test)
predictions_ftu
</code></pre>
<p>As previously, let's calculate this model's score.</p>
<pre><code class="language-python">ftu_score = linreg_ftu.score(X_ftu_test, df_test[<span class="hljs-string">&quot;ZFYA&quot;</span>])
print(ftu_score)
</code></pre>
<pre><code><span class="hljs-attribute">0</span>.<span class="hljs-number">0917442226187073</span>

</code></pre>
<pre><code class="language-python">RMSE_ftu = np.sqrt(mean_squared_error(df_test[<span class="hljs-string">&quot;ZFYA&quot;</span>], predictions_ftu))
print(RMSE_ftu)
</code></pre>
<pre><code><span class="hljs-attribute">0</span>.<span class="hljs-number">8840061503773576</span>

</code></pre>
<h3 id="latent-variable-model">Latent variable model</h3>
<p>Still according to <sup class="footnote-ref"><a href="#fn2" id="fnref2:3">[2]</a></sup>, a <strong>Level 2</strong> approach will model latent ‘fair’ variables which are parents of observed variables.</p>
<p>If we consider a predictor parameterised by $\theta$, such as:</p>
<p>$$
\hat{Y} \equiv g_\theta (U, X_{\nsucc A})
$$</p>
<p>with $X_{\nsucc A} \subseteq X$ are non-descendants of $A$.
Assuming a loss function $l(\cdot,\cdot)$ and training data $\mathcal{D}\equiv\{(A^{(i), X^{(i)}, Y^{(i)}})\}$, for $i=1,2\dots,n$, the empirical loss is defined as</p>
<p>$$
L(\theta)\equiv \sum_{i=1}^n \mathbb{E}[l(y^{(i)},g_\theta(U^{(i)}, x^{(i)}_{\nsucc A}))]/n
$$</p>
<p>which has to be minimised in order to $\theta$. Each $n$ expectation is with respect to random variable $U^{(i)}$ such that</p>
<p>$$
U^{(i)}\sim P_{\mathcal{M}}(U|x^{(i)}, a^{(i)})
$$</p>
<p>where $P_{\mathcal{M}}(U|x,a)$ is the conditional distribution of the background variables as given by a causal model M that is available by assumption.</p>
<p>If this expectation cannot be calculated analytically, Markov chain Monte Carlo (MCMC) can be used to approximate it as in the following algorithm.</p>
<p>We will follow the model specified in the original paper, where the latent variable considered is $K$, which represents a student's <strong>knowledge</strong>.
$K$ will affect <code>GPA</code>, <code>LSAT</code> and the outcome, <code>FYA</code>.
The model can be defined by:</p>
<p>$$
\begin{aligned}
GPA &amp;\sim \mathcal{N}(GPA_0 + w_{GPA}^KK + w_{GPA}^RR + w_{GPA}^SS, \sigma_{GPA}) \\
LSAT &amp;\sim \text{Po}(\exp(LSAT_0 + w_{LSAT}^KK + w_{LSAT}^RR + w_L^SS)) \\
FYA &amp;\sim \mathcal{N}(w_{FYA}^KK + w_{FYA}^RR + w_{FYA}^SS, 1) \\
K &amp;\sim \mathcal{N}(0,1)
\end{aligned}
$$
The priors used will be:</p>
<p>$$
\begin{aligned}
GPA_0 &amp;\sim \mathcal{N}(0, 1) \\
LSAT_0 &amp;\sim \mathcal{N}(0, 1) \\
GPA_0 &amp;\sim \mathcal{N}(0, 1)
\end{aligned}
$$</p>
<pre><code class="language-python"><span class="hljs-keyword">import</span> pymc3 <span class="hljs-keyword">as</span> pm

K = <span class="hljs-built_in">len</span>(A)


<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">MCMC</span>(<span class="hljs-params">data, samples=<span class="hljs-number">1000</span></span>):</span>

    N = <span class="hljs-built_in">len</span>(data)
    a = np.array(data[A])

    model = pm.Model()

    <span class="hljs-keyword">with</span> model:
        <span class="hljs-comment"># Priors</span>
        k = pm.Normal(<span class="hljs-string">&quot;k&quot;</span>, mu=<span class="hljs-number">0</span>, sigma=<span class="hljs-number">1</span>, shape=(<span class="hljs-number">1</span>, N))
        gpa0 = pm.Normal(<span class="hljs-string">&quot;gpa0&quot;</span>, mu=<span class="hljs-number">0</span>, sigma=<span class="hljs-number">1</span>)
        lsat0 = pm.Normal(<span class="hljs-string">&quot;lsat0&quot;</span>, mu=<span class="hljs-number">0</span>, sigma=<span class="hljs-number">1</span>)
        w_k_gpa = pm.Normal(<span class="hljs-string">&quot;w_k_gpa&quot;</span>, mu=<span class="hljs-number">0</span>, sigma=<span class="hljs-number">1</span>)
        w_k_lsat = pm.Normal(<span class="hljs-string">&quot;w_k_lsat&quot;</span>, mu=<span class="hljs-number">0</span>, sigma=<span class="hljs-number">1</span>)
        w_k_zfya = pm.Normal(<span class="hljs-string">&quot;w_k_zfya&quot;</span>, mu=<span class="hljs-number">0</span>, sigma=<span class="hljs-number">1</span>)

        w_a_gpa = pm.Normal(<span class="hljs-string">&quot;w_a_gpa&quot;</span>, mu=np.zeros(K), sigma=np.ones(K), shape=K)
        w_a_lsat = pm.Normal(<span class="hljs-string">&quot;w_a_lsat&quot;</span>, mu=np.zeros(K), sigma=np.ones(K), shape=K)
        w_a_zfya = pm.Normal(<span class="hljs-string">&quot;w_a_zfya&quot;</span>, mu=np.zeros(K), sigma=np.ones(K), shape=K)

        sigma_gpa_2 = pm.InverseGamma(<span class="hljs-string">&quot;sigma_gpa_2&quot;</span>, alpha=<span class="hljs-number">1</span>, beta=<span class="hljs-number">1</span>)

        mu = gpa0 + (w_k_gpa * k) + pm.math.dot(a, w_a_gpa)

        <span class="hljs-comment"># Observed data</span>
        gpa = pm.Normal(
            <span class="hljs-string">&quot;gpa&quot;</span>,
            mu=mu,
            sigma=pm.math.sqrt(sigma_gpa_2),
            observed=<span class="hljs-built_in">list</span>(data[<span class="hljs-string">&quot;UGPA&quot;</span>]),
            shape=(<span class="hljs-number">1</span>, N),
        )
        lsat = pm.Poisson(
            <span class="hljs-string">&quot;lsat&quot;</span>,
            pm.math.exp(lsat0 + w_k_lsat * k + pm.math.dot(a, w_a_lsat)),
            observed=<span class="hljs-built_in">list</span>(data[<span class="hljs-string">&quot;LSAT&quot;</span>]),
            shape=(<span class="hljs-number">1</span>, N),
        )
        zfya = pm.Normal(
            <span class="hljs-string">&quot;zfya&quot;</span>,
            mu=w_k_zfya * k + pm.math.dot(a, w_a_zfya),
            sigma=<span class="hljs-number">1</span>,
            observed=<span class="hljs-built_in">list</span>(data[<span class="hljs-string">&quot;ZFYA&quot;</span>]),
            shape=(<span class="hljs-number">1</span>, N),
        )

        step = pm.Metropolis()
        trace = pm.sample(samples, step)

    <span class="hljs-keyword">return</span> trace
</code></pre>
<pre><code class="language-python">train_estimates = MCMC(df_train)
</code></pre>
<pre><code><span class="hljs-selector-tag">Multiprocess</span> <span class="hljs-selector-tag">sampling</span> (<span class="hljs-number">4</span> chains in <span class="hljs-number">4</span> jobs)
<span class="hljs-selector-tag">CompoundStep</span>
&gt;<span class="hljs-selector-tag">Metropolis</span>: <span class="hljs-selector-attr">[sigma_gpa_2]</span>
&gt;<span class="hljs-selector-tag">Metropolis</span>: <span class="hljs-selector-attr">[w_a_zfya]</span>
&gt;<span class="hljs-selector-tag">Metropolis</span>: <span class="hljs-selector-attr">[w_a_lsat]</span>
&gt;<span class="hljs-selector-tag">Metropolis</span>: <span class="hljs-selector-attr">[w_a_gpa]</span>
&gt;<span class="hljs-selector-tag">Metropolis</span>: <span class="hljs-selector-attr">[w_k_zfya]</span>
&gt;<span class="hljs-selector-tag">Metropolis</span>: <span class="hljs-selector-attr">[w_k_lsat]</span>
&gt;<span class="hljs-selector-tag">Metropolis</span>: <span class="hljs-selector-attr">[w_k_gpa]</span>
&gt;<span class="hljs-selector-tag">Metropolis</span>: <span class="hljs-selector-attr">[lsat0]</span>
&gt;<span class="hljs-selector-tag">Metropolis</span>: <span class="hljs-selector-attr">[gpa0]</span>
&gt;<span class="hljs-selector-tag">Metropolis</span>: <span class="hljs-selector-attr">[k]</span>

</code></pre>
<pre><code><span class="hljs-attribute">Sampling</span> <span class="hljs-number">4</span> chains for <span class="hljs-number">1</span>_<span class="hljs-number">000</span> tune and <span class="hljs-number">1</span>_<span class="hljs-number">000</span> draw iterations (<span class="hljs-number">4</span>_<span class="hljs-number">000</span> + <span class="hljs-number">4</span>_<span class="hljs-number">000</span> draws total) took <span class="hljs-number">87</span> seconds.
<span class="hljs-attribute">The</span> rhat statistic is larger than <span class="hljs-number">1</span>.<span class="hljs-number">4</span> for some parameters. The sampler did not converge.
<span class="hljs-attribute">The</span> estimated number of effective samples is smaller than <span class="hljs-number">200</span> for some parameters.

</code></pre>
<p>Let's plot a single trace for $k^{(i)}$.</p>
<pre><code class="language-python"><span class="hljs-keyword">import</span> matplotlib.pyplot <span class="hljs-keyword">as</span> plt
<span class="hljs-keyword">import</span> seaborn <span class="hljs-keyword">as</span> sns
<span class="hljs-keyword">from</span> plotutils <span class="hljs-keyword">import</span> *

<span class="hljs-comment"># Thin the samples before plotting</span>
k_trace = train_estimates[<span class="hljs-string">&quot;k&quot;</span>][:, <span class="hljs-number">0</span>].reshape(-<span class="hljs-number">1</span>, <span class="hljs-number">1</span>)[<span class="hljs-number">0</span>::<span class="hljs-number">100</span>]
plt.subplot(<span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">1</span>)
plt.hist(k_trace, color=colours[<span class="hljs-number">0</span>], bins=<span class="hljs-number">100</span>)
plt.subplot(<span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">2</span>)
plt.scatter(<span class="hljs-built_in">range</span>(<span class="hljs-built_in">len</span>(k_trace)), k_trace, s=<span class="hljs-number">1</span>, c=colours[<span class="hljs-number">0</span>])
plt.show()
</code></pre>
<p><img src="./images/counterfactual-fairness_1.png" alt="counterfactual-fairness_1"></p>
<pre><code class="language-python">train_k = np.mean(train_estimates[<span class="hljs-string">&quot;k&quot;</span>], axis=<span class="hljs-number">0</span>).reshape(-<span class="hljs-number">1</span>, <span class="hljs-number">1</span>)
train_k
</code></pre>
<p>We can now estimate $k$ using the test data:</p>
<pre><code class="language-python">test_map_estimates = MCMC(df_test)
</code></pre>
<pre><code><span class="hljs-selector-tag">Multiprocess</span> <span class="hljs-selector-tag">sampling</span> (<span class="hljs-number">4</span> chains in <span class="hljs-number">4</span> jobs)
<span class="hljs-selector-tag">CompoundStep</span>
&gt;<span class="hljs-selector-tag">Metropolis</span>: <span class="hljs-selector-attr">[sigma_gpa_2]</span>
&gt;<span class="hljs-selector-tag">Metropolis</span>: <span class="hljs-selector-attr">[w_a_zfya]</span>
&gt;<span class="hljs-selector-tag">Metropolis</span>: <span class="hljs-selector-attr">[w_a_lsat]</span>
&gt;<span class="hljs-selector-tag">Metropolis</span>: <span class="hljs-selector-attr">[w_a_gpa]</span>
&gt;<span class="hljs-selector-tag">Metropolis</span>: <span class="hljs-selector-attr">[w_k_zfya]</span>
&gt;<span class="hljs-selector-tag">Metropolis</span>: <span class="hljs-selector-attr">[w_k_lsat]</span>
&gt;<span class="hljs-selector-tag">Metropolis</span>: <span class="hljs-selector-attr">[w_k_gpa]</span>
&gt;<span class="hljs-selector-tag">Metropolis</span>: <span class="hljs-selector-attr">[lsat0]</span>
&gt;<span class="hljs-selector-tag">Metropolis</span>: <span class="hljs-selector-attr">[gpa0]</span>
&gt;<span class="hljs-selector-tag">Metropolis</span>: <span class="hljs-selector-attr">[k]</span>

</code></pre>
<pre><code><span class="hljs-attribute">Sampling</span> <span class="hljs-number">4</span> chains for <span class="hljs-number">1</span>_<span class="hljs-number">000</span> tune and <span class="hljs-number">1</span>_<span class="hljs-number">000</span> draw iterations (<span class="hljs-number">4</span>_<span class="hljs-number">000</span> + <span class="hljs-number">4</span>_<span class="hljs-number">000</span> draws total) took <span class="hljs-number">35</span> seconds.
<span class="hljs-attribute">The</span> rhat statistic is larger than <span class="hljs-number">1</span>.<span class="hljs-number">4</span> for some parameters. The sampler did not converge.
<span class="hljs-attribute">The</span> estimated number of effective samples is smaller than <span class="hljs-number">200</span> for some parameters.

</code></pre>
<pre><code class="language-python">test_k = np.mean(test_map_estimates[<span class="hljs-string">&quot;k&quot;</span>], axis=<span class="hljs-number">0</span>).reshape(-<span class="hljs-number">1</span>, <span class="hljs-number">1</span>)
test_k
</code></pre>
<p>We now build the Level 2 predictor, using $k$ as the input.</p>
<pre><code class="language-python">linreg_latent = LinearRegression()
</code></pre>
<pre><code class="language-python">linreg_latent = linreg_latent.fit(train_k, df_train[<span class="hljs-string">&quot;ZFYA&quot;</span>])
</code></pre>
<pre><code class="language-python">predictions_latent = linreg_latent.predict(test_k)
predictions_latent
</code></pre>
<pre><code class="language-python">latent_score = linreg_latent.score(test_k, df_test[<span class="hljs-string">&quot;ZFYA&quot;</span>])
print(latent_score)
</code></pre>
<pre><code><span class="hljs-attribute">0</span>.<span class="hljs-number">008509520014148064</span>

</code></pre>
<pre><code class="language-python">RMSE_latent = np.sqrt(mean_squared_error(df_test[<span class="hljs-string">&quot;ZFYA&quot;</span>], predictions_latent))
print(RMSE_latent)
</code></pre>
<pre><code><span class="hljs-attribute">0</span>.<span class="hljs-number">9236245677858551</span>

</code></pre>
<h3 id="additive-error-model">Additive error model</h3>
<p>Finally, in <strong>Level 3</strong>, we model <code>GPA</code>, <code>LSAT</code>, and <code>FYA</code> as continuous variables with additive error terms
independent of race and sex<sup class="footnote-ref"><a href="#fn3" id="fnref3">[3]</a></sup>.</p>
<p>This corresponds to</p>
<p>$$
\begin{aligned}
GPA &amp;= b_G + w^R_{GPA}R + w^S_{GPA}S + \epsilon_{GPA}, \epsilon_{GPA} \sim p(\epsilon_{GPA}) \\
LSAT &amp;= b_L + w^R_{LSAT}R + w^S_{LSAT}S + \epsilon_{LSAT}, \epsilon_{LSAT} \sim p(\epsilon_{LSAT}) \\
FYA &amp;= b_{FYA} + w^R_{FYA}R + w^S_{FYA}S + \epsilon_{FYA} , \epsilon_{FYA} \sim p(\epsilon_{FYA})
\end{aligned}
$$
We estimate the error terms $\epsilon_{GPA}, \epsilon_{LSAT}$ by first fitting two models that each use race and sex to individually
predict <code>GPA</code> and <code>LSAT</code>. We then compute the residuals of each model (<em>e.g.</em>, $\epsilon_{GPA} =GPA−\hat{Y}<em>{GPA}(R, S)$).
We use these residual estimates of $\epsilon</em>{GPA}, \epsilon_{LSAT}$ to predict $FYA$. In <sup class="footnote-ref"><a href="#fn2" id="fnref2:4">[2]</a></sup> this is called <em>Fair Add</em>.
Since the process is similar for the individual predictions for <code>GPA</code> and <code>LSAT</code>, we will write a method to avoid repetion.</p>
<pre><code class="language-python"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">calculate_epsilon</span>(<span class="hljs-params">data, var_name, protected_attr</span>):</span>
    X = data[protected_attr]
    y = data[var_name]

    linreg = LinearRegression()
    linreg = linreg.fit(X, y)

    predictions = linreg.predict(X)

    <span class="hljs-keyword">return</span> data[var_name] - predictions
</code></pre>
<p>Let's apply it to each variable, individually.
First we calculate $\epsilon_{GPA}$:</p>
<pre><code class="language-python">epsilons_gpa = calculate_epsilon(df, <span class="hljs-string">&quot;UGPA&quot;</span>, A)
epsilons_gpa
</code></pre>
<p>Next, we calculate $\epsilon_{LSAT}$:</p>
<pre><code class="language-python">epsilons_LSAT = calculate_epsilon(df, <span class="hljs-string">&quot;LSAT&quot;</span>, A)
epsilons_LSAT
</code></pre>
<p>Let's visualise the $\epsilon$ distribution quickly:</p>
<pre><code class="language-python"><span class="hljs-keyword">import</span> matplotlib.pyplot <span class="hljs-keyword">as</span> plt
<span class="hljs-keyword">import</span> seaborn <span class="hljs-keyword">as</span> sns

plt.subplot(<span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">1</span>)
plt.hist(epsilons_gpa, color=colours[<span class="hljs-number">0</span>], bins=<span class="hljs-number">100</span>)
plt.title(<span class="hljs-string">&quot;$\epsilon_{GPA}$&quot;</span>)
plt.xlabel(<span class="hljs-string">&quot;$\epsilon_{GPA}$&quot;</span>)

plt.subplot(<span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">2</span>)
plt.hist(epsilons_LSAT, color=colours[<span class="hljs-number">1</span>], bins=<span class="hljs-number">100</span>)
plt.title(<span class="hljs-string">&quot;$\epsilon_{LSAT}$&quot;</span>)
plt.xlabel(<span class="hljs-string">&quot;$\epsilon_{LSAT}$&quot;</span>)
plt.show()
</code></pre>
<p><img src="./images/counterfactual-fairness_2.png" alt="counterfactual-fairness_2">
We finally use the calculated $\epsilon$ to train a model in order to predict <code>FYA</code>.
We start by getting the subset of the $\epsilon$ which match the training indices.</p>
<pre><code class="language-python">X = np.hstack(
    (
        np.array(epsilons_gpa[df_train.index]).reshape(-<span class="hljs-number">1</span>, <span class="hljs-number">1</span>),
        np.array(epsilons_LSAT[df_train.index]).reshape(-<span class="hljs-number">1</span>, <span class="hljs-number">1</span>),
    )
)
X
</code></pre>
<pre><code class="language-python">linreg_fair_add = LinearRegression()

linreg_fair_add = linreg_fair_add.fit(
    X,
    df_train[<span class="hljs-string">&quot;ZFYA&quot;</span>],
)
</code></pre>
<p>We now use this model to calculate the predictions</p>
<pre><code class="language-python">X_test = np.hstack(
    (
        np.array(epsilons_gpa[df_test.index]).reshape(-<span class="hljs-number">1</span>, <span class="hljs-number">1</span>),
        np.array(epsilons_LSAT[df_test.index]).reshape(-<span class="hljs-number">1</span>, <span class="hljs-number">1</span>),
    )
)

predictions_fair_add = linreg_fair_add.predict(X_test)
predictions_fair_add
</code></pre>
<p>And as previously, we calculate the model's score:</p>
<pre><code class="language-python">fair_add_score = linreg_fair_add.score(X_test, df_test[<span class="hljs-string">&quot;ZFYA&quot;</span>])
print(fair_add_score)
</code></pre>
<pre><code><span class="hljs-attribute">0</span>.<span class="hljs-number">04475841449183948</span>

</code></pre>
<pre><code class="language-python">RMSE_fair_add = np.sqrt(mean_squared_error(df_test[<span class="hljs-string">&quot;ZFYA&quot;</span>], predictions_fair_add))
print(RMSE_fair_add)
</code></pre>
<pre><code><span class="hljs-attribute">0</span>.<span class="hljs-number">9065835039365202</span>

</code></pre>
<h3 id="comparison">Comparison</h3>
<p>The scores, so far, are:</p>
<pre><code class="language-python">print(<span class="hljs-string">f&quot;Unfair score:\t<span class="hljs-subst">{score_unfair}</span>&quot;</span>)
print(<span class="hljs-string">f&quot;FTU score:\t<span class="hljs-subst">{ftu_score}</span>&quot;</span>)
print(<span class="hljs-string">f&quot;L2 score:\t<span class="hljs-subst">{latent_score}</span>&quot;</span>)
print(<span class="hljs-string">f&quot;Fair add score:\t<span class="hljs-subst">{fair_add_score}</span>&quot;</span>)
</code></pre>
<pre><code><span class="hljs-attribute">Unfair</span> score:   <span class="hljs-number">0</span>.<span class="hljs-number">12701634112845117</span>
<span class="hljs-attribute">FTU</span> score:  <span class="hljs-number">0</span>.<span class="hljs-number">0917442226187073</span>
<span class="hljs-attribute">L2</span> score:   <span class="hljs-number">0</span>.<span class="hljs-number">008509520014148064</span>
<span class="hljs-attribute">Fair</span> add score: <span class="hljs-number">0</span>.<span class="hljs-number">04475841449183948</span>

</code></pre>
<pre><code class="language-python">print(<span class="hljs-string">f&quot;Unfair RMSE:\t<span class="hljs-subst">{RMSE_unfair}</span>&quot;</span>)
print(<span class="hljs-string">f&quot;FTU RMSE:\t<span class="hljs-subst">{RMSE_ftu}</span>&quot;</span>)
print(<span class="hljs-string">f&quot;L2 RMSE:\t<span class="hljs-subst">{RMSE_latent}</span>&quot;</span>)
print(<span class="hljs-string">f&quot;Fair add RMSE:\t<span class="hljs-subst">{RMSE_fair_add}</span>&quot;</span>)
</code></pre>
<pre><code><span class="hljs-attribute">Unfair</span> RMSE:    <span class="hljs-number">0</span>.<span class="hljs-number">8666709890234552</span>
<span class="hljs-attribute">FTU</span> RMSE:   <span class="hljs-number">0</span>.<span class="hljs-number">8840061503773576</span>
<span class="hljs-attribute">L2</span> RMSE:    <span class="hljs-number">0</span>.<span class="hljs-number">9236245677858551</span>
<span class="hljs-attribute">Fair</span> add RMSE:  <span class="hljs-number">0</span>.<span class="hljs-number">9065835039365202</span>

</code></pre>
<h2 id="measuring-counterfactual-fairness">Measuring counterfactual fairness</h2>
<p>First, we will measure two quantities, the <strong>Statistical Parity Difference</strong> (SPD)<sup class="footnote-ref"><a href="#fn4" id="fnref4">[4]</a></sup> and <strong>Disparate impact</strong> (DI)<sup class="footnote-ref"><a href="#fn5" id="fnref5">[5]</a></sup>.</p>
<h3 id="statistical-parity-difference-/-disparate-impact">Statistical Parity Difference / Disparate Impact</h3>
<pre><code class="language-python"><span class="hljs-keyword">from</span> fairlearn.metrics <span class="hljs-keyword">import</span> demographic_parity_difference, demographic_parity_ratio

parities = []
impacts = []

<span class="hljs-keyword">for</span> a <span class="hljs-keyword">in</span> A:
    parity = demographic_parity_difference(df_train[<span class="hljs-string">&quot;ZFYA&quot;</span>], df_train[<span class="hljs-string">&quot;ZFYA&quot;</span>], 
                                                sensitive_features = df_train[a])
    di = demographic_parity_ratio(df_train[<span class="hljs-string">&quot;ZFYA&quot;</span>], df_train[<span class="hljs-string">&quot;ZFYA&quot;</span>], 
                                                sensitive_features = df_train[a])
    parities.append(parity)
    impacts.append(di)
</code></pre>
<pre><code class="language-python">df_parities = pd.DataFrame({<span class="hljs-string">&#x27;protected&#x27;</span>:A,<span class="hljs-string">&#x27;parity&#x27;</span>:parities,<span class="hljs-string">&#x27;impact&#x27;</span>:impacts})
</code></pre>
<pre><code class="language-python"><span class="hljs-keyword">import</span> matplotlib.pyplot <span class="hljs-keyword">as</span> plt
<span class="hljs-keyword">from</span> plotutils <span class="hljs-keyword">import</span> *

fig = plt.figure()

ax = fig.add_subplot(<span class="hljs-number">111</span>)
ax2 = ax.twinx()

fig.suptitle(<span class="hljs-string">&#x27;Statistical Parity Difference and Disparate Impact&#x27;</span>)

width = <span class="hljs-number">0.4</span>
df_parities.plot(x =<span class="hljs-string">&#x27;protected&#x27;</span>, y = <span class="hljs-string">&#x27;parity&#x27;</span>, kind = <span class="hljs-string">&#x27;bar&#x27;</span>, ax = ax, width = width, 
       position=<span class="hljs-number">1</span>, color=colours[<span class="hljs-number">0</span>], legend=<span class="hljs-literal">False</span>)

df_parities.plot(x =<span class="hljs-string">&#x27;protected&#x27;</span>, y = <span class="hljs-string">&#x27;impact&#x27;</span>, kind = <span class="hljs-string">&#x27;bar&#x27;</span>, ax = ax2, width = width, 
       position = <span class="hljs-number">0</span>, color = colours[<span class="hljs-number">1</span>], legend = <span class="hljs-literal">False</span>)

ax.axhline(y = <span class="hljs-number">0.1</span>, linestyle = <span class="hljs-string">&#x27;dashed&#x27;</span>, alpha = <span class="hljs-number">0.7</span>, color = colours[<span class="hljs-number">0</span>])
ax2.axhline(y = <span class="hljs-number">0.55</span>, linestyle = <span class="hljs-string">&#x27;dashed&#x27;</span>, alpha = <span class="hljs-number">0.7</span>, color = colours[<span class="hljs-number">1</span>])


patches, labels = ax.get_legend_handles_labels()
ax.legend(patches, [<span class="hljs-string">&#x27;Stat Parity Diff&#x27;</span>], loc = <span class="hljs-string">&#x27;upper left&#x27;</span>)

patches, labels = ax2.get_legend_handles_labels()
ax2.legend(patches, [<span class="hljs-string">&#x27;Disparate Impact&#x27;</span>], loc = <span class="hljs-string">&#x27;upper right&#x27;</span>)



labels = [item.get_text() <span class="hljs-keyword">for</span> item <span class="hljs-keyword">in</span> ax.get_xticklabels()]

<span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-built_in">len</span>(A)):
    labels[i] = A[i]

ax.set_xticklabels(labels)
ax.set_xlabel(<span class="hljs-string">&#x27;Protected Features&#x27;</span>)

ax.set_ylabel(<span class="hljs-string">&#x27;Statistical Parity Difference&#x27;</span>)
ax2.set_ylabel(<span class="hljs-string">&#x27;Disparate Impact&#x27;</span>)

plt.show()
</code></pre>
<p><img src="./images/counterfactual-fairness_3.png" alt="counterfactual-fairness_3"></p>
<h3 id="finding-sensitive-features">Finding sensitive features</h3>
<p>Typically a $SPD &gt; 0.1$ and a $DI &lt; 0.9$ might indicate discrimination on those features.
All <em>protected attributes</em> fail the SPD test and, in our dataset, we have two features (<code>Hispanic</code> and <code>Mexican</code>) which clearly fail the DI test.</p>
<pre><code class="language-python"><span class="hljs-keyword">for</span> a <span class="hljs-keyword">in</span> [<span class="hljs-string">&quot;Mexican&quot;</span>, <span class="hljs-string">&quot;Hispanic&quot;</span>]:
    spd = demographic_parity_difference(y_true=df_train[<span class="hljs-string">&quot;ZFYA&quot;</span>], 
                                        y_pred=df_train[<span class="hljs-string">&quot;ZFYA&quot;</span>], 
                                        sensitive_features = df_train[a])
    print(<span class="hljs-string">f&quot;SPD(<span class="hljs-subst">{a}</span>) = <span class="hljs-subst">{spd}</span>&quot;</span>)
    di = demographic_parity_ratio(y_true=df_train[<span class="hljs-string">&quot;ZFYA&quot;</span>], 
                                  y_pred=df_train[<span class="hljs-string">&quot;ZFYA&quot;</span>], 
                                  sensitive_features = df_train[a])
    print(<span class="hljs-string">f&quot;DI(<span class="hljs-subst">{a}</span>) = <span class="hljs-subst">{di}</span>&quot;</span>)
</code></pre>
<pre><code><span class="hljs-function"><span class="hljs-title">SPD</span><span class="hljs-params">(Mexican)</span></span> = <span class="hljs-number">0.0014017257538768636</span>
<span class="hljs-function"><span class="hljs-title">DI</span><span class="hljs-params">(Mexican)</span></span> = <span class="hljs-number">0.5556529360210342</span>
<span class="hljs-function"><span class="hljs-title">SPD</span><span class="hljs-params">(Hispanic)</span></span> = <span class="hljs-number">0.003272247102713093</span>
<span class="hljs-function"><span class="hljs-title">DI</span><span class="hljs-params">(Hispanic)</span></span> = <span class="hljs-number">0.34227833235466826</span>

</code></pre>
<pre><code class="language-python">
</code></pre>
<hr class="footnotes-sep">
<section class="footnotes">
<ol class="footnotes-list">
<li id="fn1"  class="footnote-item"><p>McIntyre, Frank, and Michael Simkovic. &quot;Are law degrees as valuable to minorities?.&quot; International Review of Law and Economics 53 (2018): 23-37. <a href="#fnref1" class="footnote-backref">↩</a></p>
</li>
<li id="fn2"  class="footnote-item"><p>Kusner, Matt J., Joshua Loftus, Chris Russell, and Ricardo Silva. &quot;Counterfactual fairness.&quot; In Advances in neural information processing systems, pp. 4066-4076. 2017. <a href="#fnref2" class="footnote-backref">↩</a> <a href="#fnref2:1" class="footnote-backref">↩</a> <a href="#fnref2:2" class="footnote-backref">↩</a> <a href="#fnref2:3" class="footnote-backref">↩</a> <a href="#fnref2:4" class="footnote-backref">↩</a></p>
</li>
<li id="fn3"  class="footnote-item"><p>That may in turn be correlated with one-another. <a href="#fnref3" class="footnote-backref">↩</a></p>
</li>
<li id="fn4"  class="footnote-item"><p>See {ref}<code>fairness:demographic-parity-difference</code>. <a href="#fnref4" class="footnote-backref">↩</a></p>
</li>
<li id="fn5"  class="footnote-item"><p>See {ref}<code>fairness:disparate-impact</code>. <a href="#fnref5" class="footnote-backref">↩</a></p>
</li>
</ol>
</section>
        <div class="footer">
            <span class="cc-symbol">&#127341;</span> 2020 CC BY Rui Vieira
        </div>
    </div>

    <div id="sidebar">
        <div id="sidebar-search">
            <input id="search_terms" type="search" placeholder="Search terms" />
            <button id="search_button" onclick="search()">?</button>
        </div>
        <div id="sidebar-home"><a href="/">Home</a></div>
        <div id="sidebar-all-pages"><a href="/content.html">All pages</a></div>
        <div id="sidebar-graph"><a href="/graph.html">Link network</a></div>

        <div id="sidebar-contents">
            <h3>Contents</h3>
            <ul>
<li><a href="#building-counterfactually-fair-models">Building counterfactually fair models</a>
<ul>
<li><a href="#data">Data</a></li>
<li><a href="#pre-processing">Pre-processing</a></li>
<li><a href="#protected-attributes">Protected attributes</a></li>
<li><a href="#training-and-testing-subsets">Training and testing subsets</a></li>
</ul></li>
<li><a href="#models">Models</a>
<ul>
<li><a href="#unfair-model">Unfair model</a></li>
<li><a href="#full-model">Full model</a></li>
<li><a href="#fairness-through-unawareness-ftu">Fairness through unawareness (FTU)</a></li>
<li><a href="#latent-variable-model">Latent variable model</a></li>
<li><a href="#additive-error-model">Additive error model</a></li>
<li><a href="#comparison">Comparison</a></li>
</ul></li>
<li><a href="#measuring-counterfactual-fairness">Measuring counterfactual fairness</a>
<ul>
<li><a href="#statistical-parity-difference-/-disparate-impact">Statistical Parity Difference / Disparate Impact</a></li>
<li><a href="#finding-sensitive-features">Finding sensitive features</a></li>
</ul></li>
</ul>

                        <h3>Backlinks</h3>
            <ul>
                                <li><a href="/explainability.html">Explainability</a><sup>&#5833</sup></li>
                                <li><a href="/index.html">index</a><sup>&#5833</sup></li>
                            </ul>
                    </div>



        <div class="footer">
            modified 1 month ago        </div>

    </div>
    <div>

        <script>
            const input = document.getElementById('search_terms');
            const highlight = new URLSearchParams(document.location.search).get("h");

            if (highlight!=null) {
                const markInstance = new Mark("#content");
                markInstance.mark(highlight);
            }

            input.addEventListener("keyup", function(event) {
                // Number 13 is the "Enter" key on the keyboard
                if (event.keyCode === 13) {
                    // Cancel the default action, if needed
                    event.preventDefault();
                    // Trigger the button element with a click
                    search_button.click();
                }
            });

            let search = function() {
                const query = new URLSearchParams({"q": input.value});
                console.log(query.toString());
                window.location.href = "/search.html?" + query.toString();
            }
        </script>
        <script>
            document.addEventListener("DOMContentLoaded", function() {
                renderMathInElement(
                    document.body,
                    {
                        delimiters: [
                            {left: "$$", right: "$$", display: true},
                            {left: "\\[", right: "\\]", display: true},
                            {left: "$", right: "$", display: false},
                            {left: "\\(", right: "\\)", display: false}
                        ]
                    }
                );
            });
        </script>

</body>
</html>