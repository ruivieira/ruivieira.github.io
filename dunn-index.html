<!doctype html><html lang=en-uk><head><script data-goatcounter=https://ruivieira-dev.goatcounter.com/count async src=//gc.zgo.at/count.js></script>
<script type=module src=/js/deeplinks/deeplinks.js></script>
<link rel=preload href=/lib/fonts/fa-brands-400.woff2 as=font type=font/woff2 crossorigin=anonymous><link rel=preload href=/lib/fonts/fa-regular-400.woff2 as=font type=font/woff2 crossorigin=anonymous><link rel=preload href=/lib/fonts/fa-solid-900.woff2 as=font type=font/woff2 crossorigin=anonymous><link rel=preload href=/lib/JetBrainsMono/web/woff2/JetBrainsMono-Regular.woff2 as=font type=font/woff2 crossorigin=anonymous><link rel=stylesheet href=/css/kbd.css type=text/css><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><title>Dunn index · Rui Vieira</title><link rel=canonical href=https://ruivieira.dev/dunn-index.html><meta name=viewport content="width=device-width,initial-scale=1"><meta name=robots content="all,follow"><meta name=googlebot content="index,follow,snippet,archive"><meta property="og:title" content="Dunn index"><meta property="og:description" content="There are several ways to measure the robustness of a clustering algorithm. Three commonly used metrics are the Dunn index, Davis-Bouldin index and Silhoutte index.
But before we start, let&rsquo;s introduce some concepts.
We are interested in clustering algorithms for a dataset \(\mathcal{D}\) with \(N\) elements in a $n$-dimensional real space, that is:
\[ \mathcal{D} = {x_1, x_2, \ldots, x_N} \in \mathbb{R}^p \]
The clustering algorithm will create a set \(C\) of \(K\) distinct disjoint groups from \(\mathcal{D}\) \(C={c_1, c_2, \ldots, c_k}\), such that:"><meta property="og:type" content="article"><meta property="og:url" content="https://ruivieira.dev/dunn-index.html"><meta property="article:section" content="posts"><meta property="article:modified_time" content="2022-01-03T21:56:06+00:00"><meta name=twitter:card content="summary"><meta name=twitter:title content="Dunn index"><meta name=twitter:description content="There are several ways to measure the robustness of a clustering algorithm. Three commonly used metrics are the Dunn index, Davis-Bouldin index and Silhoutte index.
But before we start, let&rsquo;s introduce some concepts.
We are interested in clustering algorithms for a dataset \(\mathcal{D}\) with \(N\) elements in a $n$-dimensional real space, that is:
\[ \mathcal{D} = {x_1, x_2, \ldots, x_N} \in \mathbb{R}^p \]
The clustering algorithm will create a set \(C\) of \(K\) distinct disjoint groups from \(\mathcal{D}\) \(C={c_1, c_2, \ldots, c_k}\), such that:"><link rel=stylesheet href=https://ruivieira.dev/css/styles.css><!--[if lt IE 9]><script src=https://oss.maxcdn.com/html5shiv/3.7.2/html5shiv.min.js></script>
<script src=https://oss.maxcdn.com/respond/1.4.2/respond.min.js></script><![endif]--><link rel=icon type=image/png href=https://ruivieira.dev/images/favicon.ico></head><body class="max-width mx-auto px3 ltr"><div class="content index py4"><div id=header-post><a id=menu-icon href=#><i class="fas fa-bars fa-lg"></i></a>
<a id=menu-icon-tablet href=#><i class="fas fa-bars fa-lg"></i></a>
<a id=top-icon-tablet href=# onclick='$("html, body").animate({scrollTop:0},"fast")' style=display:none aria-label="Top of Page"><i class="fas fa-chevron-up fa-lg"></i></a>
<span id=menu><span id=nav><ul><li><a href=/>Home</a></li><li><a href=/map/>All pages</a></li><li><a href=/search.html>Search</a></li></ul></span><br><span id=actions><ul><li><a class=icon href=https://ruivieira.dev/elisp.html aria-label=Previous><i class="fas fa-chevron-left" aria-hidden=true onmouseover='$("#i-prev").toggle()' onmouseout='$("#i-prev").toggle()'></i></a></li><li><a class=icon href=https://ruivieira.dev/doom-emacs.html aria-label=Next><i class="fas fa-chevron-right" aria-hidden=true onmouseover='$("#i-next").toggle()' onmouseout='$("#i-next").toggle()'></i></a></li><li><a class=icon href=# onclick='$("html, body").animate({scrollTop:0},"fast")' aria-label="Top of Page"><i class="fas fa-chevron-up" aria-hidden=true onmouseover='$("#i-top").toggle()' onmouseout='$("#i-top").toggle()'></i></a></li><li><a class=icon href=# aria-label=Share><i class="fas fa-share-alt" aria-hidden=true onmouseover='$("#i-share").toggle()' onmouseout='$("#i-share").toggle()' onclick='return $("#share").toggle(),!1'></i></a></li></ul><span id=i-prev class=info style=display:none>Previous post</span>
<span id=i-next class=info style=display:none>Next post</span>
<span id=i-top class=info style=display:none>Back to top</span>
<span id=i-share class=info style=display:none>Share post</span></span><br><div id=share style=display:none><ul><li><a class=icon href="http://www.facebook.com/sharer.php?u=https%3a%2f%2fruivieira.dev%2fdunn-index.html" aria-label=Facebook><i class="fab fa-facebook" aria-hidden=true></i></a></li><li><a class=icon href="https://twitter.com/share?url=https%3a%2f%2fruivieira.dev%2fdunn-index.html&text=Dunn%20index" aria-label=Twitter><i class="fab fa-twitter" aria-hidden=true></i></a></li><li><a class=icon href="http://www.linkedin.com/shareArticle?url=https%3a%2f%2fruivieira.dev%2fdunn-index.html&title=Dunn%20index" aria-label=Linkedin><i class="fab fa-linkedin" aria-hidden=true></i></a></li><li><a class=icon href="https://pinterest.com/pin/create/bookmarklet/?url=https%3a%2f%2fruivieira.dev%2fdunn-index.html&is_video=false&description=Dunn%20index" aria-label=Pinterest><i class="fab fa-pinterest" aria-hidden=true></i></a></li><li><a class=icon href="mailto:?subject=Dunn%20index&body=Check out this article: https%3a%2f%2fruivieira.dev%2fdunn-index.html" aria-label=Email><i class="fas fa-envelope" aria-hidden=true></i></a></li><li><a class=icon href="https://getpocket.com/save?url=https%3a%2f%2fruivieira.dev%2fdunn-index.html&title=Dunn%20index" aria-label=Pocket><i class="fab fa-get-pocket" aria-hidden=true></i></a></li><li><a class=icon href="http://reddit.com/submit?url=https%3a%2f%2fruivieira.dev%2fdunn-index.html&title=Dunn%20index" aria-label=reddit><i class="fab fa-reddit" aria-hidden=true></i></a></li><li><a class=icon href="http://www.tumblr.com/share/link?url=https%3a%2f%2fruivieira.dev%2fdunn-index.html&name=Dunn%20index&description=There%20are%20several%20ways%20to%20measure%20the%20robustness%20of%20a%20clustering%20algorithm.%20Three%20commonly%20used%20metrics%20are%20the%20Dunn%20index%2c%20Davis-Bouldin%20index%20and%20Silhoutte%20index.%0aBut%20before%20we%20start%2c%20let%26rsquo%3bs%20introduce%20some%20concepts.%0aWe%20are%20interested%20in%20clustering%20algorithms%20for%20a%20dataset%20%5c%28%5cmathcal%7bD%7d%5c%29%20with%20%5c%28N%5c%29%20elements%20in%20a%20%24n%24-dimensional%20real%20space%2c%20that%20is%3a%0a%5c%5b%20%5cmathcal%7bD%7d%20%3d%20%7bx_1%2c%20x_2%2c%20%5cldots%2c%20x_N%7d%20%5cin%20%5cmathbb%7bR%7d%5ep%20%5c%5d%0aThe%20clustering%20algorithm%20will%20create%20a%20set%20%5c%28C%5c%29%20of%20%5c%28K%5c%29%20distinct%20disjoint%20groups%20from%20%5c%28%5cmathcal%7bD%7d%5c%29%20%5c%28C%3d%7bc_1%2c%20c_2%2c%20%5cldots%2c%20c_k%7d%5c%29%2c%20such%20that%3a" aria-label=Tumblr><i class="fab fa-tumblr" aria-hidden=true></i></a></li><li><a class=icon href="https://news.ycombinator.com/submitlink?u=https%3a%2f%2fruivieira.dev%2fdunn-index.html&t=Dunn%20index" aria-label="Hacker News"><i class="fab fa-hacker-news" aria-hidden=true></i></a></li></ul></div><div id=toc><h4>Contents</h4><nav id=TableOfContents><ul><li><a href=#dunn-index>Dunn index</a></li></ul></nav><h4>Related</h4><nav><ul><li class="header-post toc"><a href=https://ruivieira.dev/distance-metrics.html>Distance metrics</a></li></ul></nav></div></span></div><article class=post itemscope itemtype=http://schema.org/BlogPosting><header><h1 class=posttitle itemprop="name headline">Dunn index</h1><div class=meta><div class=postdate>Updated <time datetime="2022-01-03 21:56:06 +0000 GMT" itemprop=datePublished>2022-01-03</time>
<span class=commit-hash>(261c776)</span></div></div></header><div class=content itemprop=articleBody><p>There are several ways to measure the robustness of a clustering algorithm. Three commonly used metrics are the <strong><strong>Dunn index</strong></strong>, <strong><strong>Davis-Bouldin index</strong></strong> and <strong><strong>Silhoutte index</strong></strong>.</p><p>But before we start, let&rsquo;s introduce some concepts.</p><p>We are interested in clustering algorithms for a dataset \(\mathcal{D}\) with \(N\) elements in a $n$-dimensional real space, that is:</p><p>\[
\mathcal{D} = {x_1, x_2, \ldots, x_N} \in \mathbb{R}^p
\]</p><p>The clustering algorithm will create a set \(C\) of \(K\) distinct disjoint groups from \(\mathcal{D}\) \(C={c_1, c_2, \ldots, c_k}\), such that:</p><p>\[
\cup_{c_k\in C}c_k=\mathcal{D} \\
c_k \cap c_l \neq \emptyset \forall k\neq l
\]</p><p>Each group (or cluster) \(c_k\), will have a <strong><strong>centroid</strong></strong>, \(\bar{c}_k\), which is the mean vector of its elements such that:</p><p>\[
\bar{c}_k=\frac{1}{|c_k|}\sum_{x_i \in c_k}x_i
\]</p><p>We will also make use of the dataset&rsquo;s mean vector, \(\bar{\mathcal{D}}\), defined as:</p><p>\[
\bar{\mathcal{D}}=\frac{1}{N}\sum_{x_i \in X}x_i
\]</p><h2 id=dunn-index>Dunn index</h2><p>The <strong><strong>Dunn index</strong></strong> aims at quantifying the compactness and variance of the clustering.
A cluster is considered <strong><strong>compact</strong></strong> if there is small variance between members of the cluster.
This can be calculated using \(\Delta(c_k)\), where</p><p>\[
\Delta(c_k) = \max_{x_i, x_j \in c_k}{d_e(x_i, x_j)}
\]</p><p>and \(d_e\) is the <a href=/distance-metrics.html>Euclidean distance</a> defined as:</p><p>\[
d_e=\sqrt{\sum_{j=1}^p (x_{ij}-x_{kj})^2}.
\]</p><p>A cluster is considered <strong>well separated</strong> if the cluster are far-apart. This can quantified using</p><p>\[
\delta(c_k, c_l) = \min_{x_i \in c_k}\min_{x_j\in c_l}{d_e(x_i, x_j)}.
\]</p><p>Given these quantities, the <strong>Dunn index</strong> for a set of clusters \(C\), \(DI( C)\), is then defined by:</p><p>\[
DI( C)=\frac{\min_{c_k \in C}{\delta(c_k, c_l)}}{\max_{c_k\in C}\Delta(c_k)}
\]</p><p>A higher <strong>Dunn Index</strong> will indicate compact, well-separated clusters, while a lower index will indicate less compact or less well-separated clusters.</p><p>We can now try to calculate the metric for the dataset we&rsquo;ve created previously.
Let&rsquo;s simulate some data and apply the Dunn index from scratch.
First, we will create a compact and well-separated dataset using the <a href=https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make%5C%5Fblobs.html>make_blobs</a> method in <code>scikit-learn</code>.
We will create a dataset of \(\mathbb{R}^2\) data (for easier plotting), with three clusters.</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=font-weight:700>from</span> <span style=color:#555>sklearn.datasets</span> <span style=font-weight:700>import</span> make_blobs
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>X, y <span style=font-weight:700>=</span> make_blobs(n_samples<span style=font-weight:700>=</span><span style=color:#099>1000</span>,
</span></span><span style=display:flex><span>                  centers<span style=font-weight:700>=</span><span style=color:#099>3</span>,
</span></span><span style=display:flex><span>                  n_features<span style=font-weight:700>=</span><span style=color:#099>2</span>,
</span></span><span style=display:flex><span>                  random_state<span style=font-weight:700>=</span><span style=color:#099>23</span>)
</span></span></code></pre></div><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=font-weight:700>import</span> <span style=color:#555>pandas</span> <span style=font-weight:700>as</span> <span style=color:#555>pd</span>
</span></span><span style=display:flex><span><span style=font-weight:700>from</span> <span style=color:#555>plotnine</span> <span style=font-weight:700>import</span> <span style=font-weight:700>*</span>
</span></span><span style=display:flex><span><span style=font-weight:700>from</span> <span style=color:#555>plotnine.data</span> <span style=font-weight:700>import</span> <span style=font-weight:700>*</span>
</span></span><span style=display:flex><span><span style=font-weight:700>from</span> <span style=color:#555>plotutils</span> <span style=font-weight:700>import</span> <span style=font-weight:700>*</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>data <span style=font-weight:700>=</span> pd<span style=font-weight:700>.</span>DataFrame(X, columns<span style=font-weight:700>=</span>[<span style=color:#b84>&#34;x1&#34;</span>, <span style=color:#b84>&#34;x2&#34;</span>])
</span></span><span style=display:flex><span>data[<span style=color:#b84>&#34;y&#34;</span>] <span style=font-weight:700>=</span> y
</span></span><span style=display:flex><span>data[<span style=color:#b84>&#34;y&#34;</span>] <span style=font-weight:700>=</span> data<span style=font-weight:700>.</span>y<span style=font-weight:700>.</span>astype(<span style=color:#b84>&#39;category&#39;</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>ggplot(data<span style=font-weight:700>=</span>data) <span style=font-weight:700>+</span> \
</span></span><span style=display:flex><span>    geom_point(mapping<span style=font-weight:700>=</span>aes(x<span style=font-weight:700>=</span><span style=color:#b84>&#34;x1&#34;</span>, y<span style=font-weight:700>=</span><span style=color:#b84>&#34;x2&#34;</span>, colour<span style=font-weight:700>=</span><span style=color:#b84>&#34;y&#34;</span>)) <span style=font-weight:700>+</span> \
</span></span><span style=display:flex><span>    scale_color_manual(values<span style=font-weight:700>=</span>[colours[<span style=color:#099>0</span>], colours[<span style=color:#099>1</span>], colours[<span style=color:#099>2</span>]]) <span style=font-weight:700>+</span> \
</span></span><span style=display:flex><span>    theme_classic()
</span></span></code></pre></div><p>We now cluster the data[^2] and we will have, as expected three distinct clusters, plotted below.</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=font-weight:700>from</span> <span style=color:#555>sklearn</span> <span style=font-weight:700>import</span> cluster
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>k_means <span style=font-weight:700>=</span> cluster<span style=font-weight:700>.</span>KMeans(n_clusters<span style=font-weight:700>=</span><span style=color:#099>3</span>)
</span></span><span style=display:flex><span>k_means<span style=font-weight:700>.</span>fit(data)
</span></span><span style=display:flex><span>y_pred <span style=font-weight:700>=</span> k_means<span style=font-weight:700>.</span>predict(data)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>prediction <span style=font-weight:700>=</span> pd<span style=font-weight:700>.</span>concat([data, pd<span style=font-weight:700>.</span>DataFrame(y_pred, columns<span style=font-weight:700>=</span>[<span style=color:#b84>&#39;pred&#39;</span>])], axis <span style=font-weight:700>=</span> <span style=color:#099>1</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>clus0 <span style=font-weight:700>=</span> prediction<span style=font-weight:700>.</span>loc[prediction<span style=font-weight:700>.</span>pred <span style=font-weight:700>==</span> <span style=color:#099>0</span>]
</span></span><span style=display:flex><span>clus1 <span style=font-weight:700>=</span> prediction<span style=font-weight:700>.</span>loc[prediction<span style=font-weight:700>.</span>pred <span style=font-weight:700>==</span> <span style=color:#099>1</span>]
</span></span><span style=display:flex><span>clus2 <span style=font-weight:700>=</span> prediction<span style=font-weight:700>.</span>loc[prediction<span style=font-weight:700>.</span>pred <span style=font-weight:700>==</span> <span style=color:#099>2</span>]
</span></span><span style=display:flex><span>k_list <span style=font-weight:700>=</span> [clus0<span style=font-weight:700>.</span>values, clus1<span style=font-weight:700>.</span>values,clus2<span style=font-weight:700>.</span>values]
</span></span></code></pre></div><p>Let&rsquo;s focus now on two of these cluster, let&rsquo;s call them \(c_k\) and \(c_l\).</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>ck <span style=font-weight:700>=</span> k_list[<span style=color:#099>0</span>]
</span></span><span style=display:flex><span>cl <span style=font-weight:700>=</span> k_list[<span style=color:#099>1</span>]
</span></span></code></pre></div><p>We know we have to calculate the distance between the points in \(c_k\) and \(c_l\). We know that the `len(ck)=len(cl)=333` we create</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=font-weight:700>import</span> <span style=color:#555>numpy</span> <span style=font-weight:700>as</span> <span style=color:#555>np</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>values <span style=font-weight:700>=</span> np<span style=font-weight:700>.</span>ones([<span style=color:#999>len</span>(ck), <span style=color:#999>len</span>(cl)])
</span></span><span style=display:flex><span>values
</span></span></code></pre></div><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-text data-lang=text><span style=display:flex><span>array([[1., 1., 1., ..., 1., 1., 1.],
</span></span><span style=display:flex><span>       [1., 1., 1., ..., 1., 1., 1.],
</span></span><span style=display:flex><span>       [1., 1., 1., ..., 1., 1., 1.],
</span></span><span style=display:flex><span>       ...,
</span></span><span style=display:flex><span>       [1., 1., 1., ..., 1., 1., 1.],
</span></span><span style=display:flex><span>       [1., 1., 1., ..., 1., 1., 1.],
</span></span><span style=display:flex><span>       [1., 1., 1., ..., 1., 1., 1.]])
</span></span></code></pre></div><p>For each pair of points, we then get the norm of \(x_i-x_j\). For instance, for \(i=0\in c_k\) and \(i=1\in c_l\), we would have:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>values[<span style=color:#099>0</span>, <span style=color:#099>1</span>] <span style=font-weight:700>=</span> np<span style=font-weight:700>.</span>linalg<span style=font-weight:700>.</span>norm(ck[<span style=color:#099>0</span>]<span style=font-weight:700>-</span>cl[<span style=color:#099>1</span>])
</span></span><span style=display:flex><span><span style=color:#999>print</span>(ck[<span style=color:#099>0</span>], cl[<span style=color:#099>1</span>])
</span></span><span style=display:flex><span><span style=color:#999>print</span>(values[<span style=color:#099>0</span>, <span style=color:#099>1</span>])
</span></span></code></pre></div><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-text data-lang=text><span style=display:flex><span>[-5.37039106  3.47555168  2.          0.        ] [ 5.46312794 -3.08938807  1.          1.        ]
</span></span><span style=display:flex><span>12.746119711608184
</span></span></code></pre></div><p>The calculation of \(\delta(c_k, c_l)\) between two clusters \(c_k\) and \(c_l\) will be defined as follows:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=font-weight:700>import</span> <span style=color:#555>numpy</span> <span style=font-weight:700>as</span> <span style=color:#555>np</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=font-weight:700>def</span> <span style=color:#900;font-weight:700>δ</span>(ck, cl):
</span></span><span style=display:flex><span>    values <span style=font-weight:700>=</span> np<span style=font-weight:700>.</span>ones([<span style=color:#999>len</span>(ck), <span style=color:#999>len</span>(cl)])
</span></span><span style=display:flex><span>    <span style=font-weight:700>for</span> i <span style=font-weight:700>in</span> <span style=color:#999>range</span>(<span style=color:#099>0</span>, <span style=color:#999>len</span>(ck)):
</span></span><span style=display:flex><span>        <span style=font-weight:700>for</span> j <span style=font-weight:700>in</span> <span style=color:#999>range</span>(<span style=color:#099>0</span>, <span style=color:#999>len</span>(cl)):
</span></span><span style=display:flex><span>            values[i, j] <span style=font-weight:700>=</span> np<span style=font-weight:700>.</span>linalg<span style=font-weight:700>.</span>norm(ck[i]<span style=font-weight:700>-</span>cl[j])
</span></span><span style=display:flex><span>    <span style=font-weight:700>return</span> np<span style=font-weight:700>.</span>min(values)
</span></span></code></pre></div><p>So, for our two clusters above, \(\delta(c_k, c_l)\) will be:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>δ(ck, cl)
</span></span></code></pre></div><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-text data-lang=text><span style=display:flex><span>8.13474311744193
</span></span></code></pre></div><p>Within a single cluster \(c_k\), we can calculate \(\Delta(c_k)\) similarly as:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=font-weight:700>def</span> <span style=color:#900;font-weight:700>Δ</span>(ci):
</span></span><span style=display:flex><span>    values <span style=font-weight:700>=</span> np<span style=font-weight:700>.</span>zeros([<span style=color:#999>len</span>(ci), <span style=color:#999>len</span>(ci)])
</span></span><span style=display:flex><span>    <span style=font-weight:700>for</span> i <span style=font-weight:700>in</span> <span style=color:#999>range</span>(<span style=color:#099>0</span>, <span style=color:#999>len</span>(ci)):
</span></span><span style=display:flex><span>        <span style=font-weight:700>for</span> j <span style=font-weight:700>in</span> <span style=color:#999>range</span>(<span style=color:#099>0</span>, <span style=color:#999>len</span>(ci)):
</span></span><span style=display:flex><span>            values[i, j] <span style=font-weight:700>=</span> np<span style=font-weight:700>.</span>linalg<span style=font-weight:700>.</span>norm(ci[i]<span style=font-weight:700>-</span>ci[j])
</span></span><span style=display:flex><span>    <span style=font-weight:700>return</span> np<span style=font-weight:700>.</span>max(values)
</span></span></code></pre></div><p>So, for instance, for our \(c_k\) and \(c_l\) we would have:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#999>print</span>(Δ(ck))
</span></span><span style=display:flex><span><span style=color:#999>print</span>(Δ(cl))
</span></span></code></pre></div><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-text data-lang=text><span style=display:flex><span>6.726025773561468
</span></span><span style=display:flex><span>6.173844284636552
</span></span></code></pre></div><p>We can now define the <strong>Dunn index</strong> as</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=font-weight:700>def</span> <span style=color:#900;font-weight:700>dunn</span>(k_list):
</span></span><span style=display:flex><span>    δs <span style=font-weight:700>=</span> np<span style=font-weight:700>.</span>ones([<span style=color:#999>len</span>(k_list), <span style=color:#999>len</span>(k_list)])
</span></span><span style=display:flex><span>    Δs <span style=font-weight:700>=</span> np<span style=font-weight:700>.</span>zeros([<span style=color:#999>len</span>(k_list), <span style=color:#099>1</span>])
</span></span><span style=display:flex><span>    l_range <span style=font-weight:700>=</span> <span style=color:#999>list</span>(<span style=color:#999>range</span>(<span style=color:#099>0</span>, <span style=color:#999>len</span>(k_list)))
</span></span><span style=display:flex><span>    <span style=font-weight:700>for</span> k <span style=font-weight:700>in</span> l_range:
</span></span><span style=display:flex><span>        <span style=font-weight:700>for</span> l <span style=font-weight:700>in</span> (l_range[<span style=color:#099>0</span>:k]<span style=font-weight:700>+</span>l_range[k<span style=font-weight:700>+</span><span style=color:#099>1</span>:]):
</span></span><span style=display:flex><span>            δs[k, l] <span style=font-weight:700>=</span> δ(k_list[k], k_list[l])
</span></span><span style=display:flex><span>            Δs[k] <span style=font-weight:700>=</span> Δ(k_list[k])
</span></span><span style=display:flex><span>            di <span style=font-weight:700>=</span> np<span style=font-weight:700>.</span>min(δs)<span style=font-weight:700>/</span>np<span style=font-weight:700>.</span>max(Δs)
</span></span><span style=display:flex><span>    <span style=font-weight:700>return</span> di
</span></span></code></pre></div><p>and calculate the <strong>Dunn index</strong> for our clustered values list as</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>dunn(k_list)
</span></span></code></pre></div><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-text data-lang=text><span style=display:flex><span>0.14867620697065728
</span></span></code></pre></div><p>Intuitively, we can expect a dataset with less well-defined clusters to have a lower <strong>Dunn index</strong>. Let&rsquo;s try it.
We first generate the new dataset.</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>X, y <span style=font-weight:700>=</span> make_blobs(n_samples<span style=font-weight:700>=</span><span style=color:#099>1000</span>,
</span></span><span style=display:flex><span>                  centers<span style=font-weight:700>=</span><span style=color:#099>3</span>,
</span></span><span style=display:flex><span>                  n_features<span style=font-weight:700>=</span><span style=color:#099>2</span>,
</span></span><span style=display:flex><span>                  cluster_std<span style=font-weight:700>=</span><span style=color:#099>10.0</span>,
</span></span><span style=display:flex><span>                  random_state<span style=font-weight:700>=</span><span style=color:#099>24</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>df <span style=font-weight:700>=</span> pd<span style=font-weight:700>.</span>DataFrame(X, columns<span style=font-weight:700>=</span>[<span style=color:#b84>&#39;A&#39;</span>, <span style=color:#b84>&#39;B&#39;</span>])
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>k_means <span style=font-weight:700>=</span> cluster<span style=font-weight:700>.</span>KMeans(n_clusters<span style=font-weight:700>=</span><span style=color:#099>3</span>)
</span></span><span style=display:flex><span>k_means<span style=font-weight:700>.</span>fit(df)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#998;font-style:italic>#K-means training</span>
</span></span><span style=display:flex><span>y_pred <span style=font-weight:700>=</span> k_means<span style=font-weight:700>.</span>predict(df)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>prediction <span style=font-weight:700>=</span> pd<span style=font-weight:700>.</span>concat([df,pd<span style=font-weight:700>.</span>DataFrame(y_pred, columns<span style=font-weight:700>=</span>[<span style=color:#b84>&#39;pred&#39;</span>])], axis <span style=font-weight:700>=</span> <span style=color:#099>1</span>)
</span></span><span style=display:flex><span>prediction[<span style=color:#b84>&#34;pred&#34;</span>] <span style=font-weight:700>=</span> prediction<span style=font-weight:700>.</span>pred<span style=font-weight:700>.</span>astype(<span style=color:#b84>&#39;category&#39;</span>)
</span></span></code></pre></div><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>ggplot(data<span style=font-weight:700>=</span>prediction) <span style=font-weight:700>+</span>\
</span></span><span style=display:flex><span>geom_point(mapping<span style=font-weight:700>=</span>aes(x<span style=font-weight:700>=</span><span style=color:#b84>&#34;A&#34;</span>, y<span style=font-weight:700>=</span><span style=color:#b84>&#34;B&#34;</span>, colour<span style=font-weight:700>=</span><span style=color:#b84>&#34;pred&#34;</span>)) <span style=font-weight:700>+</span> \
</span></span><span style=display:flex><span>scale_color_manual(values<span style=font-weight:700>=</span>[colours[<span style=color:#099>0</span>], colours[<span style=color:#099>1</span>], colours[<span style=color:#099>2</span>]]) <span style=font-weight:700>+</span> theme_classic()
</span></span></code></pre></div><figure><img src=./.ob-jupyter/144ccb9ee25c619a88afebae6bb845a962b06eae.png></figure><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-text data-lang=text><span style=display:flex><span>&lt;ggplot: (317671277)&gt;
</span></span></code></pre></div><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>clus0 <span style=font-weight:700>=</span> prediction<span style=font-weight:700>.</span>loc[prediction<span style=font-weight:700>.</span>pred <span style=font-weight:700>==</span> <span style=color:#099>0</span>]
</span></span><span style=display:flex><span>clus1 <span style=font-weight:700>=</span> prediction<span style=font-weight:700>.</span>loc[prediction<span style=font-weight:700>.</span>pred <span style=font-weight:700>==</span> <span style=color:#099>1</span>]
</span></span><span style=display:flex><span>clus2 <span style=font-weight:700>=</span> prediction<span style=font-weight:700>.</span>loc[prediction<span style=font-weight:700>.</span>pred <span style=font-weight:700>==</span> <span style=color:#099>2</span>]
</span></span><span style=display:flex><span>k_list <span style=font-weight:700>=</span> [clus0<span style=font-weight:700>.</span>values, clus1<span style=font-weight:700>.</span>values,clus2<span style=font-weight:700>.</span>values]
</span></span></code></pre></div><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>dunn(k_list)
</span></span></code></pre></div><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-text data-lang=text><span style=display:flex><span>0.019563892388205984
</span></span></code></pre></div></div></article><div id=footer-post-container><div id=footer-post><div id=nav-footer style=display:none><ul><li><a href=/>Home</a></li><li><a href=/map/>All pages</a></li><li><a href=/search.html>Search</a></li></ul></div><div id=toc-footer style=display:none><nav id=TableOfContents><ul><li><a href=#dunn-index>Dunn index</a></li></ul></nav></div><div id=share-footer style=display:none><ul><li><a class=icon href="http://www.facebook.com/sharer.php?u=https%3a%2f%2fruivieira.dev%2fdunn-index.html" aria-label=Facebook><i class="fab fa-facebook fa-lg" aria-hidden=true></i></a></li><li><a class=icon href="https://twitter.com/share?url=https%3a%2f%2fruivieira.dev%2fdunn-index.html&text=Dunn%20index" aria-label=Twitter><i class="fab fa-twitter fa-lg" aria-hidden=true></i></a></li><li><a class=icon href="http://www.linkedin.com/shareArticle?url=https%3a%2f%2fruivieira.dev%2fdunn-index.html&title=Dunn%20index" aria-label=Linkedin><i class="fab fa-linkedin fa-lg" aria-hidden=true></i></a></li><li><a class=icon href="https://pinterest.com/pin/create/bookmarklet/?url=https%3a%2f%2fruivieira.dev%2fdunn-index.html&is_video=false&description=Dunn%20index" aria-label=Pinterest><i class="fab fa-pinterest fa-lg" aria-hidden=true></i></a></li><li><a class=icon href="mailto:?subject=Dunn%20index&body=Check out this article: https%3a%2f%2fruivieira.dev%2fdunn-index.html" aria-label=Email><i class="fas fa-envelope fa-lg" aria-hidden=true></i></a></li><li><a class=icon href="https://getpocket.com/save?url=https%3a%2f%2fruivieira.dev%2fdunn-index.html&title=Dunn%20index" aria-label=Pocket><i class="fab fa-get-pocket fa-lg" aria-hidden=true></i></a></li><li><a class=icon href="http://reddit.com/submit?url=https%3a%2f%2fruivieira.dev%2fdunn-index.html&title=Dunn%20index" aria-label=reddit><i class="fab fa-reddit fa-lg" aria-hidden=true></i></a></li><li><a class=icon href="http://www.tumblr.com/share/link?url=https%3a%2f%2fruivieira.dev%2fdunn-index.html&name=Dunn%20index&description=There%20are%20several%20ways%20to%20measure%20the%20robustness%20of%20a%20clustering%20algorithm.%20Three%20commonly%20used%20metrics%20are%20the%20Dunn%20index%2c%20Davis-Bouldin%20index%20and%20Silhoutte%20index.%0aBut%20before%20we%20start%2c%20let%26rsquo%3bs%20introduce%20some%20concepts.%0aWe%20are%20interested%20in%20clustering%20algorithms%20for%20a%20dataset%20%5c%28%5cmathcal%7bD%7d%5c%29%20with%20%5c%28N%5c%29%20elements%20in%20a%20%24n%24-dimensional%20real%20space%2c%20that%20is%3a%0a%5c%5b%20%5cmathcal%7bD%7d%20%3d%20%7bx_1%2c%20x_2%2c%20%5cldots%2c%20x_N%7d%20%5cin%20%5cmathbb%7bR%7d%5ep%20%5c%5d%0aThe%20clustering%20algorithm%20will%20create%20a%20set%20%5c%28C%5c%29%20of%20%5c%28K%5c%29%20distinct%20disjoint%20groups%20from%20%5c%28%5cmathcal%7bD%7d%5c%29%20%5c%28C%3d%7bc_1%2c%20c_2%2c%20%5cldots%2c%20c_k%7d%5c%29%2c%20such%20that%3a" aria-label=Tumblr><i class="fab fa-tumblr fa-lg" aria-hidden=true></i></a></li><li><a class=icon href="https://news.ycombinator.com/submitlink?u=https%3a%2f%2fruivieira.dev%2fdunn-index.html&t=Dunn%20index" aria-label="Hacker News"><i class="fab fa-hacker-news fa-lg" aria-hidden=true></i></a></li></ul></div><div id=actions-footer><a id=menu-toggle class=icon href=# onclick='return $("#nav-footer").toggle(),!1' aria-label=Menu><i class="fas fa-bars fa-lg" aria-hidden=true></i> Menu</a>
<a id=toc-toggle class=icon href=# onclick='return $("#toc-footer").toggle(),!1' aria-label=TOC><i class="fas fa-list fa-lg" aria-hidden=true></i> TOC</a>
<a id=share-toggle class=icon href=# onclick='return $("#share-footer").toggle(),!1' aria-label=Share><i class="fas fa-share-alt fa-lg" aria-hidden=true></i> share</a>
<a id=top style=display:none class=icon href=# onclick='$("html, body").animate({scrollTop:0},"fast")' aria-label="Top of Page"><i class="fas fa-chevron-up fa-lg" aria-hidden=true></i> Top</a></div></div></div><footer id=footer><div class=footer-left>Copyright &copy; 2022 Rui Vieira</div><div class=footer-right><nav><ul><li><a href=/>Home</a></li><li><a href=/map/>All pages</a></li><li><a href=/search.html>Search</a></li></ul></nav></div></footer></div></body><link rel=stylesheet href=/css/fa.min.css><script src=/js/jquery-3.6.0.min.js></script>
<script src=/js/mark.min.js></script>
<script src=/js/main.js></script>
<script>MathJax={tex:{inlineMath:[["$","$"],["\\(","\\)"]]},svg:{fontCache:"global"}}</script><script type=text/javascript id=MathJax-script async src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js></script></html>