[{"categories":null,"contents":"I\u0026rsquo;ve been using a minimalist blog setup for some time now.\nI was having something of a framework fatigue after switching between a few static site generators. Each new generator I decided to try implied usually either learning a new programming language (Python, Ruby, Go) to perform basic setup and a new template engine syntax1. Typically I wasn\u0026rsquo;t using the vast majority of the features available for each generator. And finally, most of the generators I tried over the years rely on heavy configuration if I want to maintain the site organisation and look.\nI\u0026rsquo;ve discussed with some friends and colleagues why it\u0026rsquo;s my opinion that a plain HTML blog is still superior to other solutions (such as Markdown coupled with some generator framework). I\u0026rsquo;ll leave my arguments to a future post.\nHowever, I am still using some form of a generator. The blog writing process at the moment is the following:\nI write the content of the post to an HTML fragment (no HEAD, for instance). All files are HTML and in the same folder. I have a shell script to walk through the files in the input folder and add a common header, footer and process all code blocks with syntax highlighting. Save the \u0026ldquo;processed\u0026rdquo; files to an output folder Upload (currently to Github to be served via Github pages). The HTML fragments are minimal, for instance:\n\u0026lt;h1\u0026gt;A post-modern title\u0026lt;/h1\u0026gt; \u0026lt;p\u0026gt;Yes, this could be an entire blog post.\u0026lt;/p\u0026gt; The point is, where do we draw the line on what a static generator is? For this post, I won\u0026rsquo;t consider a loose collection of specialised scripts to be a static generator. There is no configuration, no convention, no theming ability 2. You can argue that this is what many generators do, but I think that\u0026rsquo;s beyond the scope of this short post.\nSince my static blog is straightforward, with minimal markup, why not create something equally simple for RSS generation? To do so, I\u0026rsquo;ve decided to go the way of \u0026ldquo;handcrafted\u0026rdquo; HTML.\nHowever, I was accustomed to a static site generator to generate some goodies, such as syndication feeds automatically.\nI\u0026rsquo;ve decided to add an RSS feed to the site, using minimal dependencies (only shell scripting and a couple of universal user-land tools such as grep and cat). This approach has the added benefit that it is applicable to expose other types of data as an RSS feed, such as server and periodic job logs.\nWe start by adding the feed header to the index.xml:\n\u0026lt;?xml version=\u0026#34;1.0\u0026#34; encoding=\u0026#34;utf-8\u0026#34;?\u0026gt; \u0026lt;rss version=\u0026#34;2.0\u0026#34;\u0026gt; \u0026lt;channel\u0026gt; \u0026lt;title\u0026gt;Rui Vieira\u0026#39;s blog\u0026lt;/title\u0026gt; \u0026lt;description\u0026gt;Rui Vieira\u0026#39;s personal blog\u0026lt;/description\u0026gt; \u0026lt;link\u0026gt;https://ruivieira.dev/\u0026lt;/link\u0026gt; \u0026lt;lastBuildDate\u0026gt;$(date)\u0026lt;/lastBuildDate\u0026gt; EOF The RSS 2.0 specification3 is quite simple in terms of the minimum requirements for a valid feed. The mandatory \u0026ldquo;header\u0026rdquo; fields are:\ntitle, the name of the channel. link, the URL to the HTML website corresponding to the channel. description, phrase or sentence describing the channel. In terms of feed items, according to the specification, at least one of title or description must be present, and all remaining elements are optional.\nWe use the following in this feed:\ntitle, the title of the item. link, the URL of the item. pubDate indicates when the item was published. pubDate needs to conform with RFC 822.\n[!INFO] Just as interesting tidbit, RFC 822 (which defines Internet Text Message formats) is one of the core email RFCs. It predates [https://en.wikipedia.org/wiki/ISO_8601]ISO 8601 by six years (1982) and it\u0026rsquo;s itself based on 1977\u0026rsquo;s [https://tools.ietf.org/html/rfc733]RFC 733.\nWe then loop over all the input files to build the RSS entries.\nFILES=input/*.html for FILE in $FILES do FILENAME=\u0026#34;${FILE##*/}\u0026#34; FILENAME=\u0026#34;${FILENAME%.*}\u0026#34; # extract title ... # write entry to index.xml done Using Bash First, extract the title. The actual title is not inside the \u0026lt;title\u0026gt; tag, but on the first header \u0026lt;h1\u0026gt;.\ncat output/nb-estimation.html |\\ grep -E \u0026#34;\u0026lt;h1.*\u0026gt;(.*?)\u0026lt;/h1\u0026gt;\u0026#34; |\\ sed \u0026#39;s/.*\u0026lt;h1.*\u0026gt;\\(.*\\)\u0026lt;\\/h1\u0026gt;.*/\\1/\u0026#39; The first produces:\n\u0026lt;div id=\u0026#34;main\u0026#34;\u0026gt; \u0026lt;h1 id=\u0026#34;negative-binomial-estimation\u0026#34;\u0026gt;Negative Binomial estimation\u0026lt;/h1\u0026gt; \u0026lt;/div\u0026gt; While the second produces:\nNegative Binomial estimations Now, what happens if we have more than one \u0026lt;h1\u0026gt; header? UNIX pipelines to the rescue. We simple retrieve the first line of the matching grep, by inserting a head -1.\nTo get the modified date of $FILE we can use:\ndate -r $FILE.html The final RSS feed build is:\ncat \u0026gt;output/index.xml \u0026lt;\u0026lt;EOF \u0026lt;?xml version=\u0026#34;1.0\u0026#34; encoding=\u0026#34;utf-8\u0026#34;?\u0026gt; \u0026lt;rss version=\u0026#34;2.0\u0026#34;\u0026gt; \u0026lt;channel\u0026gt; \u0026lt;title\u0026gt;Rui Vieira\u0026#39;s blog\u0026lt;/title\u0026gt; \u0026lt;description\u0026gt;Rui Vieira\u0026#39;s personal blog\u0026lt;/description\u0026gt; \u0026lt;link\u0026gt;https://ruivieira.dev/\u0026lt;/link\u0026gt; \u0026lt;lastBuildDate\u0026gt;$(date)\u0026lt;/lastBuildDate\u0026gt; EOF FILES=input/*.html for FILE in $FILES do FILENAME=\u0026#34;${FILE##*/}\u0026#34; FILENAME=\u0026#34;${FILENAME%.*}\u0026#34; TITLE=$(cat output/$FILENAME.html | grep -E \u0026#34;\u0026lt;h1.*\u0026gt;(.*?)\u0026lt;/h1\u0026gt;\u0026#34; | head -1 | sed \u0026#39;s/.*\u0026lt;h1.*\u0026gt;\\(.*\\)\u0026lt;\\/h1\u0026gt;.*/\\1/\u0026#39;) cat \u0026gt;\u0026gt;output/index.xml \u0026lt;\u0026lt;EOF \u0026lt;item\u0026gt; \u0026lt;title\u0026gt;$TITLE\u0026lt;/title\u0026gt; \u0026lt;link\u0026gt;https://ruivieira.dev/$FILENAME.html\u0026lt;/link\u0026gt; \u0026lt;pubDate\u0026gt;$(date -r output/$FILENAME.html)\u0026lt;/pubDate\u0026gt; \u0026lt;/item\u0026gt; EOF done cat \u0026gt;\u0026gt;output/index.xml \u0026lt;\u0026lt;EOF \u0026lt;/channel\u0026gt; \u0026lt;/rss\u0026gt; EOF Using Python Another possibility is to use a specialised tool to extract an RSS item from an HTML file. To do so, we need to parse the necessary data and replace the extraction part of the loop. This is, after all, along the lines of the UNIX Philosohopy 4: create specialised tools with a focus on modularity and reusability.\nTo do, we create a simple script called post_title.py. It uses the Beautiful Soup library, which you can install using:\n$ pip install beautifulsoup4 The script reads an HTML file, extract the title and return:\nfrom bs4 import BeautifulSoup import sys with open(sys.argv[1], \u0026#39;r\u0026#39;) as file: data = file.read() soup = BeautifulSoup(data, features=\u0026#34;html.parser\u0026#34;) print(soup.h1.string) This script can now be used to replace the title extraction:\ncat \u0026gt;output/index.xml \u0026lt;\u0026lt;EOF \u0026lt;?xml version=\u0026#34;1.0\u0026#34; encoding=\u0026#34;utf-8\u0026#34;?\u0026gt; \u0026lt;rss version=\u0026#34;2.0\u0026#34;\u0026gt; \u0026lt;channel\u0026gt; \u0026lt;title\u0026gt;Rui Vieira\u0026#39;s blog\u0026lt;/title\u0026gt; \u0026lt;description\u0026gt;Rui Vieira\u0026#39;s personal blog\u0026lt;/description\u0026gt; \u0026lt;link\u0026gt;https://ruivieira.dev/\u0026lt;/link\u0026gt; \u0026lt;lastBuildDate\u0026gt;$(date)\u0026lt;/lastBuildDate\u0026gt; EOF FILES=input/*.html for FILE in $FILES do FILENAME=\u0026#34;${FILE##*/}\u0026#34; FILENAME=\u0026#34;${FILENAME%.*}\u0026#34; cat \u0026gt;\u0026gt;output/index.xml \u0026lt;\u0026lt;EOF \u0026lt;item\u0026gt; \u0026lt;title\u0026gt;$(post_title.py $FILENAME.html)\u0026lt;/title\u0026gt; \u0026lt;link\u0026gt;https://ruivieira.dev/$FILENAME.html\u0026lt;/link\u0026gt; \u0026lt;pubDate\u0026gt;$(date -r output/$FILENAME.html)\u0026lt;/pubDate\u0026gt; \u0026lt;/item\u0026gt; EOF done cat \u0026gt;\u0026gt;output/index.xml \u0026lt;\u0026lt;EOF \u0026lt;/channel\u0026gt; \u0026lt;/rss\u0026gt; EOF The reason why the whole RSS feed is not generated in Python is to have the title extraction as a \u0026ldquo;function\u0026rdquo; which can map to whichever logic the shell script is using.\nHope this could be useful to you. Happy coding!\nAs it turns out \u0026hellip; I reverted to using a static site generator. More information can be found in the page site details.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nApart from plain CSS theming, that is.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nhttps://validator.w3.org/feed/docs/rss2.html\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nSee Unix philosophy.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"/semi-handcrafted-rss.html","tags":null,"title":"(Semi) handcrafted RSS"},{"categories":null,"contents":" The original page seems to have disappeared from the Internet1 (see: link rot), so the original contents are kept here.\nWhen making this website, I wanted a simple, reasonable way to make it look good on most displays. Not counting any minimization techniques, the following 58 bytes worked well for me:\nmain { max-width: 38rem; padding: 2rem; margin: auto; } Let\u0026rsquo;s break this down.\nmax-width: 38rem It appears that the default font size for most browsers is 16px, so 38rem is 608px. Supporting 600px displays at a minimum seems reasonable.\npadding: 2rem If the display\u0026rsquo;s width goes under 38rem, then this padding keeps things looking pretty good until around 256px. While this may seem optional, it actually hits two birds with one stone - the padding also provides sorely-needed top and bottom whitespace.\nmargin: auto This is really all that is needed to center the page, because main is a block element under semantic html5.\nA key insight: it took me a surprising number of iterations to arrive at this point. Perhaps that speaks to the fact that I know nothing about \u0026ldquo;modern\u0026rdquo; web development, or, as i\u0026rsquo;m more inclined to believe, just how hard it is to keep it simple in a world of complication.\n[!Update] Following some discussion (see footer), I\u0026rsquo;ve since changed the padding to 1.5rem for a happier compromise between mobile and desktop displays.\n[!Update 2] The ch unit was brought to my attention here, and I quite like it! I\u0026rsquo;ve since changed to 70ch / 2ch, which looks nearly the same with 2 less bytes, except that the padding is a little bit smaller (a good thing for mobile).\nA cached version can still be seen, hopefully, at http://web.archive.org/web/20210318102514/https://jrl.ninja/etc/1/\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"/58-bytes-of-css-to-look-great-nearly-everywhere.html","tags":null,"title":"58 bytes of CSS to look great nearly everywhere"},{"categories":null,"contents":"Recently, I\u0026rsquo;ve been following with interest the development of the Crystal language.\nCrystal is a statically typed language with a syntax resembling Ruby\u0026rsquo;s. The main features which drawn me to it were its simple boilerplate-free syntax (which is ideal for quick prototyping), tied with the ability to compile directly to native code along with a dead simple way of creating bindings to existing C code.\nThese features make it quite attractive, in my opinion, for scientific computing. To test it against more popular languages, I\u0026rsquo;ve decided to run the Gibbs sampling examples created in Darren Wilkinson\u0026rsquo;s blog.\nI recommend reading this post, and in fact, if you are interested in Mathematics and scientific computing in general, I strongly recommend you follow the blog.\nAs explained in the linked post, I will make a Gibbs sampler for\n$$ f\\left(x,y\\right)=kx^2\\exp\\left\\lbrace-xy^2-y^2+2y-4x\\right\\rbrace $$\nwith\n$$ \\begin{aligned} x|y \u0026amp;\\sim Ga\\left(3,y^2+4\\right) \\\\ y|x \u0026amp;\\sim N\\left(\\frac{1}{1+x},\\frac{1}{2\\left(1+x\\right)}\\right) \\end{aligned} $$\nThe original examples were ran again, without any code alterations. I\u0026rsquo;ve just added the Crystal version.\nThis implementation uses a very simple wrapper I wrote to the famous GNU Scientific Library (GSL).\nrequire \u0026#34;../libs/gsl/statistics.cr\u0026#34; require \u0026#34;math\u0026#34; def gibbs(n : Int = 50000, thin : Int = 1000) x = 0.0 y = 0.0 puts \u0026#34;Iter x y\u0026#34; (0..n).each do |i| (0..thin).each do |j| x = Statistics::Gamma.sample(3.0, y\\*y+4.0) y = Statistics::Normal.sample(1.0/(x+1.0), 1.0/Math.sqrt(2.0\\*x+2.0)) end puts \u0026#34;#{i} #{x} #{y}\u0026#34; end end gibbs (As you can see, the Crystal code is quite similar to the Python one).\nTo make sure it\u0026rsquo;s a fair comparison, I ran it in compiled (and optimised mode build using\n$ crystal build gibbs.cr --release $ time ./gibbs \u0026gt; gibbs_crystal.csv Looking at the results, you can see that they are consistent with the other implementations:\nThe timings for each of the different versions1 were\nLanguage Time (s) R 364.8 Python 144.0 Scala 9.896 Crystal 5.171 C 5.038 So there you have it. A Ruby-like language which can easily compete with C performance-wise.\nI sincerely hope that Crystal gets some traction in the scientific community.\nThat of course won\u0026rsquo;t depend solely on its merits but rather on an active community along with a strong library ecosystem.\nThis is lacking at the moment, simply because it is relatively new language with the specs and standard library still being finalised.\nRan in a 1.7 GHz Intel Core i7 Macbook Air.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"/a-gibbs-sampler-in-crystal.html","tags":null,"title":"A Gibbs Sampler in Crystal"},{"categories":null,"contents":"Recently when discussing the Crystal language and specifically the Gibbs sample blog post with a colleague, he mentioned that the Python benchmark numbers looked a bit off and not consistent with his experience of numerical programming in Python.\nTo recall, the numbers were:\nLanguage Time(s) R 364.8 Python 144.0 Scala 9.896 Crystal 5.171 C 5.038 To have a better understanding of what is happening, I\u0026rsquo;ve decided to profile and benchmark that code (running on Python 3.6).\nThe code is the following:\nimport random, math def gibbs(N=50000, thin=1000): x = 0 y = 0 print(\u0026#34;Iter x y\u0026#34;) for i in range(N): for j in range(thin): x = random.gammavariate(3, 1.0 / (y y + 4)) y = random.gauss(1.0 / (x + 1), 1.0 / math.sqrt(2 x + 2)) print(i,x,y) Profiling this code with cProfile gives the following results:\nName Call count Time (ms) Percentage gammavariate 50000000 141267 52.1% gauss 50000000 65689 24.2% \u0026lt;built-in method math.log\u0026gt; 116628436 18825 6.9% \u0026lt;method 'random' of '_random.Random' objects\u0026gt; 170239973 17155 6.3% \u0026lt;built-in method math.sqrt\u0026gt; 125000000 12352 4.6% \u0026lt;built-in method math.exp\u0026gt; 60119980 7276 2.7% \u0026lt;built-in method math.cos\u0026gt; 25000000 3338 1.2% \u0026lt;built-in method math.sin\u0026gt; 25000000 3336 1.2% \u0026lt;built-in method builtins.print\u0026gt; 50001 1030 0.4% gibbs.py 1 271396 100.0% The results look different than the original ones on account of being performed on a different machine. However, we will just look into the relative code performance between different implementations and whether the code itself has room for optimisation.\nSurprisingly, the console I/O took a much smaller proportion of the execution time than I expected (0.4%).\nOn the other hand, as expected, the bulk of the execution time is spent on the gammavariate and gauss methods.\nThese methods, however, are provided by the Python\u0026rsquo;s standard library random, which underneath makes heavy usage of C code (mainly by usage of the random() function).\nFor the second run of the code, I\u0026rsquo;ve decided to use numpy to sample from the Gamma and Normal distributions. The new code, gibbs_np.py, is provided below.\nimport numpy as np import math def gibbs(N=50000, thin=1000): x = 0 y = 0 print(\u0026#34;Iter x y\u0026#34;) for i in range(N): for j in range(thin): x = np.random.gamma(3, 1.0 / (y y + 4)) y = np.random.normal(1.0 / (x + 1), 1.0 / math.sqrt(2 x + 2)) print(i,x,y) if __name__ == \u0026#34;main\u0026#34;: gibbs() We can see from the plots below that the results from both modules are identical.\nThe profiling results for the numpy version were:\nName Call count Time (ms) Percentage \u0026lt;method 'gamma' of 'mtrand.RandomState' objects\u0026gt; 50000000 121211 45.8% \u0026lt;method 'normal' of 'mtrand.RandomState' objects\u0026gt; 50000000 83092 31.4% \u0026lt;built-in method math.sqrt\u0026gt; 50000000 6127 2.3% \u0026lt;built-in method builtins.print\u0026gt; 50001 920 0.3% gibbs_np.py 1 264420 100.0% A few interesting results from this benchmark were the fact that using numpy or random didn\u0026rsquo;t make much difference overall (264.4 and 271.3 seconds, respectively).\nThis is despite the fact that, apparently, the Gamma sampling seems to perform better in numpy but the Normal sampling seems to be faster in the random library.\nYou will notice that we\u0026rsquo;ve still used Python\u0026rsquo;s built-in math.sqrt since it is known that for scalar usage it out-performs numpy\u0026rsquo;s equivalent.\nUnfortunately, in my view, we are just witnessing a fact of life: Python is not the best language for number crunching.\nSince the bulk of the computational time, as we\u0026rsquo;ve seen, is due to the sampling of the Normal and Gamma distributions, it is clear that in our code there is little room for optimisation except the sampling methods themselves.\nA few possible solutions would be to:\nConvert the code to Cython Use FFI to call a highly optimised native library which provides Gamma and Normal distributions (such as GSL) Nevertheless, personally I still find Python a great language for quick prototyping of algorithms and with an excellent scientific computing libraries ecosystem. Keep on Pythoning.\n","permalink":"/a-simple-python-benchmark-exercise.html","tags":null,"title":"A simple Python benchmark exercise"},{"categories":null,"contents":"In this blog post I would like to talk a little bit about recommendation engines in general and how to build a streaming recommendation engine on top of Apache Spark.\nI will start by introducing the concept of collaborative filterig, and focus in two variants: batch and streaming Alternating Least Squares (ALS). I will look at the principles of a streaming distributed recommendation engine on Spark and finally, I\u0026rsquo;ll talk about practical issues when using these methods.\nRecommendation engines So what are \u0026ldquo;recommendation engines\u0026rdquo;?\nRecommendation engines are a popular method to match users, products and historical data on user behaviour.\nCollaborative filtering In the majority of cases, we assume there\u0026rsquo;s a unique mapping between a user $x$, a product $y$ and rating $\\mathsf{R}_{x,y}$.\n$$ \\left(x,y\\right) \\mapsto \\mathsf{R}_{x,y} $$\nThe \u0026ldquo;collaborative\u0026rdquo; aspect refers to the fact that we are using collective information from a group of users and \u0026ldquo;filtering\u0026rdquo; is simply a synonym for \u0026ldquo;prediction\u0026rdquo;.\nSo, we use collaborative filtering quite frequently in our daily life and it really seems like common sense.\nThe main principle is that if a group of people tend to collectively have similar tastes, it is more likely that they agree on an unknown product.\nLet\u0026rsquo;s imagine that you have a number of friends with whom you share a very similar musical taste, let\u0026rsquo;s call it A and another group, B, compared to which you have very different musical tastes.\nIf group A and group B both recommend you a new album which they regard highly, which one would you pick?\nYou will probably pick the album from group A, right? So that\u0026rsquo;s collaborative filtering in a nutshell.\nBonus question\nWhat if an album is considered really bad by group B? Does it mean you\u0026rsquo;ll like it?\nIt\u0026rsquo;s difficult to tell. Because group A has relevance to you, it\u0026rsquo;s easy to match. Because B is too dissimilar, a low rating is not very informative.\nAlternating Least Squares (ALS) One of the most popular collaborative filtering methods is Alternating Least Squares (ALS).\nIn ALS we assume that the available rating data can be represented in a sparse matrix form, that is, we will assume a sequential ordering of both users and products. Each entry of the matrix will then represent the rating for a unique pair of user and products.\nIf we then consider ratings data as a matrix, let\u0026rsquo;s call it $\\mathsf{R}$, the user and product ids will represent coordinates in a ratings matrix and the actual rating will be the value for that particular entry. To keep the notation consistent with the above we simply call the entry $(x,y)$ as $\\mathsf{R}_{x,y}$. This will look something like the matrix represented in the figure below.\nThe idea behind ALS is to factorise the ratings matrix $\\mathsf{R}_{x,y}$ into two matrices $\\mathsf{U}$ and $\\mathsf{P}$, which in turn, when multiplied back, will return an approximation of the original ratings matrix, that is:\n$$ \\mathsf{R} \\approx \\hat{\\mathsf{R}} = \\mathsf{U}^T \\mathsf{P} $$\nTo \u0026ldquo;predict\u0026rdquo; a missing rating for a user $x$ and product $y$, we can simply multiply two vectors, namely the $x$ row from the user latent factors and the $y$ column from the product latent factors, $\\hat{\\mathsf{R}}_{x,y}$, that is:\n$$ \\hat{\\mathsf{R}}_{x,y} = \\mathsf{U}_x^T \\mathsf{P}_y $$\nThere are several ways to tackle this factorisation problem and we will cover two of them in here. We will first look at a batch method, which aims at factorising using the whole of the ratings matrix and a stochastic gradient descent method, which uses a single observation at a time.\nBatch ALS This factorisation is performed by first defining an (objective) loss function (here called $\\ell$).\nA general form is represented below where, as before, $\\mathsf{R} _{x,y}$ is the true rating and $\\hat{\\mathsf{R}} _{x,y}$ is the predicted rating, calculated as seen previously. The remaining terms are simply regularisation terms to help prevent overfitting.\n$$ \\ell = \\sum c _{x,y} \\left(\\mathsf{R} _{x,y} - \\underbrace{\\mathsf{U}_x^T \\mathsf{P} _y} _{\\hat{\\mathsf{R}} _{x,t}}\\right)^2 + \\lambda\\left(\\left\\lVert \\mathsf{U} \\right\\rVert^2 + \\left\\lVert \\mathsf{P} \\right\\rVert^2\\right) $$\nThe value of $(c_{x,y})$ constitutes a penalisation function and will depend on whether we are considering explicit or implicit feedback. If we consider the known ratings as our training dataset $\\mathcal{T}$, then, in the case of explicit feedback we have\n$$ c_{x,y} = \\begin{cases} 0,\\qquad\\text{if}\\ \\left(x,y\\right) \\notin \\mathcal{T} \\\\ 1,\\qquad\\text{if}\\ \\left(x,y\\right) \\in \\mathcal{T} \\end{cases} $$\nConstraining our loss function to only include known ratings. The implicit feedback case is different (and a possible future topic) and for the remainder of this post we will only consider the explicit feedback case. Given the above, we can then simplify our loss function, in the explicit feedback case, to\n$$ \\ell = \\sum _{x,y \\in \\mathcal{T}} \\left(\\mathsf{R} _{x,y} -\\hat{\\mathsf{R}} _{x,y}\\right)^2 + \\lambda\\left(\\left\\lVert \\mathsf{U} \\right\\rVert^2 + \\left\\lVert \\mathsf{P} \\right\\rVert^2\\right) $$\nMinimizing $\\ell$ is however an NP-hard problem, due to its non-convexity. However, if we treat $\\mathsf{U}$ as constant, then $\\ell$ is a convex in relation to $\\mathsf{P}$ and if we treat $\\mathsf{P}$ as constant, $\\ell$ is convex in relation to $\\mathsf{U}$. We can then alternate between fixing $\\mathsf{U}$ and $\\mathsf{P}$, changing the values such that the loss function $\\ell$ (above) is minimized. This procedure is then repeated until we reach convergence.\nThe way that ALS works is, in simplified terms, to find the factors $\\mathsf{U}$ and $\\mathsf{P}$, which when multiplied together provide an approximation of our ratings matrix $\\mathsf{R}$, as we\u0026rsquo;ve seen previously.\nOnce we have the factors $\\mathsf{U}$ and $\\mathsf{P}$, we can then predict the missing values in $\\mathsf{R}$ by using the approximation $\\hat{\\mathsf{R}}$.\nIt is clear that in a real world scenario we would have many missing ratings, simply due to the assumption that no user rates all products (if they did, the case for a recommendation engine will be significantly weaker). ALS is designed to deal with sparse matrices and to fill the blanks using predicted values. After factorization, our approximated ratings matrix will look something like this:\nAs mentioned previously, the first step is then to minimise the loss function. In this case we take the partial derivatives and set them to zero and fortunately this has a closed form solution. We get a system of linear equations which we can easily implement. The system will correspond to the solution of\n$$ \\frac{\\partial \\ell}{\\partial \\mathsf{U}_x}=0, \\qquad \\frac{\\partial \\ell}{\\partial \\mathsf{P}_y}=0. $$\nWe start by solving the user latent factor minimisation using:\n$$ \\begin{aligned} \\frac{1}{2}\\frac{\\partial \\ell}{\\partial \\mathsf{U} _x}\u0026amp;=0 \\\\\n\\frac{1}{2}\\frac{\\partial}{\\partial \\mathsf{U} _ x} \\sum _ {x,y \\in \\mathcal{T}} \\left(\\mathsf{R} _ {x,y} - \\mathsf{U} _ x^T \\mathsf{P } _ y\\right)^2 + \\lambda\\left(\\left\\lVert \\mathsf{U} \\right\\rVert^2 + \\left\\lVert \\mathsf{P} \\right\\rVert^2\\right)\u0026amp;=0 \\\\\n-\\sum _{x,y \\in \\mathcal{T}} \\left(\\mathsf{R} _ {x,y} - \\mathsf{U} _x^T \\mathsf{P}_y\\right)\\mathsf{P} _y^T + \\lambda \\mathsf{U}\\ _ x^T\u0026amp;=0\\\\\n-\\left(\\mathsf{R} _ x -\\mathsf{U} _ x^T \\mathsf{P}^T\\right)\\mathsf{P} + \\lambda \\mathsf{U} _x^T\u0026amp;=0\\\\\n\\mathsf{U} _ x^T\\left(\\mathsf{P}^T \\mathsf{P} + \\lambda \\boldsymbol{\\mathsf{I}}\\right) \u0026amp;= \\mathsf{R} _ x \\mathsf{P} \\\\\n\\mathsf{U} _ x^T \u0026amp;= \\mathsf{R} _ x \\mathsf{P} \\left(\\mathsf{P}^T \\mathsf{P} + \\lambda \\boldsymbol{\\mathsf{I}}\\right)^{-1}. \\end{aligned} $$\nSimilarly, we can solve for the product latent factor by using:\n$$ \\begin{aligned} \\frac{1}{2}\\frac{\\partial \\ell}{\\partial \\mathsf{P} _y}\u0026amp;=0 \\\\ -\\sum _{x,y \\in \\mathcal{T}} \\left(\\mathsf{R} _{x,y} - \\mathsf{P} _y^T \\mathsf{U} _x\\right)\\mathsf{U}_x^T + \\lambda \\mathsf{P} _y^T\u0026amp;=0\\\\ -\\left(\\mathsf{R}_y - \\mathsf{P} _y^T \\mathsf{U}^T\\right)\\mathsf{U} + \\lambda \\mathsf{P} _y^T\u0026amp;=0\\\\ \\mathsf{P} _y^T\\left(\\mathsf{U}^T \\mathsf{U} + \\lambda \\boldsymbol{\\mathsf{I}}\\right) \u0026amp;= \\mathsf{R} _y \\mathsf{U} \\\\ \\mathsf{P} _y^T \u0026amp;= \\mathsf{R} _y \\mathsf{U} \\left(\\mathsf{U}^T \\mathsf{U} + \\lambda \\boldsymbol{\\mathsf{I}}\\right)^{-1}. \\end{aligned} $$\nWe can then calculate each factor iteratively, by fixing the other one and solving the estimator. While this process is alternated, an error measure (usually the Root Mean Squared Error), or $RMSE$ is calculated (as below) between the rating matrix approximation given by the latent factors and the ratings which we have, $\\mathcal{T}$. This method is guaranteed to converge and when we consider out approximation to be good enough, or after a set number of iterations we can then stop the refinement.\n$$ RMSE = \\sqrt{\\frac{1}{n}\\sum _{x,y \\in \\mathcal{T}}\\lvert \\hat{\\mathsf{R}} _{x,y} - \\mathsf{R} _{x,y}\\rvert} $$\nAfter the latent factors are estimated, we can then use them to try to recreate the original ratings matrix with the approximation as we\u0026rsquo;ve seen. The missing ratings in the original matrix will now be filled by values which minimize the least squares recursion and these are taken as the ratings \u0026ldquo;predictions\u0026rdquo;.\nTo illustrate the working of ALS, let\u0026rsquo;s assume we have a very quirky shop that only ever sells 300 products and has exactly 300 customers. On top of that, users are allowed to use 8 bit number to rate the products. We will also assume in this unusual shop that every user has rated every product.\nNow we\u0026rsquo;re humans, and we visualise patterns in colour more easily than in numbers. We will assign a palette to the ratings, so that each rating corresponds to a colour.\nI think you know where this is going \u0026hellip; we make up this final ratings matrix so now we can visualise the ALS progress.\nSo how do we perform this factorisation? The initial step is to fill the latent factors $\\mathsf{U}$ and $\\mathsf{P}$) with random values. Since at this point, we assume we don\u0026rsquo;t have any ratings, having random factors will lead to an initial random guess of the ratings matrix.\nWe then proceed to calculate each factor matrix, as we\u0026rsquo;ve seen, by calculating one using the estimator while keeping the other one constant and then alternating. We can see by the movie below that at each iteration the approximation to the original ratings gets better, stabilising after a few steps.\nThis is to be expected, in this case, since this would be the simplest implementation of ALS: a batch ALS on a single machine where we know all the ratings.\nThere should have been a video here but your browser does not seem to support it. So a fair question that arises is: why can\u0026rsquo;t we update this model and perform recommendations in a streaming fashion using this method?\nAfter all, if users add product ratings, we can simply update the predictions by recalculating the factors!\nThe problem is that when a new rating is added, or when new users and new products are added, we need to recalculate the entirety of the $\\mathsf{U}$ and $\\mathsf{P}$ matrices, and to do so, we need to have access to all of the data, $\\mathsf{R}$.\nStreaming ALS Ideally, we want a method that would allow us to update $\\mathsf{U}$ and $\\mathsf{P}$ using one observation, $\\mathsf{R}_{x,y}$ at a time\nIt turns out that the Stochastic Gradient Descent (or SGD)1 method allows us to do precisely that. We\u0026rsquo;ll look at the specific variant of SGD we\u0026rsquo;ve used which is called Bias-Stochastic Gradient Descent (B-SGD).\nIt is important to keep in mind, under a certain point of view, both methods aim at the same thing.\nThey both try to factorise the ratings matrix as latent factors, which would then be used to perform predictions. The main differences are of course, how the data is used (batch or one observation at the time) and how the factorisation is calculated.\nIn the SGD case we use the concept of biases in both users and items. The bias is a measure of how consistently a product is rated by different users. The bias of rating $(x,y)$, that is the rating given by user $x$ to product $y$, can be calculated as the sum of $\\mu$, an overall average rating and the observed deviations of user $x$, which we call $b_x$, and the observed deviations of product $y$, called $b_y$, that is:\n$$ b_{x,y} = \\mu + b_x + b_y $$\nThis bias information is now incorporated in the rating prediction. We can see that the SGD prediction is simply the batch prediction plus the corresponding bias term\n$$ \\hat{\\mathsf{R}} _{x,y} = b _{x,y} + \\underbrace{\\mathsf{U}^T _x \\times \\mathsf{P} _y} _{batch} $$\nIf we take the loss function definition for the batch method (and still considering the explicit feedback case), we can then replace the predicted rating formulation with our new one. We have, as before, some regularisation terms, but now also include a new regularisation term for the bias components,\nbut we don\u0026rsquo;t need to go into that.\n$$ \\ell _{SGD} = \\sum _{x,y \\in \\mathcal{T}} \\left(\\mathsf{R} _{x,y} - b _{x,y} - \\hat{\\mathsf{R}} _{x,y}\\right)^2 + \\lambda\\left(\\left\\lVert \\mathsf{U} \\right\\rVert^2 + \\left\\lVert \\mathsf{P} \\right\\rVert^2 + b _x^2 + b _y^2\\right) $$\nSince calculating the full gradient is computationally very expensive, we calculate it for a single observation. As we can see, the SGD method allows us to update the user and product specific bias as well as a single user and product latent factor row given a single rating.\nProvided we have a single rating, the rating of user $x$ for product $y$, we can update the biases as well as the latent vectors for user $x$ and for product $y$, that is, we no longer need to update the entire matrices $\\mathsf{U}$ and $\\mathsf{P}$, while still maintaining a convergence property.\nProvided with a learning rate $\\gamma$ and defining our prediction error as\n$$ \\epsilon _ {x,y}=\\mathsf{R} _{x,y}-\\hat{\\mathsf{R}} _{x,y}, $$\nthe biases and latent factors can now be updated in the opposite direction of the calculated gradient, proportionally to the learning rate, such that\n$$ \\begin{aligned} b _x \u0026amp;\\leftarrow b _x + \\gamma \\left(\\epsilon _{x,y}-\\lambda _x b _x\\right) \\\\ b _y \u0026amp;\\leftarrow b _y + \\gamma \\left(\\epsilon _{x,y}-\\lambda _y b _y\\right) \\\\ \\mathsf{U} _x \u0026amp;\\leftarrow \\mathsf{U} _x + \\gamma \\left(\\epsilon _{x,y}\\mathsf{P} _y - \\lambda^\\prime _x \\mathsf{U} _x\\right) \\\\ \\mathsf{P} _y \u0026amp;\\leftarrow \\mathsf{P} _y + \\gamma \\left(\\epsilon _{x,y}\\mathsf{U} _x - \\lambda^\\prime _y \\mathsf{P} _y\\right) \\end{aligned} $$\nSo the practical difference, in terms of streaming data is evident now. Given that, in both methods, the objective is to estimate the latent factors, given the ratings: with batch ALS, whenever we get a new rating, we need to fully recalculate the factors iteratively until we reach convergence. Conversely, with an SGD based factorisation, whenever we have a new rating, we can simply estimate the relevant row and column in the latent factors, by calculating the gradients and adjusting its values.\nNext we show the previous manufactured ratings matrix being factorised using B-SGD. We now simply recalculate the biases and a single latent factor vector, one observation at the time. We can see that, as expected, the convergence is slower (we are using a single observation at each step) but in the end, it produces a similar result.\nThere should have been a video here but your browser does not seem to support it. Now, this works fine for a single machine implementing streaming ALS. But we are interested in scaling this to something larger than this example so we will use a distributed implementation of ALS. And this is were it can start to get tricky. As it is the case with distributed algorithms, there are some pitfalls which we need to avoid in order to have a performant implementation. We will look at a few of these by looking at the Apache Spark\u0026rsquo;s and its default ALS implementation.\nApache Spark As probably most of you are familiar with, Spark is an Apache community project which aims at providing a modern platform for distributed computations. Spark provides several core data structures, such as RDDs (Resilient Distributed Datasets), Dataframes and Datasets.\nThe RDD is an immutable, distributed typed collection of objects. The RDD is partitioned across the cluster. This allows the spark operations, such as function mapping, to be applied to each subset of the RDD in parallel at each partition.\nFor the streaming ALS application we will use RDDs to implement the algorithm. Spark\u0026rsquo;s MLlib provides a collaborative filtering implementation based on the distributed batch ALS which we\u0026rsquo;ve covered previously. The API is quite simple and to train a model we need:\nval model = ALS.train(ratings, rank, iterations, lambda) case class Rating(int user, int product, double rating) val ratings: RDD[Rating] val rank: int val iterations: int val lambda: Double An RDD containing the ratings. The RDD has elements of the class Rating, which is basically a wrapper around a tuple of user id, product id and rating. This RDD corresponds to all the entries in our ratings matrix used previously. The rank which corresponds to the number of elements in our latent factor vectors (this would be the number of columns or rows in our $\\mathsf{U}$ and $\\mathsf{P}$ matrices). A stopping criteria in terms of iterations for the ALS. And finally, we set the lambda parameter, a regularisation parameter, which we\u0026rsquo;ve shown to be a part of the loss function\u0026rsquo;s regularisation. Since we have the data, the question is then how to choose the parameters. A typical method is to split the original ratings data into two random sets, one for training and one for validation. We then proceed to train the model to several different parameters, usually according to a grid search, and calculate some error measure between the predicted and validation ratings, choosing the parameters which minimise the error.\nOnce the model is trained, we get a MatrixFactorizationModel instance, which is basically a wrapper for the latent factors as RDDs.\nval model = ALS.train(ratings, rank, iterations, lambda) model: MatrixFactorizationModel class MatrixFactorizationModel { val userFeatures: RDD[(Int, Array[Double])] val productFeatures: RDD[(Int, Array[Double])] } One we have the trained model, we can now perform predictions.\nStreaming data We now want to build a streaming recommender system. For this scenario, we will assume that the observations take the form of a Spark\u0026rsquo;s Discretised Stream or DStream. With DStreams we consume the stream as mini-batches of RDDs over a certain interval window.\nWe can for instance, use the first mini-batch to initialise the model and the following batches to continuously train the model.\nOne immediate advantage of using observations as a stream is that we no longer need to keep the entirety of the data in memory or read it from storage. If we consider the batch implementation with a very large dataset if we had a single new observation and wanted to retrain the model, we would have to, for instance, read several million ratings from a database. With a streaming variant we can use that single observation to update the latent factors.\nWe will try to recreate Spark\u0026rsquo;s batch ALS API by allowing model training using a ratings RDD, however this time, we consume each RDD from the stream mini-batch and will incrementally train the model as observations trickle in.\nFirst, we start by establishing the quantities and data structures needed to implement streaming ALS.\nWe\u0026rsquo;ve seen in the previous slides that the recursions for the gradient calculation take the following form, here presented in pseudo-code:\nuserBias += gamma * (error - lambda * userBias) userFeature(i) += gamma * (error prodFeature(j) - lambda * userFeature(i)) We create a Factor class to encapsulate the features and the corresponding bias.\nWe recall that in the Spark ALS implementation, the features were stored in RDDs typed as a tuple of (id, Array), where now we have an equivalent form of (id, Factor) which allows us to capture the bias. I\u0026rsquo;ll now provide a quick overview of the steps required to go from the initial ratings stream to the trained model, in terms of Spark\u0026rsquo;s RDD operations.\nSimilarly to Spark\u0026rsquo;s ALS, we can assume that the model data will be in the form of Rating\u0026rsquo;s RDDs. These, as we\u0026rsquo;ve seen, will correspond to a mini-batch of ratings from our data stream. We first need to create the initial user and product latent factors for the observed data and we would start by creating two separate RDDs from the data, one keyed by user id, the other by product id.\nFor each entry of those new ~RDDs~ (the user and product indexed ones) we will now generate a random vector of features. This can be done by simply filling a vector of size ~rank~ with random uniform values, but as you will recall, we now also have a bias associated with each entry, which will initially also be set to a random value.\nWe now join the incoming ratings, with the generated user factors (using the user id as the key) getting a resulting ~RDD~ consisting of product ids, user ids, ratings and user factors (and the same thing for products and product factors).\nFinally, we have these two joint ~RDDs~ which have all the necessary quantities needed to calculate the partial gradient in each element. Recalling how to calculate a predicted rating in streaming ALS, we need the global bias, the user and product bias and the corresponding user and product latent vectors.\nThis is straightforward to calculate for each element as we can see from the pseudo-code.\nHere the ~dot~ function is simply a function to calculate the dot product treating the two factor arrays as vectors.\n$$ \\hat{\\mathsf{R}}_{x,y}=\\mu + b_x + b_y + \\mathsf{U}_x^T \\times \\mathsf{P}_y $$\nprediction = dot(userFactors.features, itemFactors.features) + userFactors.bias + itemFactors.bias + bias Given the prediction, the error is also straightforward to calculate, since the real rating is also included in this RDD.\n$$ \\epsilon _{x,y} = \\mathsf{R} _{x,y} - \\hat{\\mathsf{R}} _{x,y} $$\neps = rating - prediction And now, since we have the error, we can also easily calculate the update term for the user and product features. As mentioned previously, ~gamma~ and ~lambda~ are known model parameters which we pick ourselves when instantiating the streaming ALS algorithm.\n$$ \\gamma \\left(\\epsilon_{x,y}\\mathsf{U}_x-\\lambda^{\\prime}\\mathsf{P}_y\\right) $$\n(0 until rank).map { i =\u0026gt; gamma * (eps * userFactors.features(i) - lambda * itemFactors.features(i)) } Finally, we update the user and product biases given the model parameters and the previous bias.\n$$ \\gamma \\left(\\epsilon_{x,y}-\\lambda_y b_y\\right) $$\ngamma * (eps - lambda * itemFactors.bias) These calculated gradients can now be mapped to a new ~RDD~ which we will use to update the final biases and latent factors.\nThe last step is to split the gradients according to user and product, and finally, when in possession of all the individual gradients, we reduce them into latent factors by performing an aggregated sum for each user and product.\nSo these steps define the entirety of the streaming ALS operation. For each observation window, we calculate the latent factors ~RDD~, and on the following window we update these factors, given the current observations.\nWe\u0026rsquo;ve covered the initialisation case, that is, we assumed the case where the model is not initialised and we received the first mini-batch of ratings. If, on the following window, we receive ratings for previously unseen user or products, the procedure is exactly the same, that is, we generate random factors and update as described.\nNow I\u0026rsquo;ll just quickly cover the case where we get some ratings from users or for some product we\u0026rsquo;ve already seen. For this new set of observations, we proceed exactly as previously, that is, we split the data into separate RDDs, each one containing the ratings but keyed by user and product id. I\u0026rsquo;ll assume that we get a mixture of completely new data, that is, unseen user and products and some ratings for previously seen users and products shown in red.\nThe difference now is that, instead of assigning random factors and biases to each entry of these ~RDDs~, we perform a full outer join between them and the current latent factors.\nThe strategy is then to keep the matching existing latent factor and create random features and biases just for the user and product entries we haven\u0026rsquo;t seen before.\nNow that we are in possession of this joint ~RDD~, we can apply exactly the same steps as previously to update the factors and repeat these steps for all future incoming observations, allowing us to continuously update the model.\nIt is now easy to see that, in the limit situation where we only have one new rating, we would now only have to update a single entry of the latent factors ~RDD~, in contrast with the batch method, where the entirety of the factors would be used in the ALS step.\nLet\u0026rsquo;s look at some results comparing the streaming implementation with the Spark\u0026rsquo;s batch implementation.\nThe dataset we have chosen to use in these tests is one of the MovieLens\u0026rsquo; datasets. These dataset are a widely used data in recommendation engine research. They are managed by the Lens corporation and are freely available for non-commercial applications.\nThese datasets come in several variants, namely a small variant, useful for a quick algorithm prototyping and testing, and a full variant, with approximately 26 million ratings (from 45,000 movies and 270,000 users), useful for a more comprehensive testing and benchmark.\nThe data is available as a set of Comma Separated Value files, each containing different variables, but we are mainly interested in the ~ratings~ file which contains four variables: a unique user and movie id, represented as integers, a rating represented by a value from 0 to 5 with steps of 0.5 and a timestamp for when the movie was rated by this user.\nFirst, we\u0026rsquo;ll start by training a batch ALS model using the MovieLens data. We assume that we already have the observations as an ~RDD~ of ratings and simply split the data into 80% for training and 20% for validation.\nval split: Array[RDD[Rating]] = ratings.randomSplit(0.8, 0.2) val model = ALS.train(split(0), rank, iter, lambda) Here, we won\u0026rsquo;t show the steps to determine the best parameters for this dataset, were we performed a simple parameter grid search over a number of possible candidates. The Spark ALS API is quite simple and to train the model we simply pass the training ~RDD~ and the parameters.\nWe can now use the remaining 20% of the observations to calculate the RMSE between the model predictions and the actual ratings.\nWe now can persist the validation ~RDD~, so we can use the exact same one for the streaming ALS run.\nval predictions: RDD[Rating] = model .predict(split(1).map { x =\u0026gt;(x.user, x.product)) } val pairs = predictions .map(x =\u0026gt; ((x.user, x.product), x.rating)) .join( split(1) .map(x =\u0026gt; ((x.user, x.product), x.rating)) .values val RMSE = math.sqrt( pairs.map(x =\u0026gt; math.pow(x._1 - x._2, 2)).mean()) In order to test the streaming version, we first need to define a data source. We start with the original MovieLens data and remove all the ratings from the validation observations.\nWe then create a simulated stream of observations using Kafka, with an interval of 5 seconds and with 1000 observations in each mini-batch. These are arbitrary numbers, chosen just for practical reasons. We could have, for instance, a single observation in each mini-batch.\nIt is not guaranteed that the best parameters (namely ~rank~ and ~lambda~) chosen for the batch version are the best for the streaming implementation, however we\u0026rsquo;ve decided to use the same ones. For each mini-batch we then incrementally train the model and calculate the RMSE up to that point. Given the actual ratings in the validation set and the model\u0026rsquo;s prediction, the RMSE calculation is the same as in the batch version. And looking at the results, we can see that with each mini-batch (of 1000 observations), the RMSE from the streaming version (in blue) is edging towards the batch value (plotted as the horizontal dashed line).\nCaveats However, streaming ALS has pitfalls which we have to take into account.\nCold start An issue, which is shared with batch ALS, is usually called the cold start problem. This refers to initial point in a recommender engine where we have too few observations to make meaningful predictions. As we now know, when having a small number of ratings, since our latent factors are initialised to random numbers, most of our predicted ratings will also be random.\nAlthough this is not an exclusive problem to the streaming implementation, we might be tempted, since the system is suited for realtime recommendations, to immediately start serving predictions. It might be wise to exercise caution and train the model offline with a larger dataset or at least perform some model diagnostics to check how sensible our predictions are.\nHyperparameter estimation Another challenge we encounter is hyperparameter estimation.\nIn the batch ALS case, we can perform a grid search for instance and estimate the hyperparameters. If, after some time, we find ourselves with a new ratings or even new product and users, we can simply repeat this procedure using the totality of the data. As an example, if in batch ALS at any point we wish to estimate the model with a different rank, this would be perfectly acceptable.\nIn the streaming case, we can\u0026rsquo;t do that. When we have a new batch of observations, we assume that previous ones were discarded since they are already incorporated in the latent factors. If they weren\u0026rsquo;t and we keep all the observations in the stream, we might as well use batch ALS.\nA solution is to perform a grid search in parallel from the start and prune the least performant models as time progresses. This has the disadvantage of being expensive in terms of resources, since we have to keep several models simultaneously and again, we have the cold start problem surfacing.\nThis means that we have no guarantee that the best parameters for a initial batch with few observations will still be the best further on.\nPerformance Also, there are some performance considerations. As we\u0026rsquo;ve seen, we implement some operations which can be costly in a Spark setting. We have several join operations which can lead to a considerable amount of data shuffling between partitions.\nCare must be taken into choosing an appropriate partitioning strategy to minimise data shuffling.\nSpark\u0026rsquo;s implementation of batch ALS uses a specific method called blocked ALS, which computes outgoing and ingoing links between user and products vectors and then partitioning them in blocks in order to minimise data transfer between nodes.\nAlso, to make predictions we might have to try and perform random access to the latent factors ~RDDs~. This also can be quite inefficient since we are using ~lookup~ methods.\nIf you want to get straight setting up your own distributed recommendation engine, I highly suggest you start with Spark\u0026rsquo;s builtin solution. I would highly recommended looking at the ~jiminy~ project (part of the radanalytics.io community), a micro-service oriented complete recommendation engine, ready to deploy on OpenShift.\nThe engine is split into services such as a predictor and a modeler, along with a front-end and tools to simplify tasks (such as using the MovieLens data) and it\u0026rsquo;s a great way to look at how to put a modern recommender engine together and also a great code read.\nVinagre, J., Jorge, A. M., \u0026amp; Gama, J. (2014). Fast incremental matrix factorization for recommendation with positive-only feedback. In , International Conference on User Modeling, Adaptation, and Personalization (pp. 459470).\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"/a-streaming-als-implementation.html","tags":null,"title":"A streaming ALS implementation"},{"categories":null,"contents":"Environments Create To create a new environment foo use\n$ conda create --name foo And to activate it use\n$ conda activate foo ","permalink":"/anaconda.html","tags":null,"title":"Anaconda"},{"categories":null,"contents":"Ansible notes.\nInstallation Debian/Ubuntu $ sudo apt update $ sudo apt install software-properties-common $ sudo add-apt-repository --yes --update ppa:ansible/ansible $ sudo apt install ansible Reference Installing packages To install a remote deb package with Ansible we can use, for instance\n- name: Install Wezterm (Ubuntu) apt: deb: https://github.com/wez/wezterm/releases/download/20211205-192649-672c1cc1/wezterm-20211205-192649-672c1cc1.Ubuntu20.04.deb ","permalink":"/ansible.html","tags":null,"title":"Ansible"},{"categories":null,"contents":"A common introductory problem in Bayesian changepoint detection is the record of UK coal mining disasters from 1851 to 1962. More information can be found in Carlin, Gelfand and Smith (1992).\nAs we can see from the plot below, the number of yearly disasters ranges from 0 to 6 and we will assume that at some point within this time range a change in the accident rate has occured.\nThe number of yearly disasters can be modelled as a Poisson with a unknown rate depending on the changepoint $k$:\n$$ y_t \\sim \\text{Po}\\left(\\rho\\right),\\qquad \\rho = \\begin{cases} \\mu, \u0026amp; \\text{if}\\ t=1,2,\\dots,k \\\\ \\lambda, \u0026amp; \\text{if}\\ t = k +1, k + 2, \\dots,m \\end{cases} $$\nOur objective is to estimate in which year the change occurs (the changepoint $k$) and the accident rate before ($\\mu$) and after ($\\lambda$) the changepoint amounting to the parameter set $\\Phi = \\left\\lbrace\\mu,\\lambda,k\\right\\rbrace$.\nWe will use Crystal (with crystal-gsl) to perform the estimation.\nWe start by placing independent priors on the parameters:\n$k \\sim \\mathcal{U}\\left(0, m\\right)$ $\\mu \\sim \\mathcal{G}\\left(a_1, b_1\\right)$ $\\lambda \\sim \\mathcal{G}\\left(a_2, b_2\\right)$ For the remainder we\u0026rsquo;ll set $a_1=a_2=0.5$, $c_1=c_2=0$ and $d_1=d_2=1$.\nThe joint posterior of $\\Phi$ is then:\n$$ \\pi\\left(\\Phi|Y\\right) \\propto p\\left(Y|\\Phi\\right) \\pi\\left(k\\right) \\pi\\left(\\mu\\right) \\pi\\left(\\lambda\\right), $$\nwhere the likelihood is\n$$ \\begin{aligned} p\\left(Y|\\Phi\\right) \u0026amp;= \\prod_{i=1}^{k} p\\left(y_i|\\mu,k\\right) \\prod_{i=k+1}^{m} p\\left(y_i|\\lambda,k\\right) \\\\ \u0026amp;= \\prod_{i=1}^{k} \\frac{\\mu^{y_i}e^{-\\mu}}{y_i!} \\prod_{i=k+1}^{m} \\frac{\\lambda^{y_i}e^{-\\lambda}}{y_i!}. \\end{aligned} $$\nAs such, the full joint posterior can be written as:\n$$ \\begin{aligned} \\pi\\left(\\Phi|Y\\right) \u0026amp;\\propto \\prod_{i=1}^{k} \\frac{\\mu^{y_i}e^{-\\mu}}{y_i!} \\prod_{i=k+1}^{m} \\frac{\\lambda^{y_i}e^{-\\lambda}}{y_i!} \\left(\\mu^{a_1-1} e^{-\\mu b_1}\\right) \\left(\\lambda^{a_2-1} e^{-\\lambda b_2}\\right) \\frac{1}{m} \\\\ \u0026amp;= \\mu^{a_1 + \\sum_{1}^{k}y_i - 1}e^{-\\mu\\left(k+b_1\\right)} \\lambda^{a_2 + \\sum_{k+1}^{m}y_i - 1}e^{-\\lambda\\left(m-k+b_2\\right)} \\end{aligned}. $$\nIt follows that the full conditionals are, for $\\mu$:\n$$ \\begin{aligned} \\pi\\left(\\mu|\\lambda,k,Y\\right) \u0026amp;\\propto \\mu^{a_1 + \\sum_{i=1}^{k}y_i-1}e^{-\\mu\\left(k+b_1\\right)} \\\\ \u0026amp;= \\mathcal{G}\\left(a_1+\\sum_{i=1}^{k}y_i, k + b_1\\right) \\end{aligned} $$\nWe can define the $\\mu$ update as:\ndef mu_update(data : Array(Int), k : Int, b1 : Float64) : Float64 Gamma.sample(0.5 + data[0..k].sum, k + b1) end The full conditional for $\\lambda$ is:\n$$ \\begin{aligned} \\pi\\left(\\lambda|\\mu,k,Y\\right) \u0026amp;\\propto \\lambda^{a_2 + \\sum_{i=k+1}^{m}y_i-1}e^{-\\lambda\\left(m-k+b_2\\right)} \\\\ \u0026amp;= \\mathcal{G}\\left(a_2+\\sum_{i=k+1}^{m}y_i, m - k + b_2\\right), \\end{aligned} $$\nwhich we implement as:\ndef lambda_update(data : Array(Int), k : Int, b2 : Float64) : Float64 Gamma.sample(0.5 + data[(k+1)..M].sum, M - k + b2) end The next step is to take\n$$ \\begin{aligned} b_1 \u0026amp;\\sim \\mathcal{G}\\left(a_1 + c_1,\\mu + d_1\\right) \\\\ b_2 \u0026amp;\\sim \\mathcal{G}\\left(a_2 + c_2,\\lambda + d_2\\right), \\end{aligned} $$\nwhich we will implement as:\ndef b1_update(mu : Float64) : Float64 Gamma.sample(0.5, mu + 1.0) end def b2_update(lambda : Float64) : Float64 Gamma.sample(0.5, lambda + 1.0) end And finally we choose the next year, $k$, according to\n$$ p\\left(k|Y,\\Phi\\right)=\\frac{L\\left(Y|\\Phi\\right)}{\\sum_{k^{\\prime}} L\\left(Y|\\Phi^{\\prime}\\right)} $$\nwhere\n$$ L\\left(Y|\\Phi\\right) = e^{\\left(\\lambda-\\mu\\right)k}\\left(\\frac{\\mu}{\\lambda}\\right)^{\\sum_i^k y_i} $$\nimplemented as\ndef l(data : Array(Int), k : Int, lambda : Float64, mu : Float64) : Float64 Math::E**((lambda - mu)*k) * (mu / lambda)**(data[0..k].sum) end So, let\u0026rsquo;s start by writing our initials conditions:\niterations = 100000 b1 = 1.0 b2 = 1.0 M = data.size # number of data points # parameter storage mus = Array(Float64).new(iterations, 0.0) lambdas = Array(Float64).new(iterations, 0.0) ks = Array(Int32).new(iterations, 0) We can then cast the priors:\nmus[0] = Gamma.sample(0.5, b1) lambdas[0] = Gamma.sample(0.5, b2) ks[0] = Random.new.rand(M) And define the main body of our Gibbs sampler:\n(1...iterations).map { |i| k = ks[i-1] mus[i] = mu_update(data, k, b1) lambdas[i] = lambda_update(data, k, b2) b1 = b1_update(mus[i]) b2 = b2_update(lambdas[i]) ks[i] = Multinomial.sample((0...M).map { |kk| l(data, kk, lambdas[i], mus[i]) }) } Looking at the results, we see that the mean value of $k$ is 38.761, which seems\nto indicate that the change in accident rates occurred somewhere near $1850+38.761\\approx 1889$.\nWe can visually check this by looking at the graph below. Also plotted are the density for the accident rates before ($\\mu$) and after ($\\lambda$) the change.\nOf course, one the main advantages of implementing the solution in Crystal is not only the boilerplate-free code, but the execution speed.\nCompared to an equivalent implementation in ~R~ the Crystal code executed roughly 17 times faster.\nLanguage Time (s) R 58.678 Crystal 3.587 ","permalink":"/bayesian-estimation-of-changepoints.html","tags":null,"title":"Bayesian estimation of changepoints"},{"categories":null,"contents":"ArXivist A bot which periodically toots a paper published on ArXiV. The main Mastodon page of the bot can be found at https://botsin.space/@arxivstats. The current queue can be found at https://w6118k.deta.dev/\n","permalink":"/bots.html","tags":null,"title":"Bots"},{"categories":null,"contents":"The major guidelines of the Brutalist web design 1 are:\nContent is readable on all reasonable screens and devices  This guideline is followed by this site. The vast majority of the pages work with all major browsers, implement a responsive design. They also work with Javascript disabled as described in site details. It even works with unreasonable browsers, screens and devices, such as Internet Explorer 6 (2001) and NCA Mosaic 2 (1993).\nOnly hyperlinks and buttons respond to clicks.  This guideline is followed by this site. Buttons and hyperlinks (textual and TOC links) are the only way to navigate content (except for standard browser navigation, i.e. back button)\nHyperlinks are underlined and buttons look like buttons.  Not really followed. Hyperlinks are highlighted, rather than underlined.\n This guideline is now mostly followed by this site. Hyperlinks look like hyperlink and buttons look like buttons, though.\nThe back button works as expected.  This guideline is followed by this site. This site does not implement routers, history hijacking, etc. Hyperlinks and browser navigation are the only way to get around.\nView content by scrolling.  Vertical scrolling, specifically.\nDecoration when needed and no unrelated content.  This guideline is followed by this site. Decoration is judicious and mostly through colour. No splash images too.\nPerformance is a feature.  Performance is an important concern.\nAn area of work is image optimisation, since this the heaviest part of the site Javascript is minimal and the goal is to decrease dependence on it, not increase. https://brutalist-web.design/\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"/brutalist-web-design.html","tags":null,"title":"Brutalist web design"},{"categories":null,"contents":"Notes on Clojure.\nReference Concatenating strings (require \u0026#39;[clojure.string :as string]) (string/join [\u0026#34;foo\u0026#34; \u0026#34;bar\u0026#34;]) List files recursively Clojure To list files recursively in Clojure1\n(file-seq \u0026#34;/etc\u0026#34;) Babashka If using Babashka, the following works:\n(file-seq (clojure.java.io/file \u0026#34;/etc\u0026#34;)) Filter by extension (filter #(.endsWith (.toString %) \u0026#34;.conf\u0026#34;) (file-seq \u0026#34;/etc\u0026#34;)) Get home directory (def home (System/getProperty \u0026#34;user.home\u0026#34;)) Filter collection Use the syntax (filter predicate collection). For instance, to filter a collection of strings that end with bar, do:\n(def strings [\u0026#34;Foobar\u0026#34; \u0026#34;Barfoo\u0026#34; \u0026#34;Foobaz\u0026#34; \u0026#34;Barbar\u0026#34;]) (filter (fn [s] (clojure.string/ends-with? s \u0026#34;bar\u0026#34;)) strings) Compare with the Java version.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"/clojure.html","tags":null,"title":"Clojure"},{"categories":null,"contents":"To prototype and test almost any application some type of input data is needed. Getting the right data can be difficult for several reasons, including strict licenses, a considerable amount of data engineering to shape the data to our requirements and the setup of dedicated data producers. Additionally, in modern applications, we are often interested in realtime/streaming and distributed processing of data with platforms such as Apache Kafka and Apache Spark and deployment in a cloud environment like OpenShift1 with tools such as oshinko.\nSimulating data is not trivial, since we might want to capture complex characteristic to evaluate our algorithms in conditions similar to the real world. In this post I\u0026rsquo;ll introduce a tool, timeseries-mock, which allows for a simple, containerised deployment of a data simulator along with some of the theory behind the data generation.\nState-space models A common way of modelling these patterns is to use state-space models (SSM). SSMs can be divided into a model and a observation structure.\n$$ Y_t|\\theta_t,\\Phi \\sim f\\left(y_t|\\theta_t,\\Phi_t\\right) \\ \\theta_t|\\theta_{t-1},\\Phi_t \\sim g\\left(\\theta_t|\\theta_{t-1},\\Phi_t\\right). $$\nIt is clear from the above that the state possesses a Markovian nature. The state at time $t$, $\\theta_t$ will on depend on the previous value, $\\theta_{t-1}$ and an observation at time $t$, $y_t$ will only depend on the current state, $\\theta_t$, that is:\n$$ p\\left(\\theta_{t}|\\theta_{0:t-1},y_{0:t-1}\\right)=p\\left(\\theta_{t}|\\theta_{t-1}\\right) \\ p\\left(\\theta_{t-1}|\\theta_{t:T},y_{t:T}\\right)=p\\left(\\theta_{t-1}|\\theta_{t}\\right) \\ p\\left(y_{t}|\\theta_{0:t},y_{0:t-1}\\right)=p\\left(y_{t}|\\theta_{t}\\right). $$\nIn this post we will focus on a specific instance of SSMs, namely Dynamic Generalised Linear Models (DGLMs). If you want a deeper theoretical analysis of DGLMs I strongly recommend Mike West and Jeff Harrison\u0026rsquo;s \u0026ldquo;Bayesian Forecasting and Dynamic Models\u0026rdquo; (1997). In DGLMs, the observation follows a distribution from the exponential family, $E\\left(\\dot\\right)$ such, that\n$$ Y_t|\\theta_t,\\Phi \\sim E\\left(\\eta_t,\\Phi\\right) \\ \\eta_t|\\theta_t = L\\left(\\mathsf{F}^T \\theta_t\\right) $$\nwhere $L\\left(\\dot\\right)$ is the linear predictor and the state evolves according to a multivariate normal (MVN) distribution:\n$$ \\theta_t \\sim \\mathcal{N}\\left(\\theta_t;\\mathsf{G}\\theta_{t-1},\\mathsf{W}\\right) $$\nStructure The fundamental way in which timeseries-mock works is by specifying the underlying structure and observational model in a YAML configuration file. In the following sections we will look at the options available in terms of structural and observational components and look at how to represent them. As we\u0026rsquo;ve seen from (5), the structure will allows us to define the underlying patterns of the state evolution $\\lbrace \\theta_1, \\theta_2, \\dots, \\theta_t\\rbrace$. One of the advantages of DGLMs is the ability to compose several simpler components into a single complex structure. We will then look at some of these \u0026ldquo;fundamental\u0026rdquo; components.\nMean An underlying mean component will represent a random walk scalar state which can be specified in the configuration file by\nstructure: - type: mean start: 0.0 noise: 1.5 In this case start will correspond the mean of the state prior, $m_0$, and noise will correspond to the prior\u0026rsquo;s variance, $\\tau^2$, that is\n$$ \\theta_0 \\sim \\mathcal{N}\\left(m_0, \\tau^2\\right). $$\nIn the figure below we can see the above configuration for, respectively, a higher and lower value of noise.\nSeasonality Seasonality is represented by Fourier components. A Fourier component can be completely specified by providing the period, start, noise and harmonics. The start and noise parameters are analogous to the mean components we saw previously. The period parameter refers to how long does it take for the cyclical pattern to repeat. This is done relatively to your time-point interval, such that\n$$ P = p_{\\text{fourier}}\\times p_{\\text{stream}}. $$\nThat is, if your stream\u0026rsquo;s rate is one observation every 100 milliseconds, $p_{\\text{stream}}=0.1$, and the harmonic\u0026rsquo;s period is 2000, $p_{\\text{fourier}}=1000$, then the seasonal component will repeat every $200$ seconds. The configuration example\nstructure: - type: season period: 200 harmonics: 5 start: 0.0 noise: 0.7 will create a sequence of state vectors $\\boldsymbol{\\theta}_{0:T}$ with five components, such that:\n$$ \\boldsymbol{\\theta}t = \\lbrace\\theta{1,t},\\dots,\\theta_{5,t}\\rbrace. $$\nIn this example, period refers to the number of time-points for each cycle\u0026rsquo;s repetition and harmonics to the number of Fourier harmonics used. \u0026ldquo;Simpler\u0026rdquo; cyclic patterns usually require less harmonics. In the figure below we show on the lowest and highest frequency harmonics, on the left and right respectively.\nAR-p An AR($p$) (Auto-Regressive) component can be specified using the directives:\nstructure: - type: arma start: 0.0 coefficients: 0.1,0.3,0.15 noise: 0.5 In the above example we would be creating an AR(3) component, with respective coefficients $\\phi=\\lbrace 0.1,0.3,0.15 \\rbrace$. These coefficients will take part of the state model as\n$$ \\mathsf{G} = \\begin{bmatrix} \\phi_1 \u0026amp; \\phi_2 \u0026amp; \\dots \u0026amp; \\phi_{p-1} \u0026amp; \\phi_p \\ 1 \u0026amp; 0 \u0026amp; \\dots \u0026amp; 0 \u0026amp; 0 \\ \\vdots \u0026amp; \u0026amp; \\ddots \u0026amp; \\vdots \u0026amp; \\vdots \\ 0 \u0026amp; \\dots \u0026amp; 0 \u0026amp; 1 \u0026amp; 0 \\end{bmatrix} $$\nIn the following plots we show respectively the first and second component of the AR(3) state vector.\nComposing Structural composition of DGLM structures amounts to the individual composition of the state covariance matrix and state/observational evolution matrices such that:\n$$ \\mathsf{F}^T = \\begin{bmatrix}\\mathsf{F}_1 \u0026amp; \\mathsf{F}_2 \u0026amp; \\dots \\mathsf{F}_i\\end{bmatrix}^T \\\\ \\mathsf{G} = \\text{blockdiag}\\left(\\mathsf{G}_1, \\mathsf{G}_2, \\dots, \\mathsf{G}_i\\right) \\\\ \\mathsf{W} = \\text{blockdiag}\\left(\\mathsf{W}_1, \\mathsf{W}_2, \\dots, \\mathsf{W}_i\\right) $$\nTo express the composition of structures in the YAML configuration, we simply enumerate the separate components under the structure key. As as example, to compose the previous mean and seasonal components, we would simply write:\nstructure: - type: mean start: 0.0 noise: 1.5 - type: season period: 200 harmonics: 5 start: 0.0 noise: 0.7 This would create a structure containing both an underlying mean and a seasonal component.\nObservations As we have seen from (3) that an observational model can be coupled with a structure to complete the DGLM specification. In the following sections we will look at some example observational models and in which situations they might be useful.\nContinuous Continuous observations are useful to model real valued data such as stock prices, temperature readings, etc. This can be achieved by specifying the observational component as a Gaussian distribution such that:\n$$ Y_t|\\Phi \\sim \\mathcal{N}\\left(y_t|\\eta_t, \\mathsf{W}\\right). $$\nobservations: - type: continuous noise: 1.5 The following plot shows the coupling of the structure used in the mean section with the continuous (example above) observational model.\nDiscrete Discrete observations, sometimes referred as \u0026ldquo;count data\u0026rdquo;, can be used to model integer quantities. This can be achieved by using a Poisson distribution in the observational model, such that:\n$$ Y_t|\\Phi \\sim \\text{Po}\\left(y_t|\\eta_t\\right) $$\nAn example configuration would be:\nobservations: - type: discrete In this case we will use the previous ARMA(3) structure example and couple it with a discrete observational model. The result is shown in the plot below:\nCategorical In the categorical case, we model the observations according to a binomial distribution, such that\n$$ Y_t \\sim \\text{Bin}\\left(y_t|\\eta_t,r\\right), $$\nwhere $r$ represents the number of categories. A typical example would be the case where $r=1$ which would represent a binary outcome (0 or 1). The following configuration implements this very case:\nobservations: - type: categorical categories: 1 We can see in the plot below (states on the left, observations on the right) a realisation of this stream, when using the previous seasonal example structure.\nOften, when simulating a data stream, we might be interested in the category labels themselves, rather than a numerical value. The generator allows to pass directly a list of labels and output the labelled observations. Let\u0026rsquo;s assume we wanted to generate a stream of random DNA nucleotides (C, T, A and G). The generator allows to pass the labels directly and output the direct mapping between observations and label, that is:\n$$ y_t: \\lbrace 0, 1, 2, 3 \\rbrace \\mapsto \\lbrace\\text{C, T, A, G}\\rbrace $$\nobservations: - type: categorical values: C,T,A,G Using the same seasonal structure and observation model as above, the output would then be:\nComposite model In a real-world scenario we are interested in simulating multivariate data and that comprises of different observational models. For instance, combining observation components from categorical, continuous, etc.\nThe approach taken for multivariate composite models, is that the structures are composed as seen previously into a single one and the resulting state vector is then \u0026ldquo;collapsed\u0026rdquo; into a vector on natural parameters, $\\eta_t$ which are then used to sample the individual observation components.\n$$ \\theta_t = \\lbrace\\underbrace{\\theta_{1}, \\theta_{2}, \\theta_{3}}{\\eta_1}, \\underbrace{\\theta{4}, \\theta_{5}, \\theta_{6}}_{\\eta_2}\\rbrace \\ y = f(\\eta_t) \\ = \\lbrace f_1(\\eta_1), f_2(\\eta_2)\\rbrace $$\nThe model composition can be expressed by grouping the different structures and observations under a compose key:\ncompose: - structure: # component 1 - type: mean start: 0.0 noise: 0.5 - observations: - type: continuous noise: 0.5 - structure: # component 2 - type: mean start: 5.0 noise: 3.7 - observations: - type: continuous noise: 1.5 Examples We will look at two separate examples, one that creates a stream of simulated stock prices and one that generates a fake HTTP log. We assume we want to simulate a stream of per-day stock prices for 3 different companies, each with different characteristics. In this case, we will model the following:\nCompany A\u0026rsquo;s stocks start at quite a high value ($700) and are quite stable throughout time Company B\u0026rsquo;s stocks start slightly lower than A ($500) and are quite stable in the long run, but show heavy fluctuation from day to day Company C\u0026rsquo;s stocks start at $600 are very unpredictable Since we will be using per-day data we won\u0026rsquo;t be streaming this in realtime! We can map each daily observation to a second in our stream so we will specify a period=1. All stocks will exhibit a small monthly effect (period=30), which will be indicated by a noise=0.01 and a yearly effect (period=365) with a noise=2.5.\nThe resulting configuration will be:\ncompose: - structure: # company A - type: mean start: 700 noise: 0.01 # low structural variance - type: season period: 30 # monthly seasonality noise: 0.1 - type: season period: 365. # yearly seasonality noise: 1.7 - observations: - type: continuous noise: 0.05 # low observational variance - structure: # company B - type: mean start: 500 noise: 0.01 # low structural variance - type: season period: 30 # monthly seasonality noise: 0.7 - type: season period: 365. # yearly seasonality noise: 3.7 - observations: - type: continuous noise: 3.00 # higher observational variance - structure: # company C - type: mean start: 600 noise: 3.0 # higher structural variance - type: season period: 30 # monthly seasonality noise: 0.1 - type: season period: 365. # yearly seasonality noise: 0.25 - observations: - type: continuous noise: 4.0 # higher observational variance A realisation of this stream looks like the figure below.\nTo generate the fake HTTP log we will make the following assumptions:\nWe will have a request type (GET, POST, PUT) which will vary following a random walk A set of visited pages which, for illustration purposes, will be limited to (/site/page.htm, /site/index.htm and /internal/example.htm). We also want that the URLs visited follow a seasonal pattern. An IP address in the IPv4 format (i.e. 0-255.0-255.0-255.0-255) It is clear that for all variables the appropriate observational model is the categorical one. For the request type and the visited page we can pass directly the category name in the configuration file and for the IP we simply need four categorical observations with $r=255$.\nIf the underlying structure is the same, a useful shortcut to specify several observation component is the replicate key. In this particular example to generate four 0-255 numbers with an underlying mean as the structure, we simple use:\n- replicate: 4 structure: - type: mean start: 0.0 noise: 2.1 observations: type: categorical categories: 255 The full configuration for the HTTP log simulation could then be something like this:\ncompose: - structure: - type: mean start: 0.0 noise: 0.01 observations: type: categorical values: GET,POST,PUT - structure: - type: mean start: 0.0 noise: 0.01 - type: season start: 1.0 period: 15 noise: 0.2 observations: type: categorical values: /site/page.htm,/site/index.htm,/internal/example.htm - replicate: 4 structure: - type: mean start: 0.0 noise: 2.1 observations: type: categorical categories: 255 [\u0026#34;PUT\u0026#34;, \u0026#34;/internal/example.htm\u0026#34;, 171, 158, 59, 89] [\u0026#34;GET\u0026#34;, \u0026#34;/internal/example.htm\u0026#34;, 171, 253, 71, 146] [\u0026#34;PUT\u0026#34;, \u0026#34;/internal/example.htm\u0026#34;, 224, 252, 9, 156] [\u0026#34;POST\u0026#34;, \u0026#34;/site/index.htm\u0026#34;, 143, 253, 6, 126] [\u0026#34;POST\u0026#34;, \u0026#34;/site/page.htm\u0026#34;, 238, 254, 2, 48] [\u0026#34;GET\u0026#34;, \u0026#34;/site/page.htm\u0026#34;, 228, 252, 52, 126] [\u0026#34;POST\u0026#34;, \u0026#34;/internal/example.htm\u0026#34;, 229, 234, 103, 233] [\u0026#34;GET\u0026#34;, \u0026#34;/internal/example.htm\u0026#34;, 185, 221, 109, 195] ... Setting up the generator As I have mentioned in the beginning of this post, we want to fit the data simulation solution into a cloud computing workflow. To illustrate this we will use the OpenShift platform which allows for the deployment of containerised applications. A typical setup for a streaming data processing application would be as illustrated in the figure below. We have several sources connected to a message broker, such as Apache Kafka in this case. Data might be partitioned into \u0026ldquo;topics\u0026rdquo; which are then consumed by different applications, each performing data processing, either independently or in a distributed manner.\nAn advantage of ~timeseries-mock~ would then be to replace the \u0026ldquo;real\u0026rdquo; data sources with a highly configurable simulator either for the prototyping or initial testing phase. If we consider our previous example of the \u0026ldquo;fake\u0026rdquo; HTTP log generation, an application for Web log analytics could be prototyped and tested with simulated log data very quickly, without being blocked by the lack of suitable real data. Since the data is consumed by proxy via the message broker\u0026rsquo;s topics, we could later on replace the simulator with real data sources seamlessly without an impact on any of the applications. To setup the generator (and assuming Kafka and your consumer application are already running on OpenShift) we only need to perform two steps:\nWrite the data specifications in a YAML configuration Use the s2i to deploy the simulator The s2i functionality of OpenShift allows to create deployment ready images by simply pointing to a source code location. In this case we could simply write:\n$ oc new-app centos/python-36-centos7~https://github.com/ruivieira/timeseries-mock \\ -e KAFKA_BROKERS=kafka:9092 \\ -e KAFKA_TOPIC=example \\ -e CONF=examples/mean_continuous.yml \\ --name=emitter In this case, we would deploy a simulator generating data according to the specifications in mean_continuous.yml. This data will be sent to the topic example of a Kafka broker running on port 9092.\nThe stream will be ready to consume and message payload will a stream of serialised JSON strings. In the case of the simulated HTTP log this would be:\n{ name: \u0026#34;HTTP log\u0026#34; values: [\u0026#34;GET\u0026#34;, \u0026#34;/internal/example.htm\u0026#34;, 185, 221, 109, 195] } name - the name given to this stream in the configuration file values - a single observation for this stream After consuming the data it is straight-forward to do any post-processing if needed. For instance, the values above could be easily transformed into a standard Apache Web server log line.\nI hope you found this tool useful, simple to use and configure. Some future work includes adding more observations distributions beyond the exponential family and the ability to directly add transformation rules to the generated observations. If you have any suggestions, use cases (or found an issue!), please let me know in the repository.\nIf you have any comments please let me know on Mastodon (or Twitter).\nHappy coding!\nhttps://www.openshift.com/\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"/containerised-streaming-data-generation-using-state-space-models.html","tags":null,"title":"Containerised Streaming Data Generation using State-Space Models"},{"categories":null,"contents":"Docker is a containerization and manager tool. Docker is a kind of isolated space where an application runs by using system resources.\nTips Extract image locally To extract a container\u0026rsquo;s image locally (e.g. using Docker) the following can be used:\ndocker export $CONTAINER_NAME \u0026gt; output.tar Alternatively, and using the $IMAGE_ID we can do:\ndocker save $IMAGE_ID$ \u0026gt; output.tar ","permalink":"/containers.html","tags":null,"title":"Containers"},{"categories":null,"contents":"Main documentation is available here.\nSetup Requirements For the purpose of these instructions we will assume the following are installed:\nPython 3.9.0 virtualenv A new venv can be created with virtualenv env1 and activated with source venv/bin/activate. Once the environment is active we can install the cookiecutter package using pip install cookiecutter.\nThe create of the cookiecutter project can be done with\n$ cookiecutter https://github.com/drivendata/cookiecutter-data-science For the remainder of this text we will call the of the project you\u0026rsquo;ve just created as $PROJ.\nMore details at Python.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"/cookiecutter-data-science.html","tags":null,"title":"Cookiecutter data science"},{"categories":null,"contents":"Similarity Let\u0026rsquo;s create two datasets, $\\mu_1$ and $\\mu_2$ such that\n$$ \\mu_i = {x_1,\\dots,x_n} \\sim {\\mathcal{U}_1(-1,1),\\dots,\\mathcal{U}_n(-1,1)} $$\nWe will use $N=100$ observations for a vector sized $n=10$.\nimport numpy as np import pandas as pd import scipy.stats as stats from scipy.spatial.distance import squareform n = 10 N = 100 np.random.seed(0) mu_1 = np.random.normal(loc=0, scale=1, size=(N, n)) mu_2 = np.random.normal(loc=0, scale=1, size=(N, n)) We now add some noise, $\\epsilon=0.6$, to $\\mu_2$ such that\n$$ \\mu_2 = \\epsilon \\mu_2 + (1-\\epsilon)*\\mu_1 $$\nepsilon = 0.6 mu_2 = epsilon*mu_2 + (1-epsilon)*mu_1 We use Pandas to calculate the correlation matrix:\nC1 = pd.DataFrame(mu_1).corr() C2 = pd.DataFrame(mu_2).corr() And we plot the correlation matrices.\nimport seaborn as sns import matplotlib.pyplot as plt f,axes = plt.subplots(1,2, figsize=(10,5)) sns.set_style(\u0026#34;white\u0026#34;) for ix, m in enumerate([C1,C2]): sns.heatmap(m, cmap=\u0026#34;RdBu_r\u0026#34;, center=0, vmin=-1, vmax=1, ax=axes[ix], square=True, cbar_kws={\u0026#34;shrink\u0026#34;: .5}, xticklabels=True) axes[ix].set(title=f\u0026#34;$C_{ix+1}$\u0026#34;) Spearman correlation Calculate similarity using Spearman correlation between the top triangle of the covariance matrices $C_1$ and $C_2$.\nindices = np.triu_indices(C1.shape[0], k=1) print(C1[indices]) ","permalink":"/correlation-matrix.html","tags":null,"title":"Correlation matrix"},{"categories":null,"contents":"Building counterfactually fair models Data To evaluate counterfactual fairness we will be using the \u0026ldquo;law school\u0026rdquo; dataset1.\nThe Law School Admission Council conducted a survey across 163 law schools in the United States. It contains information on 21,790 law students such as their entrance exam scores (LSAT), their grade-point average (GPA) collected prior to law school, and their first year average grade (FYA). Given this data, a school may wish to predict if an applicant will have a high FYA. The school would also like to make sure these predictions are not biased by an individuals race and sex. However, the LSAT, GPA, and FYA scores, may be biased due to social factors.\nWe start by importing the data into a [Pandas]] DataFrame.\nimport warnings warnings.filterwarnings(\u0026#34;ignore\u0026#34;) import pandas as pd df = pd.read_csv(\u0026#34;data/law_data.csv\u0026#34;, index_col=0) df.head() Pre-processing We now pre-process the data. We start by creating categorical \u0026ldquo;dummy\u0026rdquo; variables according to the race variable.\ndf = pd.get_dummies(df, columns=[\u0026#34;race\u0026#34;], prefix=\u0026#34;\u0026#34;, prefix_sep=\u0026#34;\u0026#34;) df.iloc[:, : 7].head() df.iloc[:, 7 :].head() We also want to expand the sex variable into male / female categorical variables and remove the original.\ndf[\u0026#34;male\u0026#34;] = df[\u0026#34;sex\u0026#34;].map(lambda x: 1 if x == 2 else 0) df[\u0026#34;female\u0026#34;] = df[\u0026#34;sex\u0026#34;].map(lambda x: 1 if x == 1 else 0) df = df.drop(axis=1, columns=[\u0026#34;sex\u0026#34;]) df.iloc[:, 0:7].head() df.iloc[:, 7:].head() We will also convert the entrance exam scores (LSAT) to a discrete variable.\ndf[\u0026#34;LSAT\u0026#34;] = df[\u0026#34;LSAT\u0026#34;].astype(int) df.iloc[:, :6].head() df.iloc[:, 6:].head() Protected attributes Counterfactual fairness enforces that a distribution over possible predictions for an individual should remain unchanged in a world where an individuals protected attributes $A$ had been different in a causal sense. Let\u0026rsquo;s start by defining the /protected attributes/. Obvious candidates are the different categorical variables for ethnicity (Asian, White, Black, etc) and gender (male, female).\nA = [ \u0026#34;Amerindian\u0026#34;, \u0026#34;Asian\u0026#34;, \u0026#34;Black\u0026#34;, \u0026#34;Hispanic\u0026#34;, \u0026#34;Mexican\u0026#34;, \u0026#34;Other\u0026#34;, \u0026#34;Puertorican\u0026#34;, \u0026#34;White\u0026#34;, \u0026#34;male\u0026#34;, \u0026#34;female\u0026#34;, ] Training and testing subsets We will now divide the dataset into training and testing subsets. We will use the same ratio as in 2, that is 20%.\nfrom sklearn.model_selection import train_test_split df_train, df_test = train_test_split(df, random_state=23, test_size=0.2); Models Unfair model As detailed in 2, the concept of counterfactual fairness holds under three levels of assumptions of increasing strength.\nThe first of such levels is Level 1, where $\\hat{Y}$ is built using only the observable non-descendants of $A$. This only requires partial causal ordering and no further causal assumptions, but in many problems there will be few, if any, observables which are not descendants of protected demographic factors.\nFor this dataset, since LSAT, GPA, and FYA are all biased by ethnicity and gender, we cannot use any observed features to construct a Level 1 counterfactually fair predictor as described in Level 1.\nInstead (and in order to compare the performance with Level 2 and 3 models) we will build two unfair baselines.\nA Full model, which will be trained with the totality of the variables An Unaware model (FTU), which will be trained will all the variables, except the protected attributes $A$. Let\u0026rsquo;s proceed with calculating the Full model.\nFull model As mentioned previously, the full model will be a simple linear regression in order to predict ZFYA using all of the variables.\nfrom sklearn.linear_model import LinearRegression linreg_unfair = LinearRegression() The inputs will then be the totality of the variabes (protected variables $A$, as well as UGPA and LSAT).\nimport numpy as np X = np.hstack( ( df_train[A], np.array(df_train[\u0026#34;UGPA\u0026#34;]).reshape(-1, 1), np.array(df_train[\u0026#34;LSAT\u0026#34;]).reshape(-1, 1), ) ) print(X) As for our target, we are trying to predict ~ZFYA~ (first year average grade).\ny = df_train[\u0026#34;ZFYA\u0026#34;] y[:10] We fit the model:\nlinreg_unfair = linreg_unfair.fit(X, y) And perform some predictions on the test subset.\nX_test = np.hstack( ( df_test[A], np.array(df_test[\u0026#34;UGPA\u0026#34;]).reshape(-1, 1), np.array(df_test[\u0026#34;LSAT\u0026#34;]).reshape(-1, 1), ) ) X_test predictions_unfair = linreg_unfair.predict(X_test) predictions_unfair We will also calculate the /unfair model/ score for future use.\nscore_unfair = linreg_unfair.score(X_test, df_test[\u0026#34;ZFYA\u0026#34;]) print(score_unfair) from sklearn.metrics import mean_squared_error RMSE_unfair = np.sqrt(mean_squared_error(df_test[\u0026#34;ZFYA\u0026#34;], predictions_unfair)) print(RMSE_unfair) Fairness through unawareness (FTU) As also mentioned in 2, the second baseline we will use is an Unaware model (FTU), which will be trained will all the variables, except the protected attributes $A$.\nlinreg_ftu = LinearRegression() We will create the inputs as previously, but without using the protected attributes, $A$.\nX_ftu = np.hstack( ( np.array(df_train[\u0026#34;UGPA\u0026#34;]).reshape(-1, 1), np.array(df_train[\u0026#34;LSAT\u0026#34;]).reshape(-1, 1), ) ) X_ftu And we fit the model:\nlinreg_ftu = linreg_ftu.fit(X_ftu, y) Again, let\u0026rsquo;s perform some predictions on the test subset.\nX_ftu_test = np.hstack( (np.array(df_test[\u0026#34;UGPA\u0026#34;]).reshape(-1, 1), np.array(df_test[\u0026#34;LSAT\u0026#34;]).reshape(-1, 1)) ) X_ftu_test predictions_ftu = linreg_ftu.predict(X_ftu_test) predictions_ftu As previously, let\u0026rsquo;s calculate this model\u0026rsquo;s score.\nftu_score = linreg_ftu.score(X_ftu_test, df_test[\u0026#34;ZFYA\u0026#34;]) print(ftu_score) RMSE_ftu = np.sqrt(mean_squared_error(df_test[\u0026#34;ZFYA\u0026#34;], predictions_ftu)) print(RMSE_ftu) Latent variable model Still according to 2, a Level 2 approach will model latent fair variables which are parents of observed variables.\nIf we consider a predictor parameterised by $\\theta$, such as:\n$$ \\hat{Y} \\equiv g_\\theta (U, X_{\\nsucc A}) $$\nwith $X_{\\nsucc A} \\subseteq X$ are non-descendants of $A$. Assuming a loss function $l(.,.)$ and training data $\\mathcal{D}\\equiv{(A^{(i), X^{(i)}, Y^{(i)}})}$, for $i=1,2\\dots,n$, the empirical loss is defined as\n$$ L(\\theta)\\equiv \\sum_{i=1}^n \\mathbb{E}[l(y^{(i)},g_\\theta(U^{(i)}, x^{(i)}_{\\nsucc A}))]/n $$\nwhich has to be minimised in order to $\\theta$. Each $n$ expectation is with respect to random variable $U^{(i)}$ such that\n$$ U^{(i)}\\sim P_{\\mathcal{M}}(U|x^{(i)}, a^{(i)}) $$\nwhere $P_{\\mathcal{M}}(U|x,a)$ is the conditional distribution of the background variables as given by a causal model $M$ that is available by assumption.\nIf this expectation cannot be calculated analytically, Markov chain Monte Carlo (MCMC) can be used to approximate it as in the following algorithm.\nWe will follow the model specified in the original paper, where the latent variable considered is $K$, which represents a student\u0026rsquo;s knowledge. $K$ will affect GPA, LSAT and the outcome, FYA. The model can be defined by:\n$$ \\begin{aligned} GPA \u0026amp;\\sim \\mathcal{N}(GPA_0 + w_{GPA}^KK + w_{GPA}^RR + w_{GPA}^SS, \\sigma_{GPA}) \\ LSAT \u0026amp;\\sim \\text{Po}(\\exp(LSAT_0 + w_{LSAT}^KK + w_{LSAT}^RR + w_L^SS)) \\ FYA \u0026amp;\\sim \\mathcal{N}(w_{FYA}^KK + w_{FYA}^RR + w_{FYA}^SS, 1) \\ K \u0026amp;\\sim \\mathcal{N}(0,1) \\end{aligned} $$\nThe priors used will be:\n$$ \\begin{aligned} GPA_0 \u0026amp;\\sim \\mathcal{N}(0, 1) \\ LSAT_0 \u0026amp;\\sim \\mathcal{N}(0, 1) \\ GPA_0 \u0026amp;\\sim \\mathcal{N}(0, 1) \\end{aligned} $$\nimport pymc3 as pm K = len(A) def MCMC(data, samples=1000): N = len(data) a = np.array(data[A]) model = pm.Model() with model: # Priors k = pm.Normal(\u0026#34;k\u0026#34;, mu=0, sigma=1, shape=(1, N)) gpa0 = pm.Normal(\u0026#34;gpa0\u0026#34;, mu=0, sigma=1) lsat0 = pm.Normal(\u0026#34;lsat0\u0026#34;, mu=0, sigma=1) w_k_gpa = pm.Normal(\u0026#34;w_k_gpa\u0026#34;, mu=0, sigma=1) w_k_lsat = pm.Normal(\u0026#34;w_k_lsat\u0026#34;, mu=0, sigma=1) w_k_zfya = pm.Normal(\u0026#34;w_k_zfya\u0026#34;, mu=0, sigma=1) w_a_gpa = pm.Normal(\u0026#34;w_a_gpa\u0026#34;, mu=np.zeros(K), sigma=np.ones(K), shape=K) w_a_lsat = pm.Normal(\u0026#34;w_a_lsat\u0026#34;, mu=np.zeros(K), sigma=np.ones(K), shape=K) w_a_zfya = pm.Normal(\u0026#34;w_a_zfya\u0026#34;, mu=np.zeros(K), sigma=np.ones(K), shape=K) sigma_gpa_2 = pm.InverseGamma(\u0026#34;sigma_gpa_2\u0026#34;, alpha=1, beta=1) mu = gpa0 + (w_k_gpa * k) + pm.math.dot(a, w_a_gpa) # Observed data gpa = pm.Normal( \u0026#34;gpa\u0026#34;, mu=mu, sigma=pm.math.sqrt(sigma_gpa_2), observed=list(data[\u0026#34;UGPA\u0026#34;]), shape=(1, N), ) lsat = pm.Poisson( \u0026#34;lsat\u0026#34;, pm.math.exp(lsat0 + w_k_lsat * k + pm.math.dot(a, w_a_lsat)), observed=list(data[\u0026#34;LSAT\u0026#34;]), shape=(1, N), ) zfya = pm.Normal( \u0026#34;zfya\u0026#34;, mu=w_k_zfya * k + pm.math.dot(a, w_a_zfya), sigma=1, observed=list(data[\u0026#34;ZFYA\u0026#34;]), shape=(1, N), ) step = pm.Metropolis() trace = pm.sample(samples, step, progressbar = False) return trace train_estimates = MCMC(df_train) Let\u0026rsquo;s plot a single trace for $k^{(i)}$.\nimport matplotlib.pyplot as plt import seaborn as sns from plotutils import * # Thin the samples before plotting k_trace = train_estimates[\u0026#34;k\u0026#34;][:, 0].reshape(-1, 1)[0::100] plt.subplot(1, 2, 1) plt.hist(k_trace, color=colours[0], bins=100) plt.subplot(1, 2, 2) plt.scatter(range(len(k_trace)), k_trace, s=1, c=colours[0]) plt.show() train_k = np.mean(train_estimates[\u0026#34;k\u0026#34;], axis=0).reshape(-1, 1) train_k We can now estimate $k$ using the test data:\ntest_map_estimates = MCMC(df_test) test_k = np.mean(test_map_estimates[\u0026#34;k\u0026#34;], axis=0).reshape(-1, 1) test_k We now build the Level 2 predictor, using $k$ as the input.\nlinreg_latent = LinearRegression() linreg_latent = linreg_latent.fit(train_k, df_train[\u0026#34;ZFYA\u0026#34;]) predictions_latent = linreg_latent.predict(test_k) predictions_latent latent_score = linreg_latent.score(test_k, df_test[\u0026#34;ZFYA\u0026#34;]) print(latent_score) RMSE_latent = np.sqrt(mean_squared_error(df_test[\u0026#34;ZFYA\u0026#34;], predictions_latent)) print(RMSE_latent) Additive error model Finally, in Level 3, we model GPA, LSAT, and FYA as continuous variables with additive error terms independent of race and sex3.\nThis corresponds to\n$$ \\begin{aligned} GPA \u0026amp;= b_G + w^R_{GPA}R + w^S_{GPA}S + \\epsilon_{GPA}, \\epsilon_{GPA} \\sim p(\\epsilon_{GPA}) \\ LSAT \u0026amp;= b_L + w^R_{LSAT}R + w^S_{LSAT}S + \\epsilon_{LSAT}, \\epsilon_{LSAT} \\sim p(\\epsilon_{LSAT}) \\ FYA \u0026amp;= b_{FYA} + w^R_{FYA}R + w^S_{FYA}S + \\epsilon_{FYA} , \\epsilon_{FYA} \\sim p(\\epsilon_{FYA}) \\end{aligned} $$\nWe estimate the error terms $\\epsilon_{GPA}, \\epsilon_{LSAT}$ by first fitting two models that each use race and sex to individually predict GPA and LSAT. We then compute the residuals of each model (e.g., $\\epsilon_{GPA} =GPA\\hat{Y}{GPA}(R, S)$). We use these residual estimates of $\\epsilon{GPA}, \\epsilon_{LSAT}$ to predict $FYA$. In 2 this is called Fair Add.\nSince the process is similar for the individual predictions for GPA and LSAT, we will write a method to avoid repetion.\ndef calculate_epsilon(data, var_name, protected_attr): X = data[protected_attr] y = data[var_name] linreg = LinearRegression() linreg = linreg.fit(X, y) predictions = linreg.predict(X) return data[var_name] - predictions Let\u0026rsquo;s apply it to each variable, individually. First we calculate $\\epsilon_{GPA}$:\nepsilons_gpa = calculate_epsilon(df, \u0026#34;UGPA\u0026#34;, A) epsilons_gpa Next, we calculate $\\epsilon_{LSAT}$:\nepsilons_LSAT = calculate_epsilon(df, \u0026#34;LSAT\u0026#34;, A) epsilons_LSAT Let\u0026rsquo;s visualise the $\\epsilon$ distribution quickly:\nimport matplotlib.pyplot as plt import seaborn as sns plt.subplot(1, 2, 1) plt.hist(epsilons_gpa, color=colours[0], bins=100) plt.title(\u0026#34;$\\epsilon_{GPA}$\u0026#34;) plt.xlabel(\u0026#34;$\\epsilon_{GPA}$\u0026#34;) plt.subplot(1, 2, 2) plt.hist(epsilons_LSAT, color=colours[1], bins=100) plt.title(\u0026#34;$\\epsilon_{LSAT}$\u0026#34;) plt.xlabel(\u0026#34;$\\epsilon_{LSAT}$\u0026#34;) plt.show() We finally use the calculated $\\epsilon$ to train a model in order to predict FYA. We start by getting the subset of the $\\epsilon$ which match the training indices.\nX = np.hstack( ( np.array(epsilons_gpa[df_train.index]).reshape(-1, 1), np.array(epsilons_LSAT[df_train.index]).reshape(-1, 1), ) ) X linreg_fair_add = LinearRegression() linreg_fair_add = linreg_fair_add.fit( X, df_train[\u0026#34;ZFYA\u0026#34;], ) We now use this model to calculate the predictions\nX_test = np.hstack( ( np.array(epsilons_gpa[df_test.index]).reshape(-1, 1), np.array(epsilons_LSAT[df_test.index]).reshape(-1, 1), ) ) predictions_fair_add = linreg_fair_add.predict(X_test) predictions_fair_add And as previously, we calculate the model\u0026rsquo;s score:\nfair_add_score = linreg_fair_add.score(X_test, df_test[\u0026#34;ZFYA\u0026#34;]) print(fair_add_score) RMSE_fair_add = np.sqrt(mean_squared_error(df_test[\u0026#34;ZFYA\u0026#34;], predictions_fair_add)) print(RMSE_fair_add) Comparison The scores, so far, are:\nprint(f\u0026#34;Unfair score:\\t{score_unfair}\u0026#34;) print(f\u0026#34;FTU score:\\t{ftu_score}\u0026#34;) print(f\u0026#34;L2 score:\\t{latent_score}\u0026#34;) print(f\u0026#34;Fair add score:\\t{fair_add_score}\u0026#34;) print(f\u0026#34;Unfair RMSE:\\t{RMSE_unfair}\u0026#34;) print(f\u0026#34;FTU RMSE:\\t{RMSE_ftu}\u0026#34;) print(f\u0026#34;L2 RMSE:\\t{RMSE_latent}\u0026#34;) print(f\u0026#34;Fair add RMSE:\\t{RMSE_fair_add}\u0026#34;) Measuring counterfactual fairness First, we will measure two quantities, the Statistical Parity Difference (SPD)4 and Disparate impact (DI)5.\nStatistical Parity Difference / Disparate Impact from fairlearn.metrics import demographic_parity_difference, demographic_parity_ratio\nparities = [] impacts = [] for a in A: parity = demographic_parity_difference(df_train[\u0026#34;ZFYA\u0026#34;], df_train[\u0026#34;ZFYA\u0026#34;], sensitive_features = df_train[a]) di = demographic_parity_ratio(df_train[\u0026#34;ZFYA\u0026#34;], df_train[\u0026#34;ZFYA\u0026#34;], sensitive_features = df_train[a]) parities.append(parity) impacts.append(di) df_parities = pd.DataFrame({\u0026#39;protected\u0026#39;:A,\u0026#39;parity\u0026#39;:parities,\u0026#39;impact\u0026#39;:impacts}) import matplotlib.pyplot as plt from plotutils import * fig = plt.figure() ax = fig.add_subplot(111) ax2 = ax.twinx() fig.suptitle(\u0026#39;Statistical Parity Difference and Disparate Impact\u0026#39;) width = 0.4 df_parities.plot(x =\u0026#39;protected\u0026#39;, y = \u0026#39;parity\u0026#39;, kind = \u0026#39;bar\u0026#39;, ax = ax, width = width, position=1, color=colours[0], legend=False) df_parities.plot(x =\u0026#39;protected\u0026#39;, y = \u0026#39;impact\u0026#39;, kind = \u0026#39;bar\u0026#39;, ax = ax2, width = width, position = 0, color = colours[1], legend = False) ax.axhline(y = 0.1, linestyle = \u0026#39;dashed\u0026#39;, alpha = 0.7, color = colours[0]) ax2.axhline(y = 0.55, linestyle = \u0026#39;dashed\u0026#39;, alpha = 0.7, color = colours[1]) patches, labels = ax.get_legend_handles_labels() ax.legend(patches, [\u0026#39;Stat Parity Diff\u0026#39;], loc = \u0026#39;upper left\u0026#39;) patches, labels = ax2.get_legend_handles_labels() ax2.legend(patches, [\u0026#39;Disparate Impact\u0026#39;], loc = \u0026#39;upper right\u0026#39;) labels = [item.get_text() for item in ax.get_xticklabels()] for i in range(len(A)): labels[i] = A[i] ax.set_xticklabels(labels) ax.set_xlabel(\u0026#39;Protected Features\u0026#39;) ax.set_ylabel(\u0026#39;Statistical Parity Difference\u0026#39;) ax2.set_ylabel(\u0026#39;Disparate Impact\u0026#39;) plt.show() Finding sensitive features Typically a $SPD \u0026gt; 0.1$ and a $DI \u0026lt; 0.9$ might indicate discrimination on those features. All protected attributes fail the SPD test and, in our dataset, we have two features (~Hispanic~ and ~Mexican~) which clearly fail the DI test.\nfor a in [\u0026#34;Mexican\u0026#34;, \u0026#34;Hispanic\u0026#34;]: spd = demographic_parity_difference(y_true=df_train[\u0026#34;ZFYA\u0026#34;], y_pred=df_train[\u0026#34;ZFYA\u0026#34;], sensitive_features = df_train[a]) print(f\u0026#34;SPD({a}) = {spd}\u0026#34;) di = demographic_parity_ratio(y_true=df_train[\u0026#34;ZFYA\u0026#34;], y_pred=df_train[\u0026#34;ZFYA\u0026#34;], sensitive_features = df_train[a]) print(f\u0026#34;DI({a}) = {di}\u0026#34;) McIntyre, Frank, and Michael Simkovic. \u0026ldquo;Are law degrees as valuable to minorities?.\u0026rdquo; International Review of Law and Economics 53 (2018): 23-37.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nKusner, Matt J., Joshua Loftus, Chris Russell, and Ricardo Silva. \u0026ldquo;Counterfactual fairness.\u0026rdquo; In Advances in neural information processing systems, pp. 4066-4076. 2017.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\u0026#160;\u0026#x21a9;\u0026#xfe0e;\u0026#160;\u0026#x21a9;\u0026#xfe0e;\u0026#160;\u0026#x21a9;\u0026#xfe0e;\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nThat may in turn be correlated with one-another.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nSee {ref}fairness:demographic-parity-difference.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nSee {ref}fairness:disparate-impact.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"/counterfactual-fairness.html","tags":null,"title":"Counterfactual Fairness"},{"categories":null,"contents":"Here we will look at how to build a counterfactually fair model, as detailed in Counterfactual Fairness, specifically the \u0026ldquo;Fair Add\u0026rdquo; model.\nThis implementation will rely mostly on Apache Commons Math1 linear regression implementations, namely the Ordinary Least Squares (OLS) regression2. We start then by adding the relevant Maven dependencies:\n\u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.apache.commons\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;commons-math3\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;3.6.1\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt; Data will be passed as a RealMatrix3. This matrix will have dimensions $N\\times f$, where $N$ is the number of observations and $f$ is the number of features.\nWe can instatiate the model using\n// RealMatrix data = ... final CounterfactuallyFairModel model = new CounterfactuallyFairModel(data); We will then need the following information:\nThe protected attributes indices, protectedIndices The variable indices, variableIndices The target variable index, targetIndex Assuming that we have the same variables as in the counterfactual fairness example, let\u0026rsquo;s say that the protected attributes have in the data matrix, column numbers 5, 6, 7, 8, 9, 10, 11, 12, 13, 14 and the model variables (LSAT and UGPA) have indices 1, 0 and the target (ZFYA) has index 2. We then calculate the counterfactually fair model using:\nmodel.calculate(new int[]{5, 6, 7, 8, 9, 10, 11, 12, 13, 14}, new int[]{1, 0}, 2); The calculate method performs the following:\npublic void calculate(int[] protectedIndices, int[] variableIndices, int targetIndex) { final RealMatrix residuals = new Array2DRowRealMatrix(this.data.getRowDimension(), variableIndices.length); for (int i = 0; i \u0026lt; variableIndices.length; i++) { final int index = variableIndices[i]; final RealVector varResidual = this.calculateEpsilon(protectedIndices, index); residuals.setColumn(i, varResidual.toArray()); } // predict target from residuals final OLSMultipleLinearRegression regression = new OLSMultipleLinearRegression(); regression.newSampleData(this.data.getColumn(targetIndex), residuals.getData()); } As in counterfactual Fairness , we calculate a regression model to predict each of the variable (LSAT and UGPA) using the protected variables. The resulting residuals, $\\epsilon_{LSAT}$ and $\\epsilon_{UGPA}$ will in turn be used to calculate another regression model in order to predict the target variable ZFYA.\nThe residuals are calculated using the calculateEpsilon method, which consists of:\npublic RealVector calculateEpsilon(int[] protectedIndices, int targetIndex) { int[] protectedRows = new int[this.data.getRowDimension()]; Arrays.setAll(protectedRows, i -\u0026gt; i); final RealMatrix _x = this.data.getSubMatrix(protectedRows, protectedIndices); final RealVector _y = this.data.getSubMatrix(protectedRows, new int[]{targetIndex}).getColumnVector(0); final OLSMultipleLinearRegression regression = new OLSMultipleLinearRegression(); regression.newSampleData(_y.toArray(), _x.getData()); return new ArrayRealVector(regression.estimateResiduals()); } Which simply calculates a regression model for the variables using the protected attributes and returning a RealVector with the residual $\\epsilon$.\nhttps://commons.apache.org/proper/commons-math/.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nhttps://commons.apache.org/proper/commons-math/javadocs/api-3.6/org/apache/commons/math3/stat/regression/OLSMultipleLinearRegression.html\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nhttps://commons.apache.org/proper/commons-math/javadocs/api-3.6/org/apache/commons/math3/linear/RealMatrix.html\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"/counterfactual-fairness-in-java.html","tags":null,"title":"Counterfactual Fairness in Java"},{"categories":null,"contents":"A special type of Explainability.\nDesiderata According to Verma et al 1 the counterfactual desiderata is:\nValidity Actionability Sparsity Data manifold closeness Causality Amortised inference Validity We assume that a counterfactual is valid if it solves the optimisation as states in Wachter et al2. If we defined the loss function as\n$$ L(x,x^{\\prime},y^{\\prime},\\lambda)=\\lambda\\times(\\hat{f}(x^{\\prime})y^{\\prime})^2+d(x,x^{\\prime}), $$\nwe can define the counterfactual as\n$$ \\arg \\underset{x^{\\prime}}{\\min}\\underset{\\lambda}{\\max} \\lambda\\times(\\hat{f}(x^{\\prime})y^{\\prime})^2+d(x,x^{\\prime}) $$\nwhere:\n$x \\in \\mathcal{X}$ is the original data point $x^{\\prime} \\in \\mathcal{X}$ is the counterfactual $y^{\\prime} \\in \\mathcal{Y}$ is the desired label $d$ is a distance metric to measure the distance between $x$ and $x^{\\prime}$. this could be a L1 or L2 distance, a quadratic distance, etc. Actionability Still according to 2, actionability refers to the ability of a counterfactual method to separate between mutable and immutable features. Immutable, and additionally legally protected features, shouldn\u0026rsquo;t be changed by a counterfactual implementation. Formally, if we defined our set of mutable (or actionable) features as $\\mathcal{A}$, we have\n$$ \\arg \\underset{x^{\\prime} \\in \\mathcal{A}}{\\min}\\underset{\\lambda}{\\max} \\lambda\\times(\\hat{f}(x^{\\prime})y^{\\prime})^2+d(x,x^{\\prime}) $$\nSparsity According to 2 Shorter counterfactuals are easier to understand and an effective counterfactual implementation should change the least amount of features as possible. If a sparsity penalty term is added to our definition\n$$ g(x^{\\prime}-x) $$\nwhich increases the more features are changed and could be a L0 or [Distance metrics#Manhattan distance L1|L1]] metric, for instance. We can then define the counterfactual as\n$$ \\arg \\underset{x^{\\prime} \\in \\mathcal{A}}{\\min}\\underset{\\lambda}{\\max} \\lambda\\times(\\hat{f}(x^{\\prime})y^{\\prime})^2+d(x,x^{\\prime})+g(x^{\\prime}-x) $$\nData manifold closeness Still according to 2, data manifold closeness is the property which guarantees that the counterfactual will be as close to the training data as possible. This can translate into a more \u0026ldquo;realistic\u0026rdquo; counterfactual, since it is possible that the counterfactual would take extreme or never seen before values in order to satisfy the previous conditions. Formally, we can write a penalty term for the adherence to the training data manifold, $\\mathcal{X}$ as $l(x^{\\prime};\\mathcal{X})$ and the define the counterfactual as\n$$ \\arg \\underset{x^{\\prime} \\in \\mathcal{A}}{\\min}\\underset{\\lambda}{\\max} \\lambda\\times(\\hat{f}(x^{\\prime})y^{\\prime})^2+d(x,x^{\\prime})+g(x^{\\prime}-x)+l(x^{\\prime};\\mathcal{X}) $$\nCausality Causality refers to the property where feature changes will impact dependent features. That is, we no longer assume that all features are independent. This implies that the counterfactual method needs to mantain the causal relations between features.\nAmortised inference Amortised inference refers to the property of a counterfactual search to provide multiple counterfactuals for a single data point.\nAlternative methods Constraint solvers An alternative method to find counterfactuals is to use constraint solvers. This is explored more in-depth in Counterfactuals with Constraint Solvers.\nVerma, Sahil, John Dickerson, and Keegan Hines. \u0026ldquo;Counterfactual Explanations for Machine Learning: A Review.\u0026rdquo; arXiv preprint arXiv:2010.10596 (2020).\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nWachter, Sandra, Brent Mittelstadt, and Chris Russell. \u0026ldquo;Counterfactual explanations without opening the black box: Automated decisions and the GDPR.\u0026rdquo; Harv. JL \u0026amp; Tech. 31 (2017): 841.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\u0026#160;\u0026#x21a9;\u0026#xfe0e;\u0026#160;\u0026#x21a9;\u0026#xfe0e;\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"/counterfactuals.html","tags":null,"title":"Counterfactuals"},{"categories":null,"contents":"Scoring An implementation on how to calculate counterfactuals with Constraint Solvers (namely OptaPlanner) is available here.\nThis implementation satisfies several criteria of the counterfactuals.\nThe penalisation score is represented with a BendableBigDecimalScore 1, having three \u0026ldquo;hard\u0026rdquo; levels and two \u0026ldquo;soft\u0026rdquo; levels.\nThe first hard level component, 1, penalises the score according to the distance between the prediction, $y^{\\prime}$ for the currently proposed solution, $x^{\\prime}$ and the original prediction $y$, that is this our $(\\hat{f}(x^{\\prime})-y^{\\prime})^2$. This corresponds to the counterfactual\u0026rsquo;s validity.\nThe actionability is score with 2. This component penalises the score according to number of immutable features which were changed in the counterfactual.\nA confidence score component, 3 is use to, optionally, impose a minimum confidence threshold to the counterfactual\u0026rsquo;s associated prediction, $x^{\\prime}$.\nFinally, the feature distance, 4, penalises the score according to the feature distance. This is the representation of\n$$ d(x, x^{\\prime}). $$\nIn the concrete implementation linked above, the distance, $d$, chosen is a Manhattan (or $L^1$) distance calculated feature-wise. This also corresponds to the validity property.\nImplementation Entities are defined by classes such as Integer, Categorical, Boolean or Float, as shown in 5. Each of the features, shown in 6, is created as an instance of one of these entities. For instance, feature1 would be of type Integer and feature2 would be of type Categorical, etc.\nThe original data point, $x$ is represented by this set of features (6).\nA planning solution (PlanningSolution), illustrated in 7 will produce candidate solutions (shown in 8)\nFor each solution, we propose a new set of features ($x^{\\prime}$) as a counterfactual candidate. For instance, ~solution A~ in 8.\nIn the following section we will look at how each component is calculated. We will refer to each \u0026ldquo;hard\u0026rdquo; level component as $H_1, H_2$ and $H_3$ and the \u0026ldquo;soft\u0026rdquo; component as $S_1$. The overal score consists, then, of $S={H_1, H_2, H_3, S_1 }$\nPrediction distance The first component of the score, 1 is established by sending the proposed counterfactual $x^{\\prime}$, 8 to a predictive model, 9 and calculating the distance between the desired outcome, $y^{\\prime}$ and the model\u0026rsquo;s prediction. This is done component wise, for each feature of the output. That is, for a prediction with $N$ features, we calculate\n$$ H_1=\\left(\\sum_i^Nf(x^{\\prime}_i) - y^{\\prime}_i\\right)^2 $$\nTolerance For numerical features, the above score ($H_1$) will cause the counterfactual to be invalid, unless the distance between the outcomes and proposed values is exactly zero.\nWe can solve this problem by introducing a \u0026ldquo;tolerance\u0026rdquo; adjustment, which allows proposed values to be accepted if they are \u0026ldquo;close enough\u0026rdquo; to the goal.\nTo make the tolerance scale-invariant and unit-less we can use a relative change and set the distance to zero, if smaller than the threshold $t$, that is\n$$ d = \\begin{cases} 0,\u0026amp;\\qquad\\text{if},\\frac{\\vert f(x\u0026rsquo;_i)-y\u0026rsquo;_i\\vert}{\\max(\\vert f(x\u0026rsquo;_i)\\vert,\\vert y\u0026rsquo;_i\\vert)} \u0026lt; t \\\\ \\vert f(x\u0026rsquo;_i)-y\u0026rsquo;_i\\vert,\u0026amp;\\qquad\\text{otherwise} \\end{cases} $$\nand compare to the threshold $t$. As an example, for a goal $y\u0026rsquo;_i=3$ and a threshold of $t=0.01$, around the goal we would have the distance as in the figure below:\nThis would however fail for the edge case where $y^{\\prime}_i=0$ as we can see below:\nTo solve this, we can introduce a special case for $y^{\\prime}_i=0$, such that:\n$$ d_{g=0} = \\begin{cases} 0,\u0026amp;\\qquad \\text{if}\\ |f(x^{\\prime}_i)|\u0026lt; t\\\\ \\lVert f(x^{\\prime}_i) - y^{\\prime}_i \\rVert,\u0026amp;\\qquad\\text{otherwise} \\end{cases} $$\nSo that we have now the desired behaviour at $y^{\\prime}_i=0$:\nGower distance An alternative metric for the outcome distance (and mixed variables in general) is the site/Machine learning/Gower distance.\nActionability score For the second component, the actionability score, 2. We calculate the number of features for the protected set $\\mathcal{A}$, which have a different value from the original. That is, assuming we have a certain number of protectd features $M$, such that $\\mathcal{A}={A_1,A_2,\\dots,A_M}$, we calculate:\n$$ H_2 = \\sum_{a \\in \\mathcal{A}} \\mathbb{1}(x_a \\neq x^{\\prime}_a), $$\nConfidence score For each feature $i$, if we have a prediction confidence, $p_i(f(x^{\\prime}))$, we calculate the number of features which have a confidence below a certain predefined threshold, $P_i$. If the threshold is not defined, this component will always be zero and not influence the counterfactual selection. Assuming we have defined a threshold for all $N$ features, $P = {P_1, P_2, \\dots, P_N}$ we calculate this score as\n$$ H_3 = \\sum_i^N \\mathbb{1} \\left( p_i \\left( f(x^{\\prime}) \u0026lt; P_i \\right) \\right) $$\nFeature distance Considering that each datapoint $x$ consists of different $N$ features, such that $x=\\left(f_1,\\dots,f_n\\right)$ and that each feature might be numerical or categorical2, we calculate the distance between a datapoint $x$ and a potential counterfactual $x^{\\prime}$:\n$$ d\\left(x,x^{\\prime}\\right)=\\sum_{i=1}^Nd^{\\prime}\\left(x_i,x_i^{\\prime}\\right) $$ $$ d^{\\prime}\\left(x_i,x_i^{\\prime}\\right)= \\begin{cases} \\left(x_i-x_i^{\\prime}\\right)^2,\\quad\\text{if}\\ x_i,x_i^{\\prime}\\in\\mathbb{N} \\lor x_i,x_i^{\\prime}\\in\\mathbb{R}\\ 1-\\delta_{x,x^{\\prime}},\\quad\\text{if}\\ x_i,x_i^{\\prime}\\ \\text{categorical} \\end{cases} $$\nSince in many scenarios we might not have access to the training data, the above distance are not normalised. In the event that we do have access to training data, then we can use the standard deviation ($SD$) to normalise the features. The $SD$ can be calculated as:\n$$ SD=\\sqrt{\\frac{1}{N}\\sum_{i=1}^N\\left(x_i-\\bar{x}\\right)^2} $$\nso that, in this case, we scale the numerical features with\n$$ \\bar{d}^{\\prime}\\left(x_i,x_i^{\\prime}\\right)= \\frac{\\left(x_i-x_i^{\\prime}\\right)^2}{SD}. $$\nSearching To search for a counterfactual, we start by specifying a search domain for each feature. This will include:\nAn upper and lower bounds for numerical features, respectively $\\mathcal{D}_l, \\mathcal{D}_u$ A set of categories for categorical features, $\\mathcal{C}$ $\\mathcal{B}={0,1}$ for the specific case of boolean/binary values Typically these values would be either established by someone with domain knowledge, or by values that might reflect our expectation for the actual counterfactual (for instance, an ~age~ would have realistic values).\nThe algorithm used for the search is Tabu search3 (Glover, 1989).\nBendableBigDecimalScore documentation\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nHere we are considering binary and boolean values as categorical.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\ncite:glover1989tabu Glover, F. (1989). Tabu searchpart i. ORSA Journal on computing, 1(3), 190206.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"/counterfactuals-with-constraint-solvers.html","tags":null,"title":"Counterfactuals with Constraint Solvers"},{"categories":null,"contents":"Installation Fedora To install Deno on Fedora, first download the installation file:\ncurl -fsSL https://deno.land/x/install/install.sh | sh And then add the following to your shell\u0026rsquo;s profile (e.g. ~/.bashrc):\nexport DENO_INSTALL=\u0026#34;/home/$USER/.deno\u0026#34; export PATH=\u0026#34;$DENO_INSTALL/bin:$PATH\u0026#34; Topis Deno types ","permalink":"/deno.html","tags":null,"title":"Deno"},{"categories":null,"contents":"Union types function add(a: any, b: any) { if (typeof a === \u0026#39;number\u0026#39; \u0026amp;\u0026amp; typeof b === \u0026#39;number\u0026#39;) { return a + b; } if (typeof a === \u0026#39;string\u0026#39; \u0026amp;\u0026amp; typeof b === \u0026#39;string\u0026#39;) { return a.concat(b); } throw new Error(\u0026#39;Parameters must be numbers or strings\u0026#39;); } return add(true, false); function add(a: number | string, b: number | string) { if (typeof a === \u0026#39;number\u0026#39; \u0026amp;\u0026amp; typeof b === \u0026#39;number\u0026#39;) { return a + b; } if (typeof a === \u0026#39;string\u0026#39; \u0026amp;\u0026amp; typeof b === \u0026#39;string\u0026#39;) { return a.concat(b); } throw new Error(\u0026#39;Parameters must be numbers or strings\u0026#39;); } return add(\u0026#39;a\u0026#39;, \u0026#39;b\u0026#39;); ","permalink":"/deno-types.html","tags":null,"title":"Deno types"},{"categories":null,"contents":"Digital gardens are places where information grows.\nChallenges Chronological is the wrong metaphor, but how to capture time and sequence? As an example, have a Git commit hash as well as a recent changes history. Each page already display a commit hash to give it a context as well as a recently updated section in the index. The RSS1 also provides a temporal source of truth.\nNetworking Digital gardens nurture the creation of complex networks from simple concept associations. Quoting Tim Berners-Lee\u0026rsquo;s2 \u0026ldquo;Weaving the Web\u0026rdquo;3 (2000):\nOne of the beautiful things about physics is its ongoing quest to find simple rules that describe the behavior of very small, simple objects. Once found, these rules can often be scaled up to describe the behavior of monumental systems in the real world. []\nIf the rules governing hypertext links between servers and browsers stayed simple, then our web of a few documents could grow to a global web. The art was to define the few basic, common rules of protocol that would allow one computer to talk to another, in such a way that when all computers everywhere did it, the system would thrive, not break down.\nRSS 2.0 file.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nhttps://en.wikipedia.org/wiki/Tim_Berners-Lee\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nFischetti, Mark., Berners-Lee, Tim., Fischetti, Mark., Berners-Lee, Tim. Weaving the Web: The Original Design and Ultimate Destiny of the World Wide Web by Its Inventor. N.p.: Paw Prints, 2008.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"/digital-garden.html","tags":null,"title":"Digital Garden"},{"categories":null,"contents":"L-p metrics Manhattan distance (L1) Given two vectors $p$ and $q$, such that\n$$ \\begin{aligned} p \u0026amp;= \\left(p_1, p_2, \\dots,p_n\\right) \\ q \u0026amp;= \\left(q_1, q_2, \\dots,q_n\\right) \\end{aligned} $$\nwe define the Manhattan distance as:\n$$ d_1(p, q) = |p - q|1 = \\sum{i=1}^n |p_i-q_i| $$\nEuclidean distance (L2) In general, for points given by Cartesian coordinates in $n$-dimensional Euclidean space, the distance is\n$$ d(p,q)=\\sqrt {(p_{1}-q_{1})^{2}+(p_{2}-q_{2})^{2}+\\dots +(p_{i}-q_{i})^{2}+\\dots +(p_{n}-q_{n})^{2}} $$\nCluster distances Within-cluster sum of squares (WCSS) Given a set of observations ($x_1, x_2,\\dots,x_n$), where each observation is a $d$-dimensional real vector, $k$-means clustering aims to partition the $n$ observations into $k$ ($\\leq n$) sets $S=\\lbrace S_1, S_2, \\dots, S_k\\rbrace$ so as to minimize the within-cluster sum of squares (WCSS) (i.e. variance). Formally, the objective is to find:\n$$ {\\underset {\\mathbf {S} }{\\operatorname {arg,min} }}\\sum {i=1}^{k}\\sum {\\mathbf {x} \\in S{i}}\\left|\\mathbf {x} -{\\boldsymbol {\\mu }}{i}\\right|^{2}={\\underset {\\mathbf {S} }{\\operatorname {arg,min} }}\\sum {i=1}^{k}|S{i}|\\operatorname {Var} S_{i} $$\nwhere $\\mu_i$ is the mean of points in $S_i$. This is equivalent to minimizing the pairwise squared deviations of points in the same cluster:\n$$ {\\displaystyle {\\underset {\\mathbf {S} }{\\operatorname {arg,min} }}\\sum {i=1}^{k},{\\frac {1}{2|S{i}|}},\\sum {\\mathbf {x} ,\\mathbf {y} \\in S{i}}\\left|\\mathbf {x} -\\mathbf {y} \\right|^{2}} $$\nThe equivalence can be deduced from identity\n$${\\displaystyle \\sum {\\mathbf {x} \\in S{i}}\\left|\\mathbf {x} -{\\boldsymbol {\\mu }}{i}\\right|^{2}=\\sum {\\mathbf {x} \\neq \\mathbf {y} \\in S{i}}(\\mathbf {x} -{\\boldsymbol {\\mu }}{i})({\\boldsymbol {\\mu }}_{i}-\\mathbf {y} )}. $$\nBecause the total variance is constant, this is equivalent to maximizing the sum of squared deviations between points in different clusters (between-cluster sum of squares, BCSS) which follows from the law of total variance.\nDunn index A full explanation is available at Dunn index.\nGower distance A full explanation with examples is available at site/Machine learning/Gower distance.\n","permalink":"/distance-metrics.html","tags":null,"title":"Distance metrics"},{"categories":null,"contents":"Architecture Algorithm Input: Dataset $\\mathcal{D}_{train}$, Instance $x$, lenght of explanation $\\mathcal{K}$ Initialise $\\mathcal{Y} \\leftarrow {}$ Initialise cluster for $i$ /in/ $1,\\dots,N$ do $C_i \\leftarrow {i}$ end Initialise clusters to merge $\\mathcal{S} \\leftarrow$ for $i$ /in/ $1\\dots N$ while /no more clusters are available for merging/ do Pick two most similar cluster with minimum distance $d$: $(j,k) \\leftarrow \\arg\\min_{d(j,k)} \\in \\mathcal{S}$ Create new cluster $C_l \\leftarrow C_j \\bigcup C_k$ Mark $j$ and $k$ unavailable to merge if $C_l \\neq i$ in $1\\dots N$ then Mark $l$ as available, $\\mathcal{S} \\leftarrow \\mathcal{S} \\bigcup {l}$ end foreach $i \\in \\mathcal{S}$ do Update similarity matrix by computing distance $d(i, l)$ end end while $i$ /in/ $1,\\dots,n$ do $d(\\mathbf{x}i, \\mathbf{x})=\\sqrt{(x{i1}-x_1)^2+\\dots+(x_{im}-x_m)^2}$ end $ind \\leftarrow$ Find indices for the $k$ smallest distance $d(\\mathbf{x}_i, \\mathbf{x})$ $\\hat{y} \\leftarrow$ Get majority label for $x \\in ind$ $n^s \\leftarrow$ Filter $\\mathcal{D}_{train}$ based on $\\hat{y}$ foreach $i$ /in/ $1, \\dots, n$ do $\\mathcal{Y} \\leftarrow $ Pairwise distance of each instance in cluster $n^s$ with the original instance $x$ end $\\omega \\leftarrow$ LinearRegression$(n^s, \\mathcal{Y}, \\mathcal{K})$ return $\\omega$ ","permalink":"/dlime.html","tags":null,"title":"DLIME"},{"categories":null,"contents":"Configuration is at the same time the most fun and biggest challenge of any Emacs installation. I think it\u0026rsquo;s a good time to recall Anthony Bourdains thoughts about mise en place.\nMise-en-place is the religion of all good line cooks. Do not mess with a line cooks meez  meaning his setup, his carefully arranged supplies of sea salt, rough-cracked pepper, softened butter, cooking oil, wine, backups, and so on. As a cook, your station, and its condition, its state of readiness, is an extension of your nervous system The universe is in order when your station is set up the way you like it: you know where to find everything with your eyes closed, everything you need during the course of the shift is at the ready at arms reach, your defenses are deployed. If you let your mise-en-place run down, get dirty and disorganized, youll quickly find yourself spinning in place and calling for backup. I worked with a chef who used to step behind the line to a dirty cooks station in the middle of a rush to explain why the offending cook was falling behind. Hed press his palm down on the cutting board, which was littered with peppercorns, spattered sauce, bits of parsley, bread crumbs and the usual flotsam and jetsam that accumulates quickly on a station if not constantly wiped away with a moist side towel. You see this? hed inquire, raising his palm so that the cook could see the bits of dirt and scraps sticking to his chefs palm. Thats what the inside of your head looks like now.\n Anthony Bourdain, from Kitchen Confidential.\nThis an annotated version of my DOOM Emacs configuration. The source files can be found on Github or Sourcehut.\nInstallation The most basic installation of Doom Emacs consists of:\n$ git clone --depth 1 https://github.com/doomemacs/doomemacs ~/.emacs.d $ ~/.emacs.d/bin/doom install Languages Python virtualenv In order to use Python\u0026rsquo;s virtualenv, the virtualenvwrapper.el1 module is used. This is done by adding to packages.el\n(package! virtualenvwrapper) The only configuration I use for this library is setting my virtualenv root location in config.el\n(use-package! virtualenvwrapper) (after! virtualenvwrapper (setq venv-location \u0026#34;~/.virtualenvs/\u0026#34;) ) From a Python project the virtual environment can be selected by using M-x venv-workon.\nBlack formatter To allow formatting of Python blocks in org-mode and elsewhere, add python-black 2 to packages.el.\n(package! python-black) Then we configure it with\n(use-package! python-black :after python :hook (python-mode . python-black-on-save-mode-enable-dwim)) We should install black-machiatto 3 to allow formatting of partial regions.\nGo The following modules must be enabled in init.el:\n(go +lsp) in the lang section lsp in the tools section snippets in the editor section gopls should be installed.\nTemplates To enable support of Go templates, install the lang/web packages by adding to init.el.\n(web +html) This will install the web-mode package4 To specify the sepcific template engine as Go M-x web-mode-set-engine to go.\nRunning doom sync will finish the setup.\nPikchr Support for Pikchr5 is added via the pikchr-mode package. By adding\n(package! pikchr-mode) It is necessary to install the pikchr binary according to the instructions in Pikchr. That page also contains examples.\norg-mode org-agenda To disable completed items to show up in agenda views, the following is set in config.el\n(setq org-agenda-skip-scheduled-if-done t) (setq org-agenda-skip-deadline-if-done t) Key bindings For Doom Emacs we add some additional key-bindings to the org-mode specifically. These bindings go under the local org leader key with the u prefix (which in my case is ) SPC m u. They are\nSPC m u a, archive an org-agenda item using #org-archive-subtree-default r, get a random note using #org-randomnote Pretty bullets The default Doom Emacs org-mode bullets are *. To get \u0026ldquo;pretty\u0026rdquo; bullets you need to add the +pretty flag to init.el:\n(org +pretty ) ; organize your plain life in plain text Optionally, you can specify the bullet\u0026rsquo;s hierarchy glyphs with\n(setq org-superstar-headline-bullets-list \u0026#39;(\u0026#34;\u0026#34; \u0026#34;\u0026#34; \u0026#34;\u0026#34; \u0026#34;\u0026#34; \u0026#34;\u0026#34;) ) UI Prettify symbols Since Emacs 24.46 there is a builtin prettify-symbols-mode. It can be customized by changing prettify-symbols-alist. These strings will be replace by our selection, typically an unicode symbol. In this configuration we use the following:\n(defun my/pretty-symbols () (setq prettify-symbols-alist \u0026#39;((\u0026#34;#+begin_src python\u0026#34; . \u0026#34;\u0026#34;) (\u0026#34;#+begin_src elisp\u0026#34; . \u0026#34;\u0026#34;) (\u0026#34;#+begin_src jupyter-python\u0026#34; . \u0026#34;\u0026#34;) (\u0026#34;#+end_src\u0026#34; . \u0026#34;\u0026#34;) (\u0026#34;#+results:\u0026#34; . \u0026#34;\u0026#34;) (\u0026#34;#+RESULTS:\u0026#34; . \u0026#34;\u0026#34;)))) This will, for instance, replace the beginning of Python org-babel~ blocks with the single symbol. To register the prettify list with each mode we use\n(add-hook \u0026#39;org-mode-hook \u0026#39;my/pretty-symbols) Or we can register the prettify-symbols-mode as a global mode\n(global-prettify-symbols-mode +1) company company is great, but it can get in the way when using on org-mode buffers. To disable add the following:\n(after! org ;; disable auto-complete in org-mode buffers (remove-hook \u0026#39;org-mode-hook #\u0026#39;auto-fill-mode) ;; disable company too (setq company-global-modes \u0026#39;(not org-mode)) ;; ... ) Beacon Beacon highlights the current cursor line after major movements. Especially useful for HDPi screens. Add it on packages.el:\n(package! beacon) And enable the global minor-mode on config.el with:\n;; global beacon minor-mode (use-package! beacon) (after! beacon (beacon-mode 1)) Focus This mode relies on the Focus package that dims regions not on \u0026hellip; focus. Since the package is on MELPA, it can be installed by adding to packages.el:\n(package! focus) Next, require it from config.el\n(use-package! focus) And call it by setting the mode with M-x focus-mode.\nNavigation Treemacs treemacs7 is a great file navigation explorer for Emacs. However, since it has its own iternal concept of project, it needs an external helper to be able to synchronise with other project management tools, such as projectile.\nTo be able to synchronise treemacs and projectile the treemacs-projectile module must be used. It can be activated using\n(use-package treemacs-projectile :after (treemacs projectile)) (after! (treemacs projectile) (treemacs-project-follow-mode 1)) This guarantees that when moving to a buffer of a different projectile project, the treemacs tree will reflect that.\nDirvish Dirvish offers a suitable replacement/enhancement for dired with features such as improved UI and image preview. The only requirement for dirvish is the ls alternative exa It is available from MELPA which means you can add it to packages.el with\n(package! dirvish) and then enable it on config.el with\n(use-package! dirvish) Tools vterm An interesting talk on the advantages of using vterm as the default Emacs terminal emulator can be found can found at EmacsConf 2021 \u0026ldquo;A Tour of vterm\u0026rdquo;. To use vterm as the Emacs shell, the respective section in init.el should be selected:\n(doom! :term ;;eshell ; the elisp shell that works everywhere ;;shell ; simple shell REPL for Emacs ;;term ; basic terminal emulator for Emacs vterm ; the best terminal emulation in Emacs ) We also need to install libvterm, with in macOS can be done with\nbrew install libvterm and in Linux with\n$ sudo apt-get install -y libvterm-dev # Ubuntu $ sudo dnf -y install libvterm # Fedora deadgrep For full-text search, deadgrep8 is used, which leverages ripgrep. To install ripgrep on Linux, run\n$ sudo apt install ripgrep # Ubuntu $ sudo dnf install ripgrep # Fedora Dynamic modules The best place to put dynamic modules in Doom Emacs is inside the actual .doom.d folder. This allows to load modules from Doom\u0026rsquo;s \u0026ldquo;private dir\u0026rdquo;. For instance, for a module called my_module and sub-directory named modules we would create\nmkdir ~/.doom.d/modules and then add to config.el\n(add-to-list \u0026#39;load-path (expand-file-name \u0026#34;modules/my_module\u0026#34; doom-private-dir)) (def-package! my_module :commands (mymodule-foo mymodule-bar)) https://github.com/porterjamesj/virtualenvwrapper.el\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nhttps://github.com/emacs-vault/emacs-python-black\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nhttps://github.com/wbolster/black-macchiato\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nhttps://web-mode.org/\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nhttps://pikchr.org/\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nhttps://www.emacswiki.org/emacs/PrettySymbol\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nhttps://github.com/Alexander-Miller/treemacs\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nhttps://github.com/Wilfred/deadgrep\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"/doom-emacs.html","tags":null,"title":"DOOM Emacs"},{"categories":null,"contents":"A page on Drools.\n","permalink":"/drools.html","tags":null,"title":"Drools"},{"categories":null,"contents":"There are several ways to measure the robustness of a clustering algorithm. Three commonly used metrics are the Dunn index, Davis-Bouldin index and Silhoutte index.\nBut before we start, let\u0026rsquo;s introduce some concepts.\nWe are interested in clustering algorithms for a dataset $\\mathcal{D}$ with $N$ elements in a $n$-dimensional real space, that is:\n$$ \\mathcal{D} = {x_1, x_2, \\ldots, x_N} \\in \\mathbb{R}^p $$\nThe clustering algorithm will create a set $C$ of $K$ distinct disjoint groups from $\\mathcal{D}$ $C={c_1, c_2, \\ldots, c_k}$, such that:\n$$ \\bigcup_{c_k\\in C}c_k=\\mathcal{D} \\ c_k \\bigcap c_l \\neq \\emptyset \\forall k\\neq l $$\nEach group (or cluster) $c_k$, will have a centroid, $\\bar{c}_k$, which is the mean vector of its elements such that:\n$$ \\bar{c}k=\\frac{1}{|c_k|}\\sum{x_i \\in c_k}x_i $$\nWe will also make use of the dataset\u0026rsquo;s mean vector, $\\bar{\\mathcal{D}}$, defined as:\n$$ \\bar{\\mathcal{D}}=\\frac{1}{N}\\sum_{x_i \\in X}x_i $$\nDunn index The Dunn index aims at quantifying the compactness and variance of the clustering. A cluster is considered compact if there is small variance between members of the cluster. This can be calculated using $\\Delta(c_k)$, where\n$$ \\Delta(c_k) = \\max_{x_i, x_j \\in c_k}{d_e(x_i, x_j)} $$\nand $d_e$ is the Euclidian distance defined as:\n$$ d_e=\\sqrt{\\sum_{j=1}^p (x_{ij}-x_{kj})^2}. $$\nA cluster is considered well separated if the cluster are far-apart. This can quantified using\n$$ \\delta(c_k, c_l) = \\min_{x_i \\in c_k}\\min_{x_j\\in c_l}{d_e(x_i, x_j)}. $$\nGiven these quantities, the Dunn index for a set of clusters $C$, $DI(C)$, is then defined by:\n$$ DI(C)=\\frac{\\min_{c_k \\in C}{\\delta(c_k, c_l)}}{\\max_{c_k\\in C}\\Delta(c_k)} $$\nA higher Dunn Index will indicate compact, well-separated clusters, while a lower index will indicate less compact or less well-separated clusters.\nWe can now try to calculate the metric for the dataset we\u0026rsquo;ve created previously. Let\u0026rsquo;s simulate some data and apply the Dunn index from scratch. First, we will create a compact and well-separated dataset using the make_blobs method in scikit-learn. We will create a dataset of $\\mathbb{R}^2$ data (for easier plotting), with three clusters.\nfrom sklearn.datasets import make_blobs X, y = make_blobs(n_samples=1000, centers=3, n_features=2, random_state=23) import pandas as pd from plotnine import * from plotnine.data import * from plotutils import * data = pd.DataFrame(X, columns=[\u0026#34;x1\u0026#34;, \u0026#34;x2\u0026#34;]) data[\u0026#34;y\u0026#34;] = y data[\u0026#34;y\u0026#34;] = data.y.astype(\u0026#39;category\u0026#39;) \u0026lt;ggplot: (8753936888634)\u0026gt; We now cluster the data and we will have, as expected three distinct clusters, plotted below.\nfrom sklearn import cluster k_means = cluster.KMeans(n_clusters=3) k_means.fit(data) y_pred = k_means.predict(data) prediction = pd.concat([data, pd.DataFrame(y_pred, columns=[\u0026#39;pred\u0026#39;])], axis = 1) clus0 = prediction.loc[prediction.pred == 0] clus1 = prediction.loc[prediction.pred == 1] clus2 = prediction.loc[prediction.pred == 2] k_list = [clus0.values, clus1.values,clus2.values] Let\u0026rsquo;s focus now on two of these cluster, let\u0026rsquo;s call them $c_k$ and $c_l$.\nck = k_list[0] cl = k_list[1] We know we have to calculate the distance between the points in $c_k$ and $c_l$. We know that the len(ck)=len(cl)=333 we create\nimport numpy as np values = np.ones([len(ck), len(cl)]) values array([[1., 1., 1., ..., 1., 1., 1.], [1., 1., 1., ..., 1., 1., 1.], [1., 1., 1., ..., 1., 1., 1.], ..., [1., 1., 1., ..., 1., 1., 1.], [1., 1., 1., ..., 1., 1., 1.], [1., 1., 1., ..., 1., 1., 1.]]) For each pair of points, we then get the norm of $x_i-x_j$. For instance, for $i=0\\in c_k$ and $i=1\\in c_l$, we would have:\nvalues[0, 1] = np.linalg.norm(ck[0]-cl[1]) print(ck[0], cl[1]) print(values[0, 1]) [1.76127766 9.39696306 0. 0. ] [ 5.46312794 -3.08938807 1. 1. ] 13.100101521169044 The calculation of $\\delta(c_k, c_l)$ between two clusters $c_k$ and $c_l$ will be defined as follows:\nimport numpy as np def (ck, cl): values = np.ones([len(ck), len(cl)]) for i in range(0, len(ck)): for j in range(0, len(cl)): values[i, j] = np.linalg.norm(ck[i]-cl[j]) return np.min(values) So, for our two clusters above, $\\delta(c_k, c_l)$ will be:\n(ck, cl) 8.70692734880369 Within a single cluster $c_k$, we can calculate $\\Delta(c_k)$ similarly as:\ndef (ci): values = np.zeros([len(ci), len(ci)]) for i in range(0, len(ci)): for j in range(0, len(ci)): values[i, j] = np.linalg.norm(ci[i]-ci[j]) return np.max(values) So, for instance, for our $c_k$ and $c_l$ we would have:\nprint((ck)) print((cl)) 5.8077337425156745 6.173844284636552 We can now define the Dunn index as\ndef dunn(k_list): s = np.ones([len(k_list), len(k_list)]) s = np.zeros([len(k_list), 1]) l_range = list(range(0, len(k_list))) for k in l_range: for l in (l_range[0:k]+l_range[k+1:]): s[k, l] = (k_list[k], k_list[l]) s[k] = (k_list[k]) di = np.min(s)/np.max(s) return di and calculate the Dunn index for our clustered values list as\ndunn(k_list) 0.14867620697065728 Intuitively, we can expect a dataset with less well-defined clusters to have a lower Dunn index. Let\u0026rsquo;s try it. We first generate the new dataset.\nX, y = make_blobs(n_samples=1000, centers=3, n_features=2, cluster_std=10.0, random_state=24) df = pd.DataFrame(X, columns=[\u0026#39;A\u0026#39;, \u0026#39;B\u0026#39;]) k_means = cluster.KMeans(n_clusters=3) k_means.fit(df) #K-means training y_pred = k_means.predict(df) prediction = pd.concat([df,pd.DataFrame(y_pred, columns=[\u0026#39;pred\u0026#39;])], axis = 1) prediction[\u0026#34;pred\u0026#34;] = prediction.pred.astype(\u0026#39;category\u0026#39;) \u0026lt;ggplot: (8753936988097)\u0026gt; clus0 = prediction.loc[prediction.pred == 0] clus1 = prediction.loc[prediction.pred == 1] clus2 = prediction.loc[prediction.pred == 2] k_list = [clus0.values, clus1.values,clus2.values] dunn(k_list) 0.019563892388205984 ","permalink":"/dunn-index.html","tags":null,"title":"Dunn index"},{"categories":null,"contents":"Elisp is the programming language used to program and configure Emacs.\nSome Elisp topics:\nElisp snippets ","permalink":"/elisp.html","tags":null,"title":"Elisp"},{"categories":null,"contents":"Snippets Execute command in shell buffer comint-send-string is the function we\u0026rsquo;re looking for.1\nIt takes a PROCESS and a STRING. You can get the process from the shell buffer, and conveniently the shell function returns the buffer, so you can streamline it all into something like:\n(defun my-server () \u0026#34;SSH to my.server.com in `shell\u0026#39; buffer.\u0026#34; (interactive) (comint-send-string (get-buffer-process (shell)) \u0026#34;ssh my.server.com\\n\u0026#34;)) Where the (shell) call will take care of creating the shell buffer and/or process if necessary.2\nFree variables In some situations, for instance when setting a variable in a DOOM Emacs like\n(setq my/variable \u0026#34;value\u0026#34;) You might get the warning\nWarning: assignment to free variable `er/try-expand-list\u0026#39; This is because set and setq do not declare lexical variables3. The solution is to use4\n(defvar my/variable \u0026#34;value\u0026#34;) Lists to arrays List of lists To convert a list of lists, e.g.\n(defvar mylist ((\u0026#34;1\u0026#34; \u0026#34;a\u0026#34;) (\u0026#34;2\u0026#34; \u0026#34;b\u0026#34;) (\u0026#34;3\u0026#34; \u0026#34;c\u0026#34;))) The method to convert a single element from the list is\n(require \u0026#39;cl) (coerce (nth 0 mylist) \u0026#39;vector) ;; [\u0026#34;1\u0026#34; \u0026#34;a\u0026#34;] We now just need to map this for all the list\n(mapcar (lambda (arg) (coerce arg \u0026#39;vector)) mylist) Reference defcustom You can specify variables using defcustom so that you and others can then use Emacss customize feature to set their values. (You cannot use customize to write function definitions; but you can write defuns in your .emacs file. Indeed, you can write any Lisp expression in your .emacs~ file.)5\nFootnotes SQLite Emacs provides functionality to interact with SQLite databases. For these examples we will use the emacSQL package. To open a database file /tmp/foo.sqlite we issue:\n(defvar db (emacsql-sqlite \u0026#34;~/tmp/foo.sqlite\u0026#34;)) We can then issue SELECT statements with\n;; Query the database for results: (emacsql db [:select [name id] :from people :where (\u0026gt; salary 62000)]) ;; =\u0026gt; ((\u0026#34;Susan\u0026#34; 1001)) shell is built on top of the comint library.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nn.b. if there\u0026rsquo;s an existing one, shell will re-use that.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nOnly let does.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nBut not quote the symbol.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nThe docs for defcustom: https://www.gnu.org/software/emacs/manual/html_node/eintr/defcustom.html\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"/elisp-snippets.html","tags":null,"title":"Elisp snippets"},{"categories":null,"contents":"Notes on Emacs My current flavour/distribution of Emacs is DOOM Emacs spacemacs1 DOOM Emacs2. Yes, DOOM Emacs is my preferred configuration framework at the moment. I have used Spacemacs for a long time, but the performance increase by switching to DOOM Emacs is simply to great to be ignored. Recently, I have also migrated to the Emacs 28 branch which includes Elisp native compilation. It is a thing of beauty in terms of speed.\nThis page refers to broad Emacs base configuration. For specific Emacs recipes check the Emacs cookbook.\nInstallation macOS Many people swear by emacs-mac as the gold standard for macOS Emacs distributions. To install it, using homebrew simply run\n$ brew tap railwaycat/emacsmacport $ brew install --cask emacs-mac-spacemacs-icon Alternatively, you can choose whatever icon you want from here. A good alternative is for instance --with-emacs-big-sur-icon. On Linux you can use my con, if you wish.\nHowever, emacs-mac is, at the time of writing, targetting Emacs 27. If you want to use Emacs 28 (and don\u0026rsquo;t want to build it from source) a good option if emacs-plus. To install it use:\n$ brew install emacs-plus@28 --with-native-comp --with-modern-doom3-icon Fedora At the time of writing3, Fedora only supports, officially, Emacs 27. If you want to try Emacs 28 (and the native compilation feature) you need to install it using\n$ sudo dnf copr enable deathwish/emacs-pgtk-nativecomp and then (if Emacs is already present)\n$ sudo dnf upgrade emacs otherwise run\n$ sudo dnf install emacs Ubuntu For Ubuntu (and other Linux distros, including Fedora) an option is to use the Snap store. The available Snap can install Emacs 28 with native compilation enabled. To use it, simply run\n$ sudo snap install emacs --edge --classic From source In December 2021 the pgtk branch of Emacs was merged into master4. Among other features this adds full Wayland support and better font rendering for the Emacs GUI. This version can be compiled from source with just a few simple steps.\nDependencies To build this version in Linux (I\u0026rsquo;m assuming Ubuntu and variants) you will need the following dependencies:\nautoconf build-essential, provides GCC, make, libc, etc. libgtk-3-dev, Gnome dependencies libgnutls28-dev, provides libgnutls28 libtiff5-dev, libgif-dev, libjpeg-dev, libpng-dev and libxpm-dev provide image support libncurses-dev, provides terminal support texinfo, for Info documentation libxml2-dev for XML support I also wanted to include Emacs\u0026rsquo; native JSON support and Elisp native compilation, so will additionally need:\nlibjansson4, libjansson-dev, provides native JSON support libgccjit0, libgccjit-11-dev, gcc-11 and g++-115 A one-liner to install all dependencies is:\n$ sudo apt update $ sudo apt install build-essential libgtk-3-dev libgnutls28-dev \\ libtiff5-dev libgif-dev libjpeg-dev libpng-dev libxpm-dev \\ libncurses-dev texinfo \\ libxml2-dev \\ jansson4 libjansson-dev \\ libgccjit0 libgccjit-11-dev gcc-11 g++-11 If building on Fedora, the dependency names will naturally be slightly different. In this case we would use\n$ sudo dnf update $ sudo dnf install @development-tools autoconf \\ gtk3-devel gnutls-devel \\ libtiff-devel giflib-devel libjpeg-devel libpng-devel libXpm-devel \\ ncurses-devel texinfo \\ libxml2-devel \\ jansson jansson-devel \\ libgccjit libgccjit-devel Building We can then clone the Emacs repo and run autogen. The steps from now on should be the same for most GNU/Linux distributions.\n$ git clone git://git.sv.gnu.org/emacs.git $ cd emacs $ ./autogen.sh To use native Elisp compilation we will use gcc-115 so before starting the build run:\n$ export CC=/usr/bin/gcc-11 CXX=/usr/bin/gcc-11 On Fedora, if are sure you have gcc 11, simply export\n$ export CC=/usr/bin/gcc CXX=/usr/bin/gcc Emacs supports --with-xxx to enable support for feature xxx, which in our case will be:\n--with-pgtk, native GTK support --with-json, native JSON support --with-native-compilation, native compilation support --with-xml2, XML support --with-modules, support for dynamic modules --with-mailutils, support for mail-utils So we can run the configure script with\n$ ./configure \\ --with-native-compilation \\ --with-json \\ --with-pgtk \\ --with-xml2 \\ --with-modules \\ --with-mailutils We can now start the build and installation (you can choose the approriate number for your machine for the parallel run, in this case 8):\n$ make -j8 $ sudo make install If everything completes successfully you should have Emacs 29 available in /usr/local/bin\n$ emacs --version GNU Emacs 29.0.50 Copyright (C) 2021 Free Software Foundation, Inc. GNU Emacs comes with ABSOLUTELY NO WARRANTY. You may redistribute copies of GNU Emacs under the terms of the GNU General Public License. For more information about these matters, see the file named COPYING. Configuration Annotated DOOM Emacs config Completion I recently move from helm6 to ivy. The main reason is that apparently helm development has stalled.\nThe important stuff? Icon I\u0026rsquo;ve also made a vaporwave Emacs icon which you can find in [https://github.com/ruivieira/emacs-vaporwave-icon]here.\nCursor A cursor should blink, in my opinion. For a history of the blinking cursor see The Forgotten History of the Blinking Cursor. You can instruct Emacs to blink it with the appropriate mode7.\n(blink-cursor-mode 1) Themes An excellent resource for Emacs themes is Peach MELPA. Some selected theme are below.\nHere is a list of good themes for Emacs:\nzenburn ample alect cyberpunk gruvbox The theme that I\u0026rsquo;m currently using is spacemacs-light. It works really well the new org-mode styling. A more detailed rundown of the specific theming can be found at DOOM Emacs.\nElisp The main page for Elisp You can check my solutions for the Euler project in Elisp: Project Euler in Elisp Command line Running org files Dynamic modules Since Emacs 25.1, Emacs can use Dynamic Modules.\nhttps://www.spacemacs.org/\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nhttps://github.com/hlissner/doom-emacs\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nFedora 32/33.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nhttps://mail.gnu.org/archive/html/emacs-devel/2021-12/msg01732.html\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nThis also works with gcc-10.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nThe main Helm maintainer is Thierry Volpiatto.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nhttps://www.emacswiki.org/emacs/NonBlinkingCursor\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"/emacs.html","tags":null,"title":"Emacs"},{"categories":null,"contents":"Ordering JSON by key value Run json-pretty-print-buffer-ordered[^spacemacslayer]\n[^spacemacslayer] If using spacemacs, it requires the Javascript layer\nProjects Creating a projectile project To create a projectile project either use a valid Git repository or create the special .projectile file in root.\nFuzzy search Do a SPC p f, but make sure the projectile project is created.\nSplit screen The standard Emacs keys are valid here (even for Spacemacs).\nSPC w h Indent a block of text Select the block with V and then press Ctrl-x TAB. Then use h or l to move it along\n","permalink":"/emacs-cookbook.html","tags":null,"title":"Emacs cookbook"},{"categories":null,"contents":"An Emacs package for Quarkus.\n","permalink":"/emacs-quarkus.html","tags":null,"title":"Emacs Quarkus"},{"categories":null,"contents":" Root Mean Squared Error R-squared ","permalink":"/error-metrics.html","tags":null,"title":"Error metrics"},{"categories":null,"contents":"Topics Counterfactuals Counterfactuals with Constraint Solvers LIME and the deterministic version, DLIME Fairness Counterfactual Fairness (also how to create counterfactually fair models in Java) Resources TrustyAI Explainability Toolkit1 pre-print, https://arxiv.org/abs/2104.12717 A nice presentation on AI/ML explainability: https://explainml-tutorial.github.io/neurips20 Software omnixai Literature Kakogeorgiou, Ioannis, and Konstantinos Karantzalos. \u0026ldquo;Evaluating Explainable Artificial Intelligence Methods for Multi-label Deep Learning Classification Tasks in Remote Sensing.\u0026rdquo; arXiv preprint arXiv:2104.01375 (2021). Geada, Rob, Tommaso Teofili, Rui Vieira, Rebecca Whitworth and Daniele Zonca. TrustyAI Explainability Toolkit. (2021).\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"/explainability.html","tags":null,"title":"Explainability"},{"categories":null,"contents":"Summary ","permalink":"/extending-junit.html","tags":null,"title":"Extending JUnit"},{"categories":null,"contents":"Techniques The most common tecnhiques for feature scaling are normalisation and standardisation. For the examples, we will use the reference dataframe\nimport pandas as pd import numpy as np import matplotlib.pyplot as plt df = pd.DataFrame({\u0026#39;x\u0026#39;: np.random.rand(100)*10.0, \u0026#39;y\u0026#39;: np.random.rand(100)*2.0}) print(df) x y 0 7.338272 0.963962 1 9.282307 0.799143 2 2.505291 0.664340 3 3.212283 0.137100 4 4.370920 0.383998 .. ... ... 95 1.454787 0.773893 96 3.847065 1.478079 97 4.198221 0.308595 98 9.986268 0.298912 99 4.940190 0.916740 [100 rows x 2 columns] Min-Max scaler A common scaler which transforms the original space between $[A, B]$ to another space $[A^{\\prime}, B^{\\prime}]$. Typically, $[A^{\\prime}, B^{\\prime}]=[0, 1]$. The transformation is:\n$$ x^{\\prime}=\\frac{x-x_{min}}{x_{max}-x_{min}} $$\nThe Min-Max scaler works best when no normality is assumed and it is very sensitive to outliers.\nExample from sklearn.preprocessing import MinMaxScaler scaler = MinMaxScaler() df_scaled = pd.DataFrame(scaler.fit_transform(df), columns=[\u0026#39;x\u0026#39;,\u0026#39;y\u0026#39;]) ","permalink":"/feature-scaling.html","tags":null,"title":"Feature scaling"},{"categories":null,"contents":" The Fedora1 OS A Linux distribution Upgrading To upgrade a Fedora distribution (to, say, version 36), run:\n$ sudo dnf upgrade --refresh $ sudo dnf system-upgrade download --releasever=36 After the download of new packages is finished, run:\nsudo dnf system-upgrade reboot Build tools Run\nsudo dnf install @development-tools https://docs.fedoraproject.org/en-US/project/\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"/fedora.html","tags":null,"title":"Fedora"},{"categories":null,"contents":"A page on Flask things.\n","permalink":"/flask.html","tags":null,"title":"Flask"},{"categories":null,"contents":"A recommended graphical Gemini browser is Lagrange or Amfora for a text-based/terminal browser.\nSyntax Gemini syntax is quite similar to Markdown\nSetup Using the agate server\n","permalink":"/gemini.html","tags":null,"title":"Gemini"},{"categories":null,"contents":"Introduction This is a page containing the topics for Git1.\nGit cookbook https://git-scm.com/\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"/git.html","tags":null,"title":"Git"},{"categories":null,"contents":"Tree Truncate history To truncate Git history, that is discard all commits before a specific one, you can do the following. Assume I have a commit with a certain hash, say abc123, and want to drop all commits before this one. Create a new orphan branch (name not important) called, say, truncated.\n$ git checkout --orphan truncated abc123 And then rebase master (or any other main branch) on top of truncated.\n$ git commit -m \u0026#34;Truncate history\u0026#34; $ git rebase --onto truncated abc123 master Your new branch truncated will be free of master history.\nHooks pre-push The pre-push script is called by git push, when the push actually happens. If the exit status is 0, then the push will proceed, otherwise it will be stopped.\nThe script is supplied with the following arguments:\n$1 -- Name of the remote to which the push is being done (Ex: origin) $2 -- URL to which the push is being done (Ex: https://\u0026lt;host\u0026gt;:\u0026lt;port\u0026gt;/\u0026lt;username\u0026gt;/\u0026lt;project_name\u0026gt;.git) Information about the commits which are being pushed is supplied as lines to the standard input in the form:\n\u0026lt;local_ref\u0026gt; \u0026lt;local_sha1\u0026gt; \u0026lt;remote_ref\u0026gt; \u0026lt;remote_sha1\u0026gt; Sample values:\nlocal_ref = refs/heads/master local_sha1 = 68a07ee4f6af8271dc40caae6cc23f283122ed11 remote_ref = refs/heads/master remote_sha1 = efd4d512f34b11e3cf5c12433bbedd4b1532716f ","permalink":"/git-cookbook.html","tags":null,"title":"Git cookbook"},{"categories":null,"contents":"Notes on GitHub actions.\n","permalink":"/github-actions.html","tags":null,"title":"GitHub actions"},{"categories":null,"contents":"Some notes regarding the Go language. Some topics have graduated to their own page:\nGo cookbook Go resource bundling Go filesystem operations Setup First things first. How to install the Go language in different OSes.\nFedora You can either use dnf directly and simply run\n$ sudo dnf install golang-bin This might not install the latest and greatest. If you want to use the most recent version, download it directly from https://golang.org/doc/install#install. If applicable, delete any previous /usr/local/go directory with\n$ sudo rm -rf /usr/local/go Next, extract the archive file with\n$ sudo tar -C /usr/local -xzf /home/foo/tmp/go-$VERSION.linux-amd64.tar.gz And add /usr/local/go/bin to the $PATH.\nmacOS The best way to perform an attended installation of Go in macOS is to simply download the installer from https://go.dev/dl/ and running it.\nLanguage design Go doesn\u0026rsquo;t have sets The Go language, notoriously, does not have1 some common data structures like sets. There are two main reasons for that:\n+Go does not have generics2 Go relies on you writing your own data structures, generally Go lacks generics, which prevent writing a \u0026hellip; well, generic and efficient set implementation. Also, writing your own (non-generic) set with maps is quite straight-forward.2 The usual structure for a type T is map[T]bool, where the key is the element and the value is just a placeholder. For instance, for a int set, we can add elements:\ns := map[int]bool{1: true, 3: true} s[1] = true // already presenvar t s[2] = true // adds new element Some other techniques for maps replacing sets:\nSet union set_1 := map[int]bool{1:true, 2:true, 3:false} set_2 := map[int]bool{1:false, 2:false, 4:false} set_union := map[int]bool{} for k, _ := range set_1{ set_union[k] = true } for k, _ := range set_2{ set_union[k] = true } fmt.Println(set_union) Set intersection set_1 := map[int]bool{1:true, 2:true, 3:false} set_2 := map[int]bool{1:false, 2:false, 4:false} set_intersection := map[int]bool{} for k,_ := range set_1 { if set_2[k] { set_intersection[k] = true } } fmt.Println(set_intersection) To convert a (map) set to an array:\narray := make([]int, 0) set_1 := map[int]bool{1:true, 2:true, 3:false} for k := range set_1 { array = append(array, k) } fmt.Println(array) CI GitHub A potential workflow for GitHub is to use GitHub Actions for Go. An example workflow file, .github/workflows/test.yml, which runs go test (see Testing in Go) and go vet is:\non: [push, pull_request\\] name: Test jobs: test: strategy: matrix: go-version: [1.14.x, 1.15.x] os: [ubuntu-latest] runs-on: ${{ matrix.os }} steps: - name: Install Go uses: actions/setup-go@v2 with: go-version: ${{ matrix.go-version }} - name: Checkout code uses: actions/checkout@v2 - name: Test run: go test ./... - name: Vet run: go vet ./... Containers Minimal example A minimal example of a Go container configuration for a web server running on port 8080:\n# Start from the latest golang base image FROM golang:latest # Add Maintainer Info LABEL maintainer=\u0026#34;Rui Vieira\u0026#34; # Set the Current Working Directory inside the container WORKDIR /app # Copy go mod and sum files COPY go.mod go.sum ./ # Download all dependencies. Dependencies will be cached if the go.mod and go.sum files are not changed RUN go mod download # Copy the source from the current directory to the Working Directory inside the container COPY . . # Build the Go app RUN go build -o main . # Expose port 8080 to the outside world EXPOSE 8080 # Command to run the executable CMD [\u0026#34;./main\u0026#34;] Reference Conversions How to convert a string to byte array? The conversion is simple:\nb := []byte(\u0026#34;This is a string\u0026#34;) Collections Sort map keys alphabetically If a map contains string keys, i.e. var myMap map[string]T, we must sort the map keys independently. For instance:\nkeys := make([]string, 0) for k, _ := range myMap { keys = append(keys, k) } sort.Strings(keys) for _, k := range keys { fmt.Println(k, myMap[k]) } Check for element If we consider a collection, say, []string collection, the way to check for an element already present is, for instance:\nfunc existsIn(needle string, haystack []string) bool { for _, element := range haystack { if element == needle { return true } } return false } Templates Check if variable empty In a Go template you check if a variable is empty by doing:\n{{if .Items}} \u0026lt;ul\u0026gt; {{range .Items}} \u0026lt;li\u0026gt;{{.Name}}\u0026lt;/li\u0026gt; {{end}} \u0026lt;/ul\u0026gt; {{end}} Looping over a map Looping over the map var data map[string]bool in a Go template:\n{{range $index, $element := .}} {{$index}}: {{$element}} {{end}} Processes Executing external processes Executing an external process and directing input and output to Stdout and Stderr.\ncmd := exec.Command(\u0026#34;ls\u0026#34;, \u0026#34;-1ao\u0026#34;) cmd.Stdout = os.Stdout cmd.Stderr = os.Stderr err := cmd.Run() if err != nil { log.Fatalf(\u0026#34;cmd.Run() failed with %s\\\\n\u0026#34;, err) } Testing in Go Place the tests in your place of choosing, but keep the package declaration. Test functions should be parameterised as (t *testing.T) and start with the prefix Test, for instance:\npackage main func TestFoo(t *testing.T) { value := Foo(5, 5) // ... assertions The test files themselves must have the suffix *_test.go. Call the tests with go test.\nOutput Printing struct keys To print a struct along with its keys, we can use the Printf switch as in the official documentation. That is,\nfmt.Printf(\u0026#34;%+v\\n\u0026#34;, myStruct) Date and time Check if a date is empty If a date is unassigned, the .IsZero() method can be used to check it\na := time.Time{} a.IsZero() // This will be true As of the time of writing, that is Go 1.15.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nGo indeed has generics starting with 1.18.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"/go.html","tags":null,"title":"Go"},{"categories":null,"contents":"Notes on Go filesystem operations.\nCopying files Go does not have an utility method to copy files. We have to rely on writing our own implementation using the reading and writing functionality in other packages. As an example:\npackage main import ( \u0026#34;io\u0026#34; \u0026#34;log\u0026#34; \u0026#34;os\u0026#34; ) func main() { from, err := os.Open(\u0026#34;./foo.txt\u0026#34;) if err != nil { log.Fatal(err) } defer from.Close() to, err := os.OpenFile(\u0026#34;./bar.txt\u0026#34;, os.O_RDWR|os.O_CREATE, 0666) if err != nil { log.Fatal(err) } defer to.Close() _, err = io.Copy(to, from) if err != nil { log.Fatal(err) } } Path operations Basepath To get the basepath of a path string we use the Dir method:\nfilepath.Dir(\u0026#34;/etc/foo/file.txt\u0026#34;) // \u0026#34;/etc/foo\u0026#34; Check if directory exists if _, err := os.Stat(\u0026#34;/etc/foo/\u0026#34;); os.IsNotExist(err) { // do something because it does not exist } Create nested directories Use MkdirAll:\nos.MkdirAll(\u0026#34;/etc/long/nested/path/to/create\u0026#34;, os.ModePerm) ","permalink":"/go-filesystem-operations.html","tags":null,"title":"Go filesystem operations"},{"categories":null,"contents":"Notes on the installation and usage of pkger.\nInstallation done with\ngo get github.com/markbates/pkger/cmd/pkger pkger works by bundling the resources with a code-generated pkg.go. The configuration of assets to be bundled is done by reflection at compile time and not direct configuration. This is done by replacing standard Go file operations with pkger proxy ones, such as:\ntype Pkger interface { Parse(p string) (Path, error) Current() (here.Info, error) Info(p string) (here.Info, error) Create(name string) (File, error) MkdirAll(p string, perm os.FileMode) error Open(name string) (File, error) Stat(name string) (os.FileInfo, error) Walk(p string, wf filepath.WalkFunc) error Remove(name string) error RemoveAll(path string) error } type File interface { Close() error Info() here.Info Name() string Open(name string) (http.File, error) Path() Path Read(p []byte) (int, error) Readdir(count int) ([]os.FileInfo, error) Seek(offset int64, whence int) (int64, error) Stat() (os.FileInfo, error) Write(b []byte) (int, error) } Example Bundling a Go template file.\ntmplFile, _ := pkger.Open(\u0026#34;/templates/page.tmpl\u0026#34;) tmplBytes, _ := ioutil.ReadAll(tmplFile) tmplString := string(tmplBytes) tpl, err := template.New(\u0026#34;page\u0026#34;).Parse(tmplString) _ = tpl.Execute(f, ...) The bundling is simply done by running\npkger and building as usual\ngo build ","permalink":"/go-resource-bundling.html","tags":null,"title":"Go resource bundling"},{"categories":null,"contents":"For features $x_i={x_{i1},\\dots,x_{ip}}$ and $x_j={x_{j1},\\dots,x_{jp}}$, the Gower similarity matrix 1 can be defined as\n$$ S_{\\text{Gower}}(x_i, x_j) = \\frac{\\sum_{k=1}^p s_{ijk}\\delta_{ijk}}{\\sum_{k=1}^p \\delta_{ijk}}. $$\nFor each feature $k=1,\\dots,p$ a score $s_{ijk}$ is calculated. A quantity $\\delta_{ijk}$ is also calculated having possible values ${0,1}$ depending on whether the variables $x_i$ and $x_j$ can be compared or not (/e.g./ if they have different types).\nA special case2 for when no missing values exist can be formulated as the mean of the Gower similarity scores, that is:\n$$ S_{\\text{Gower}}(x_i, x_j) = \\frac{\\sum_{k=1}^p s_{ijk}}{p}. $$\nThe score $s_{ijk}$ calculation will depend on the type of variable and below we will see some examples.\nThis similarity score will take values between $0 \\leq s_{ijk} \\leq 1$ with $0$ representing maximum similarity and $1$ no similarity.\nScoring Numerical variables For numerical variables the score can be calculated as\n$$ s_{ijk} = 1 - \\frac{|x_{ik}-x_{jk}|}{R_k}. $$\nThis is simply a L1 distance between the two values normalised by a quantity $R_k$. The quantity $R_k$ refers to the range of feature (population /or/ sample).\nCategorical variables For categorical variables we will use following score:\n$$ s_{ijk} = 1{x_{ik}=x_{jk}} $$\nThis score will be $1$ if the categories are the same and $0$ if they are not.\nIn reality the score $S_{\\text{Gower}}(x_i, x_j)$ will be a /similarity score/ taking values between $1$ (for equal points) and $0$ for extremely dissimilar points. In order to turn this value into a /distance metric/ we can convert it using (for instance)\n$$ d_{\\text{Gower}} = \\sqrt{1-S_{\\text{Gower}}}. $$\nThis will take values of $1$ for the furthest points and $0$ for the same points.\nExample Here we will use the special case when no missing values exist. A test dataset can be:\nimport pandas as pd df = pd.DataFrame({ \u0026#34;Sex1\u0026#34;: [\u0026#39;M\u0026#39;, \u0026#39;M\u0026#39;, \u0026#39;F\u0026#39;, \u0026#39;F\u0026#39;, \u0026#39;F\u0026#39;, \u0026#39;M\u0026#39;, \u0026#39;M\u0026#39;, \u0026#39;F\u0026#39;, \u0026#39;F\u0026#39;, \u0026#39;F\u0026#39;], \u0026#34;Sex2\u0026#34;: [\u0026#39;M\u0026#39;, \u0026#39;M\u0026#39;, \u0026#39;F\u0026#39;, \u0026#39;F\u0026#39;, \u0026#39;F\u0026#39;, \u0026#39;F\u0026#39;, \u0026#39;F\u0026#39;, \u0026#39;M\u0026#39;, \u0026#39;M\u0026#39;, \u0026#39;M\u0026#39;], \u0026#34;Age1\u0026#34;: [15, 15, 15, 15, 15, 15, 15, 15, 15, 15], \u0026#34;Age2\u0026#34;: [15, 36, 58, 78, 100, 15, 36, 58, 78, 100] }) df Sex1 Sex2 Age1 Age2 0 M M 15 15 1 M M 15 36 2 F F 15 58 3 F F 15 78 4 F F 15 100 5 M F 15 15 6 M F 15 36 7 F M 15 58 8 F M 15 78 9 F M 15 100 For the numerical variable (age) we can define the range as $R_{\\text{age}}=\\left(\\max{\\text{age}}-\\min{\\text{age}}\\right)$.\n_fields = [\u0026#34;Age1\u0026#34;, \u0026#34;Age2\u0026#34;] age_min = df[_fields].min().min() age_max = df[_fields].max().max() R_age = age_max - age_min We can now calculate the score for each numerical field\ndef s_numeric(x1, x2, R): return 1 - abs(x1-x2)/R df[\u0026#39;s_age\u0026#39;] = df.apply(lambda x: s_numeric(x[\u0026#39;Age1\u0026#39;], x[\u0026#39;Age2\u0026#39;], R_age), axis=1) df Sex1 Sex2 Age1 Age2 s_age 0 M M 15 15 1.000000 1 M M 15 36 0.752941 2 F F 15 58 0.494118 3 F F 15 78 0.258824 4 F F 15 100 0.000000 5 M F 15 15 1.000000 6 M F 15 36 0.752941 7 F M 15 58 0.494118 8 F M 15 78 0.258824 9 F M 15 100 0.000000 For categorical variables we can define the following score function:\ndef s_categorical(x1, x2): return 1 if x1==x2 else 0 df[\u0026#39;s_sex\u0026#39;] = df.apply(lambda x: s_categorical(x[\u0026#39;Sex1\u0026#39;], x[\u0026#39;Sex2\u0026#39;]), axis=1) df Sex1 Sex2 Age1 Age2 s_age s_sex 0 M M 15 15 1.000000 1 1 M M 15 36 0.752941 1 2 F F 15 58 0.494118 1 3 F F 15 78 0.258824 1 4 F F 15 100 0.000000 1 5 M F 15 15 1.000000 0 6 M F 15 36 0.752941 0 7 F M 15 58 0.494118 0 8 F M 15 78 0.258824 0 9 F M 15 100 0.000000 0 We can now calculate the final score using\nimport math df[\u0026#39;s\u0026#39;] = df.apply(lambda x: (x[\u0026#39;s_age\u0026#39;] + x[\u0026#39;s_sex\u0026#39;])/2.0, axis=1) df[\u0026#39;d\u0026#39;] = df.apply(lambda x: math.sqrt(1.0 - x[\u0026#39;s\u0026#39;]), axis=1) df Sex1 Sex2 Age1 Age2 s_age s_sex s d 0 M M 15 15 1.000000 1 1.000000 0.000000 1 M M 15 36 0.752941 1 0.876471 0.351468 2 F F 15 58 0.494118 1 0.747059 0.502933 3 F F 15 78 0.258824 1 0.629412 0.608760 4 F F 15 100 0.000000 1 0.500000 0.707107 5 M F 15 15 1.000000 0 0.500000 0.707107 6 M F 15 36 0.752941 0 0.376471 0.789639 7 F M 15 58 0.494118 0 0.247059 0.867722 8 F M 15 78 0.258824 0 0.129412 0.933053 9 F M 15 100 0.000000 0 0.000000 1.000000 Range impact Varying bounds Let\u0026rsquo;s visualise how the choice of range can affect the scoring, if can set it arbitrarily. First let\u0026rsquo;s pick two random points, $x_1=(30, M)$ and $x_2=(35, F)$.\nWe will vary the bounds from a $15\\leq x_{min}\u0026lt;30$ and $35\u0026lt; x_{max} \\leq 100$.\ndef score(x1, x2, R): s_0 = 1-abs(x1[0]-x2[0])/R s_1 = 1 if x1[1]==x2[1] else 0 return (s_0 + s_1)/2.0 def distance(s): return math.sqrt(1.0-s) import numpy as np x1 = (30, \u0026#39;M\u0026#39;) x2 = (35, \u0026#39;F\u0026#39;) bmin = np.linspace(15, 30, num=1000) bmax = np.linspace(36, 100, num=1000) scores_min = [distance(score(x1, x2, 100-bm)) for bm in bmin] scores_max = [distance(score(x1, x2, bm-15)) for bm in bmax] Let\u0026rsquo;s try with more separated points\nx1 = (16, \u0026#39;M\u0026#39;) x2 = (90, \u0026#39;F\u0026#39;) bmin = np.linspace(15, 16, num=1000, endpoint=False) bmax = np.linspace(91, 100, num=1000) scores_min = [distance(score(x1, x2, 100-bm)) for bm in bmin] scores_max = [distance(score(x1, x2, bm-16)) for bm in bmax] Varying range directly We will now try to see how the distance between two point comparisons (very close, very far) changes when varying the range directly. We will choose two sets of points, $x_1=(1000, M), x_2=(1001, F)$ and $x_1=(500, M), x_2=(50000, F)$. The range will vary between\n$$ \\max(x_1, x_2)-\\min(x_1, x_2)\u0026lt;R\u0026lt;100000. $$\nWe are also interested on the weight the categorical variable will have on the final distance with varying bounds, so we will also calculate them for an alternative $x_2\u0026rsquo;=(1001, M)$ anf $x_2\u0026rsquo;=(50000, M)$.\nFor the first set of points we will have:\nx1 = (1000.0, \u0026#39;M\u0026#39;) x2 = (1001.0, \u0026#39;F\u0026#39;) MAX_RANGE = 100000 R = np.linspace(max(x1[0], x2[0])-min(x1[0], x2[0]), MAX_RANGE, num=100000) distances_M = [distance(score(x1, x2, i)) for i in R] distances_F = [distance(score(x1, (x2[0], \u0026#39;M\u0026#39;), i)) for i in R] And for far away points we will have:\nx1 = (500.0, \u0026#39;M\u0026#39;) x2 = (50000.0, \u0026#39;F\u0026#39;) MAX_RANGE = 100000 R = np.linspace(max(x1[0], x2[0])-min(x1[0], x2[0]), MAX_RANGE, num=100000) distances_M = [distance(score(x1, x2, i)) for i in R] distances_F = [distance(score(x1, (x2[0], \u0026#39;M\u0026#39;), i)) for i in R] Categorical impact Predictably, in the scenario where we calculate the mean of the Gower distances, for a point $x$ with $p$ features, $x=(x_{1},\\dots,x_{p})$, the contribution to the final distance of a categorial variable will be either $0$ or $1/p$, regardless of the range.\nMissing range For the previous examples the range $R$ was available, but how to calculate the mixed distance when the numerical range is absent?\nA possible way is to use scale each feature using unit scaling:\n$$ f_u(x) = \\frac{x}{||x||} $$\nWe will visualise how a difference varying from $-1000 \\leq \\delta \\leq 1000$ varies with the $f_u(\\delta)$ transformation.\ndef f_unit(x): return np.exp(x)/(np.exp(x)+1.0) def ilogit(eta): return 1.0 - 1.0/(np.exp(eta)+1) delta = np.linspace(-10, 10, num=20000) transformed = [ilogit(abs(x)) for x in delta] cite:gower1971general Gower, J. C. (1971). A general coefficient of similarity and some of its properties. Biometrics, (), 857\u0026ndash;871.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nThis is for instance the case we deal with in the Counterfactuals with Constraint Solvers.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"/gower-distance.html","tags":null,"title":"Gower distance"},{"categories":null,"contents":"Installation macOS You can install GPG on macOS using:\n$ brew install gpg Workflow Getting existing key To get an existing key id use:\ngpg --list-secret-keys --keyid-format LONG The key id will be available at\ngpg --list-secret-keys --keyid-format LONG /home/foo/.gnupg/pubring.gpg ------------------------------ sec rsa2048/7FFFC09ACAC05FD0 2017-06-02 [SC] [expires: 2019-06-02] 5538B0F643277336BA7F0E457FFFC09ACAC05FD0 uid [ultimate] foo \u0026lt;foo@[example.com](http://realaddress.com)\u0026gt; ssb rsa2048/95E8A289DFE77A84 2017-06-02 [E] [expires: 2019-06-02] The key id we want, in this case is 7FFFC09ACAC05FD0.\nTroubleshooting macOS Can\u0026rsquo;t access keychain from UI If a program cannot access the keychain from the UI, probably there\u0026rsquo;s some problem in prompting you for the passphrase. You can install, for instance pinentry to solve this. Install it with\n$ brew install pinentry-mac and then register pinentry as the passphrase input option:\n$ echo \u0026#34;pinentry-program /usr/local/bin/pinentry-mac\u0026#34; \u0026gt;\u0026gt; ~/.gnupg/gpg-agent.conf ","permalink":"/gpg.html","tags":null,"title":"GPG"},{"categories":null,"contents":"Missing headers On macOS, if you find missing headers when installing polyglot packages, try:\n$ export CPATH=`xcrun --show-sdk-path`/usr/include ","permalink":"/graalvm.html","tags":null,"title":"GraalVM"},{"categories":null,"contents":"Page on Grad-CAM.\n","permalink":"/grad-cam.html","tags":null,"title":"Grad-CAM"},{"categories":null,"contents":" Hill-climbing optimisation ","permalink":"/gradient-free-optimisation.html","tags":null,"title":"Gradient-free optimisation"},{"categories":null,"contents":"I have been working in a new library called gulp which you can find on https://github.com/ruivieira/gulp.\nOn the project\u0026rsquo;s page there are some usage examples but I will try to summarise the main points here.\nThe purpose of this library is to facilitate the parallel development of R and Java code, using rJava as the bridge. Creating bindings in rJava is quite simple, the tricky part of the process (in my opinion) being the maintenance of the bindings (usually done by hand) when refactoring your code.\nAs an example, let\u0026rsquo;s assume you have the following Java class:\n@ExportClassReference(value=\u0026#34;test\u0026#34;) public class Test { // Java code } That you wish to call from R.\n","permalink":"/gulp.html","tags":null,"title":"gulp"},{"categories":null,"contents":"Global maximum Let\u0026rsquo;s try it with the function\n$$ f(x,y) = e^{-\\left(x^2+y^2\\right)} $$\nimport numpy as np import matplotlib.pyplot as plt from plotutils import * x = np.linspace(-2.0, 2.0, 1000) y = np.linspace(-2.0, 2.0, 1000) X, Y = np.meshgrid(x, y) Z = np.exp(-(X ** 2 + Y ** 2)) fig, ax = plt.subplots(1, 1) cp = ax.contourf(X, Y, Z, cmap=cmaps[1]) ax.set_title(\u0026#34;f(x,y)\u0026#34;) ax.set_xlabel(\u0026#34;x\u0026#34;) ax.set_ylabel(\u0026#34;y\u0026#34;) plt.show() from gradient_free_optimizers import HillClimbingOptimizer search_space = { \u0026#34;x\u0026#34;: x, \u0026#34;y\u0026#34;: y, } opt = HillClimbingOptimizer(search_space) def f(pos): x = pos[\u0026#34;x\u0026#34;] y = pos[\u0026#34;y\u0026#34;] z = np.exp(-(x**2 + y**2)) return z result = opt.search(f, n_iter=30000, verbosity=[\u0026#39;print_times\u0026#39;]) Evaluation time : 0.9117169380187988 sec [28.97 %] Optimization time : 2.235219955444336 sec [71.03 %] Iteration time : 3.1469368934631348 sec [9533.08 iter/sec] opt.best_para {'x': -0.002002002002001957, 'y': 0.002002002002002179} Local maximum Let\u0026rsquo;s try it with the function\n$$ f(x,y) = e^{-\\left(x^2+y^2\\right)}+2e^{-\\left((x-1.7)^2+(y-1.7)^2\\right)} $$\nx = np.linspace(-1.0, 3.0, 1000) y = np.linspace(-1.0, 3.0, 1000) X, Y = np.meshgrid(x, y) Z = np.exp(-(X**2 + Y**2))+2*np.exp(-((X-1.7)**2+(Y-1.7)**2)) fig,ax=plt.subplots(1,1) cp = ax.contourf(X, Y, Z, cmap=cmaps[1]) ax.set_title(\u0026#39;f(x,y)\u0026#39;) ax.set_xlabel(\u0026#39;x\u0026#39;) ax.set_ylabel(\u0026#39;y\u0026#39;) plt.show() opt = HillClimbingOptimizer(search_space) def f(pos): x = pos[\u0026#34;x\u0026#34;] y = pos[\u0026#34;y\u0026#34;] z = np.exp(-(x**2 + y**2))+2*np.exp(-((x-1.7)**2+(y-1.7)**2)) return z result = opt.search(f, n_iter=30000, verbosity=[\u0026#39;print_times\u0026#39;]) Evaluation time : 0.9092590808868408 sec [29.73 %] Optimization time : 2.148698091506958 sec [70.27 %] Iteration time : 3.057957172393799 sec [9810.47 iter/sec] opt.best_para {'x': 1.6956956956956954, 'y': 1.6956956956956954} ","permalink":"/hill-climbing-optimisation.html","tags":null,"title":"Hill-climbing optimisation"},{"categories":null,"contents":"HTML Tricks Meters A meter element is availble natively for HTML.\n\u0026lt;label for=\u0026#34;value1\u0026#34;\u0026gt;Low\u0026lt;/label\u0026gt; \u0026lt;meter id=\u0026#34;value1\u0026#34; min=\u0026#34;0\u0026#34; max=\u0026#34;100\u0026#34; low=\u0026#34;30\u0026#34; high=\u0026#34;75\u0026#34; optimum=\u0026#34;80\u0026#34; value=\u0026#34;25\u0026#34;\u0026gt;\u0026lt;/meter\u0026gt; \u0026lt;label for=\u0026#34;value2\u0026#34;\u0026gt;Medium\u0026lt;/label\u0026gt; \u0026lt;meter id=\u0026#34;value2\u0026#34; min=\u0026#34;0\u0026#34; max=\u0026#34;100\u0026#34; low=\u0026#34;30\u0026#34; high=\u0026#34;75\u0026#34; optimum=\u0026#34;80\u0026#34; value=\u0026#34;50\u0026#34;\u0026gt;\u0026lt;/meter\u0026gt; \u0026lt;label for=\u0026#34;value3\u0026#34;\u0026gt;High\u0026lt;/label\u0026gt; \u0026lt;meter id=\u0026#34;value3\u0026#34; min=\u0026#34;0\u0026#34; max=\u0026#34;100\u0026#34; low=\u0026#34;30\u0026#34; high=\u0026#34;75\u0026#34; optimum=\u0026#34;80\u0026#34; value=\u0026#34;80\u0026#34;\u0026gt;\u0026lt;/meter\u0026gt; Low Medium High Ordered list start Change the starting point of an ordered list.\n\u0026lt;ol start=\u0026#34;11\u0026#34;\u0026gt; \u0026lt;li\u0026gt;Eleven\u0026lt;/li\u0026gt; \u0026lt;li\u0026gt;Twelve\u0026lt;/li\u0026gt; \u0026lt;li\u0026gt;Thirteen\u0026lt;/li\u0026gt; \u0026lt;li\u0026gt;Fourteen\u0026lt;/li\u0026gt; \u0026lt;/ol\u0026gt; Eleven Twelve Thirteen Fourteen HTML Native Search \u0026lt;div class=\u0026#34;wrapper\u0026#34;\u0026gt; \u0026lt;h1\u0026gt;HTML native search\u0026lt;/h1\u0026gt; \u0026lt;input list=\u0026#34;items\u0026#34;\u0026gt; \u0026lt;datalist id=\u0026#34;items\u0026#34;\u0026gt; \u0026lt;option value=\u0026#34;Rui\u0026#34;\u0026gt; \u0026lt;option value=\u0026#34;Vieira\u0026#34;\u0026gt; \u0026lt;option value=\u0026#34;ruivieira.dev\u0026#34;\u0026gt; \u0026lt;option value=\u0026#34;HTML\u0026#34;\u0026gt; \u0026lt;/datalist\u0026gt; \u0026lt;/div\u0026gt; Native HTML Search Fieldset element \u0026lt;form\u0026gt; \u0026lt;fieldset\u0026gt; \u0026lt;legend\u0026gt;Best editor\u0026lt;/legend\u0026gt; \u0026lt;input type=\u0026#34;radio\u0026#34; id=\u0026#34;emacs\u0026#34; name=\u0026#34;editor\u0026#34;\u0026gt; \u0026lt;label for=\u0026#34;emacs\u0026#34;\u0026gt;Emacs\u0026lt;/label\u0026gt;\u0026lt;br/\u0026gt; \u0026lt;input type=\u0026#34;radio\u0026#34; id=\u0026#34;vim\u0026#34; name=\u0026#34;editor\u0026#34;\u0026gt; \u0026lt;label for=\u0026#34;vim\u0026#34;\u0026gt;Vim\u0026lt;/label\u0026gt;\u0026lt;br/\u0026gt; \u0026lt;input type=\u0026#34;radio\u0026#34; id=\u0026#34;nano\u0026#34; name=\u0026#34;editor\u0026#34;\u0026gt; \u0026lt;label for=\u0026#34;nano\u0026#34;\u0026gt;nano\u0026lt;/label\u0026gt; \u0026lt;/fieldset\u0026gt; \u0026lt;/form\u0026gt; Best editor \u0026lt;input type=\u0026quot;radio\u0026quot; id=\u0026quot;emacs\u0026quot; name=\u0026quot;editor\u0026quot;\u0026gt; \u0026lt;label for=\u0026quot;emacs\u0026quot;\u0026gt;Emacs\u0026lt;/label\u0026gt;\u0026lt;br/\u0026gt; \u0026lt;input type=\u0026quot;radio\u0026quot; id=\u0026quot;vim\u0026quot; name=\u0026quot;editor\u0026quot;\u0026gt; \u0026lt;label for=\u0026quot;vim\u0026quot;\u0026gt;Vim\u0026lt;/label\u0026gt;\u0026lt;br/\u0026gt; \u0026lt;input type=\u0026quot;radio\u0026quot; id=\u0026quot;nano\u0026quot; name=\u0026quot;editor\u0026quot;\u0026gt; \u0026lt;label for=\u0026quot;nano\u0026quot;\u0026gt;nano\u0026lt;/label\u0026gt; Spellchecking \u0026lt;label for=\u0026#34;input1\u0026#34;\u0026gt;spellcheck=\u0026#34;true\u0026#34;\u0026lt;/label\u0026gt; \u0026lt;input type=\u0026#34;text\u0026#34; id=\u0026#34;input1\u0026#34; spellcheck=\u0026#34;true\u0026#34;\u0026gt; \u0026lt;label for=\u0026#34;input2\u0026#34;\u0026gt;spellcheck=\u0026#34;false\u0026#34;\u0026lt;/label\u0026gt; \u0026lt;input type=\u0026#34;text\u0026#34; id=\u0026#34;input2\u0026#34; spellcheck=\u0026#34;false\u0026#34;\u0026gt; spellcheck=\u0026ldquo;true\u0026rdquo; spellcheck=\u0026ldquo;false\u0026rdquo; Sliders \u0026lt;label for=\u0026#34;volume\u0026#34;\u0026gt;Volume (goes to 11): \u0026lt;/label\u0026gt; \u0026lt;input type=\u0026#34;range\u0026#34; id=\u0026#34;volume\u0026#34; name=\u0026#34;volume\u0026#34; min=\u0026#34;0\u0026#34; max=\u0026#34;11\u0026#34;\u0026gt; Volume (goes to 11): Details \u0026lt;details\u0026gt; \u0026lt;summary\u0026gt; Spoiler \u0026lt;/summary\u0026gt; \u0026lt;p\u0026gt; Keyser Sze is a myth. \u0026lt;/p\u0026gt; \u0026lt;/details\u0026gt; Spoiler \u0026lt;p\u0026gt; Keyser Sze is a myth. \u0026lt;/p\u0026gt; Mark \u0026lt;p\u0026gt;This is an \u0026lt;mark\u0026gt;example with highlight\u0026lt;/mark\u0026gt; since it\u0026#39;s so important\u0026lt;/p\u0026gt; This is an example with highlight since it's so important\n","permalink":"/html.html","tags":null,"title":"HTML"},{"categories":null,"contents":"Templates Special pages By default, the type for a piece of content is inherited from the the contents section. So, the file you create for content at content/posts/my-post.md automatically has a type of posts. However, you may want to keep my-post.md within that section because you want to rely on Hugos default behavior to render the page to yoursite.com/posts/my-post, but you want it to render according to a different layout. In this case, you can specify a type for the content that overrides the default behavior. Types are always singular.\nYou can then put specific layouts in a layout folder of the same name as the type (hence why it works with mylayout). You are telling Hugo that your About page, while living inside the root content section, is of the specific mylayout type. Further, you could even specify a layout that Hugo should use to render your About page:\n+++ date = \u0026#34;2017-04-17T11:01:21-04:00\u0026#34; draft = false title = \u0026#34;About\u0026#34; type = \u0026#34;mylayout\u0026#34; layout = \u0026#34;speciallayout\u0026#34; +++ In this example, Hugo will render the page according to what you create in ./themes/mytheme/layouts/mylayout/speciallayout.html. If you do not specify the layout, Hugo then looks for the the next layout in the lookup order.\nLooping with indices To loop a collection along with the index you can use:\n{{range $index, $element := .collection}} index:{{ $index }} name:{{ $collection.name }} {{ end }} ","permalink":"/hugo.html","tags":null,"title":"Hugo"},{"categories":null,"contents":"Stardate 96893.29.\nYou are the USS Euler\u0026rsquo;s Science Officer at a moment when the computer graphical displays and voice systems went down. You only have enough deuterium for a short travel and need to find the nearest star system. This is not a simple matter of looking at a chart. You have multiple dimensions in which you can travel. In a bid for galactic peace, the Federation mandated that both Emacs and Vim should be installed in all computers. You open your favourite editor and, fortunately, know exactly how to formulate the solution to your problem: a $d$-dimensional nearest neighbour algorithm.\nGiven a dataset $\\mathcal{D}$ of $n$ points in a space $X$ we want to be able to tell which are the closest point to a query point $q \\in X$, preferably in a way which is computationally cheaper than brute force methods (e.g. iterating through all of the points) which typically solve this problem in $\\mathcal{O}(dn)$ 1.\n$X$ could have $d$ dimensions (that is $\\mathcal{D} \\subset X : \\mathbb{R}^d$) and we define closest using2 Minkowski distance metrics, that is:\n$$ L_m = \\left(\\sum_{i=1}^d |p_i - q_i|^m\\right)^{\\frac{1}{m}},\\qquad p,q \\in X : \\mathbb{R}^d. $$\nA potential solution for this problem would be to use kd-trees, which for low dimension scenarios provide $\\mathcal{O}(\\log n)$ query times 3. However, as the number of dimensions increase (as quickly as $d\u0026gt;2$) the query times also increase as $2^d$.\nThe case can be made then for approximate nearest neighbour (NN) algorithms and that\u0026rsquo;s precisely what we will discuss here, namely the Balanced Box-Decomposition Tree (BBD, 1). The definition of approximate NN for a query point $q$ can be given as\n$$ \\text{dist}(p, q) \\leq (1+\\epsilon)\\text{dist}(p^{\\star},q),\\qquad \\epsilon \u0026gt; 0, $$\nwhere $p$ is the approximate NN and $p^{\\star}$ is the true NN. Let\u0026rsquo;s consider, for the sake of visualisation, a small two dimensional dataset $\\mathcal{D} \\to \\mathbb{R}^2$ as shown in Figure 1.\nFigure 1. A small test dataset in $\\mathbb{R}^2,n=7$.\nSpace decomposition BBD trees belong to the category of hierarchical space decomposition trees. In BBD trees, specifically, space is divided in $d$-dimensional rectangles and cells. Cells can either represent another $d$-dimensional rectangle or the intersection of two rectangles (one, the outer box fully enclosing the other, the inner box). Another important distinction of BBD trees is that rectangle\u0026rsquo;s size (in this context, the largest length in all of the $d$ dimensions) is bounded by a constant value. The space decomposition must follow an additional rule which is boxes must be sticky. If we consider a inner box $[x_{inner}, y_{inner}]$ contained in a outer box $[x_{outer}, y_{outer}]$, such that\n$$ [x_{inner}, y_{inner}] \\subseteq [x_{outer}, y_{outer}], $$\nthen, considering $w = y_{inner} - x_{inner}$, the box is considered sticky if either\n$$ \\begin{aligned} x_{inner}-x_{outer} = 0 \u0026amp;\\lor x_{inner}-x_{outer} \\nleq w \\\\ y_{outer}-y_{inner} = 0 \u0026amp;\\lor y_{outer}-y_{inner} \\nleq w. \\end{aligned} $$\nAn illustration of the stickiness concept can viewed in the diagram below.\nFigure 2. Visualisation of the \u0026ldquo;stickiness\u0026rdquo; criteria for $\\mathbb{R}^2$ rectangles.\nStickiness provides some important geometric properties to the space decomposition which will be discussed further on. The actual process of space decomposition will produce a tree of nodes, each with an associated $d$-dimensional rectangle enclosing a set of points. Each node will be further decomposed into children nodes, containing a region of space with a subset of the parent\u0026rsquo;s data points. If a node has no children it will be called a leaf node. The division process can occur either by means of:\na fair split, this is done by partitioning the space with an hyperplane, resulting in a low and high children nodes a shrink, splitting the box into a inner box (the inner child) and a outer box (the outer child). Figure 3. \u0026ldquo;Fair split\u0026rdquo; and \u0026ldquo;shrinking\u0026rdquo; division strategies example in $\\mathbb{R}^2$ with respective high/low and outer/inner children.\nThe initial node of the tree, the root node, will include all the dataset points, $\\mathcal{D}$. In the Figure 4 we can see a representation of the root node for the dataset presented above. We can see the node boundaries in dashed red lines as well as the node\u0026rsquo;s center, marked as $\\mu_{root}$.\nFigure 4. Associated cell for the BBD-tree root node for the example dataset. Node boundaries in red and node centre labelled as $\\mu_{root}$.\nThe actual method to calculate the division can either be based on the midpoint algorithm or the middle interval algorithm. The method used for these examples is the latter, for which more details can be found in 4. The next step is to divide the space according to the previously mentioned rules. As an example, we can see the root node\u0026rsquo;s respective children in Figure 5.\nFigure 5. BBD-tree root node\u0026rsquo;s lower (left) and upper (right) children. Node boundaries in red and centres labelled with a red cross.\nThis process is repeated until the child nodes are leaves and cannot be divided anymore. To better visualise the construction process it would be helpful to have a larger tree, so we will now consider still the 2-dimensional case, but now with a larger dataset (Figure 6), consisting of 2000 samples in total, each half from a bivariate Gaussian distribution:\n$$ \\begin{aligned} \\text{X}_1 \u0026amp;\\sim \\mathcal{N}([0,0], \\mathbf{I}) \\\\ \\text{X}_2 \u0026amp;\\sim \\mathcal{N}([3, 3], \\mathbf{I}). \\end{aligned} $$\nFigure 6. Larger example dataset in $\\mathbb{R}^2$ consisting of a realisation of $n=2000$ from two bivariate Gaussian distributions centred in $\\mu_1=(0,0)$ and $\\mu_2=(3,3)$ and with $\\Sigma=\\mathbf{I}$.\nWith this larger dataset, we have enough points to illustrate the tree node building. This time, we will start from the root node and always follow either the \u0026ldquo;lower\u0026rdquo; nodes or the \u0026ldquo;upper\u0026rdquo; nodes (as show in Figure 7). We can clearly see the cells getting smaller, until finally we have a single point included (i.e. a leaf node).\nFigure 7. BBD-tree node building process for the bivariate dataset. On the left we traverse the upper tree nodes and on the right the lower tree nodes.\nThis division process illustrates an important property of BBD-trees. Although other space decomposition algorithms (such as kd-trees) display a geometric reduction of number of points enclosed in each cell, methods such as the BBD-tree, which impose constraints on the cell\u0026rsquo;s size aspect ratio as stated before, display not only a geometric reduction in the number of points, but also in the cell\u0026rsquo;s size as well. The construction cost of a BBD-tree is $\\mathcal{O}(dn \\log n)$ and the tree itself will have $\\mathcal{O}(n)$ nodes and $\\mathcal{O}(\\log n)$ height.\nTree querying Now that we have successfully constructed a BBD-tree, we want to actually find the (approximate) nearest neighbour of an arbitrary query point $q$ (Figure 8).\nFigure 8. Query point $q$ (red) for the bivariate dataset.\nThe first step consists in descending the tree in order to locate the smallest cell containing the query point $q$. This process is illustrated for the bivariate data in Figure 9.\nFigure 9. BBD-tree descent to locate the smallest cell containing $q$ (red).\nOnce the cell has been located, we proceed to enumerate all the leaf nodes contained by it and calculate our distance metric $L_2$ in this case) between the query point $q$ and the leaf nodes, eventually declaring the point with the smallest $L_2$ as the aproximate NN. BBD-trees provide strong guarantees that the ANN will be located within this cell and not in a neighbouring cell. In Figure 10 we zoomed in the smallest cell containing $q$ and show the associated calculated $L_2$ distance for each node.\nFigure 10. $L_2$ distance between leaf nodes and the query point $q$ inside the smallest cell containing $q$.\nAn important property of BBD-trees is that the tree structure does not need to be recalculated if we change either $\\epsilon$ or if we decide to use another $L_m$ distance metric 1. The query time for a point $q$ in a BBD-tree is $\\mathcal{O}(\\log n)$. For comparison, if you recall, the query time for a brute force method is typically $\\mathcal{O}(dn)$.\nFiltering and k-NN Great. Now that you solved the USS Euler\u0026rsquo;s problem, you want to make a suggestion to the federation. Where to place several star-bases and how to divide the system\u0026rsquo;s coverage between them. An immediate generalisation of this method is easily applicable to the problem of clustering. Note that, at the moment, we are not concerned with determining the \u0026ldquo;best\u0026rdquo; clusters for our data5. Given a set of points $Z = {z_1, z_2, \\dots, z_n}$, we are concerned now in partitioning the data in clusters centred in each of the $Z$ points. A way of looking at this, is that we are building, for each point $z_n$ a Voronoi cell $V(z_n)$. This is achieved by a method called filtering. Filtering, in general terms, works by walking the tree with the list of candidate centres ($Z$) and pruning points from the candidate list as we move down. We will denote an arbitrary node as $n$, $z^{\\star}_w$ and $n_w$ respectively as the candidate and the node weight, $z^{\\star}_n$ and $n_n$ as the candidate and node count. The algorithm steps, as detailed in 6, are detailed below:\nFilter($n$, $Z$) { $C \\leftarrow n.cell$ if ($n$ is a leaf) { $z^{\\star} \\leftarrow$ the closest point in $Z$ to $n.point$ $z^{\\star}_w \\leftarrow z^{\\star}_w + n.point$ $z^{\\star}_n \\leftarrow z^{\\star}_n + 1\\qquad$ } *else { $z^{\\star} \\leftarrow$ the closest point in $Z$ to $C$\u0026rsquo;s midpoint for each ($z \\in Z \\setminus {z^{\\star}}$) { if ($z.isFarther(z^{\\star},C)$) { $Z \\leftarrow Z \\setminus {z}$ } if ($|Z|=1$) { $z^{\\star}_w \\leftarrow z^{\\star}_w + n_w$ $z^{\\star}_n \\leftarrow z^{\\star}_n + n_n$ } else { Filter($n_{left}, Z$) Filter($n_{right}, Z$) } } To illustrate the assignment of data points to the centres, we will consider the previous bivariate Gaussian data along with two centres, $z_1 = {0,0}$ and $z_2 = {3, 3}$. Figure 11 shows the process of splitting the dataset $\\mathcal{D}$ into two clusters, namely the subsets of data points closer to $z_1$ or $z_2$.\nFigure 11. Assignment of points in $\\mathcal{D}$ to $Z$. Data points coloured according to the assigned center. Lines represent the distance from the cells midpoint to $Z$.\nWe can see in Figure 12 the final cluster assignment of the data points. With a $\\mathbb{R}^2$ dataset and only two centres the organisation of points follows a simple perpendicular bisection of the segment connecting the centres, as expected.\nFigure 12. Final $\\mathcal{D}$ point assignment to clusters centred in $z_1$ and $z_2$.\nIn Figure 13 we can see more clearly the dataset clusters changing when center $z_1$ is moving around the plane. BBD-trees can play an important role in improving $k$-means performance, as described in 6.\nFigure 13. Dynamic assignment of points to a cluster using a BBD-tree.\nThis concludes a (short) introduction to BBD-trees, I hope you enjoyed it. If you have any comments or suggestions, please let me know at Mastodon.\nArya, S., Mount, D. M., Netanyahu, N. S., Silverman, R., \u0026amp; Wu, A. Y. (1998). An optimal algorithm for approximate nearest neighbor searching fixed dimensions. Journal of the ACM. https://doi.org/10.1145/293347.293348\u0026#160;\u0026#x21a9;\u0026#xfe0e;\u0026#160;\u0026#x21a9;\u0026#xfe0e;\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nThe $L_m$ distance may be pre-computed in this method to avoid recalculation for each query.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nFriedman, J. H., \u0026amp; Bentley, J. L. (1977). RA Finkel. An algorithm for finding best matches in logarithmic expected time. ACM Transactions on Mathematical Software, 3(3), 209-226.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nCallahan, P. B., \u0026amp; Kosaraju, S. R. (1995). A decomposition of multidimensional point sets with applications to k-nearest-neighbors and n-body potential fields. Journal of the ACM, 42(1), 67-90.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nThis would be a k-means problem. I intend to write a blog post on k-means clustering (and the role BBD-trees can play) in the future.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nKanungo, T., Mount, D. M., Netanyahu, N. S., Piatko, C. D., Silverman, R., \u0026amp; Wu, A. Y. (2002). An efficient k-means clustering algorithms: Analysis and implementation. IEEE Transactions on Pattern Analysis and Machine Intelligence, 24(7), 881892. https://doi.org/10.1109/TPAMI.2002.1017616\u0026#160;\u0026#x21a9;\u0026#xfe0e;\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"/introduction-to-balanced-box-decomposition-trees.html","tags":null,"title":"Introduction to Balanced Box-Decomposition Trees"},{"categories":null,"contents":"Isolation Forests (IFs), presented in Liu1 et. al (2012), are a popular algorithm used for outlier classification. In a very simplified way, the method consists of building an ensemble of Isolation Trees (ITs) for a given data set and observations are deemed anomalies if they have short adjusted average path lengths on the ITs.\nITs, which will be covered shortly, have several properties in common with a fundamental data structure: the Binary Search Tree (BSTs). In a very simplified way, BSTs are a special instance of tree structures where keys are kept in such an order that a node search is performed by iteratively (or recursively) choosing a left or right branch based on a quantitative comparison (e.g. lesser or greater). Node insertion is performed by doing a tree search, using the method described previously, until reaching an external node, where the new node will be inserted. This allows for efficient node searches since, on average, half the tree will not be visited. To illustrate this assume the values $x=[1, 10, 2, 4, 3, 5, 26, 9, 7, 54]$ and the respective insertion on a BST. The intermediate steps would then be as shown below.\nOne of the properties of BSTs is that, with randomly generated data, the path between the root node and the outliers will typically be shorter. We can see from the illustration below that, with our example data, the path length for (say) 7 is twice the length than for the suspicious value of 54. This property will play an important role in the IF algorithm, as we will see further on.\nIsolation Trees Since ITs are the fundamental component of IFs, we will start by describing their building process. We start by defining $t$ as the number of trees in the IF, $\\mathcal{D}$ as the training data (contained in an $n$-dimensional feature space, $\\mathcal{D} \\subset \\mathbb{R}^n$) and $\\psi$ as the subsampling size. The building of a IT consists then in recursively partitioning the data $\\mathcal{D}$ by sampling (without replacement) a subsample $\\mathcal{D}^{\\prime}$ of size $\\psi$. We then build an isolation tree $\\mathcal{T}^{\\prime}$ with this subsample (in order to later add it to the isolation forest $\\mathcal{F}$) and the process is repeated $t$ times.\nTo build an isolation tree $\\mathcal{T}^{\\prime}$ from the subsample we proceed as follows: if the data subsample $\\mathcal{D}^{\\prime}$ is indivisible, a tree is returned containing a single external node corresponding to the feature dimensions, $n$. If it can be divided, a series of steps must be performed. Namely, if we consider $Q = \\lbrace q_1,\\dots,q_n\\rbrace$ as the list of features in $\\mathcal{D}^{\\prime}$, we select a random feature $q \\in Q$ and a random split point $p$ such that\n$$ \\min(q) \u0026lt; p \u0026lt; \\max(q), \\qquad q \\in Q. $$\nBased on the cut-off point $p$, we filter the features into a BSTs left and right nodes according to\n$$ \\mathcal{D}_l := \\lbrace \\mathcal{D}^{\\prime} : q \\in Q, q\u0026lt;p\\rbrace \\ \\mathcal{D}_r := \\lbrace \\mathcal{D}^{\\prime} : q \\in Q, q \\geq p\\rbrace, $$\nand return an internal node having an isolation tree with left and right nodes as respectively $\\mathcal{D}_l$ and $\\mathcal{D}_r$.\nTo illustrate this (and the general method of identifying anomalies in a two dimensional feature space, $x\\in\\mathbb{R}^2$) we will look at some simulated data and its processing. We start by simulating two clusters of data from a multivariate normal distribution, one centred in $x_a=[-10, 10]$ and another centred in $x_b=[10, 10]$, with a variance of $\\Sigma=\\text{diag}(2, 2)$, that is\n$$ X_a \\sim \\mathcal{N}\\left([-10, -10], \\text{diag}(2, 2)\\right) \\ X_b \\sim \\mathcal{N}\\left([10, 10], \\text{diag}(2, 2)\\right). $$\nThe particular realisation of this simulation looks like this:\nBelow we illustrate the building of a single IT (given the data), illustrating the feature split point $p$ and respective division of the feature list into left or right IT nodes. The process is conducted recursively until the feature list is no longer divisible. As mentioned previously, this process, the creation of an IT, is repeated $t$ times in order to create the IF.\nThere should have been a video here but your browser does not seem to support it. In order to perform anomaly detection (e.g. observation scoring) we will then use the IT equivalent of the BST unsuccessful search heuristics. An external node termination in an IT is equivalent to a BST unsuccessful search. Given an observation $x$, our goal is then to calculate the score for this observation, given our defined subsampling size, that is, $s(x,\\psi)$.\nThis technique amounts to partitioning the feature space randomly until feature points are isolated. Intuitively, points in high density regions will need more partitioning steps, whereas anomalies (by definition away from high density regions) will need fewer splits. Since the building of the ITs is performed in a randomised fashion and using a subsample of the data, this density predictor can be average over a number of ITs, the Isolation Forest.\nIntuitively, this could be done by calculating the average path length for our $\\mathcal{T}n, n=1,\\dots,t$ ITs, $\\overline{h}(x)$. However, as pointed in Liu1 et. al (2012), a problem with calculating this is that maximum possible height of each $\\mathcal{T}_n$ grows as $\\mathcal{O}(\\log(\\psi))$. To compare $h(x)$ given different subsampling sizes, a normalisation factor, $c(\\psi)$ must be established. This can be calculated by\n$$ c(\\psi) = \\begin{cases} 2H(\\psi-1)-2\\frac{\\psi-1}{n},\\text{if}\\ \\psi \u0026gt;2,\\ 1, \\text{if}\\ \\psi=2,\\ 0, \\text{otherwise}, \\end{cases} $$\nwhere $H(i)$ is the harmonic number estimated by $H(i)\\approx\\log(i) + e$.\nDenoting $h_{max}$ as the tree height limit and e as the current path length, initialised as $e=0$ we can then calculate $h(x)$ recursively as:\n$$ h(x,\\mathcal{T},h_{max},e) = \\begin{cases} h(x,\\mathcal{T}{n,left},h{max},e+1) \\text{if}\\ x_a \u0026lt; q_{\\mathcal{T}} \\ h(x,\\mathcal{T}{n,right},h{max},e+1) \\text{if}\\ x_a \\geq q_{\\mathcal{T}} \\ e+c(\\mathcal{T_{n,s}}) \\text{if}\\ \\mathcal{T} \\text{is a terminal node or}\\ e \\geq h_{max}. \\end{cases} $$\nGiven these quantities we can then, finally, calculate the anomaly score, $s$ as\n$$ s(x,\\psi) = 2^{-\\frac{\\text{E}[h(x)]}{c(\\psi)}} $$\nwith $\\text{E}[h(x)]$ being the average $h(x)$ for a collection of ITs.\nParameters As mentioned in Liu1 et. al (2012), the empirical subsampling size $\\psi=2^8$ is typically enough to perform anomaly detection in a wide range of data. Regarding the number of trees, $t$ no considerable accuracy gain is usually observed with $t\u0026gt;100$. In the plots below, we can see the score calculation for two point in our data, namely an outlier ($x_o=[3.10, -12.69])$ and a normal observation ($x_n=[8.65, 9.71]$) with a varying number of trees and $\\psi=2^8$ (left) and a varying subsample size and $t=100$ (right). We can see that the score value stabilised quite early on when using $\\psi=2^8$ and that very low subsampling sizes can lead to problems when classifying anomalies.\nNow that we know how to implement an IF algorithm and calculate an anomaly score, we will try to visualise the anomaly score distribution in the vicinity of the simulated data. To do so, we simply create a two dimensional lattice enclosing our data an iteratively calculate $s(., \\psi)$. The result is show below:\nThe above steps fully define a naive isolation forest algorithm, which when applied to the previously simulated data, result in 88% of the anomalies being correctly identified.\nThanks for reading! If you have any questions or comments, please let me know on Mastodon or Twitter.\nLiu, F. T., Ting, K. M., \u0026amp; Zhou, Z. (2012). Isolation-based anomaly detection. ACM Transactions on Knowledge Discovery from Data (TKDD), 6(1), 139.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\u0026#160;\u0026#x21a9;\u0026#xfe0e;\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"/introduction-to-isolation-forests.html","tags":null,"title":"Introduction to Isolation Forests"},{"categories":null,"contents":"Summary This page contains links to most Java topics. Java, created by James Gosling, among others.\nTools Java build systems Includes Maven JReleaser Concepts Java Completable Futures Java consumer Extending JUnit Reference Get user home directory System.getProperty(\u0026#34;user.home\u0026#34;); List files recursively try (Stream\u0026lt;Path\u0026gt; walk = Files.walk(Paths.get(input))) { List\u0026lt;String\u0026gt; result = walk.filter(Files::isRegularFile) .map(x -\u0026gt;x.toString()) .collect(Collectors.toList()); result.forEach(System.out::println); } catch (IOException e) { e.printStackTrace(); } In case we want the file subset with a specific extension, txt we can filter the stream with\nList\u0026lt;String\u0026gt; result = walk.filter(Files::isRegularFile) .filter(x -\u0026gt; x.toString().endsWith(\u0026#34;.txt\u0026#34;)) .map(x -\u0026gt; x.toString()) .collect(Collectors.toList()); Setting the logger level From the CLI specify it with, for instance to set the logger level to info:\n$ mvn test -Dorg.slf4j.simpleLogger.defaultLogLevel=info ","permalink":"/java.html","tags":null,"title":"Java"},{"categories":null,"contents":"Summary Notes on Java build systems.\nMaven ","permalink":"/java-build-systems.html","tags":null,"title":"Java build systems"},{"categories":null,"contents":"Running in parallel import java.util.concurrent.CompletableFuture; import java.lang.InterruptedException; import java.util.concurrent.ExecutionException; public static void main(String[] args) throws InterruptedException, ExecutionException { CompletableFuture\u0026lt;String\u0026gt; future1 = CompletableFuture.supplyAsync(() -\u0026gt; \u0026#34;Hello\u0026#34;); CompletableFuture\u0026lt;String\u0026gt; future2 = CompletableFuture.supplyAsync(() -\u0026gt; \u0026#34;Beautiful\u0026#34;); CompletableFuture\u0026lt;String\u0026gt; future3 = CompletableFuture.supplyAsync(() -\u0026gt; \u0026#34;World\u0026#34;); CompletableFuture\u0026lt;Void\u0026gt; combinedFuture = CompletableFuture.allOf(future1, future2, future3); CompletableFuture\u0026lt;String\u0026gt; result = combinedFuture.thenApply(v -\u0026gt; future1.join() + future2.join() + future3.join()); System.out.println(result.get()); } : HelloBeautifulWorld Waiting for all Lets assume we have a completable future, $f$. This future, in turn, create $N$ additional completable futures, $f_1, f_2, \\dots, f_N$. How can we set $f$ to complete only when /all/ $f_1, f_2, \\dots, f_N$ are also completed?\nThe answer is to use a combination of allOf1 with thenRun2. According to the documentation, allOf returns a new CompletableFuture that is completed when all of the given CompletableFutures complete. In turn, thenRun will execute the given action. Let\u0026rsquo;s look at an example:\nimport java.util.concurrent.CompletableFuture; import java.util.concurrent.ExecutorService; import java.util.concurrent.Executors; import java.lang.InterruptedException; import java.util.concurrent.ExecutionException; public static void main(String[] args) throws InterruptedException, ExecutionException { CompletableFuture\u0026lt;String\u0026gt; f = new CompletableFuture\u0026lt;\u0026gt;(); int N = 10; CompletableFuture\u0026lt;String\u0026gt; f1 = CompletableFuture.completedFuture(\u0026#34;f1\u0026#34;); CompletableFuture\u0026lt;String\u0026gt; f2 = CompletableFuture.completedFuture(\u0026#34;f2\u0026#34;); CompletableFuture\u0026lt;String\u0026gt; f3 = CompletableFuture.completedFuture(\u0026#34;f3\u0026#34;); ExecutorService executor = Executors.newSingleThreadExecutor(); executor.submit(() -\u0026gt; { CompletableFuture.allOf(f1, f2, f3).thenRun(() -\u0026gt; f.complete(\u0026#34;f1,f2,f3 completed.\\nProceed to finish f.\u0026#34;)); }); f.thenAccept(v -\u0026gt; { System.out.println(v); }); Thread.sleep(100); executor.shutdown(); } : f1,f2,f3 completed. : Proceed to finish f. https://docs.oracle.com/javase/8/docs/api/java/util/concurrent/CompletableFuture.html#allOf-java.util.concurrent.CompletableFuture\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nhttps://docs.oracle.com/javase/8/docs/api/java/util/concurrent/CompletableFuture.html#thenRun-java.lang.Runnable\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"/java-completable-futures.html","tags":null,"title":"Java Completable Futures"},{"categories":null,"contents":"Introduction Applying Introduced in Java 8, the Consumer interface aims at providing additional functional programming capabilities for Java.\nConsumer defined functions do not return any value and they consist mainly of two methods:\nvoid accept(T t); default Consumer\u0026lt;T\u0026gt; andThen(Consumer\u0026lt;? super T\u0026gt; after); Let\u0026rsquo;s look at an example:\nimport java.util.ArrayList; import java.util.function.Consumer; public static void main(String[] args) { Consumer\u0026lt;String\u0026gt; say = a -\u0026gt; System.out.println(\u0026#34;Hello, \u0026#34; + a + \u0026#34;!\u0026#34;); say.accept(\u0026#34;World\u0026#34;); } : Hello, World! A Consumer can be applied in a functional way, since applying a consumer is equivalent to applying the accept method. For instance:\nimport java.util.ArrayList; import java.util.List; import java.util.function.Consumer; public static void main(String[] args) { Consumer\u0026lt;String\u0026gt; say = a -\u0026gt; System.out.println(\u0026#34;Hello, \u0026#34; + a + \u0026#34;!\u0026#34;); List\u0026lt;String\u0026gt; musketeers = new ArrayList\u0026lt;String\u0026gt;(); musketeers.add(\u0026#34;D\u0026#39;Artagnan\u0026#34;); musketeers.add(\u0026#34;Athos\u0026#34;); musketeers.add(\u0026#34;Aramis\u0026#34;); musketeers.add(\u0026#34;Porthos\u0026#34;); musketeers.stream().forEach(say); } : Hello, D\u0026#39;Artagnan! : Hello, Athos! : Hello, Aramis! : Hello, Porthos! Consumer functions can also modify reference objects. For instance:\nimport java.util.ArrayList; import java.util.List; import java.util.function.Consumer; public static void main(String[] args) { List\u0026lt;Double\u0026gt; numbers = new ArrayList\u0026lt;Double\u0026gt;(); numbers.add(1d); numbers.add(2d); numbers.add(3d); Consumer\u0026lt;List\u0026lt;Double\u0026gt;\u0026gt; square = list -\u0026gt; { for (int i = 0; i \u0026lt; list.size(); i++) { double x = list.get(i); list.set(i, x*x); }; }; System.out.println(numbers); square.accept(numbers); System.out.println(numbers); } : [1.0, 2.0, 3.0] : [1.0, 4.0, 9.0] Composing Let\u0026rsquo;s now look at how to create a chain of Consumers by composing them with the andThen method. Let\u0026rsquo;s first create a consumer which converts a string to uppercase in-place:\nimport java.util.ArrayList; import java.util.List; import java.util.function.Consumer; public static void main(String[] args) { Consumer\u0026lt;String\u0026gt; say = a -\u0026gt; System.out.println(\u0026#34;Hello, \u0026#34; + a + \u0026#34;!\u0026#34;); Consumer\u0026lt;List\u0026lt;String\u0026gt;\u0026gt; upperCaseConsumer = list -\u0026gt; { for(int i=0; i\u0026lt; list.size(); i++){ String value = list.get(i).toUpperCase(); list.set(i, value); } }; Consumer\u0026lt;List\u0026lt;String\u0026gt;\u0026gt; sayAll = list -\u0026gt; list.stream().forEach(say); } We will now create a chain by first applying upperCaseConsumer and the say to our list.\nimport java.util.ArrayList; import java.util.List; import java.util.function.Consumer; public static void main(String[] args) { Consumer\u0026lt;String\u0026gt; say = a -\u0026gt; System.out.println(\u0026#34;Hello, \u0026#34; + a + \u0026#34;!\u0026#34;); Consumer\u0026lt;List\u0026lt;String\u0026gt;\u0026gt; upperCaseConsumer = list -\u0026gt; { for(int i=0; i\u0026lt; list.size(); i++){ String value = list.get(i).toUpperCase(); list.set(i, value); } }; Consumer\u0026lt;List\u0026lt;String\u0026gt;\u0026gt; sayAll = list -\u0026gt; list.stream().forEach(say); upperCaseConsumer.andThen(sayAll).accept(musketeers); } ","permalink":"/java-consumer.html","tags":null,"title":"Java consumer"},{"categories":null,"contents":"Flat mapping Nested maps import java.util.Map; import java.util.HashMap; public static void main(String[] args) { final Map\u0026lt;String, Object\u0026gt; client = new HashMap\u0026lt;\u0026gt;(); client.put(\u0026#34;Age\u0026#34;, 43); client.put(\u0026#34;Salary\u0026#34;, 1950); client.put(\u0026#34;Existing payments\u0026#34;, 100); final Map\u0026lt;String, Object\u0026gt; loan = new HashMap\u0026lt;\u0026gt;(); loan.put(\u0026#34;Duration\u0026#34;, 15); loan.put(\u0026#34;Installment\u0026#34;, 100); final Map\u0026lt;String, Object\u0026gt; contextVariables = new HashMap\u0026lt;\u0026gt;(); contextVariables.put(\u0026#34;Client\u0026#34;, client); contextVariables.put(\u0026#34;Loan\u0026#34;, loan); System.out.println(contextVariables); } : {Loan={Installment=100, Duration=15}, Client={Salary=1950, Existing payments=100, Age=43}} ","permalink":"/java-streams.html","tags":null,"title":"Java streams"},{"categories":null,"contents":"Notes on the JPype Python library.\nConversion Numpy arrays ","permalink":"/jpype.html","tags":null,"title":"JPype"},{"categories":null,"contents":"Introduction K-means is still one of the fundamental clustering algorithms. It is used in such diverse fields as Natural Language Processing (NLP), social sciences and medical sciences.\nThe core idea behind K-means is that we want to group data in clusters. Data points will be assigned to a specific cluster depending on it\u0026rsquo;s distance to a cluster\u0026rsquo;s center, usually called the centroid.\nIt is important to note that typically, the mean distance to a centroid is used to partition the clusters, however, difference distances can be used and different pivot points. An example is the K-medoids clustering algorithm.\nWe will define the two main steps of a generic K-means clustering algorithm, namely the data assignement and the centroid update step.\nData assignement The criteria to determine whether a point is closer to one centroid is typically an Euclidean distance ($L^2$) . If we consider a set of $n$ centroids $C$, such that\n$$ C = \\lbrace c_1, c_2, \\dots, c_n \\rbrace $$\nWe assign each data point in $\\mathcal{D}=\\lbrace x_1, x_2, \\dots, x_n \\rbrace$ to the nearest centroid according to its distance, such that\n$$ \\underset{c_i \\in C}{\\arg\\min} ; dist(c_i,x)^2 $$\nAs mentioned previously $dist(.)$ is typically the standard ($L^2$) Euclidean distance. We define the subset of points assigned to a centroid $i$ as $S_i$.\nCentroid update step This step corresponds to updating the centroids using the mean of add points assign to a cluster, $S_i$. That is\n$$ c_i=\\frac{1}{|S_i|}\\sum_{x_i \\in S_i} x_i $$\nPartitioning Different algorithms can be used for cluster partitioning, for instance:\nPAM CLARA CLARANS PAM To illustrate the PAM partitioning method, we will use a synthetic dataset created along the guidelines in synthetic data generation.\nElbow method In order to use the \u0026ldquo;Elbow method\u0026rdquo; we calculate the Within-Cluster Sum of Squares (WCSS) for a varying number of clusters, $K$.\nimport numpy as np import matplotlib.pyplot as plt import pandas as pd import sklearn dataset = pd.read_csv(\u0026#39;../../data/mall-customers.zip\u0026#39;) X = dataset.iloc[:, [3, 4]].values from sklearn.cluster import KMeans wcss = [] for i in range(1, 11): kmeans = KMeans(n_clusters = i, init = \u0026#39;k-means++\u0026#39;, random_state = 42) kmeans.fit(X) wcss.append(kmeans.inertia_) from plotutils import * plt.plot(range(1, 11), wcss) plt.xlabel(\u0026#39;Number of clusters\u0026#39;) plt.ylabel(\u0026#39;WCSS\u0026#39;) plt.show() kmeans = KMeans(n_clusters = 5, init = \u0026#34;k-means++\u0026#34;, random_state = 42) y_kmeans = kmeans.fit_predict(X) y_kmeans array([2, 3, 2, 3, 2, 3, 2, 3, 2, 3, 2, 3, 2, 3, 2, 3, 2, 3, 2, 3, 2, 3, 2, 3, 2, 3, 2, 3, 2, 3, 2, 3, 2, 3, 2, 3, 2, 3, 2, 3, 2, 3, 2, 0, 2, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 1, 4, 0, 4, 1, 4, 1, 4, 0, 4, 1, 4, 1, 4, 1, 4, 1, 4, 0, 4, 1, 4, 1, 4, 1, 4, 1, 4, 1, 4, 1, 4, 1, 4, 1, 4, 1, 4, 1, 4, 1, 4, 1, 4, 1, 4, 1, 4, 1, 4, 1, 4, 1, 4, 1, 4, 1, 4, 1, 4, 1, 4, 1, 4, 1, 4, 1, 4, 1, 4, 1, 4, 1, 4, 1, 4], dtype=int32) ps = 30 plt.scatter(X[y_kmeans == 0, 0], X[y_kmeans == 0, 1], s = ps, c = colours[0], label = \u0026#39;Cluster1\u0026#39;) plt.scatter(X[y_kmeans == 1, 0], X[y_kmeans == 1, 1], s = ps, c = colours[1], label = \u0026#39;Cluster2\u0026#39;) plt.scatter(X[y_kmeans == 2, 0], X[y_kmeans == 2, 1], s = ps, c = colours[2], label = \u0026#39;Cluster3\u0026#39;) plt.scatter(X[y_kmeans == 3, 0], X[y_kmeans == 3, 1], s = ps, c = colours[3], label = \u0026#39;Cluster4\u0026#39;) plt.scatter(X[y_kmeans == 4, 0], X[y_kmeans == 4, 1], s = ps, c = colours[4], label = \u0026#39;Cluster5\u0026#39;) plt.scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 1], s = 100, c = \u0026#39;black\u0026#39;, label = \u0026#39;Centroids\u0026#39;) plt.xlabel(\u0026#39;Annual Income (k$)\u0026#39;) plt.ylabel(\u0026#39;Spending Score (1-100)\u0026#39;) plt.legend() plt.show() ","permalink":"/k-means-clustering.html","tags":null,"title":"K-means clustering"},{"categories":null,"contents":"OpenShift Installing To install a k8s operator, define the subscription with, for instance opendatahub-operator.yaml:\napiVersion: operators.coreos.com/v1alpha1 kind: Subscription metadata: name: opendatahub-operator namespace: openshift-operators spec: channel: rolling installPlanApproval: Automatic name: opendatahub-operator source: community-operators sourceNamespace: openshift-marketplace and apply it with oc apply -f opendatahub-operator.yaml.\n","permalink":"/k8s-operators.html","tags":null,"title":"k8s operators"},{"categories":null,"contents":"These are the KinD notes. Kubernetes IN Docker.\n","permalink":"/kind.html","tags":null,"title":"KinD"},{"categories":null,"contents":"Introduction Some requirements to run KNative:\nMinikube kubectl Architecture ","permalink":"/knative.html","tags":null,"title":"KNative"},{"categories":null,"contents":"","permalink":"/Kompose.html","tags":null,"title":"Kompose"},{"categories":null,"contents":"Introduction Kourier page.\n","permalink":"/kourier.html","tags":null,"title":"Kourier"},{"categories":null,"contents":"Notes on Kubernetes\nArchitecture Concepts Custom Resource Definitions A Custom Resource Definiton (CRD) is an endpoint in the Kubernetes API that stores a collection of API objects. This abstraction permits an expanding the Kubernetes API with new resource definitions.\nIngress service A service1 designed to allow external network traffic in.\nMinikube Main page on Minikube is here.\nTools Kubernetes tools page is here.\nArticles The Missing Kubernetes Type System https://kubernetes.io/docs/concepts/services-networking/ingress/\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"/kubernetes.html","tags":null,"title":"Kubernetes"},{"categories":null,"contents":"Notes on Kustomize1.\nInstallation kubectl is a Kustomize requirement. To install the binary version of Kustomize use\n$ curl -s \u0026#34;https://raw.githubusercontent.com/kubernetes-sigs/kustomize/master/hack/install_kustomize.sh\u0026#34; | bash https://kustomize.io/\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"/kustomize.html","tags":null,"title":"Kustomize"},{"categories":null,"contents":" Last week, at the North East Functional Programming meet up, we were given a code Kata consisting of the Langton\u0026rsquo;s ant algorithm.\nI\u0026rsquo;ve had a go at Scala but decided later on to put a live version in this blog.\nI considered several implementation options, such as scala.js and Elm, but in the end decided to implement it in plain Javascript.\nAdd ant\n","permalink":"/langtons-ant.html","tags":null,"title":"Langton's ant"},{"categories":null,"contents":"Notes on LaTeX.\nImages side-by-side Use the subfig package.\n\\documentclass[10pt,a4paper]{article} \\usepackage[demo]{graphicx} \\usepackage{subfig} \\begin{document} \\begin{figure}% ","permalink":"/latex.html","tags":null,"title":"LaTeX"},{"categories":null,"contents":"Notes on Linux admin.\n","permalink":"/linux-admin.html","tags":null,"title":"Linux admin"},{"categories":null,"contents":"Notes on machine learning.\nTopics Synthetic Data Generation Using Synthetic data with SDV and Gaussian copulas, Synthetic data with SDV and CTGAN and Synthetic data with SDV and CopulaGAN. Explainability Model/Concept drift Introduction to Concept Drift in Machine Learning Time-series Time-series analysis Streaming anomaly detection Clustering K-means clustering Fairness Fairness in machine learning Model fairness Metrics Error metrics Distance metrics Transformations Feature scaling Optimisation Gradient descent Stochastic Gradient Descent Stochastic Gradient descent with momentum Mini-Batch Gradient Descent Adagrad RMSProp AdaDelta Adam Gradient-free optimisation RNN LSTM Statistics Streaming statistics Thompson sampling Unsupervised methods Self-organising maps Supervised methods Methods pertaining to Supervised learning.\nTechniques such as:\nRandom Forest Frameworks Cookiecutter Data Science Scikit-learn Theory The difference between AI and Machine Learning ","permalink":"/machine-learning.html","tags":null,"title":"Machine learning"},{"categories":null,"contents":"Summary Notes on the Maven build system.\nModes Daemon This build mode relies on the Maven daemon, itself inspired by the Gradle daemon:\nThe Maven Daemon process does not reload classes unless they are changed.\nThe daemon is called with mvnd instead of mvn, for instance\n$ mvnd clean install The daemon uses multiple threads by default, with $N_{cores}-1$.\nArtifacts ber JARs There are many ways to build ber JARs, but we will talk about two, maven-assembly-plugin and maven-shade-plugin.\nAssembly The ~maven-assembly-plugin~ 1 adds all dependencies inside the final fat JAR and can be used by adding the following to your ~pom.xml~:\n\u0026lt;plugin\u0026gt; \u0026lt;groupId\u0026gt;org.apache.maven.plugins\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;maven-assembly-plugin\u0026lt;/artifactId\u0026gt; \u0026lt;configuration\u0026gt; \u0026lt;descriptorRefs\u0026gt; \u0026lt;descriptorRef\u0026gt;jar-with-dependencies\u0026lt;/descriptorRef\u0026gt; \u0026lt;/descriptorRefs\u0026gt; \u0026lt;archive\u0026gt; \u0026lt;manifest\u0026gt; \u0026lt;mainClass\u0026gt;{your.package.main.class}\u0026lt;/mainClass\u0026gt; \u0026lt;/manifest\u0026gt; \u0026lt;/archive\u0026gt; \u0026lt;/configuration\u0026gt; \u0026lt;executions\u0026gt; \u0026lt;execution\u0026gt; \u0026lt;phase\u0026gt;package\u0026lt;/phase\u0026gt; \u0026lt;goals\u0026gt; \u0026lt;goal\u0026gt;single\u0026lt;/goal\u0026gt; \u0026lt;/goals\u0026gt; \u0026lt;/execution\u0026gt; \u0026lt;/executions\u0026gt; \u0026lt;/plugin\u0026gt; Maven plugins should be specified inside a \u0026lt;plugins\u0026gt;\u0026lt;/plugins\u0026gt; scope, which itself must be inside \u0026lt;build\u0026gt;\u0026lt;/build\u0026gt; scope. As you can see from execution phase, the ber JAR is built when calling mvn package.\nShade The ~maven-shade-plugin~ 2 also adds all dependencies inside the final fat JAR, but additionally executes shading. Add the following to your ~pom.xml~:\n\u0026lt;plugin\u0026gt; \u0026lt;groupId\u0026gt;org.apache.maven.plugins\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;maven-shade-plugin\u0026lt;/artifactId\u0026gt; \u0026lt;configuration\u0026gt; \u0026lt;transformers\u0026gt; \u0026lt;transformer implementation=\u0026#34;org.apache.maven.plugins.shade.resource.ManifestResourceTransformer\u0026#34;\u0026gt; \u0026lt;mainClass\u0026gt;{your.package.main.class}\u0026lt;/mainClass\u0026gt; \u0026lt;/transformer\u0026gt; \u0026lt;/transformers\u0026gt; \u0026lt;/configuration\u0026gt; \u0026lt;executions\u0026gt; \u0026lt;execution\u0026gt; \u0026lt;phase\u0026gt;package\u0026lt;/phase\u0026gt; \u0026lt;goals\u0026gt; \u0026lt;goal\u0026gt;shade\u0026lt;/goal\u0026gt; \u0026lt;/goals\u0026gt; \u0026lt;/execution\u0026gt; \u0026lt;/executions\u0026gt; \u0026lt;/plugin\u0026gt; Testing Excluding tests To exclude test from Maven use the following notation3\n# Exclude specific class with (!) $ mvn test -Dtest=!ExcludedClass # Exclude specific method $ mvn test -Dtest=!ExcludedClass#excludedMethod # Exclude more than one method $ mvn test -Dtest=!ExcludedClass#excludedMethod+excludedMethod2 # Exclude a package with a wildcard (*) $ mvn test -Dtest=!dev.ruivieira.test.Excluded* https://maven.apache.org/plugins/maven-assembly-plugin/\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nhttps://maven.apache.org/plugins/maven-shade-plugin/\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nIf using zsh remember to escape !.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"/maven.html","tags":null,"title":"Maven"},{"categories":null,"contents":"It is said that patience is a virtue but the truth is that no one likes waiting (especially waiting around: this interesting article explores why people prefer walking 8 minutes to the airports baggage claim and having the bags ready rather than waiting the same amount of time entirely in the claim area).\nAnyone performing computationally heavy work, such as Monte Carlo methods, will know that these are usually computationally expensive algorithms which, even in modern hardware, can result in waiting times in the magnitude of hours, days and even weeks.\nThese long running times coupled with the fact that in certain cases it is not easy to accurately predict how long a certain number of iterations will take, usually leads to a tiresome behaviour of constantly checking for good (or bad) news.\nAlthough it is perfectly possible to specify that your simulation should stop after a certain amount of time (especially valid for very long simulations), this doesnt seem to be the standard practice.\nIn this post Ill detail my current setup for being notified exactly of when simulations are finished. To implement this setup, the following stack is required:\nA JDK Apache Maven1 A messaging service Pushbullet A smartphone, tablet, smartwatch (or any other internet enabled device) To start, we can create an account in Pushbullet, which will involve, in the simplest case, signing up using some authentication service such as Google.\nNext, we will install the client application (available for Android, iOS and most modern browsers after which we can enable notifications (at least in the Android client, Im not familiar with the iPhone version).\nSince my current work started as a plain Java project which in time evolved mainly to Scala, it consists of an unholy mixture of Maven as a build tool for Scala code.\nThis shouldn\u0026rsquo;t be a problem for other setups, but Ill just go through my specific setup (i.e. using Maven dependencies to a Scala project).\nTo implement communication between the code and the messaging service, we can use a simple library such as jpushbullet.\nThe library works well enough, although at the time of writing it only supports Pushbullets v1 API but not the newer v2 API.\nSince the project, unfortunately, is not in Maven central, you should build it from scratch. Fortunately, in a sensibly configured machine, this is trivial.\nIn the machine where you plan to perform the simulations, clone and build jpushbullet.\n$ git clone git@github.com:silk8192/jpushbullet.git $ mvn clean install Once the build is complete, you can add it as a local dependency in your projects pom.xml:\n\u0026lt;dependencies\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;com.shakethat.jpushbullet\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;jpushbullet\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;/dependencies\u0026gt; For the purpose of this example, lets assume that you have the following Object as the entry point of your simulation. The next step is to add a call to the Pushbullet service before the exit point. Please keep in mind that it is very bad practice to include your personal API key in your committed code. I strongly suggest you keep this information in a separate file (/e.g./ in ~resources~), read it at runtime and add it to ~.gitignore~.\nThat being said, place the messaging code as such:\npackage benchmarks import com.shakethat.jpushbullet.PushbulletClient\nobject MCMC { def main(args:Array[String]):Unit = { // Your MCMC code val client = new PushbulletClient(api\\_key) val devices = client.getDevices val title = \u0026#34;MCMC simulation finished\u0026#34; val body = \u0026#34;Some summary can be included\u0026#34; // n is the preferred device number client.sendNote(true, devices .getDevices .get(n) .getIden(), title, body) } } Usually, I would call this code via ~ssh~ into my main machine from Maven (using Scala Maven) as:\n$ nohup mvn scala:run -DmainClass=benchmarks.MCMC \u0026amp; Finally, when the simulation is completed, you will be notified in the client devices (you can select which ones by issuing separate sendNote calls) and include a result summary, as an example.\nMy current setup generates an R script from a template which is run by ~Rscript~ in order to produce a PDF report. However, be careful, since file quotas in Pushbullet are limited, so text notifications should be used without worry of going over the free usage tier.\nKeep in mind that there are other alternatives to jpushbullet, such as send-notification, a general notification library for Java for which the setup is quite similar.\nHope this was helpful.\nhttp://maven.apache.org\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"/mcmc-notifications.html","tags":null,"title":"MCMC notifications"},{"categories":null,"contents":"Recently I\u0026rsquo;ve been following (but not very closely, I admit) the development of the GraalVM project. The project has many interesting goals (such as Project Metropolis, increased JIT performance and others).\nHowever, having dabbled with projects such as Scala native and Kotlin native, one of the aspects of GraalVM that caught my attention was the SubstrateVM, which allegedly allows for a simple, straight-forward compilation of any Java bytecode into a native binary.\nI specifically wanted to compare the performance and memory consumption of simple scientific computing tasks when using the JVM and native executables. To do this, I picked two simple numerical simulations in the form of toy Gibbs samplers, in order to keep the cores busy for a while.\nBinomial-Beta case The first problem chosen was the one of sampling from a Beta-Binomial distribution where we have\n$$ X \\sim \\text{Binom}\\left(n,\\theta\\right) \\\\ \\theta \\sim \\text{B}\\left(a,b\\right). $$\nSince we know that\n$$ \\pi\\left(\\theta|x\\right) \\propto \\theta^x \\left(1-\\theta\\right)^{n-x}\\theta^{a-1}\\left(1-\\theta\\right)^{b-1}, $$\nWe calculate the joint density\n$$ p(x,\\theta) = \\begin{pmatrix} n \\\\ x \\end{pmatrix} \\theta^x \\left(1-\\theta\\right)^{n-x}\\frac{\\Gamma\\left(a+b\\right)}{\\Gamma(a)\\Gamma(b)}\\theta^{a-1}\\left(1-\\theta\\right)^{b-1} $$\nThe marginal distribution is a Binomial-Beta:\n$$ p\\left(x\\right)=\\begin{pmatrix} n \\\\ x \\end{pmatrix}\\frac{\\Gamma\\left(a+b\\right)}{\\Gamma(a)\\Gamma(b)}\\frac{\\Gamma\\left(a+b\\right)\\Gamma\\left(b+n-x\\right)}{\\Gamma\\left(a+b+n\\right)},\\qquad x=0,1,\\dots,n. $$\nThe code for this simulation is available here.\nThe project is setup so that Maven produces an assembly Jar file, since I\u0026rsquo;ve found that to be the easier artifact we can offer to the GraalVM\u0026rsquo;s native compiler. To enable assembly Jars we add the maven-assembly-plugin to pom.xml and specify a main class. The assembly can then be produced simply by executing\nmvn package An assembly Jar should be available in the target folder and named benchmark-gibbs-1.0-SNAPSHOT-jar-with-dependencies.jar. Both the Jar and the native executable allow to specify how many iterations the Gibbs sampler should run for (as well as the thinning factor). If nothing is specified, the default will be used, which is $50000$ iterations thinned by $100$.\nThis particular Gibbs sampler was implemented in two variants. One variant stores the samples draws of $x$ and $\\theta$ in arrays double[] while the other one simply calculates the Gibbs steps by using the previous value, that is $x_i=f(x_{i-1},\\theta_{i-1})$ and then discarding the previous values. The latter has a constant memory cost in $\\mathcal{O}(1)$ in terms of number of iterations, while the former clearly doesn\u0026rsquo;t.\nWe can them proceed with the first test, first benchmarking it under the JVM by running (for both sample history variants):\n$ /usr/bin/time -v java -jar target/benchmark-gibbs-1.0-SNAPSHOT-jar-with-dependencies.jar store 50000 100 $ /usr/bin/time -v java -jar target/benchmark-gibbs-1.0-SNAPSHOT-jar-with-dependencies.jar nostore 50000 100 (It is important to note that the time command is the executable under /usr/bin and not your shell\u0026rsquo;s builtin.) The next step is to build the native image using GraalVM\u0026rsquo;s compiler. This is also quite straight-forward and simply a matter of calling:\n$GRAALVM_BIN/native-image target/benchmark-gibbs-1.0-SNAPSHOT-jar-with-dependencies.jar where $GRAALVM_BIN is simply the location where you installed the GraalVM binaries. If the compilation is successful, you should see some information about the compilation steps, such as parsing, inlining, compiling and writing the image. Finally, if using the default, you should have a native executable available in your current directory. Again, the benchmark command is similar to the JVM step, that is:\n$ /usr/bin/time -v ./benchmark-gibbs-1.0-SNAPSHOT-jar-with-dependencies store 50000 100 $ /usr/bin/time -v ./benchmark-gibbs-1.0-SNAPSHOT-jar-with-dependencies nostore 50000 100 The results from the runs which saved the sampling history on both platforms (JVM and native) were consistent as we can see from the plots below:\nThe (peak) memory consumption and execution time for each version is presented in the table below:\nTime(s) Peak (Mb) JVM (no sample history) 110.09 320.913 native (no sample history) 130.52 273.747 JVM (sample history) 112.51 324.796 native (sample history) 130.62 274.239 Another bivariate case The second problem chosen is another bivariate model, a Gibbs sampler in this blog. The code is included in the same repositoty as the Beta-Binomial case and the setup for the benchmarks is similar. The only step needed to run this example is to change the main class in the assemply plugin section of the pom.xml from BinomialBeta to Bivariate. The benchmark results are in the table below:\nTime(s) Peak (Mb) JVM 106.92 176.541 native 121.29 273.383 Now, in this case, the results are much more interesting. The JVM version outperforms the native version in both execution time and memory consumption. I don\u0026rsquo;t have an explanation for this, but if you think you have (or have any other questions) please let me know on Mastodon or Twitter.\nThanks for reading!\n","permalink":"/mcmc-performance-on-substrate-vm.html","tags":null,"title":"MCMC performance on substrate VM"},{"categories":null,"contents":"Introduction Page on Minikube1.\nInstallation Fedora To install Minikube on Fedora use:\n$ curl -LO https://storage.googleapis.com/minikube/releases/latest/minikube-latest.x86_64.rpm $ sudo rpm -Uvh minikube-latest.x86_64.rpm Setup Most of the configuration settings can be set using the config subcommand.\nKubernetes version Set the [Kubernetes cluster version (e.g 1.19).\n$ minikube config set kubernetes-version v1.19.0 Memory Set the available amount of memory.\n$ minikube config set memory 8000 Deployment From image To build an image use the minikube command\nminikube image build -t ruivieira/my-image . Image deployment?\nhttps://minikube.sigs.k8s.io/docs/\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"/minikube.html","tags":null,"title":"Minikube"},{"categories":null,"contents":" Unfairness detection Fairness metrics ","permalink":"/model-fairness.html","tags":null,"title":"Model fairness"},{"categories":null,"contents":"Monotonic Cubic Spline interpolation (MCSI) is a popular and useful method which fits a smooth, continuous function through discrete data. MCSI has several applications in the field of computer vision and trajectory fitting. MCSI further guarantees monotonicity of the smoothed approximation, something which a cubic spline approximation alone cannot. In this post I\u0026rsquo;ll show how to implement the method developed by F. N. Fritsch and R. E. Carlson1 in the Rust2 programming language.\nRust Why Rust? Definitely this is a type of solution so simple that it can be implemented in practically any programming language we can think of. However, I do find that the best way to get acquainted with a new language and its concepts is precisely to try to implement a simple and well-know solution. Although this post does not intend to be an introduction to the Rust language, some of the fundamentals will be presented as we go along.\nIdiomatic Rust Object-Oriented Programming (OOP) has several characteristics which differ significantly from \u0026ldquo;traditional\u0026rdquo; OOP languages. Rust achieves data and behaviour encapsulation by means of defining data structure blueprints (called struct) and then defining their behaviour though a concrete implementation (through impl). As an example, a simple \u0026ldquo;class\u0026rdquo; Foo would consist of:\nstruct Foo { } impl Foo { fn new() -\u0026gt; Foo { return Foo {}; } fn method(\u0026amp;mut self) {} fn static_method() {} } pub fn main() { let mut f = Foo::new(); f.method(); Foo::static_method(); } The \u0026ldquo;constructor\u0026rdquo; is defined typically as new(), but any \u0026ldquo;static\u0026rdquo; method which returns an initialised struct can be a constructor and \u0026ldquo;object\u0026rdquo; methods include the passing of the self instance not unlike languages such as Python. The \u0026amp;mut self refers to the control or exclusive access to self and it is not directly related to mut mutability control. These concepts touch on Rust\u0026rsquo;s borrowing and ownership model which, unfortunately, are way beyond the scope of this blog post. A nice introduction is provided by the \u0026ldquo;Rust programming book\u0026rdquo; available here. Our implementation aims at building a MCSI class MonotonicCubicSpline by splitting the algorithm into the slope calculation at construction time, a Hermite interpolation function and a partial application function generator. This will follow the general structure\npub struct MonotonicCubicSpline { m_x: Vec\u0026lt;f64\u0026gt;, m_y: Vec\u0026lt;f64\u0026gt;, m_m: Vec\u0026lt;f64\u0026gt; } impl MonotonicCubicSpline { pub fn new(x : \u0026amp;Vec\u0026lt;f64\u0026gt;, y : \u0026amp;Vec\u0026lt;f64\u0026gt;) -\u0026gt; MonotonicCubicSpline { // ... } pub fn hermite(point: f64, x : (f64, f64), y: (f64, f64), m: (f64, f64)) -\u0026gt; f64 { // ... } pub fn interpolate(\u0026amp;mut self, point : f64) -\u0026gt; f64 { // ... } fn partial(x: Vec\u0026lt;f64\u0026gt;, y: Vec\u0026lt;f64\u0026gt;) -\u0026gt; impl Fn(f64) -\u0026gt; f64 { // ... } } Vec is a vector, a typed growable collection available in Rust\u0026rsquo;s standard library with documentation available here.\nMonotonic Cubic Splines MCSI hinges on the concept of cubic Hermite interpolators. The Hermite interpolation for the unit interval for a generic interval $(x_k,x_{k+1})$ is\n$$ p(x)=p_k h_{00}(t)+ h_{10}(t)(x_{k+1}-x_k)m_k + \\\\ h_{01}(t)p_{k+1} + h_{11}(t)(x_{k+1}-x_{k})m_{k+1}. $$\nThe $h_{\\star}$ functions are usually called the Hermite basis functions in the literature and here we will use the factorised forms of:\n$$ \\begin{aligned} h_{00}(t) \u0026amp;= (1+2t)(1-t)^2 \\\\ h_{10}(t) \u0026amp;= t(1-t)^2 \\\\ h_{01}(t) \u0026amp;= t^2 (3-2t) \\\\ h_{11}(t) \u0026amp;= t^2 (t-1). \\end{aligned} $$\nThis can be rewritten as\n$$ \\begin{aligned} p(x) = (p_k(1 + 2t) + \\Delta x_k m_k t)(1-t)(1-t) + \\\\ (p_{k+1} (3 -2t) + \\Delta x_k m_{k+1} (t-1))t^2 \\end{aligned} $$\nwhere\n$$ \\begin{aligned} \\Delta x_k \u0026amp;= x_{k+1} - x_k \\\\ t \u0026amp;= \\frac{x-x_k}{h}. \\end{aligned} $$\nThis associated Rust method is the above mentioned \u0026ldquo;static\u0026rdquo; MonotonicCubicSpline::hermite():\npub fn hermite(point: f64, x : (f64, f64), y: (f64, f64), m: (f64, f64)) -\u0026gt; f64 { let h = x.1 - x.0; let t = (point - x.0) / h; return (y.0 (1.0 + 2.0 t) + h m.0 t) (1.0 - t) (1.0 - t) + (y.1 (3.0 - 2.0 t) + h m.1 (t - 1.0)) t t; } where the tuples correspond to $x \\to (x_k, x_{k+1})$, $t \\to (y_k, y_{k+1})$ and $m \\to (m_k, m_{k+1})$\nFor a series of data points $(x_k, y_k)$ with $k=1,\\dots,n$ we then calculate the slopes of the secant lines between consecutive points, that is:\n$$ \\Delta_k = \\frac{\\Delta y_{k}}{\\Delta x_k},\\qquad \\text{for}\\ k=1,\\dots,n-1 $$\nwith $Delta y_k = y_{k+1}-y_k$ and $\\Delta x_k$ as defined previously.\nSince the data is represented by the vectors x : Vec\u0026lt;f64\u0026gt; and y : Vec\u0026lt;f64\u0026gt; we implement this in the \u0026ldquo;constructor\u0026rdquo;:\nlet mut secants = vec![0.0 ; n - 1]; let mut slopes = vec![0.0 ; n]; for i in 0..(n-1) { let dx = x[i + 1] - x[i]; let dy = y[i + 1] - y[i]; secants[i] = dy / dx; } The next step is to average the secants in order to get the tangents, such that\n$$ m_k = \\frac{\\Delta_{k-1}+\\Delta_k}{2},\\qquad \\text{for}\\ k=2,\\dots,n-1. $$\nThis is achieved by the code:\nslopes[0] = secants[0]; for i in 1..(n-1) { slopes[i] = (secants[i - 1] + secants[i]) * 0.5; } slopes[n - 1] = secants[n - 2]; By definition, we want to ensure monotonicity of the interpolated points, but to guarantee this we must avoid the interpolation spline to go too far from a certain radius of the control points. If we define $\\alpha_k$ and $\\beta_k$ as\n$$ \\begin{aligned} \\alpha_k \u0026amp;= \\frac{m_k}{\\Delta_k} \\\\ \\beta_k \u0026amp;= \\frac{m_{k+1}}{\\Delta_k}, \\end{aligned} $$\nto ensure the monotonicity of the interpolation we can impose the following constraint on the above quantities:\n$$ \\phi(\\alpha, \\beta) = \\alpha - \\frac{(2\\alpha+\\beta-3)^2}{3(\\alpha+\\beta-2)}\\geq 0, $$\nthat is\n$$ \\alpha + 2\\beta - 3 \\leq 0, \\text{or}\\ 2\\alpha+\\beta-3 \\leq 0 $$\nTypically the vector $(\\alpha_k, \\beta_k)$ is restricted to a circle of radius 3, that is\n$$ \\alpha^2_l + \\beta_k^2\u0026gt;9, $$\nand then setting\n$$ m_{k+1} = t\\beta_k\\Delta_k, $$\nwhere\n$$ \\begin{aligned} h \u0026amp;= \\sqrt{\\alpha^2_k + \\beta^2_k} \\\\ t \u0026amp;= \\frac{3}{h}. \\end{aligned} $$\nOne of the ways in which Rust implements polymorphism is through method dispatch. The f64 primitive provides a shorthand for the quantity $\\sqrt{\\alpha^2_k + \\beta^2_k}$ as $\\alpha.\\text{hypot}(\\beta)$. The relevant Rust code will then be:\nfor i in 0..(n-1) { if secants[i] == 0.0 { slopes[i] = 0.0; slopes[i + 1] = 0.0; } else { let alpha = slopes[i] / secants[i]; let beta = slopes[i + 1] / secants[i]; let h = alpha.hypot(beta); if h \u0026gt; 3.0 { let t = 3.0 / h; slopes[i] = t * alpha * secants[i]; slopes[i + 1] = t * beta * secants[i]; } } } We are now able to define a \u0026ldquo;smooth function\u0026rdquo; generator using MCSI. We generate a smooth function $g(.)$ given a set of $(x_k, y_k)$ points, such that\n$$ f(x_k, y_k, p) \\to g(p). $$\nPartial application Before anything, it is important to recall the difference between partial application and currying, since the two are (incorrectly) used interchangeably quite often. Function currying allows to factor functions with multiple arguments into a chain of single-argument functions, that is\n$$ f(x, y, z) = h(x)(y)(z) $$\nThe concept is prevalent in functional programming, since its initial formalisation3. Partial application, however, generally aims at using an existing function conditioned on some argument as a basis to build functions with a reduced arity. In this case this would be useful since ultimately we want to create a smooth, continuous function based on the control points $(x_k, y_k)$. The partial application implementation is done in Rust as\npub fn partial(x: Vec\u0026lt;f64\u0026gt;, y: Vec\u0026lt;f64\u0026gt;) -\u0026gt; impl Fn(f64) -\u0026gt; f64 { move |p| { let mut spline = MonotonicCubicSpline::new(\u0026amp;x, \u0026amp;y); spline.interpolate(p) } } An example of how to generate a concrete smoothed continuous function from a set of control points can be:\nlet x = vec![0.0, 2.0, 3.0, 10.0]; let y = vec![1.0, 4.0, 8.0, 10.5]; let g = partial(x, y); // calculate an interpolated point let point = g(0.39); The full code can be found here.\nFritsch, F. N., \u0026amp; Carlson, R. E. (1980). Monotone piecewise cubic interpolation. SIAM Journal on Numerical Analysis, 17(2), 238246.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nhttps://www.rust-lang.org/\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nCurry, H. B., Feys, R., Craig, W., Hindley, J. R., \u0026amp; Seldin, J. P. (1958). Combinatory logic. : North-Holland Amsterdam.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"/monotonic-cubic-spline-interpolation-with-some-rust.html","tags":null,"title":"Monotonic Cubic Spline interpolation (with some Rust)"},{"categories":null,"contents":"To instal navi1 on Linux you can use\n$ bash \u0026lt;(curl -sL https://raw.githubusercontent.com/denisidoro/navi/master/scripts/install) When installing navi on Ubuntu 20 you will get the error\ninvalid preview window layout: up:2:nohidden invalid preview window layout: up:2:nohidden invalid preview window layout: up:2:nohidden This is very likely due to the fact that the fzf2 version is too old (perhaps 0.20).\nTo upgrade to the latest version of fzf use:\n$ git clone --depth 1 https://github.com/junegunn/fzf.git ~/.fzf $ ~/.fzf/install Importing cheatsheets You can import cheatsheets from any git repository that includes .cheat files:\n$ navi repo add https://github.com/ruivieira/cheatsheets External variables It is possible to populate variables with external data in navi. To do so, specify how fzf will extract the data. For instance, the command\ndocker stop \u0026lt;container_id\u0026gt; will stop the container \u0026lt;container_id\u0026gt;, which we could extract from the output of\n$ docker ps We can then specify docker ps and the input of docker stop and instruct fzf on the data\u0026rsquo;s format. For instance, here (line 2) we say that from the command of docker ps we should extract the first column, delimited by spaces and remove the first line, since it is the header.\ndocker stop \u0026lt;container_id\u0026gt; $ container_id: docker ps --- --column 1 --header-lines 1 --delimiter \u0026#39;\\s\\s+\u0026#39; https://github.com/denisidoro/navi\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nhttps://github.com/junegunn/fzf\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"/navi.html","tags":null,"title":"Navi"},{"categories":null,"contents":"Notes on Neovim.\nInstallation Fedora To install Neovim on Fedora simply run:\nsudo dnf install -y neovim python3-neovim vim-plug To install vim-plug1 run:\nsh -c \u0026#39;curl -fLo \u0026#34;${XDG_DATA_HOME:-$HOME/.local/share}\u0026#34;/nvim/site/autoload/plug.vim --create-dirs \\ https://raw.githubusercontent.com/junegunn/vim-plug/master/plug.vim\u0026#39; The config file if not existing can created at ~/.config/nvim/init.vim.\nThe syntax is (for instance to add Crystal support):\ncall plug#begin() Plug \u0026#39;vim-crystal/vim-crystal\u0026#39; call plug#end() https://github.com/junegunn/vim-plug#neovim\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"/neovim.html","tags":null,"title":"Neovim"},{"categories":null,"contents":"In the Random Forest algorithm, we build a decision tree (DT) based on a certain training dataset. This tree will be split in order to minimise some criteria function.\nHowever, it is not desirable that individual DTs get too large with too many splits, so a common approach is to train each tree with a subset of the training data (sampled with replacement). This will ensure that individual tree maintain a manageable size, while the variance of the tree ensemble is reduced and the overall bias is not altered.\nThis training subset is usually called the bootstrap samples. In the image below, we can see an illustration of the sampling with replacement.\n","permalink":"/oob-score-in-random-forests.html","tags":null,"title":"OOB score in random forests"},{"categories":null,"contents":"Notes on OpenShift1. To deploy operators on OpenShift see operators.\nOpenShift proper App selection To get the name of all resources matching a certain label:\n$ oc get all --selector app=$APPNAME -o name To delete said resources use:\n$ oc delete all --selector app=$APPNAME Image loading CodeReady Containers SSH into VM To SSH into a running CRC machine use\nssh -i ~/.crc/machines/crc/id_ecdsa core@192.168.130.11 From inside the VM you can issue normal oc commands, such as listing the nodes\noc get nodes --context admin --cluster crc --kubeconfig /opt/kubeconfig https://www.redhat.com/en/technologies/cloud-computing/openshift\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"/openshift.html","tags":null,"title":"OpenShift"},{"categories":null,"contents":"Typically the hyper-parameters which will have the most significant impact on the behaviour of a random forest are the following:\nhe number of decision trees in a random forest The split criteria Maximum depth of individual trees Maximum number of leaf nodes Random features per split umber of samples in bootstrap dataset We will look at each of these hyper-parameters individually with examples of how to select them.\nData To understand how we can optimise the hyperparameters in a random forest model, we will use scikit-learn\u0026rsquo;s RandomForestClassifier and a subset of Titanic1 dataset.\nFirst, we will import the features and labels using Pandas.\nimport pandas as pd train_features = pd.read_csv(\u0026#34;data/svm-hyperparameters-train-features.csv\u0026#34;) train_label = pd.read_csv(\u0026#34;data/svm-hyperparameters-train-label.csv\u0026#34;) Let\u0026rsquo;s look at a random sample of entries from this dataset, both for features and labels.\ntrain_features.sample(10) Some of the available features are:\nPclass, ticket class Sex Age, age in years Sibsp, number of siblings/spouses aboard Parch, number of parents/children aboard Fare, passenger fare train_label.sample(10) The outcome label indicates whether a passenger survived the disaster.\nAs part of the typical initial steps for model training, we will prepare the data by splitting it into a training and testing subset.\nfrom sklearn.model_selection import train_test_split X_train, X_test, y_train, y_test = train_test_split(train_features, train_label, test_size=0.33, random_state=23) Naive model First we will train a \u0026ldquo;naive\u0026rdquo; model, that is a model using the defaults provided by RandomForestClassifier2. These defaults are:\nn_estimators = 10 criterion=gini max_depth=None min_samples_split=2 min_samples_leaf=1 min_weight_fraction_leaf=0.0 max_features=auto max_leaf_nodes=None min_impurity_decrease=0.0 min_impurity_split=None bootstrap=True oob_score=False n_jobs=1 random_state=None verbose=0 warm_start=False class_weight=None We will instantiate a random forest classifier:\nfrom sklearn.ensemble import RandomForestClassifier rf = RandomForestClassifier() And training it using the X_train and y_train subsets using the appropriate fit method3.\ntrue_labels = train_label.values.ravel() rf.fit(X_train, y_train.values.ravel()) We can now evaluate trained naive model\u0026rsquo;s score.\nfrom sklearn.metrics import precision_score predicted_labels = rf.predict(X_test) precision_score(y_test, predicted_labels) Hyperparameter search A simple example of a generic hyperparameter search using the GridSearchCV method in scikit-learn. The score used to measure the \u0026ldquo;best\u0026rdquo; model is the mean_test_score, but other metrics could be used, such as the Out-of-bag (OOB) error.\nparameters = { \u0026#34;n_estimators\u0026#34;:[5,10,50,100,250], \u0026#34;max_depth\u0026#34;:[2,4,8,16,32,None] } rfc = RandomForestClassifier() from sklearn.model_selection import GridSearchCV cv = GridSearchCV(rfc,parameters,cv=5) cv.fit(X_train, y_train.values.ravel()) def display(results): print(f\u0026#39;Best parameters are: {results.best_params_}\u0026#39;) print(\u0026#34;\\n\u0026#34;) mean_score = results.cv_results_[\u0026#39;mean_test_score\u0026#39;] std_score = results.cv_results_[\u0026#39;std_test_score\u0026#39;] params = results.cv_results_[\u0026#39;params\u0026#39;] for mean,std,params in zip(mean_score,std_score,params): print(f\u0026#39;{round(mean,3)} + or -{round(std,3)} for the {params}\u0026#39;) display(cv) Parameters Number of decision trees This is specified using the n_estimators hyper-parameter on the random forest initialisation.\nTypically, a higher number of trees will lead to greater accuracy at the expense of model size and training time.\ncv = GridSearchCV(rfc,{\u0026#34;n_estimators\u0026#34;:[2, 4, 8, 16, 32, 64, 128, 256, 512]},cv=5) cv.fit(X_train, y_train.values.ravel()) results = pd.DataFrame({\u0026#34;n_estimators\u0026#34;: [param[\u0026#34;n_estimators\u0026#34;] for param in cv.cv_results_[\u0026#39;params\u0026#39;]], \u0026#34;mean_score\u0026#34;: list(cv.cv_results_[\u0026#39;mean_test_score\u0026#39;]), \u0026#34;std_score\u0026#34;: cv.cv_results_[\u0026#39;std_test_score\u0026#39;]}) results from plotnine import * ( ggplot(results) + geom_boxplot(aes(x=\u0026#39;factor(n_estimators)\u0026#39;, y=\u0026#39;mean_score\u0026#39;)) + geom_errorbar(aes(x=\u0026#39;factor(n_estimators)\u0026#39;, ymin=\u0026#39;mean_score - std_score\u0026#39;, ymax=\u0026#39;mean_score + std_score\u0026#39;)) + theme_classic() + xlab(\u0026#39;Number of trees\u0026#39;) + ylab(\u0026#39;Mean score\u0026#39;) ) The split criteria At each node, a random forest decides, according to a specific algorithm, which feature and value split the tree. Therefore, the choice of splitting algorithm is crucial for the random forest\u0026rsquo;s performance.\nSince, in this example, we are dealing with a classification problem, the choices of split algorithm are, for instance:\nGini Entropy If we were dealing with a random forest for regression, other methods (such as MSE) would be a possible choice. We will now compare both split algorithms as specified above, in training a random forest with our data:\nrfc = RandomForestClassifier(n_estimators=256) cv = GridSearchCV(rfc,{\u0026#34;criterion\u0026#34;: [\u0026#34;gini\u0026#34;, \u0026#34;entropy\u0026#34;]},cv=5) cv.fit(X_train, y_train.values.ravel()) results = pd.DataFrame({\u0026#34;criterion\u0026#34;: [param[\u0026#34;criterion\u0026#34;] for param in cv.cv_results_[\u0026#39;params\u0026#39;]], \u0026#34;mean_score\u0026#34;: list(cv.cv_results_[\u0026#39;mean_test_score\u0026#39;]), \u0026#34;std_score\u0026#34;: cv.cv_results_[\u0026#39;std_test_score\u0026#39;]}) results Maximum depth of individual trees In theory, the \u0026ldquo;longer\u0026rdquo; the tree, the more splits it can have and better accommodate the data. However, at the tree level can this can lead to overfitting. Although this is a problem for decision trees, it is not necessarily a problem for the ensemble, the random forest. Although the key is to strike a balance between trees that aren\u0026rsquo;t too large or too short, there\u0026rsquo;s no universal heuristic to determine the size. Let\u0026rsquo;s try a few option for maximum depth:\nrfc = RandomForestClassifier(n_estimators=256, criterion=\u0026#34;entropy\u0026#34;) cv = GridSearchCV(rfc,{\u0026#39;max_depth\u0026#39;: [2, 4, 8, 16, 32, None]},cv=5) cv.fit(X_train, y_train.values.ravel()) results = pd.DataFrame({\u0026#34;max_depth\u0026#34;: [param[\u0026#34;max_depth\u0026#34;] for param in cv.cv_results_[\u0026#39;params\u0026#39;]], \u0026#34;mean_score\u0026#34;: list(cv.cv_results_[\u0026#39;mean_test_score\u0026#39;]), \u0026#34;std_score\u0026#34;: cv.cv_results_[\u0026#39;std_test_score\u0026#39;]}) results = results.dropna() results from plotnine import * ( ggplot(results) + geom_boxplot(aes(x=\u0026#39;factor(max_depth)\u0026#39;, y=\u0026#39;mean_score\u0026#39;)) + theme_classic() + xlab(\u0026#39;Max tree depth\u0026#39;) + ylab(\u0026#39;Mean score\u0026#39;) ) Maximum number of leaf nodes This hyperparameter can be of importance to other topics, such as explainability.\nIt is specified in scikit-learn using the max_leaf_nodes parameter. Let\u0026rsquo;s try a few different values:\nrfc = RandomForestClassifier(n_estimators=256, criterion=\u0026#34;entropy\u0026#34;, max_depth=8) cv = GridSearchCV(rfc,{\u0026#39;max_leaf_nodes\u0026#39;: [2**i for i in range(1, 8)]},cv=5) cv.fit(X_train, y_train.values.ravel()) results = pd.DataFrame({\u0026#34;max_leaf_nodes\u0026#34;: [param[\u0026#34;max_leaf_nodes\u0026#34;] for param in cv.cv_results_[\u0026#39;params\u0026#39;]], \u0026#34;mean_score\u0026#34;: list(cv.cv_results_[\u0026#39;mean_test_score\u0026#39;]), \u0026#34;std_score\u0026#34;: cv.cv_results_[\u0026#39;std_test_score\u0026#39;]}) results = results.dropna() results from plotnine import * ( ggplot(results) + geom_boxplot(aes(x=\u0026#39;factor(max_leaf_nodes)\u0026#39;, y=\u0026#39;mean_score\u0026#39;)) + geom_errorbar(aes(x=\u0026#39;factor(max_leaf_nodes)\u0026#39;, ymin=\u0026#39;mean_score - std_score\u0026#39;, ymax=\u0026#39;mean_score + std_score\u0026#39;)) + theme_classic() + xlab(\u0026#39;Maximum leaf nodes\u0026#39;) + ylab(\u0026#39;Mean score\u0026#39;) ) Random features per split This is an important hyperparameter that will depend on how noisy the original data is. Typically, if the data is not very noisy, the number of used random features can be kept low. Otherwise, it needs to be kept high.\nAn important consideration is also the following trade-off:\nA low number of random features decrease the forest\u0026rsquo;s overall variance A low number of random features increases the bias A high number of random features increases computational time In scikit-learn this is specified with the max_features parameter. Assuming $N_f$ is the total number of features, some possible values for this parameter are:\nsqrt, this will take the max_features as the rounded $\\sqrt{N_f}$ log2, as above, takes the $\\log_2(N_f)$ The actual maximum number of features can be directly specified Let\u0026rsquo;s try a simple benchmark, even though our data does not have many features to begin with:\nrfc = RandomForestClassifier(n_estimators=256, criterion=\u0026#34;entropy\u0026#34;, max_depth=8) cv = GridSearchCV(rfc,{\u0026#39;max_features\u0026#39;: [\u0026#34;sqrt\u0026#34;, \u0026#34;log2\u0026#34;, 1, 2, 3, 4, 5, 6]},cv=5) cv.fit(X_train, y_train.values.ravel()) results = pd.DataFrame({\u0026#34;max_features\u0026#34;: [param[\u0026#34;max_features\u0026#34;] for param in cv.cv_results_[\u0026#39;params\u0026#39;]], \u0026#34;mean_score\u0026#34;: list(cv.cv_results_[\u0026#39;mean_test_score\u0026#39;]), \u0026#34;std_score\u0026#34;: cv.cv_results_[\u0026#39;std_test_score\u0026#39;]}) results from plotnine import * ( ggplot(results) + geom_boxplot(aes(x=\u0026#39;factor(max_features)\u0026#39;, y=\u0026#39;mean_score\u0026#39;)) + geom_errorbar(aes(x=\u0026#39;factor(max_features)\u0026#39;, ymin=\u0026#39;mean_score - std_score\u0026#39;, ymax=\u0026#39;mean_score + std_score\u0026#39;)) + theme_classic() + xlab(\u0026#39;Maximum number of features\u0026#39;) + ylab(\u0026#39;Mean score\u0026#39;) ) Bootstrap dataset size This hyperparameter relates to the proportion of the training data to be used by decision trees.\nIt is specified in scikit-learn by max_samples and can take the value of either:\nNone, take the entirety of the samples An integer, representing the actual number of samples A float, representing a proportion between 0 and 1 or the samples to take. Let\u0026rsquo;s try a hyperparameter search with some values:\nrfc = RandomForestClassifier(n_estimators=256, criterion=\u0026#34;entropy\u0026#34;, max_depth=8, max_features=6) cv = GridSearchCV(rfc,{\u0026#39;max_samples\u0026#39;: [i/10.0 for i in range(1, 10)]},cv=5) cv.fit(X_train, y_train.values.ravel()) results = pd.DataFrame({\u0026#34;max_samples\u0026#34;: [param[\u0026#34;max_samples\u0026#34;] for param in cv.cv_results_[\u0026#39;params\u0026#39;]], \u0026#34;mean_score\u0026#34;: list(cv.cv_results_[\u0026#39;mean_test_score\u0026#39;]), \u0026#34;std_score\u0026#34;: cv.cv_results_[\u0026#39;std_test_score\u0026#39;]}) results from plotnine import * ( ggplot(results) + geom_boxplot(aes(x=\u0026#39;factor(max_samples)\u0026#39;, y=\u0026#39;mean_score\u0026#39;)) + geom_errorbar(aes(x=\u0026#39;factor(max_samples)\u0026#39;, ymin=\u0026#39;mean_score - std_score\u0026#39;, ymax=\u0026#39;mean_score + std_score\u0026#39;)) + theme_classic() + xlab(\u0026#39;Proportion bootstrap samples\u0026#39;) + ylab(\u0026#39;Mean score\u0026#39;) ) Titanic Dataset - https://www.kaggle.com/c/titanic-dataset/data\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nhttps://sklearn.org/modules/generated/sklearn.ensemble.RandomForestClassifier.html\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nhttps://sklearn.org/modules/generated/sklearn.ensemble.RandomForestClassifier.html#sklearn.ensemble.RandomForestClassifier.fit\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"/optimising-random-forest-hyperparamaters.html","tags":null,"title":"Optimising random forest hyperparameters"},{"categories":null,"contents":"Pandas1 provides high-performance, easy-to-use data structures and data analysis tools for the Python language.\nPandas basics Extending pandas dataframes https://pandas.pydata.org/\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"/pandas.html","tags":null,"title":"Pandas"},{"categories":null,"contents":" Dataframe operations Create a random dataframe import pandas as pd import numpy as np N=100 df = pd.DataFrame({ \u0026#39;a\u0026#39;:np.random.randn(N), \u0026#39;b\u0026#39;:np.random.choice( [5,7,np.nan], N), \u0026#39;c\u0026#39;:np.random.choice( [\u0026#39;foo\u0026#39;,\u0026#39;bar\u0026#39;,\u0026#39;baz\u0026#39;], N), }) df.head() a b c 0 -1.917057 7.0 baz 1 -1.867614 5.0 foo 2 0.298297 7.0 foo 3 0.498242 5.0 bar 4 0.390240 5.0 bar Concatenate dataframes Row-wise To concatenate dataframes row-wise (i.e. to append more rows to dataframes with the same structure) we can use the .concat() method. For instance, if we create a new random dataframe:\ndf_extra = pd.DataFrame({ \u0026#39;a\u0026#39;:np.random.randn(N), \u0026#39;b\u0026#39;:np.random.choice( [11,12,13], N), \u0026#39;c\u0026#39;:np.random.choice( [\u0026#39;zombie\u0026#39;,\u0026#39;woof\u0026#39;,\u0026#39;nite\u0026#39;], N), }) df_extra.head() a b c 0 0.863633 13 woof 1 -0.819802 12 woof 2 -0.202597 12 zombie 3 0.808857 11 nite 4 -2.089671 13 woof We can now concatenate an arbitray number of dataframes by passing them as a list:\ndf_all = pd.concat([df, df_extra]) df_all.sample(9) a b c 38 -0.135197 NaN foo 3 0.498242 5.0 bar 57 -0.520117 13.0 zombie 88 0.636065 12.0 zombie 33 -1.530864 13.0 zombie 44 0.596794 NaN baz 17 -0.131911 NaN baz 70 1.050289 5.0 baz 80 0.223156 NaN foo Column operations Check column existence The in keyword can be used directly to check column existence.\n\u0026#39;b\u0026#39; in df True Renaming columns df.rename(columns={\u0026#34;a\u0026#34;: \u0026#34;new_name\u0026#34;}, inplace=True) df.columns Index(['new_name', 'b', 'c'], dtype='object') Using a mapping function. In this case str.upper():\ndf.rename(columns=str.upper, inplace=True) df.columns Index(['NEW_NAME', 'B', 'C'], dtype='object') We can also use a lambda. For instance, using lambda x: x.capitalize() would result:\ndf.rename(columns=lambda x: x.capitalize(), inplace=True) df.columns Index(['New_name', 'B', 'C'], dtype='object') A list of column names can be passed directly to columns.\ndf.columns = [\u0026#34;first\u0026#34;, \u0026#34;second\u0026#34;, \u0026#34;third\u0026#34;] df.columns Index(['first', 'second', 'third'], dtype='object') Dropping columns A column can be dropped using the .drop() method along with the column keyword. For instance in the dataframe df: We can drop the second column using:\ndf.drop(columns=\u0026#39;second\u0026#39;) first third 0 -1.917057 baz 1 -1.867614 foo 2 0.298297 foo 3 0.498242 bar 4 0.390240 bar ... ... ... 95 -0.848204 bar 96 -0.552840 baz 97 2.051078 foo 98 0.770107 baz 99 1.837310 bar 100 rows  2 columns\nThe del keyword is also a possibility. However, del changes the dataframe in-place, therefore we will make a copy of the dataframe first.\ndf_copy = df.copy() df_copy first second third 0 -1.917057 7.0 baz 1 -1.867614 5.0 foo 2 0.298297 7.0 foo 3 0.498242 5.0 bar 4 0.390240 5.0 bar ... ... ... ... 95 -0.848204 7.0 bar 96 -0.552840 5.0 baz 97 2.051078 7.0 foo 98 0.770107 NaN baz 99 1.837310 7.0 bar 100 rows  3 columns\ndel df_copy[\u0026#39;second\u0026#39;] df_copy first third 0 -1.917057 baz 1 -1.867614 foo 2 0.298297 foo 3 0.498242 bar 4 0.390240 bar ... ... ... 95 -0.848204 bar 96 -0.552840 baz 97 2.051078 foo 98 0.770107 baz 99 1.837310 bar 100 rows  2 columns\nYet another possibility is to drop the column by index. For instance:\ndf.drop(columns=df.columns[1]) first third 0 -1.917057 baz 1 -1.867614 foo 2 0.298297 foo 3 0.498242 bar 4 0.390240 bar ... ... ... 95 -0.848204 bar 96 -0.552840 baz 97 2.051078 foo 98 0.770107 baz 99 1.837310 bar 100 rows  2 columns\nOr we could use ranges, for instance:\ndf.drop(columns=df.columns[0:2]) third 0 baz 1 foo 2 foo 3 bar 4 bar ... ... 95 bar 96 baz 97 foo 98 baz 99 bar 100 rows  1 columns\n","permalink":"/pandas-basics.html","tags":null,"title":"Pandas basics"},{"categories":null,"contents":"Notes on Pikchr1.\nInstallation Download pikchr~ from the downloads page To create the CLI command, compile using\n$ gcc -DPIKCHR_SHELL -o pikchr pikchr.c -lm And add it to your path.\nExamples A: box \u0026#34;head\u0026#34; fit B: box \u0026#34;tail\u0026#34; fit C: box \u0026#34;something\u0026#34; with .sw at A.nw fit wid dist(A.w, B.e) https://pikchr.org/\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"/pikchr.html","tags":null,"title":"Pikchr"},{"categories":null,"contents":"Last year (2020) we spent Christmas in \u0026ldquo;lockdown\u0026rdquo; and we tried to make ourselves our full traditional Portuguese Christmas recipes from scratch \u0026ndash; while not being in Portugal. Herein lies the first issue: there are many different \u0026ldquo;traditions\u0026rdquo;, but these are the ones that me and my partner are used to.\nTraditionally, Christmas celebrations in Portugal start on the night of Christmas eve and carry on during Christmas day. The main meals are then dinner on the 24th December and lunch on the 25th December.\nChristmas eve dinner As mentioned, we will follow two separate traditions. From my family\u0026rsquo;s side, originally from the north of Portugal (Porto), we typically have boiled salted cod with vegetables (cabbage, onion and carrots), seasoned with olive oil, vinegar and garlic. From my partner\u0026rsquo;s side, the meal typically consists of octopus rice, accompanied with salted cod fishcakes (\u0026quot;bolinhos de bacalhau\u0026quot;) and pan-fried octopus (\u0026quot;filetes de polvo\u0026quot;).\nSalted cod Salted cod is a staple from Portuguese cuisine, with possible origins in the cod salt-curing methods of Basque fishermen that ventured into Newfoundland in the 1500s1. Going through the centuries, even as recently as 1884, Portuguese writer Ea de Queiroz wrote in a letter to Oliveira Martins about his love of a \u0026ldquo;bacalhau de cebolada\u0026rdquo;2.\nThe salted cod needs to be soaked in water for at least four days to remove the excess salt (picture above, on the left).\nIt\u0026rsquo;s a really simple dish: just put everything on a pot and let it boil for approximately one hour. Since the salted cod has a really firm fleshy texture, it won\u0026rsquo;t fall apart like fresh fish when boiled for a long time. Usually there\u0026rsquo;s no need to add salt, since the cod will probably still have quite a lot of salt in it, but it never hurts to double check.\nWe will cook around two to three times the amount of cod and vegetables that we need for the actual meal. The reason for this is that the starter for the next day (oupa-velha) is made from the left-overs of the Christmas eve\u0026rsquo;s dinner. So essentially, we have to make sure we have plenty of left-overs!\nAnd here it is: ready to tuck in. As you can see, I like my salted cod with a very generous amount of olive oil and vinegar.\nOctopus rice Octopus is another northern Portuguese tradition, especially in the Minho and Trs-os-Montes, possibly due to the proximity with Galiza (Galicia) where octopus fishing has been historically a very important activity.\nNext it\u0026rsquo;s the octopus rice. Boil the octopus with just some salt for seasoning. Knowing when the octopus is ready is really an art. Make sure its not undercooked, but don\u0026rsquo;t overcook it either since it will be quite chewy. Brown chopped onions in olive oil and add the water from boiling the octopus along with rice, the octopus and chopped parsley. The rice should have a fair amount of water and not end up dry.\nPart of the octopus goes into the rice and the rest is pan-fried (\u0026quot;filetes de polvo\u0026quot;). They are battered, with eggs and flour, and deep-fried.\nWe then proceed to the cod fishcakes (\u0026quot;bolinhos de bacalhau\u0026quot;). These are done by shredding some salted code and mixing it with mashed potato, salt and parsley and then deep-fried.\nChristmas day lunch Roupa-velha The reason why we cook way more quantities than we need for the salted cod, is to make something called \u0026ldquo;roupa-velha\u0026rdquo; (literal translation \u0026ldquo;old clothes\u0026rdquo;) as a starter on the 25th. This a left-over dish and we use all the left-overs from the Christmas Eve dinner.\nStart by shredding the cooked salted cod and removing all the fish bones and skin.\nWe then add a good amount of garlic (two or three cloves at least), and prepare a pan with some olive oil. We put first the garlic and let it brown.\nWhen the garlic is brown we add all the left-overs (potato, sliced egg, carrot, cabbage and shredded code). We stir it for at least 15 minutes and add vinegar. Lots of vinegar.\nAnd here it is. Must be eaten while pipping hot.\nLamb roast Usually on the 25th of December we eat a roast (turkey, lamb, goat or pork). We went for a lamb roast. It was seasoned with lemon, rosemary, garlic, paprika, olive oil and salt for four days.\nIt is accompanied by roast potatoes and carrots and (optionally) some white rice.\nAnd here it is!\nDesserts Aletria and arroz doce Aletria is a typical Christmas dessert which is quite similar to rice pudding in taste, but instead of rice, it is done with vermicelli pasta.\nThe preparation is quite similar to rice pudding, but adding some lemon peels to the milk mix.\nA cinnamon decoration is a must, here shown with a festive \u0026ldquo;Feliz Natal\u0026rdquo; (Merry Christmas).\n\u0026ldquo;Arroz doce\u0026rdquo; (literal translation Sweet Rice) is very similar to rice pudding, also with the addition of some lemon.\nFilhs Filhs are a type of slightly sweet doughy pancake, usually sprinkled with sugar and cinnamon, traditional during Christmas. These are specific type of filh called \u0026ldquo;Filh tendida no joelho\u0026rdquo;, traditional from the Beiras Portuguese region, where the dough is stretched on top of the knee.\nThe dough has to be proven at a certain temperature. Here is the contraption we\u0026rsquo;ve used: a heating fan, heater and an Hibernate (!) book.\nThe dough must be proved (in our case at least 10 hours) so it can be stretched and fried lightly in olive oil on both sides. After draining any excess oil, they are sprinkled with a sugar and cinnamon mix.\nRabanadas \u0026ldquo;Rabanadas\u0026rdquo; are in essence very similar to \u0026ldquo;French toast\u0026rdquo;. The way to prepare them is to leave dried bread soaking in milk and drained before deep-frying. After draining, until they are mostly dry and without much excess oil, they are sprinkled (generously) with a mixture of sugar and cinnamon.\nBolo-Rei Bolo-Rei (literal translation \u0026ldquo;King cake\u0026rdquo;) is a traditional Portuguese Christmas cake. A similar recipe to the modern one can be traced to the 19th century in Loire, southern France, sold for the first time around 1869 at the Confeitaria Nacional, in Lisboa3. Since then it has become very popular and a common sight at Portuguese Christmas tables.\nAlthough it is perfectly possible to do it at home, the one we had was store-bought.\n2021 update This year (2021) we did another take on the \u0026ldquo;Portuguese Christmas abroad\u0026rdquo; theme. The recipes are still same as last year, so I\u0026rsquo;ve decided to just update with some of the 2021 photos.\nOctopus Last year I didn\u0026rsquo;t include a photo of the octopus being cooked before preparing the Octopus rice, so here it is:\nAnd here is the fried octopus for this year\u0026rsquo;s Christmas eve dinner:\nFilhs Something I\u0026rsquo;ve also forgot to add last year was a picture of the Filhs dough after proving. This year it grew a lot more than last year because we added raw bread dough instead of baker\u0026rsquo;s yeast. Using raw bread dough is actually the proper way to do it, but we didn\u0026rsquo;t have it last year.\nLast year I also didn\u0026rsquo;t include photos of the actual frying of the Filhs. They are deep-fried in olive oil, (not other vegetable oils) so for this amount of dough a considerable amount was needed. Something between 500-750ml.\nAs here is the final product for 2021:\nRabanadas The 2021 rabanadas. The full story on rabanadas is bove:\nCodfish cakes Here are the 2021 bolinhos de bacalhau (salted cod fishcakes):\nSalted cod Cabbage, carrots, onions and eggs waiting for their turn:\nThe salted cod draining a little bit Preparing the bacalhau:\nCooking everything!\nThe final product:\nAletria and arroz doce Aletria and arroz doce:\nRoupa-velha Silva, Antnio Jos Marques da (2015), \u0026ldquo;The fable of the cod and the promised sea: About Portuguese traditions of bacalhau\u0026rdquo;, in Barata, Filipe Themudo; Rocha, Joo Magalhes (eds.), Heritages and Memories from the Sea, vora, Portugal: 1st International Conference of the UNESCO Chair in Intangible Heritage and Traditional Know-How: Linking Heritage\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n\u0026ldquo;A comida como hbito e identidade: o bacalhau e os portugueses\u0026rdquo;, (ISCTE-IUL, Departamento de Antropologia, Escola de Cincias Humanas e Sociais), em 28 de fevereiro de 2013\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nde Lurdes Modesto, Maria, Afonso Praa, and Nuno Calvet. \u0026ldquo;Festas e comeres do povo portugus\u0026rdquo;. 1999.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"/portuguese-christmas-recipes.html","tags":null,"title":"Portuguese Christmas recipes"},{"categories":null,"contents":"Summary Main page for all things Python. Other pages cover specific topics, such as:\nPython environments Python collections Python Pweave Pandas Notes on Python grammar of graphics Installation Anaconda An option to install Python is to use Anaconda1. Download the appropriate installation file from https://www.anaconda.com/products/individual. And then run the installer with, e.g., sh ./Anaconda3-2021.11-Linux-x86_64.sh. The installation would typically be under $HOME/anaconda3. There is a page dedicated to configuring and using [Anaconda].\nLanguage changes In 2021, the Python steering council accepted the proposal to add a pattern-matching primitive to the language. The proposal consists of PEP634 along with PEP635 and PEP636.\npages/backlog/Python 3.10 Cool New Features for You to Try Python 3.9 changes These include new dictionary operators, topological ordering, IPv6 scoped addresses, new math and string functions, and HTTP codes. pip No binary install To install a package from source with pip specify\npip install --no-binary $PACKAGE Since requirements files are passed as command-line options to pip, you can also specify it as\nsome-package --no-binary another-package Additionaly this will also work on setup.py\u0026rsquo;s install_requires. For instance:\nsetup( install_requires=[ \u0026#34;some-package==0.0.1 --no-binary\u0026#34; ]) Modules Relative import in Python 3 If a relative import is present inside a Python 3 file (e.g. file1) inside a module (e.g. mymod), say\nfrom .foo import bar We will encounter the error\nImportError: attempted relative import with no known parent package A possible solution is to include the following in your module\u0026rsquo;s __init__.py:\nimport os, sys sys.path.append(os.path.dirname(os.path.realpath(__file__))) Ternary operator Ternary operators help reduce the amount of very small if-else blocks. Python does not have a ternary operator like other languages. However, conditionals can be used to the same effect:\ny = 7 x = 0 if (y == 1) else 1 print(x) 1 for \u0026hellip; else for-else blocks allow to capture if a condition was met inside a for-loop. For instance, consider the following for-loop:\nlocations = [\u0026#39;a\u0026#39;, \u0026#39;b\u0026#39;, \u0026#39;c\u0026#39;, \u0026#39;d\u0026#39;, \u0026#39;f\u0026#39;] treasure = False for location in locations: if location == \u0026#39;x\u0026#39;: treasure = True break if not treasure: print(\u0026#34;X marks the spot, but not found\u0026#34;) X marks the spot, but not found We can simplify the above logic using a for-else loop:\nfor location in locations: if location == \u0026#39;x\u0026#39;: break else: print(\u0026#34;X marks the spot, but not found\u0026#34;) X marks the spot, but not found Boolean unravelling and unravelling the and boolean operator. The operation can be rewritten as the function u_and:\ndef u_and(a, b): result = a if a: result = b return result For instance:\na = True ; b = None print(a and b, u_and(a, b)) a = True ; b = True print(a and b, u_and(a, b)) a = False ; b = True print(a and b, u_and(a, b)) None None True True False False or On the other hand, or cand be unravelled as:\ndef u_or(a, b): result = a if not a: result = b return result As an example:\na = True ; b = None print(a or b, u_or(a, b)) a = True ; b = True print(a or b, u_or(a, b)) a = False ; b = True print(a or b, u_or(a, b)) True True True True True True The many faces of print Concatenating arguments var1 = \u0026#34;Foo\u0026#34; var2 = \u0026#34;Bar\u0026#34; print(\u0026#34;I am \u0026#34;, var1, \u0026#34; not \u0026#34;, var2) I am Foo not Bar It is also possible to use separators by using the sep argument:\nvar1 = \u0026#34;Foo\u0026#34; var2 = \u0026#34;Bar\u0026#34; print(\u0026#34;I am\u0026#34;, var1, \u0026#34;not\u0026#34;, var2, sep=\u0026#34;!\u0026#34;) I am!Foo!not!Bar String termination The end argument allows to specify the suffix of the whole string.\nprint(\u0026#34;This is on radio\u0026#34;, end=\u0026#34; (over)\u0026#34;) This is on radio (over) Filesystem operations Get home directory For Python +3.5:\nfrom pathlib import Path home = str(Path.home()) List files recursively For Python +3.5, use glob:\nimport glob # root_dir with trailing slash (i.e. /root/dir/) root_dir = \u0026#34;./\u0026#34; for filename in glob.iglob(root_dir + \u0026#39;**/*.md\u0026#39;, recursive=True): print(filename[:-3]) ./Python ./New Features in Python 3.9 ./Understanding Decorators in Python Date operations Offset-aware operations Let\u0026rsquo;s say you have a date without timezone (offset naive), for instance:\nfrom datetime import datetime, timezone ts = datetime.now().replace(tzinfo=None) print(ts) 2022-05-24 12:04:44.440338 And you want to calculate the $\\delta$ with a datetime which has a time (offset aware). We\u0026rsquo;ll get an error.\ntry: delta = datetime.now(timezone.utc) - ts except TypeError as error: print(error) can't subtract offset-naive and offset-aware datetimes The solution is to add a timezone to the offset naive date. For instance:\nts = datetime.now(timezone.utc) delta = datetime.now(timezone.utc) - ts delta datetime.timedelta(microseconds=46) Strings Python strings are immutable, but only sometimes Packaging PyOxidizer An alternative to package Python applications is PyOxidizer.\nPyOxidizer is often used to generate binaries embedding a Python interpreter and a custom Python application. However, its configuration files support additional functionality, such as the ability to produce Windows\nhttps://www.anaconda.com/\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"/python.html","tags":null,"title":"Python"},{"categories":null,"contents":"Interpreters To install different Python interpreters I strongly recommend asdf1.\nLet\u0026rsquo;s look at to install Python in two different OSes, macOS and Fedora.\nmacOS To install asdf on a macOS, first install the general dependencies with\n$ brew install coreutils curl git then install asdf itself with\n$ brew install asdf Add to the shell, in our case zsh with:\n$ echo -e \u0026#34;\\n. $(brew --prefix asdf)/asdf.sh\u0026#34; \u0026gt;\u0026gt; ~/.zshrc Add a plugin, in our case Python, with\n$ asdf plugin add Python You can list all available versions with\n$ asdf list all Python Install a specific version, say,\n$ asdf install Python 3.9.0 Fedora To install asdf on a Fedora, first install the general dependencies\n$ sudo dnf install curl git Clone the repository\n$ git clone https://github.com/asdf-vm/asdf.git \\ ~/.asdf --branch v0.8.0 Add to zsh with\n`$ . $HOME/.asdf/asdf.sh` pyenv Compiling on macOS pyenv can be notoriously problematic on macOS. For instance, running pyenv doctor on my laptop2 will result in:\nCloning ~/.pyenv/plugins/pyenv-doctor/bin/..... Installing python-pyenv-doctor... python-build: use readline from homebrew python-build: use zlib from xcode sdk BUILD FAILED (OS X 10.15.7 using python-build 20180424) Inspect or clean up the working tree at /var/folders/c2/9d2fsqt57t10zn1f2ylp1jxw0000gn/T/python-build.20210128094523.17091 Results logged to /var/folders/c2/9d2fsqt57t10zn1f2ylp1jxw0000gn/T/python-build.20210128094523.17091.log Last 10 log lines: checking readline/readline.h, presence... no checking for readline/readline.h,... no checking readline/rlconf.h usability... yes checking readline/rlconf.h presence... yes checking for readline/rlconf.h... yes checking for SSL_library_init in -lssl... no configure: WARNING: OpenSSL \u0026lt;1.1 not installed. Checking v1.1 or beyond... checking for OPENSSL_init_ssl in -lssl... no configure: error: OpenSSL is not installed. make: *** No targets specified and no makefile found. Stop. Problem(s) detected while checking system. See https://github.com/pyenv/pyenv/wiki/Common-build-problems for known solutions. The problem in this case is that pyenv can\u0026rsquo;t find the relevant C headers for compilation of new versions. This can be fixed by using:\n$ CFLAGS=\u0026#34;-I$(brew --prefix openssl)/include \\ -I$(brew --prefix readline)/include \\ -I$(xcrun --show-sdk-path)/usr/include\u0026#34; \\ LDFLAGS=\u0026#34;-L$(brew --prefix openssl)/lib \\ -L$(brew --prefix readline)/lib \\ -L$(xcrun --show-sdk-path)/usr/lib\u0026#34; \\ pyenv doctor and the output will be:\nCloning ~/.pyenv/plugins/pyenv-doctor/bin/..... Installing python-pyenv-doctor... python-build: use readline from homebrew python-build: use zlib from xcode sdk Installed python-pyenv-doctor to /var/folders/c2/9d2fsqt57t10zn1f2ylp1jxw0000gn/T/pyenv-doctor.20210128095003.18889/prefix Congratulations! You are ready to build pythons! Poetry Poetry as Jupyter kernel To register a poetry environment (named foo) as a Jupyter kernel, run:\npoetry run python -m ipykernel install --user --name foo venv Create a new venv with the command:\n$ virtualenv venv Alternatively, create the virtualenv name foo in ~/.virtualenvs using\n$ python -m venv ~/.virtualenvs/foo and activate it using (under Bash or zsh) with:\n$ source venv/bin/activate Anaconda First download Anaconda (or Miniconda). Once installed you can proceed to create environments3.\nCreating environments An environment foo can be created using\nconda create --name foo One it is created, it can be activated using\nconda activate foo https://asdf-vm.com/\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nI\u0026rsquo;m running Big Sur at the moment of writing\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nThe remainder will assume that you have installed Anaconda, rather than Miniconda.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"/python-environments.html","tags":null,"title":"Python environments"},{"categories":null,"contents":"import pandas as pd mpg = pd.read_csv(\u0026#34;./data/mpg.csv\u0026#34;) mpg.head() mpg cylinders displacement horsepower weight acceleration model_year origin name 0 18.0 8 307.0 130 3504 12.0 70 1 chevrolet chevelle malibu 1 15.0 8 350.0 165 3693 11.5 70 1 buick skylark 320 2 18.0 8 318.0 150 3436 11.0 70 1 plymouth satellite 3 16.0 8 304.0 150 3433 12.0 70 1 amc rebel sst 4 17.0 8 302.0 140 3449 10.5 70 1 ford torino from plotnine import * from plotnine.data import * ggplot(data=mpg) +\\ geom_point(mapping=aes(x=\u0026#34;displ\u0026#34;, y=\u0026#34;hwy\u0026#34;)) + theme_classic() \u0026lt;ggplot: (324084173)\u0026gt; ggplot(data=mpg) +\\ geom_point(mapping=aes(x=\u0026#34;displ\u0026#34;, y=\u0026#34;hwy\u0026#34;, color=\u0026#34;class\u0026#34;)) + theme_classic() \u0026lt;ggplot: (324192241)\u0026gt; ggplot(data=mpg) +\\ geom_point(mapping=aes(x=\u0026#34;displ\u0026#34;, y=\u0026#34;hwy\u0026#34;, size=\u0026#34;class\u0026#34;)) + theme_classic() \u0026lt;ggplot: (324188691)\u0026gt; # Left ggplot(data=mpg) +\\ geom_point(mapping=aes(x=\u0026#34;displ\u0026#34;, y=\u0026#34;hwy\u0026#34;, alpha=\u0026#34;manufacturer\u0026#34;)) + theme_classic() \u0026lt;ggplot: (324284401)\u0026gt; # Right ggplot(data=mpg) +\\ geom_point(mapping=aes(x=\u0026#34;displ\u0026#34;, y=\u0026#34;hwy\u0026#34;, shape=\u0026#34;manufacturer\u0026#34;)) + theme_classic() \u0026lt;ggplot: (324342522)\u0026gt; ggplot(data=mpg) +\\ geom_point(mapping=aes(x=\u0026#34;displ\u0026#34;, y=\u0026#34;hwy\u0026#34;), color=\u0026#34;blue\u0026#34;) + theme_classic() \u0026lt;ggplot: (324407845)\u0026gt; ggplot(data=mpg) +\\ geom_smooth(mapping=aes(x=\u0026#34;displ\u0026#34;, y=\u0026#34;hwy\u0026#34;)) + theme_classic() \u0026lt;ggplot: (324408381)\u0026gt; ggplot(data=mpg) +\\ geom_smooth(mapping=aes(x=\u0026#34;displ\u0026#34;, y=\u0026#34;hwy\u0026#34;, linetype=\u0026#34;drv\u0026#34;)) + theme_classic() \u0026lt;ggplot: (324443965)\u0026gt; ggplot(data=mpg) +\\ geom_point(mapping=aes(x=\u0026#34;displ\u0026#34;, y=\u0026#34;hwy\u0026#34;)) +\\ geom_smooth(mapping=aes(x=\u0026#34;displ\u0026#34;, y=\u0026#34;hwy\u0026#34;)) + theme_classic() \u0026lt;ggplot: (324525996)\u0026gt; ggplot(data=mpg, mapping=aes(x=\u0026#34;displ\u0026#34;, y=\u0026#34;hwy\u0026#34;)) +\\ geom_point(mapping=aes(color=\u0026#34;class\u0026#34;)) +\\ geom_smooth() + theme_classic() \u0026lt;ggplot: (324562484)\u0026gt; ","permalink":"/python-grammar-of-graphics.html","tags":null,"title":"Python grammar of graphics"},{"categories":null,"contents":"When preparing a Jupyter1 notebook for a workshop on recommendation engines which I\u0026rsquo;ve presented with a colleague, I was faced with the following problem:\n\u0026ldquo;How to break a large class definition into several cells so it can be presented step-by-step.\u0026rdquo;\nHaving the ability to declare a rather complex (and large) Python class in separate cells has several advantages, the obvious one being the ability to fully document each method\u0026rsquo;s functionality with Markdown, rather than comments. Python does allow for functionality to be added to classes after their declaration via the assignment of methods through attributes. This is commonly known as \u0026ldquo;monkey patching\u0026rdquo; and hinges on the concepts of bound and unbound methods.\nI will show a quick and general overview of the methods that Python puts at our disposal for dynamic runtime object manipulation, but for a more in-depth please consult the official Python documentation.\nBound and unbound methods Let\u0026rsquo;s first look at bound methods. If we assume a class called Class and an instance instance, with an instance method bound and class method unbound such that\nclass Class: def bound(self): return \u0026#34;I\u0026#39;m a bound method\u0026#34; @staticmethod def unbound(): return \u0026#34;I\u0026#39;m an unbound method\u0026#34; instance = Class() Then foo is a bound method and bar is an unbound method. This definition, in practice, can be exemplified by the standard way of calling .foo(), which is\ninstance.bound() : I\u0026#39;m a bound method which in turn is equivalent to\nClass.bound(instance) : I\u0026#39;m a bound method The standard way of calling unbound is , similarly\ninstance.unbound() : I\u0026#39;m an unbound method This, however, is equivalent to\nClass.unbound() : I\u0026#39;m an unbound method In the unbound case, we can see there\u0026rsquo;s no need to pass the class instance. unbound is not bound to the class instance.\nAs mentioned before, Python allow us to change the class attributes at runtime. If we consider a method such as\ndef newBound(self): return \u0026#34;I\u0026#39;m a (new!) bound method\u0026#34; we can then add it to the class, even after declaring it. For instance:\nClass.newBound = newBound instance = Class() instance.newBound() # Class.newBound(instance) : I\u0026#39;m a (new!) bound method It is interesting to note that any type of function definition will work, since functions are first class objects in Python. As such, if the method can be written as a single statement, a ~lambda~ could also be used, i.e.\nClass.newBound = lambda self: \u0026#34;I\u0026#39;m a lambda\u0026#34; instance.newBound() : I\u0026#39;m a (new!) bound method A limitation of the \u0026ldquo;monkey patching\u0026rdquo; method, is that attributes can only be changed at the class definition level. As an example, although possible, it is not trivial to add the .newBound() method to instance.\nA solution is to either call the descriptor methods (which allow for instance attribute manipulation), or declare the instance attribute as a MethodType.\nTo illustrate this in our case:\nimport types instance.newBound = types.MethodType(newBound, instance) instance.newBound() # Prints \u0026#34;I\u0026#39;m a lambda\u0026#34; : I\u0026#39;m a (new!) bound method This method is precisely, as mentioned, to change attributes for a specific instance, so in this case, if we try to access the bound method from another instance anotherInstance, it would fail\nanotherInstance = Class() anotherInstance.newBound() # fails with AttributeError : I\u0026#39;m a lambda Abstract classes Python supports abstract classes, i.e. the definition of \u0026ldquo;blueprint\u0026rdquo; classes for which we delegate the concrete implementation of abstract methods to subclasses. In Python 3.x this is done via the @abstractmethod annotation. If we declare a class such as\nfrom abc import ABC, abstractmethod class AbstractClass(ABC): @abstractmethod def abstractMethod(self): pass we can then implement abstractMethod in all of AbstractClass\u0026rsquo;s subclasses:\nclass ConcreteClass(AbstractClass): def abstractMethod(self): print(\u0026#34;Concrete class abstract method\u0026#34;) We could, obviously, do this in Python without abstract classes, but this mechanism allows for a greater safety, since implementation of abstract methods is mandatory in this case. With regular classes, not implementing abstractMethod would simply assume we were using the parent\u0026rsquo;s definition.\nUnfortunately, monkey patching of abstract methods is not supported in Python. We could monkey patch the concrete class:\nConcreteClass.newBound = lambda self: print(\u0026#34;New \u0026#39;child\u0026#39; bound\u0026#34;) c = ConcreteClass() c.newBound() # prints \u0026#34;New \u0026#39;child\u0026#39; bound\u0026#34; : New \u0026#39;child\u0026#39; bound And we could even add a new bound method to the superclass, which will be available to all subclasses:\nAbstractClass.newBound = lambda self: print(\u0026#34;New \u0026#39;parent\u0026#39; bound\u0026#34;) c = ConcreteClass() c.newBound() # prints \u0026#34;New \u0026#39;parent\u0026#39; bound\u0026#34; : New \u0026#39;child\u0026#39; bound However, we can\u0026rsquo;t add abstract methods with monkey patching. This is a documented exception of this functionality with the specific warning that\nDynamically adding abstract methods to a class, or attempting to modify the abstraction status of a method or class once it is created, are not supported. The abstractmethod() only affects subclasses derived using regular inheritance; \u0026ldquo;virtual subclasses\u0026rdquo; registered with the ABC\u0026rsquo;s register() method are not affected.\nPrivate methods We can dynamically add and replace inner methods, such as:\nclass Class: def _inner(self): print(\u0026#34;Inner bound\u0026#34;) def __private(self): print(\u0026#34;Private bound\u0026#34;) def callNewPrivate(self): self.__newPrivate() Class._newInner = lambda self: print(\u0026#34;New inner bound\u0026#34;) c = Class() c._inner() # prints \u0026#34;Inner bound\u0026#34; c._newInner() # prints \u0026#34;New inner bound\u0026#34; : Inner bound : New inner bound However, private methods behave differently. Python enforces name mangling for private methods. As specified in the documentation:\nSince there is a valid use-case for class-private members (namely to avoid name clashes of names with names defined by subclasses), there is limited support for such a mechanism, called name mangling. Any identifier of the form __spam (at least two leading underscores, at most one trailing underscore) is textually replaced with _classname__spam, where classname is the current class name with leading underscore(s) stripped. This mangling is done without regard to the syntactic position of the identifier, as long as it occurs within the definition of a class.\nWe can then still access the private methods (although we probably shouldn\u0026rsquo;t), but monkey patching won\u0026rsquo;t work as before due to the above.\nc._Class__private() # Private bound Class.__newPrivate = lambda self: print(\u0026#34;New private bound\u0026#34;) c = Class() c._Class__newPrivate() # fails with AttributeError We have defined a new method called __newPrivate() but interestingly, this method is not private. We can see this by calling it directly (which is allowed) and by calling the new \u0026ldquo;private\u0026rdquo; method from inside the class as self.__newPrivate():\nc.__newPrivate() # prints \u0026#34;New private bound\u0026#34; c.callNewPrivate() # fails with AttributeError (can\u0026#39;t find _Class_NewPrivate) It is possible to perform some OOP abuse and declare the private method by mangling the name ourselves. In this case we could then do:\nClass._Class__newPrivate = lambda self: print(\u0026#34;New private bound\u0026#34;) c = Class() c._Class__newPrivate() # prints \u0026#34;New private bound\u0026#34; c.callNewPrivate() # prints \u0026#34;New private bound\u0026#34; Builtins Is it possible to monkey patch builtin classes in Python, e.g. int or float? In short, yes, it is.\nAlthough the usefulness is arguable and I strongly urge not to do this in any production scenario, we\u0026rsquo;ll look at how to achieve this, for the sake of completeness. A very interesting and educational read is available from the Forbidden Fruit Python module.\nPrimitive (or builtin) classes in Python are typically written in C and as such some of these meta-programming facilities require jumping through extra hoops (as well as being a Very Bad Idea). Let\u0026rsquo;s first look at the integer class representation, int.\nA int doesn\u0026rsquo;t allow bound methods to be added dynamically as previously. For instance:\np = 5 type(p) # int We can try to add a method to int to square the value of the instance:\nint.square = lambda self: self ** 2 This fails with the error TypeError: can't set attributes of built-in/extension type 'int'. The solution (as presented in Forbidden Fruit) is to first create classes to hold the ctype information of a builtin (C) class. We subclass ctypes Python representation of a C struct in native byte order and hold the signed int size and pointer to PyObject.\nimport ctypes class PyObject(ctypes.Structure): pass PyObject.fields = [ (\u0026#39;ob_refcnt\u0026#39;, ctypes.c_int), (\u0026#39;ob_type\u0026#39;, ctypes.POINTER(PyObject)), ] Next we create a holder for Python objects slots, containing a reference to the ctype structure:\nclass SlotsProxy(PyObject): _fields_ = [(\u0026#39;dict\u0026#39;, ctypes.POINTER(PyObject))] The final step is extract the PyProxyDict from the object referenced by the pointer. Ideally, we should get the builtin\u0026rsquo;s namespace so we can freely set attributes as we did previously. A helper function to retrieve the builtins (mutable) namespace can then be:\ndef patch(klass): name = klass.__name__ target = klass.__dict__ proxy_dict = SlotsProxy.from_address(id(target)) namespace = {} ctypes.pythonapi.PyDict_SetItem( ctypes.py_object(namespace), ctypes.py_object(name), proxy_dict.dict, ) return namespace[name] We can now easily patch builtin classes. Let\u0026rsquo;s try to add the square method again by first retrieving the namespace (stored below in d) and setting it directly\nd = patch(int) d[\u0026#34;square\u0026#34;] = lambda self: self ** 2 p.square() # 25 All future instance of int will also contain the square method now:\n(2 + p).square() # 49 Conclusion \u0026ldquo;Monkey patching\u0026rdquo; is usually, and rightly so, considered a code smell, due to the increased indirection and potential source of unwanted surprises. However, having the ability to \u0026ldquo;monkey patch\u0026rdquo; classes in Python allows us to write Jupyter notebooks in a more literate, fluid way rather than presenting the user with a \u0026ldquo;wall of code\u0026rdquo;. Thank you for reading. If you have any comments or suggestions please drop me a message on Mastodon.\nhttps://jupyter.org/\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"/python-monkey-patching-for-readability.html","tags":null,"title":"Python monkey patching (for readability)"},{"categories":null,"contents":"Subsetting and indexing Indexing performance Let\u0026rsquo;s assume the case where you a column BOOL with values Y or N that you want to replace with an integer 1 or 0 value. The inital1 instinct would be to do something like:\ndf[\u0026#34;BOOL\u0026#34;] = df[\u0026#34;BOOL\u0026#34;].eq(\u0026#34;Y\u0026#34;).mul(1) This will result in the warning\nSettingWithCopyWarning: A value is trying to be set on a copy of a slice from a DataFrame. Try using .loc[row_indexer,col_indexer] = value instead Pandas documentation recommends the usage of the following idiom, since it can be considerably faster:\ndf.loc[:, (\u0026#34;BOOL\u0026#34;)] = df.loc[:, (\u0026#34;BOOL\u0026#34;)].eq(\u0026#34;Y\u0026#34;).mul(1) and Pythonic?\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"/python-pandas.html","tags":null,"title":"Python Pandas"},{"categories":null,"contents":"Installing Installing pweave is a matter of simply running1:\npip3 install pweave At the moment of writing, the editor which, IMO, has the best support for pweave is Atom (using Hydrogen). To install the necessary packages run:\napm install language-weave Hydrogen apm install language-markdown atom-html-preview pdf-view I recommend using a separate pyenv for this.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"/python-pweave.html","tags":null,"title":"Python Pweave"},{"categories":null,"contents":"Page for Quarkus.\nCreating projects An example on how to create a Quarkus project:\nmvn io.quarkus:quarkus-maven-plugin:2.5.1.Final:create \\ -DprojectGroupId=com.okta.rest \\ -DprojectArtifactId=quarkus \\ -DclassName=\u0026#34;com.okta.rest.quarkus.HelloResource\u0026#34; \\ -Dpath=\u0026#34;/hello\u0026#34; \\ -Dextensions=\u0026#34;smallrye-jwt,resteasy-reactive\u0026#34; ","permalink":"/quarkus.html","tags":null,"title":"Quarkus"},{"categories":null,"contents":"Introduction A random walk is a stochastic process which describes a path made of consecutive random steps.\nGaussian In Gaussian random walk the steps follow a continuous Gaussian distribution. We will look at two different types, the univariate and multivariate kind.\nUnivariate A univariate Gaussian Random Walk, is a series of i.i.d. $\\mathcal{N}(0,1)$ random variables such that\n$$ \\begin{align*} X_0\u0026amp;=0 \\ X_t\u0026amp;=X_{t1}+\\epsilon_t \\end{align*} $$\nWhere $t=1,2,\\dots$ and $\\epsilon_t$ is a series of i.i.d. $\\mathcal{N}(0,1)$ random variables.\nLet\u0026rsquo;s illustrate a simple univariate Gaussian random walk in Python, by plotting 1000 realisations.\nimport numpy as np N = 1000 np.random.seed(23) realisations = [] for i in range(N): realisations.append(np.cumsum(np.random.normal(size=100))) import matplotlib.pyplot as plt from plotutils import * for i in range(N): plt.plot(realisations[i], c=\u0026#34;k\u0026#34;, alpha=0.1) plt.title(\u0026#34;Univariate Gaussian random walk\u0026#34;) plt.xlabel(\u0026#34;t\u0026#34;) plt.ylabel(\u0026#34;$x_t$\u0026#34;) plt.show() ","permalink":"/random-walk.html","tags":null,"title":"Random walk"},{"categories":null,"contents":"What I\u0026rsquo;m reading now and what I\u0026rsquo;ve read in the past.\nReading now Duende Meadow, by Paul Cook (1985) Past readings Books I have read recently (most recently first).\nHospital Station, by James White (1962) Retief: Envoy to New Worlds, by Keith Laumer (1972) The Ballad of Beta-2, by Samuel R. Delany (1965) Damnation Alley, by Roger Zelazny (1977) The Practicing Stoic: A Philosophical User\u0026rsquo;s Manual. (May 2021) ","permalink":"/reading-list.html","tags":null,"title":"Reading list"},{"categories":null,"contents":"Sandbox RHODS can be trialled on the RHODS developer sandbox.\n","permalink":"/rhods.html","tags":null,"title":"RHODS"},{"categories":null,"contents":"Receiver operating characteristic ROC (Receiver operating characteristic).\nimport pandas as pd data = pd.read_csv(\u0026#34;./data/credit-bias.zip\u0026#34;) We plot on the x the False-Positive rate and plot on the y the True-positive rate.\n","permalink":"/roc.html","tags":null,"title":"ROC"},{"categories":null,"contents":"A typical way of measuring the difference between observations and results from a predictor.\nThe formal definition is:\n$$ \\begin{aligned} RMSE(\\hat{\\theta}) \u0026amp;= \\sqrt{\\operatorname{MSE}(\\hat{\\theta})} \\\\ \u0026amp;= \\sqrt{\\operatorname{E}((\\hat{\\theta}-\\theta)^2)}. \\end{aligned} $$\nFor $N$ observations $Y={y_1, \\dots,y_N}$ we can express it as:\n$$ RMSE=\\sqrt{\\frac{\\sum _ {n=1}^{N}(\\hat{y} _{n}-y _{n})^{2}}{N}}. $$\nExample import numpy as np X = 2 * np.random.rand(1000,1) X_b = np.c_[np.ones((1000,1)), X] Y = 1 + 2.5 * X + np.random.randn(1000,1) ","permalink":"/root-mean-squared-error.html","tags":null,"title":"Root Mean Squared Error"},{"categories":null,"contents":"A page about RSS.\nHistory RSS has been called\nRDF Site Summary Rich Site Summary and (most recently) Really Simple Syndication Initially developed by Netscape in 1999 for the my.netscape.com portal.\nArticles It\u0026rsquo;s Time to Get Back Into RSS | Daniel Miessler Ask HN: How do you RSS? | Hacker News - https://news.ycombinator.com/item?id=23577265 https://danielmiessler.com/blog/its-time-to-get-back-into-rss/ RSS Feed Best Practises ","permalink":"/rss.html","tags":null,"title":"RSS"},{"categories":null,"contents":"Install $ curl --proto \u0026#39;=https\u0026#39; --tlsv1.2 -sSf https://sh.rustup.rs | sh $ source $HOME/.cargo/env Create a new project using $ cargo new hello_world --bin # for a binary $ cargo new hello_world # for a library Exercises Rust exercises, resolution of the rustlings exercises Reference Uploading to crates.io Use\ncargo publish Static constants \u0026ldquo;Lazy\u0026rdquo; static constants can be defined using the lazy_static macro from the lazy_static crate.\nThe crate can be added to the dependencies with\n[dependencies] lazy_static = \u0026#34;1.4.0\u0026#34; and by adding to the Rust source code:\n#[macro_use] extern crate lazy_static; Static \u0026ldquo;global\u0026rdquo; constants can then be added via:\nlazy_static! { static ref HASHMAP: HashMap\u0026lt;u32, \u0026amp;\u0026#39;static str\u0026gt; = { let mut m = HashMap::new(); m.insert(0, \u0026#34;foo\u0026#34;); m.insert(1, \u0026#34;bar\u0026#34;); m.insert(2, \u0026#34;baz\u0026#34;); m }; } List folders recursively Using the glob crate:\nuse glob::glob; fn main() { for entry in glob(\u0026#34;./**/*.md\u0026#34;).expect(\u0026#34;Failed to read glob pattern\u0026#34;) { match entry { Ok(path) =\u0026gt; println!(\u0026#34;{:?}\u0026#34;, path.display()), Err(e) =\u0026gt; println!(\u0026#34;{:?}\u0026#34;, e), } } } Duration between two times Using Rust\u0026rsquo;s standard SystemTime1, a duration between the present moment and a specific time can be calculated using:\nlet duration = SystemTime::now() .duration_since(another_system_time) .ok() .unwrap(); https://doc.rust-lang.org/std/time/struct.SystemTime.html\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"/rust.html","tags":null,"title":"Rust"},{"categories":null,"contents":"Notes on the Scala language.\n[Scala cookbook ","permalink":"/scala.html","tags":null,"title":"Scala"},{"categories":null,"contents":"val a = (0 until 10) : a: Range = Range(0, 1, 2, 3, 4, 5, 6, 7, 8, 9) println(a.map(i =\u0026gt; i + 10)) : Vector(10, 11, 12, 13, 14, 15, 16, 17, 18, 19) ","permalink":"/scala-cookbook.html","tags":null,"title":"Scala cookbook"},{"categories":null,"contents":"Collection of notes on Python\u0026rsquo;s scikit-learn machine learning library.\nOptimising random forest hyperparamaters 15 Lesser-Known Useful SkLearn Models You Should Use Now ","permalink":"/scikit-learn.html","tags":null,"title":"Scikit-learn"},{"categories":null,"contents":"This file exists solely to respond to /search URL with the related search layout template.\nNo content shown here is rendered, all content is based in the template layouts/page/search.html\nSetting a very low sitemap priority will tell search engines this is not important content.\nThis implementation uses Fusejs, jquery and mark.js\nInitial setup Search depends on additional output content type of JSON in config.toml\n[outputs] home = [\u0026#34;HTML\u0026#34;, \u0026#34;JSON\u0026#34;] Searching additional fileds To search additional fields defined in front matter, you must add it in 2 places.\nEdit layouts/_default/index.JSON This exposes the values in /index.json i.e. add category\n... \u0026#34;contents\u0026#34;:{{ .Content | plainify | jsonify }} {{ if .Params.tags }}, \u0026#34;tags\u0026#34;:{{ .Params.tags | jsonify }}{{end}}, \u0026#34;categories\u0026#34; : {{ .Params.categories | jsonify }}, ... Edit fuse.js options to Search static/js/search.js\nkeys: [ \u0026#34;title\u0026#34;, \u0026#34;contents\u0026#34;, \u0026#34;tags\u0026#34;, \u0026#34;categories\u0026#34; ] ","permalink":"/search.html","tags":null,"title":"Search Results"},{"categories":null,"contents":"Deploying machine learning models in production comes with several requirements. We must manage the model lifecycle. We need reproducibility and typically use containerised workflows.\nSeldon1 is a tool which aims at providing a production workflow for machine learning models, allowing to build model serving containers which expose well-defined APIs.\nIn this post, I\u0026rsquo;ll show how to create a simple model and how to deploy it with Seldon. The model is a customer segmentation one. The goal is to classify a customer according to a segment (0, 1 or 2), according to its age, income, whether they engaged with previous campaigns and the campaign type.\nOnce we train the model, we deploy it with Seldon in a container orchestration platform such as Kubernetes2 and OpenShift3.\nCreate data We use the Python\u0026rsquo;s Scikit-learn4 to train our model. However, we must first simulate some data to train it. We start by simulating the users age ($a$) and income ($c$). We assume income is correlated with age.\n$$ \\begin{aligned} c|a \u0026amp;\\sim \\mathcal{N}\\left(a + 20, 100\\right) \\\\ a|k \u0026amp;\\sim \\mathcal{U}\\left(A_k, B_k\\right),\\quad A=\\left\\lbrace16, 25, 50, 61\\right\\rbrace,B=\\left\\lbrace24, 49, 60, 90\\right\\rbrace \\\\ k \u0026amp;\\sim \\mathcal{M}\\left(4, \\left\\lbrace 0.15, 0.4, 0.2, 0.25\\right\\rbrace\\right) \\end{aligned} $$\nLet\u0026rsquo;s assume we have eight distinct events ($e=\\left(0, 1, \\dots, 7\\right)$). We sample them from a multinomial distribution and also assume that two different age bands have different distributions, just to add some variation.\n$$ e = \\begin{cases} \\mathcal{M}\\left(7, \\left\\lbrace 0.026, 0.195, 0.156, 0.208, 0.130, 0.205, 0.078 \\right\\rbrace\\right) \u0026amp; \\text{if}\\ a \u0026lt; 50 \\\\ \\mathcal{M}\\left(7, \\left\\lbrace 0.052, 0.143, 0.169, 0.182, 0.164, 0.182, 0.104 \\right\\rbrace\\right) \u0026amp; \\text{if}\\ a \\geq 50 \\end{cases} $$\nThe responses are calculated as 0 or 1, representing \u0026ldquo;true\u0026rdquo; or \u0026ldquo;false\u0026rdquo;, and sampled from Bernoulli distributions, with different distributions depending on the event, again just to add some variation.\n$$ r = \\begin{cases} \\text{Bernoulli}\\left(0.6\\right) \u0026amp; \\text{if}\\ e \\in \\left(2, 3, 4, 6\\right) \\\\ \\text{Bernoulli}\\left(0.4\\right) \u0026amp; \\text{if}\\ e \\in \\left(1, 5, 7\\right) \\end{cases} $$\nTo predict the response of a customer, we use a logistic model, with coefficients $\\beta_{age}=-0.0004$ and $\\beta_{income}=0.0001$. For the customer level, we use a negative binomial model with coefficients $\\beta_{age}=-0.0233$ and $\\beta_{income}=0.0054$. This results in the following distribution of customer levels:\nFinally, we create the response according to negative binomial model with coefficients $\\beta_{level}=0.1862$ and $\\beta_{response}=0.2076$. We get the following segments, stratified by age and income:\nTrain model Now that we have our simulated data, we can train a model. Generally, it is straightforward to train model data when in pandas data frame format. Let\u0026rsquo;s proceed with creating a data frame with the data we\u0026rsquo;ve just generated:\nimport pandas as pd data = { \u0026#34;age\u0026#34;: age, \u0026#34;income\u0026#34;: income, \u0026#34;class\u0026#34;: _class, \u0026#34;response\u0026#34;: response, \u0026#34;segment\u0026#34;: segment, \u0026#34;events\u0026#34;: events, } df = pd.DataFrame(data) We now create the training and testing datasets. The first thing is to define the classifier\u0026rsquo;s inputs and outputs and then splitting each of them into training and testing. Here I have used a split of 60%/40% for training and testing respectively.\nfrom sklearn.model_selection import train_test_split cols = [\u0026#34;age\u0026#34;, \u0026#34;income\u0026#34;, \u0026#34;response\u0026#34;, \u0026#34;events\u0026#34;] inputs = df[cols] outputs = df[\u0026#34;segment\u0026#34;] # split dataset X_train, X_test, y_train, y_test = train_test_split( inputs, outputs, test_size=0.4, random_state=23 ) We use a Random Forest classifier as the underlying algorithm for our model. These are available in sciki-learn with the RandomForestClassifier class. However, scikit-learn does not support categorical variables out of the box5. To deal with them, we build a Pipeline, which allows to chain multiple transformations to our data, including a categorical variable processor, such as OrdinalEncoder6. We use DataFrameMapper to apply the encoder to the response and events columns and leave the remaining unchanged.\nfrom sklearn.ensemble import RandomForestClassifier from sklearn import preprocessing from sklearn.pipeline import Pipeline def build_RF_pipeline(inputs, outputs, rf=None): if not rf: rf = RandomForestClassifier() pipeline = Pipeline( [ ( \u0026#34;mapper\u0026#34;, DataFrameMapper( [ ([\u0026#34;response\u0026#34;, \u0026#34;events\u0026#34;], preprocessing.OrdinalEncoder()), ([\u0026#34;age\u0026#34;, \u0026#34;income\u0026#34;], None), ] ), ), (\u0026#34;classifier\u0026#34;, rf), ] ) pipeline.fit(inputs, outputs) return pipeline The actual training involves a simple hyper-parameter estimation using RandomizedSearchCV. This method performs a type of parameter grid search but restricting the search to only the specified values. For the scope of this post, it is not necessary to perform an exhaustive hyperparameter estimation. The RF_estimation function returns the best-fitted model after searching with the test dataset.\ndef RF_estimation( inputs, outputs, estimator_steps=10, depth_steps=10, min_samples_split=None, min_samples_leaf=None, ): # hyper-parameter estimation n_estimators = [ int(x) for x in np.linspace(start=50, stop=100, num=estimator_steps) ] max_depth = [int(x) for x in np.linspace(3, 10, num=depth_steps)] max_depth.append(None) if not min_samples_split: min_samples_split = [1, 2, 4] if not min_samples_leaf: min_samples_leaf = [1, 2, 4] bootstrap = [True, False] random_grid = { \u0026#34;n_estimators\u0026#34;: n_estimators, \u0026#34;max_depth\u0026#34;: max_depth, \u0026#34;min_samples_split\u0026#34;: min_samples_split, \u0026#34;min_samples_leaf\u0026#34;: min_samples_leaf, \u0026#34;bootstrap\u0026#34;: bootstrap, } rf_random = RandomizedSearchCV( estimator=RandomForestClassifier(), param_distributions=random_grid, n_iter=100, scoring=\u0026#34;neg_mean_absolute_error\u0026#34;, cv=3, verbose=1, random_state=42, n_jobs=-1, ) rf_random.fit(inputs, outputs) best_random = rf_random.best_estimator_ return best_random After applying the parameter estimation, we take the best scoring model and calculate the MSE. Unsurprisingly (given the simple model and simulated data), we get a very good fit.\nrf_predictions = random_forest_pipeline.predict(X_test) print(f\u0026#34;MSE: {random_forest_pipeline.score(X_test, y_test)*100}%\u0026#34;) # MSE: 99.95% The final step is serialising the model. Serialisation is necessary since we only serve the pre-trained model. To do so, we use the joblib library and save the model to a model.pkl file.\nimport joblib # save mode in filesystem joblib.dump(random_forest_pipeline, \u0026#34;model.pkl\u0026#34;) Deploy model It is important to note that we don\u0026rsquo;t need the model training code included in the Seldon server. The purpose of Seldon is not to train models, but to deploy them and manage their lifecycle. This workflow means that a typical Seldon deployment would only include the prediction endpoint implementation and a serialised model. This provision is made by firstly create a wrapper for our model which implements the Seldon endpoints.\nSimple model We create a Python script called Model.py 7. The primary prediction endpoint uses the following signature:\ndef predict(self, X: np.ndarray, names: Iterable[str], meta: Dict = None) The wrapper is straightforward, in this example. We use the joblib library again, to load the serialised model model.pkl, and then pass through any JSON payload as inputs (X) to the model to get a prediction as well as using Python\u0026rsquo;s default logging to provide some feedback.\nimport joblib import logging class Model(object): def __init__(self): logger.info(\u0026#34;Initializing.\u0026#34;) logger.info(\u0026#34;Loading model.\u0026#34;) self.model = joblib.load(\u0026#34;model.pkl\u0026#34;) def predict(self, X, features_names): return self.model.predict_proba(X) We now build the model using the s2i (source-to-image). As the name implies, s2i\u0026rsquo;s allow to create a container image from source code, taking care of any necessary intermediate steps. Seldon support several types of builds (such as Python, R and Java)8.\nTypically s2i\u0026rsquo;s rely on certain conventions (over configuration) on your application structure. A requirement when building a Seldon model using its s2i is to provide some specific environment variables. These are usually stored in a file located in $REPO/.s2i/environment. For instance, for this model we use:\nMODEL_NAME=Model API_TYPE=REST SERVICE_TYPE=MODEL PERSISTENCE=0 The MODEL_NAME corresponds to the script we\u0026rsquo;ve created previously, Model.py and instructs Seldon to use it as the REST endpoint provider. API_TYPE defines the endpoint interface. We use the REST interface, other possibilities include gRPC, for instance.\nTo build the container image using the s2i, assuming you want an image named $NAME and tagged with $TAG, we simply need to run:\n$ s2i build $REPO \\ seldonio/seldon-core-s2i-python36:0.18 \\ $NAME:$TAG You can provide the location of your source code either by specifying a remote Git repository or by passing a local one. Once the container image builds, you can now run it using, for instance:\ndocker run -i --rm -p 5000:5000 $NAME:$TAG Let\u0026rsquo;s get a prediction from the model:\n$ curl --header \u0026#34;Content-Type: application/json\u0026#34; \\ --request POST \\ --data \u0026#39;{\u0026#34;data\u0026#34;:{\u0026#34;ndarray\u0026#34;:[34.0, 100.0, 1, 2]()}}\u0026#39; \\ http://localhost:5000/predict This will return a prediction:\n{ \u0026#34;data\u0026#34;: { \u0026#34;names\u0026#34;: [\u0026#34;t:0\u0026#34;,\u0026#34;t:1\u0026#34;,\u0026#34;t:2\u0026#34;], \u0026#34;ndarray\u0026#34;: [0.0,0.9980208571211083,0.00197914287889168]()}, \u0026#34;meta\u0026#34;: {} } This response corresponds to the probability of each segment (0, 1 and 2), respectively. We can see that a customer with this profile is classified as a segment 1 with an associated probability of 99.8%.\nWith metrics Seldon provides basic metrics by default, covering service, predictor and model name, version and image. However, you can directly add custom metrics. Going back to our Model wrapper class, we add a new method called metrics which returns custom metrics. The metrics are compatible with Prometheus and, therefore, the metric type should be familiar if you have dealt with Prometheus before. These include, for instance:\nCounters Gauges Timers Let\u0026rsquo;s add to the wrapper:\nimport joblib import logging class Model(object): def __init__(self): logger.info(\u0026#34;Initializing.\u0026#34;) logger.info(\u0026#34;Loading model.\u0026#34;) self.model = joblib.load(\u0026#34;model.pkl\u0026#34;) def predict(self, X, features_names): return self.model.predict_proba(X) # new custom metrics endpoint def metrics(self): return [ # a counter which will increase by the given value {\u0026#34;type\u0026#34;: \u0026#34;COUNTER\u0026#34;, \u0026#34;key\u0026#34;: \u0026#34;mycounter\u0026#34;, \u0026#34;value\u0026#34;: 1}, # a gauge which will be set to given value {\u0026#34;type\u0026#34;: \u0026#34;GAUGE\u0026#34;, \u0026#34;key\u0026#34;: \u0026#34;mygauge\u0026#34;, \u0026#34;value\u0026#34;: 10}, # a timer which will add sum and count metrics - assumed millisecs {\u0026#34;type\u0026#34;: \u0026#34;TIMER\u0026#34;, \u0026#34;key\u0026#34;: \u0026#34;mytimer\u0026#34;, \u0026#34;value\u0026#34;: 1.1}, ] If we now request a new prediction, as previously, we can see the custom metrics included in the model\u0026rsquo;s response.\n{ \u0026#34;data\u0026#34;: { \u0026#34;names\u0026#34;: [\u0026#34;t:0\u0026#34;,\u0026#34;t:1\u0026#34;,\u0026#34;t:2\u0026#34;], \u0026#34;ndarray\u0026#34;:[0.0,0.9980208571211083,0.00197914287889168]()}, \u0026#34;meta\u0026#34;: { \u0026#34;metrics\u0026#34;: [ {\u0026#34;key\u0026#34;:\u0026#34;mycounter\u0026#34;,\u0026#34;type\u0026#34;:\u0026#34;COUNTER\u0026#34;,\u0026#34;value\u0026#34;:1}, {\u0026#34;key\u0026#34;:\u0026#34;mygauge\u0026#34;,\u0026#34;type\u0026#34;:\u0026#34;GAUGE\u0026#34;,\u0026#34;value\u0026#34;:10}, {\u0026#34;key\u0026#34;:\u0026#34;mytimer\u0026#34;,\u0026#34;type\u0026#34;:\u0026#34;TIMER\u0026#34;,\u0026#34;value\u0026#34;:1.1}] } } These values are available via the Prometheus endpoint.\nThe model can also be easily deployed in a container platform, for instance, OpenShift. Assuming you are logged to a cluster and your image is a registry accessible by OpenShift, you can simply deploy it using:\n$ oc new-app $NAME:$TAG I hope this was useful to you. Happy coding!\nhttps://github.com/SeldonIO/seldon-core\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nhttps://kubernetes.io/\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nhttps://www.openshift.com/\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nhttps://scikit-learn.org/stable/\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nAs of the time of writing.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nOther encoders are available in scikit-learn. I recommend you experiment with some of them.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nYou can use any file name, as long as it\u0026rsquo;s consistent with .s2i/environment, which we\u0026rsquo;ll look at soon.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nMore information can be found here.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"/serving-models-with-seldon.html","tags":null,"title":"Serving models with Seldon"},{"categories":null,"contents":"According to Bash\u0026rsquo;s man:\n/bin/bash The bash executable /etc/profile The system-wide initialization file, executed for login shells ~/.bash_profile The personal initialization file, executed for login shells ~/.bashrc The individual per-interactive-shell startup file ~/.bash_logout The individual login shell cleanup file, executed when a login shell exits ~/.inputrc Individual readline initialization file With zsh, .zshrc is always read for an interactive shell, whether it\u0026rsquo;s a login one or not.\n","permalink":"/shell-configurations.html","tags":null,"title":"Shell configurations"},{"categories":null,"contents":"Reset cursor Sometimes, especially when using ANSI escape code heavy applications, your terminal state might get mangled. If that\u0026rsquo;s the case, it\u0026rsquo;s just a matter of performing a VT320 escape sequence to reset the state. For instance, in zsh, using the unhide command:\necho -en \u0026#34;\\e[?25h\u0026#34; ","permalink":"/shell-tricks.html","tags":null,"title":"Shell tricks"},{"categories":null,"contents":"Assets This site\u0026rsquo;s CSS size is 84.1Kb. This, however, includes the following dependencies:\nLaTeX processor (MathJax1) custom monospaced font (Jetbrains Mono) custom serif font (ET Book, Edward Tufte\u0026rsquo;s Book font) The base CSS is heavily inspired by 58 bytes of css to look great nearly everywhere.\nThe CSS evolution (size, number of rules, etc) can be tracked over at Project Wallace.\nThe site is generated from a set of org-mode files using Emacs, which mainly performs the following tasks:\nGather backlinks to each page Build the client-side search index convert the org-mode files to HTML2 Navigation Search The site is searchable from here. The search page also allows for query string searches using the q keyword, for instance:\n/search.html?q=statistics This allows you to add a custom search engine to most modern browsers, such as Firefox or Chrome.\nSearch is done 100% client-side, so there\u0026rsquo;s absolutely no information collected regarding your search queries.\nHighlighting of terms is also possible just adding the query parameter ?h=... to any page. For instance, to highlight the term privacy on this page, simply go to\n/site-details.html?h=privacy or click here.\nDeep linking This site implements \u0026ldquo;deep links\u0026rdquo;. This means that any section of text selected will generate a new URL which links directly to it. This link can be bookmarked or shared. Please keep in mind that this intented for transitory linking, since long-term structure of the page is not guaranteed.\nKeyword focus Some {{{focus(keywords,page)}}} are clickable and will highlight all occurences of a certain term throughtout the page. The main use case is, for instance, code-heavy pages where a certain variable or term might benefit from standing out to understand the concepts more easily.\nPrivacy No cookies are used on this site.\nAll site traffic statistics are captured using GoatCounter. Goatcounter is an open-source, privacy-friendly analytics site, which doesn\u0026rsquo;t use cookies and collects minimal information, just enough to produce a few useful summaries. In line with the desire for total transparency for all visitors, I\u0026rsquo;ve made the realtime stats dashboard for this site public and available at https://ruivieira-dev.goatcounter.com/. You can see for yourself which data is collected.\nPlease let me know if you have any concerns about this site\u0026rsquo;s privacy policy by dropping me a message at @ruivieira@mastodon.technology.\nAs mentioned previously, search is done 100% client-side, so no information is collected about your search queries.\nThe privacy record of this site can be verified independently by using Blacklight.\nJavascript If you want to disable Javascript for this site, please do! It won\u0026rsquo;t affect any of the main content. All pages will work the same with or without Javascript, except the following:\nThe link graph page The earch page Deep linking Keyword focus All of these are simply navigation helpers and the content does not depend on this functionality.\nSome sections are progressively enhanced by Javascript, but will work without it. As an example, many dates are represented by a time tag such as\n\u0026lt;time datetime=\u0026#34;2021-11-28 15:21:54 +0000 GMT\u0026#34; itemprop=\u0026#34;datePublished\u0026#34;\u0026gt;2021-11-28\u0026lt;/time\u0026gt; These are converted to a relative date by Javascript resulting in\n2021-11-28 For instance, depending on wether you have Javascript enabled or not, the above will either display \u0026ldquo;x days ago\u0026rdquo; or \u0026ldquo;2021-11-28\u0026rdquo;.\nText mode Apart from the obvious lack of images, the vast majority of this site will also work with a text-based browser (such as lynx).\nEven the code examples are quite readable (for instance, pandas dataframes are properly rendered as text tables).\nGive it a go by installing lynx and running lynx https://ruivieira.dev.\nHere\u0026rsquo;s how this page looks like using lynx:\nTo see other \u0026ldquo;supported\u0026rdquo; browsers (such as Internet Explorer 6 and NCA Mosaic 2) see the Brutalist Web Design page.\nKeeping up to date If you want to keep up to date and be notified when new content is added to this site, at the moment the best way is to follow the Mesozoic Mastodon bot.\nThis bot is part of a Git pre-push hook and will \u0026ldquo;toot\u0026rdquo; whenever changes to the site\u0026rsquo;s source are made.\nhttps://www.mathjax.org/\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nPreviously this site was written as plain HTML+CSS, this is touched upon at \u0026ldquo;(Semi) handcrafted RSS\u0026rdquo;.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"/site-details.html","tags":null,"title":"Site details"},{"categories":null,"contents":"Introduction SMILE is a Machine Learning library for Scala and Java.\nIt implements:\nClassification ","permalink":"/smile-library.html","tags":null,"title":"SMILE library"},{"categories":null,"contents":"Spearman rank correlation The Spearman correlation coefficient (or Spearman\u0026rsquo;s $\\rho$) measures rank correlation between two variables.\nAssuming monotonicity, the Spearman\u0026rsquo;s $\\rho$ will take values between $-1$ and $1$, representing completely opposite or identical ranks, respectively1.\nDue to the dependance on ranks, the Spearman\u0026rsquo;s $\\rho$ is used for ordinal value, although discrete and continous values are possible.\nIf we consider a dataset of size $n$, and $X_i, Y_i$ as the scores, we can then calculate the ranks as $\\operatorname{R}({X_i}), \\operatorname{R}({Y_i})$, and $\\rho$ as\n$$ r_s = \\rho_{\\operatorname{R}(X),\\operatorname{R}(Y)} = \\frac{\\operatorname{cov}(\\operatorname{R}(X), \\operatorname{R}(Y))} {\\sigma_{\\operatorname{R}(X)} \\sigma_{\\operatorname{R}(Y)}}, $$\nHere $\\rho$ is the Pearson correlation coefficient, but applied to the rank variables, $\\operatorname{cov}(\\operatorname{R}(X), \\operatorname{R}(Y))cov(R(X),R(Y))$ is the covariance of the rank variables, $\\sigma_{\\operatorname{R}(X)}$ and $\\sigma_{\\operatorname{R}(Y)}$ are the standard deviations of the rank variables.\nIf all the ranks are distinct integers, the simplified form can be applied\n$$ r_s = 1 - \\frac{6 \\sum d_i^2}{n(n^2 - 1)}, $$\nwhere $d_i = \\operatorname{R}(X_i) - \\operatorname{R}(Y_i)$ is the difference between the two ranks of each observation, $n$ is the number of observations.\nAssuming no repeated ranks.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"/spearman-correlation.html","tags":null,"title":"Spearman correlation"},{"categories":null,"contents":"Configuration Config file To add a known server to the config file, use the following syntax:\nHost mymachine HostName 127.0.0.1 User root Port 7654 There is no method to specify or provide on the command line the password in a non-interactive manner for ssh authentication using a OpenSSH built-in mechanism.\nServers Alternative An alternative SSH server implementation is tinyssh1\nTroubleshooting Failed signing If you get a error similar to:\nsign_and_send_pubkey: signing failed for RSA \u0026#34;/home/foo/.ssh/id_rsa\u0026#34; from agent: agent refused operation foo@example.com: Permission denied (publickey). This might be related to permission errors. The fix is:\nchmod 700 ~/.ssh chmod 600 ~/.ssh/* https://tinyssh.org/\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"/ssh.html","tags":null,"title":"SSH"},{"categories":null,"contents":"Useful algorithms:\nConformalised density and distance-based anomaly detection in time-series data1 Anomaly detection in streams with extreme value theory Robust random cut forest based anomaly detection on streams2 Time-series anomaly detection service at Microsoft3 Half-Space Trees Perhaps I should add something new.\nAnomaly detection in streams with extreme value theory A more in-detail page is available at Streaming anomaly detection with Extreme Value Theory.\nSPOT An example with streamad\u0026rsquo;s SPOT4 detector. This is available in the streamad.model.SpotDetector package.\nfrom streamad.util import StreamGenerator, UnivariateDS, plot from streamad.util.dataset import CustomDS from streamad.model import SpotDetector ds = CustomDS(\u0026#34;../../data/streamad/uniDS.csv\u0026#34;) stream = StreamGenerator(ds.data) model = SpotDetector() scores = [] for x in stream.iter_item(): score = model.fit_score(x) if score: scores.append(score) else: scores.append(0) data, label, date, features = ds.data, ds.label, ds.date, ds.features import matplotlib.pyplot as plt fig, axs = plt.subplots(2) fig.suptitle(\u0026#39;Anomaly scores for univariate data\u0026#39;) axs[0].plot(range(len(data)), data) axs[1].plot(range(len(data)), scores, c=\u0026#39;r\u0026#39;) Half-Space Trees See [Half-Space Trees].\nhttps://arxiv.org/abs/1608.04585\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nhttp://proceedings.mlr.press/v48/guha16.pdf\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nhttps://arxiv.org/abs/1906.03821\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nhttps://dl.acm.org/doi/10.1145/3097983.3098144\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"/streaming-anomaly-detection.html","tags":null,"title":"Streaming anomaly detection"},{"categories":null,"contents":"Situations where streaming statistics are useful:\nUnknown number of observations Online streaming data Dataset too big for local processing For the remainder, let\u0026rsquo;s consider a set of observations $y_i$, weights $w_i$, such that\n$$ y_1,\\dots,y_i \\in \\mathbb{R} \\ w_1,\\dots,w_i\\quad w_i \\geq 0 $$\nMean and variances A naive approach to calculating a weighted streaming mean, $\\widehat{\\mu}$ and unbiased streaming variance, , would be to calculate:\n$$ \\begin{align*} \\widehat{\\mu}\u0026amp;=\\frac{T^{(n)}}{S^{(n)}} \\ \\widehat{\\mathbb{V}}\u0026amp;=\\frac{n}{(n-1)S^{(n)}}\\left(U^{(n)}-S^{(n)}\\widehat{\\mu}^2\\right) \\end{align*} $$\nwhere\n$$ \\begin{align*} S^{(i+1)}\u0026amp;=S^{(i)}+w_i\\ T^{(i+1)}\u0026amp;=T^{(i)}+w_i y_i\\ U^{(i+1)}\u0026amp;=U^{(i)}+w_i y^2_i \\end{align*} $$\nThis calculation however does not hold for large $n$ values.\nAn alternative calculation was suggested by West1, where we calculate:\n$$ \\begin{align*} \\widehat{\\mu}\u0026amp;=\\frac{\\sum_i w_i y_i}{\\sum_i w_i} \\ \\widehat{\\mathbb{V}}\u0026amp;=\\frac{\\sum_i w_i(X_i-\\mu)^2}{\\frac{n-1}{n}\\sum_i w_i} \\end{align*} $$\nLet\u0026rsquo;s look at an example in Python.\nfrom plotutils import * import numpy as np mu = 10.0 sigma = 20.0 N = 100_000 Y = np.random.normal(loc=mu, scale=sigma, size=N) import matplotlib.pyplot as plt import matplotlib matplotlib.rc(\u0026#39;figure\u0026#39;, figsize=(12, 6)) plt.hist(Y, bins=50,color=\u0026#34;lightpink\u0026#34;) plt.show() As expected, the mean and variance when calculated in \u0026ldquo;batch\u0026rdquo; mode should be equivalent to the original values $\\mu$ and $\\sigma$.\nprint(f\u0026#34;mean: {np.mean(Y)}\u0026#34;) print(f\u0026#34;std: {np.std(Y)}\u0026#34;) mean: 10.040100563607174 std: 19.946298541957912 class StreamingStatistcs: def __init__(self): self.sum = 0.0 self.mean = 0.0 self.t = 0 self.n = 0 self.var = None def calculate(self, y, w): q = y - self.mean tmp_sum = self.sum + w r = q*w / tmp_sum self.mean += r self.t += q*r*self.sum self.sum = tmp_sum self.n += 1 if (self.sum == 0.0 or self.n \u0026lt; 2): self.var = 0 else: self.var = (self.t*self.n)/(self.sum*(self.n-1)) return (self.mean, self.var) means = [] vars = [] st = StreamingStatistcs() for y in Y: m, v = st.calculate(y, 1.0) means.append(m) vars.append(np.sqrt(v)) fig, (ax1, ax2) = plt.subplots(1, 2) fig.suptitle(\u0026#39;Streaming statistics for $N=10^5$ observations\u0026#39;) ax1.plot(means) ax1.hlines(y=10, xmin=0, xmax=N, colors=\u0026#34;red\u0026#34;) ax1.set_title(\u0026#34;Streaming mean\u0026#34;) ax2.plot(vars) ax2.hlines(y=20, xmin=0, xmax=N, colors=\u0026#34;red\u0026#34;) ax2.set_title(\u0026#34;Streaming variance\u0026#34;) plt.show() cite:\u0026amp;west1979updating West, D. (1979). Updating mean and variance estimates: an improved method. Communications of the ACM, 22(9), 532\u0026ndash;535.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"/streaming-statistics.html","tags":null,"title":"Streaming statistics"},{"categories":null,"contents":"Service systemd To enable the syncthing service at the user level on a systemd based OS (e.g. Fedora) use\nsystemctl --user enable --now syncthing.service The service might be lost after package or OS updates. To re-enable it, use:\nsystemctl --user daemon-reload systemctl --user restart syncthing.service ","permalink":"/syncthing.html","tags":null,"title":"Syncthing"},{"categories":null,"contents":"Generating synthetic data Synthetic data will be used mainly for these scenarios:\nRegression Classification Here we will mainly look at the methods provided by scikit-learn to generate synthetic datasets. For more advanced methods, such as using the SDV library please check the SDV page. It support methods such as Gaussian copulas, CTGAN and CopulaGAN.\nRegression data What does a regression consist of?\nFor this section we will mainly use scikit-learn\u0026rsquo;s make_regression method.\nFor reproducibility, we will set a random_state.\nWe will create a dataset using make_regression\u0026rsquo;s random linear regression model with input features $x=(f_1,f_2,f_3,f_4)$ and an output $y$.\nimport numpy as np import pandas as pd from sklearn.datasets import make_regression from scipy.stats import linregress N_FEATURES = 4 N_TARGETS = 1 N_SAMPLES = 100 dataset = make_regression( n_samples=N_SAMPLES, n_features=N_FEATURES, n_informative=2, n_targets=N_TARGETS, bias=0.0, effective_rank=None, tail_strength=0.5, noise=0.0, shuffle=True, coef=False, random_state=random_state, ) print(dataset[0][:10]) print(dataset[1][:10]) [[ 0.87305874 -1.63096187 0.52538404 -0.19035824] [ 1.00698671 0.79834941 -0.04057655 -0.31358605] [-0.61464273 1.65110321 0.75791487 -0.0039844 ] [-1.08536678 1.82337823 0.4612592 -1.72325306] [-1.67774847 -0.54401341 0.86347869 -0.30250463] [-0.02427254 0.75537599 -0.04644972 -0.85153564] [-0.48085576 0.82100952 -0.9390196 -0.25870492] [-0.66772841 -2.46244005 -0.19855095 -1.85756579] [-0.29810663 -0.02239635 0.25363492 -1.22688366] [ 1.48146924 0.38269965 -1.18208819 -1.31062148]] [ 20.00449025 -30.41054677 52.65371365 -119.26376184 33.78805456 -78.12189078 -88.41673748 -177.21674804 -90.13920313 -197.90799195] Let\u0026rsquo;s turn this dataset into a Pandas DataFrame:\ndf = pd.DataFrame(data=dataset[0], columns=[f\u0026#34;f{i+1}\u0026#34; for i in range(N_FEATURES)]) df[\u0026#34;y\u0026#34;] = dataset[1] df.head() f1 f2 f3 f4 y 0 0.873059 -1.630962 0.525384 -0.190358 20.004490 1 1.006987 0.798349 -0.040577 -0.313586 -30.410547 2 -0.614643 1.651103 0.757915 -0.003984 52.653714 3 -1.085367 1.823378 0.461259 -1.723253 -119.263762 4 -1.677748 -0.544013 0.863479 -0.302505 33.788055 Let\u0026rsquo;s plot the data:\nChanging the Gaussian noise level The noise parameter in make_regression allows to adjust the scale of the data\u0026rsquo;s gaussian centered noise.\ndataset = make_regression( n_samples=N_SAMPLES, n_features=N_FEATURES, n_informative=2, n_targets=N_TARGETS, bias=0.0, effective_rank=None, tail_strength=0.5, noise=2.0, shuffle=True, coef=False, random_state=random_state, ) df = pd.DataFrame(data=dataset[0], columns=[f\u0026#34;f{i+1}\u0026#34; for i in range(N_FEATURES)]) df[\u0026#34;y\u0026#34;] = dataset[1] Visualising increasing noise Let\u0026rsquo;s increase the noise by $10^i$, for $i=1, 2, 3$ and see what the data looks like.\ndf = pd.DataFrame(data=np.zeros((N_SAMPLES, 1))) def create_noisy_data(noise): return make_regression( n_samples=N_SAMPLES, n_features=1, n_informative=1, n_targets=1, bias=0.0, effective_rank=None, tail_strength=0.5, noise=noise, shuffle=True, coef=False, random_state=random_state, ) for i in range(3): data = create_noisy_data(10 ** i) df[f\u0026#34;f{i+1}\u0026#34;] = data[0] df[f\u0026#34;y{i+1}\u0026#34;] = data[1] Classification data To generate data for classification we will use the make_classification method.\nfrom sklearn.datasets import make_classification N = 4 data = make_classification( n_samples=N_SAMPLES, n_features=N, n_informative=4, n_redundant=0, n_repeated=0, n_classes=2, n_clusters_per_class=1, weights=None, flip_y=0.01, class_sep=1.0, hypercube=True, shift=0.0, scale=1.0, shuffle=True, random_state=random_state, ) df = pd.DataFrame(data[0], columns=[f\u0026#34;f{i+1}\u0026#34; for i in range(N)]) df[\u0026#34;y\u0026#34;] = data[1] df.head() f1 f2 f3 f4 y 0 -3.216 -0.416 -1.295 -1.882 0 1 -1.426 -1.257 -1.734 -1.804 0 2 2.798 -3.010 -1.085 -3.134 1 3 0.633 2.502 -1.553 1.625 1 4 1.494 0.912 -1.887 -1.457 1 Cluster separation According to the docs1, class_sep is the factor multiplying the hypercube size.\nLarger values spread out the clusters/classes and make the classification task easier.\nN_FEATURES = 4 data = make_classification( n_samples=N_SAMPLES, n_features=N_FEATURES, n_informative=4, n_redundant=0, n_repeated=0, n_classes=2, n_clusters_per_class=1, weights=None, flip_y=0.01, class_sep=3.0, hypercube=True, shift=0.0, scale=1.0, shuffle=True, random_state=None, ) df = pd.DataFrame(data[0], columns=[f\u0026#34;f{i+1}\u0026#34; for i in range(N_FEATURES)]) df[\u0026#34;y\u0026#34;] = data[1] We can make the cluster separability more difficult, by decreasing the value of class_sep.\nN_FEATURES = 4 data = make_classification( n_samples=N_SAMPLES, n_features=N_FEATURES, n_informative=4, n_redundant=0, n_repeated=0, n_classes=2, n_clusters_per_class=1, weights=None, flip_y=0.01, class_sep=0.5, hypercube=True, shift=0.0, scale=1.0, shuffle=True, random_state=None, ) df = pd.DataFrame(data[0], columns=[f\u0026#34;f{i+1}\u0026#34; for i in range(N_FEATURES)]) df[\u0026#34;y\u0026#34;] = data[1] Noise level According to the documentation2, flip_y is the fraction of samples whose class is assigned randomly.\nLarger values introduce noise in the labels and make the classification task harder.\nN_FEATURES = 4 for i in range(6): data = make_classification( n_samples=N_SAMPLES, n_features=N_FEATURES, n_informative=4, n_redundant=0, n_repeated=0, n_classes=2, n_clusters_per_class=1, weights=None, flip_y=0.1 * i, class_sep=1.0, hypercube=True, shift=0.0, scale=1.0, shuffle=False, random_state=random_state, ) df = pd.DataFrame(data[0], columns=[f\u0026#34;f{i+1}\u0026#34; for i in range(N_FEATURES)]) df[\u0026#34;y\u0026#34;] = data[1] plt.subplot(2, 3, i + 1) plt.title(f\u0026#34;flip_y={round(0.1*i,2)}\u0026#34;) plt.scatter( df[\u0026#34;f1\u0026#34;], df[\u0026#34;f2\u0026#34;], s=50, c=df[\u0026#34;y\u0026#34;].apply(lambda y: colours[y]), edgecolor=df[\u0026#34;y\u0026#34;].apply(lambda y: edges[y]), ) plt.tight_layout(pad=3.0) df = pd.DataFrame(data=np.zeros((N_SAMPLES, 1))) for i in range(3): data = make_classification( n_samples=N_SAMPLES, n_features=2, n_informative=2, n_redundant=0, n_repeated=0, n_classes=2, n_clusters_per_class=1, weights=None, flip_y=0, class_sep=i + 0.5, hypercube=True, shift=0.0, scale=1.0, shuffle=False, random_state=random_state, ) df[f\u0026#34;f{i+1}1\u0026#34;] = data[0][:, 0] df[f\u0026#34;f{i+1}2\u0026#34;] = data[0][:, 1] df[f\u0026#34;t{i+1}\u0026#34;] = data[1] It is noteworthy that many paremeters in scikit-learn for synthetic data generation allow inputs per feature or cluster. To do so, we simple pass the parameter value as an array. For instance, to\nN = 4 data = make_classification( n_samples=N_SAMPLES, n_features=N, n_informative=4, n_redundant=0, n_repeated=0, n_classes=2, n_clusters_per_class=1, weights=None, flip_y=0.01, class_sep=1.0, hypercube=True, shift=0.0, scale=1.0, shuffle=True, random_state=random_state, ) df = pd.DataFrame(data[0], columns=[f\u0026#34;f{i+1}\u0026#34; for i in range(N)]) df[\u0026#34;y\u0026#34;] = data[1] Separability from sklearn.datasets import make_blobs N_FEATURE = 4 data = make_blobs( n_samples=60, n_features=N_FEATURE, centers=3, cluster_std=1.0, center_box=(-5.0, 5.0), shuffle=True, random_state=None, ) df = pd.DataFrame(data[0], columns=[f\u0026#34;f{i+1}\u0026#34; for i in range(N_FEATURE)]) df[\u0026#34;y\u0026#34;] = data[1] To make a cluster more separable we can change cluster_std.\ndata = make_blobs( n_samples=60, n_features=N_FEATURES, centers=3, cluster_std=0.3, center_box=(-5.0, 5.0), shuffle=True, random_state=None, ) df = pd.DataFrame(data[0], columns=[f\u0026#34;f{i+1}\u0026#34; for i in range(N_FEATURES)]) df[\u0026#34;y\u0026#34;] = data[1] By decreasing cluster_std we make them less separable.\ndata = make_blobs( n_samples=60, n_features=N_FEATURES, centers=3, cluster_std=2.5, center_box=(-5.0, 5.0), shuffle=True, random_state=None, ) df = pd.DataFrame(data[0], columns=[f\u0026#34;f{i+1}\u0026#34; for i in range(N_FEATURES)]) df[\u0026#34;y\u0026#34;] = data[1] Anisotropic data data = make_blobs(n_samples=50, n_features=2, centers=3, cluster_std=1.5) transformation = [0.5, -0.5], [-0.4, 0.8]() data_0 = np.dot(data[0], transformation) df = pd.DataFrame(data_0, columns=[f\u0026#34;f{i}\u0026#34; for i in range(1, 3)]) df[\u0026#34;y\u0026#34;] = data[1] Concentric clusters Sometimes we might be interested in creating a non-separable cluster. The simples way is to create concentric clusters with the make_circles method.\nfrom sklearn.datasets import make_circles data = make_circles( n_samples=N_SAMPLES, shuffle=True, noise=None, random_state=random_state, factor=0.6 ) df = pd.DataFrame(data[0], columns=[f\u0026#34;f{i+1}\u0026#34; for i in range(2)]) df[\u0026#34;y\u0026#34;] = data[1] Adding noise The noise parameter allows to create a concentric noisy dataset.\ndata = make_circles( n_samples=N_SAMPLES, shuffle=True, noise=0.15, random_state=random_state, factor=0.6 ) df = pd.DataFrame(data[0], columns=[f\u0026#34;f{i+1}\u0026#34; for i in range(2)]) df[\u0026#34;y\u0026#34;] = data[1] Moon clusters A shape that can be useful to other methods (such as Counterfactuals, for instance) is the one generated by the make_moons method.\nfrom sklearn.datasets import make_moons data = make_moons( n_samples=N_SAMPLES, shuffle=True, noise=None, random_state=random_state ) df = pd.DataFrame(data[0], columns=[f\u0026#34;f{i+1}\u0026#34; for i in range(2)]) df[\u0026#34;y\u0026#34;] = data[1] Adding noise As usual, the noise parameter allows to control the noise.\ndata = make_moons( n_samples=N_SAMPLES, shuffle=True, noise=0.1, random_state=random_state ) df = pd.DataFrame(data[0], columns=[f\u0026#34;f{i+1}\u0026#34; for i in range(2)]) df[\u0026#34;y\u0026#34;] = data[1] Time-series data Random walk See Random walk.\nSimple periodic No trend Generate a simple HMM with a sine state and gaussian observations:\nimport numpy as np def generate_sine(period, n): cycles = n / period length = np.pi * 2 * cycles return np.sin(np.arange(0, length, length / n)) We will now get a set of $n=1000$ observations with a $p=10$ period\nN=1000 data = generate_sine(10, N) * np.random.uniform(10, size=N) Trend N=1000 data = (generate_sine(10, N) * np.random.uniform(10, size=N)) + np.arange(N)/200.0 Univariate data Using the streamad library:\nfrom streamad.util.dataset import CustomDS from streamad.util import StreamGenerator, plot ds = CustomDS(\u0026#34;../../data/streamad/uniDS.csv\u0026#34;) stream = StreamGenerator(ds.data) https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_classification.html\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nhttps://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_classification.html\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"/synthetic-data-generation.html","tags":null,"title":"Synthetic Data Generation"},{"categories":null,"contents":" Synthetic data with SDV and Gaussian copulas Synthetic data with SDV and CTGAN Synthetic data with SDV and CopulaGAN ","permalink":"/synthetic-data-generation-with-sdv.html","tags":null,"title":"Synthetic Data Generation with SDV"},{"categories":null,"contents":" import pandas as pd import warnings warnings.filterwarnings(\u0026#39;ignore\u0026#39;) data = pd.read_csv(\u0026#34;data/svm-hyperparameters-train-features.csv\u0026#34;) data.head() Pclass Sex Age SibSp Parch Fare 0 3 1 22.0 1 0 7.2500 1 1 0 38.0 1 0 71.2833 2 3 0 26.0 0 0 7.9250 3 1 0 35.0 1 0 53.1000 4 3 1 35.0 0 0 8.0500 data.describe(include=\u0026#39;all\u0026#39;) Pclass Sex Age SibSp Parch Fare count 891.000000 891.000000 891.000000 891.000000 891.000000 891.000000 mean 2.308642 0.647587 29.758889 0.523008 0.381594 32.204208 std 0.836071 0.477990 13.002570 1.102743 0.806057 49.693429 min 1.000000 0.000000 0.420000 0.000000 0.000000 0.000000 25% 2.000000 0.000000 22.000000 0.000000 0.000000 7.910400 50% 3.000000 1.000000 30.000000 0.000000 0.000000 14.454200 75% 3.000000 1.000000 35.000000 1.000000 0.000000 31.000000 max 3.000000 1.000000 80.000000 8.000000 6.000000 512.329200 from sdv.tabular import CopulaGAN model = CopulaGAN() model.fit(data) new_data = model.sample(200) new_data.head() Pclass Sex Age SibSp Parch Fare 0 3 0 32.02 0 0 18.2784 1 3 1 20.90 5 0 109.6910 2 3 0 40.80 0 1 179.5139 3 3 0 33.13 1 0 17.3447 4 1 1 20.62 0 0 9.6040 new_data.describe(include=\u0026#39;all\u0026#39;) Pclass Sex Age SibSp Parch Fare count 200.000000 200.000000 200.000000 200.000000 200.000000 200.000000 mean 2.490000 0.450000 31.404700 0.405000 0.115000 46.756109 std 0.820528 0.498742 12.297721 0.919348 0.461345 52.828023 min 1.000000 0.000000 0.450000 0.000000 0.000000 3.034900 25% 2.000000 0.000000 24.712500 0.000000 0.000000 11.768475 50% 3.000000 0.000000 30.295000 0.000000 0.000000 27.859150 75% 3.000000 1.000000 34.307500 1.000000 0.000000 58.361350 max 3.000000 1.000000 73.340000 7.000000 3.000000 381.859600 from sdv.evaluation import evaluate evaluate(new_data, data) 0.539229052965023 model = CopulaGAN( field_transformers={ \u0026#39;Pclass\u0026#39;: \u0026#39;categorical\u0026#39;, \u0026#39;Sex\u0026#39;: \u0026#39;categorical\u0026#39;, \u0026#39;Age\u0026#39;: \u0026#39;float\u0026#39;, \u0026#39;SibSp\u0026#39;: \u0026#39;boolean\u0026#39;, \u0026#39;Parch\u0026#39;: \u0026#39;integer\u0026#39;, \u0026#39;Fare\u0026#39;: \u0026#39;float\u0026#39; }, field_distributions={ \u0026#39;Fare\u0026#39;: \u0026#39;truncated_gaussian\u0026#39; } ) model.fit(data) new_data = model.sample(200) new_data.head() Pclass Sex Age SibSp Parch Fare 0 1 0 1.07 1 0 13.8318 1 3 0 15.58 0 0 46.6937 2 1 1 26.53 0 0 53.8841 3 1 0 29.58 0 0 5.2646 4 1 0 30.13 0 0 5.1415 new_data.describe(include=\u0026#39;all\u0026#39;) Pclass Sex Age SibSp Parch Fare count 200.000000 200.000000 200.000000 200.000000 200.000000 200.000000 mean 2.210000 0.410000 22.935600 0.550000 0.145000 49.155439 std 0.932873 0.493068 13.955182 0.498742 0.441531 42.733997 min 1.000000 0.000000 0.430000 0.000000 0.000000 4.749400 25% 1.000000 0.000000 13.427500 0.000000 0.000000 13.340950 50% 3.000000 0.000000 25.875000 1.000000 0.000000 34.373700 75% 3.000000 1.000000 30.225000 1.000000 0.000000 75.206275 max 3.000000 1.000000 71.340000 1.000000 3.000000 220.761200 evaluate(new_data, data) 0.4983713994365315 ","permalink":"/synthetic-data-with-svd-and-copulagan.html","tags":null,"title":"Synthetic data with SDV and CopulaGAN"},{"categories":null,"contents":" Synthetic data with SDV and CTGAN import pandas as pd import warnings warnings.filterwarnings(\u0026#39;ignore\u0026#39;) data = pd.read_csv(\u0026#34;data/svm-hyperparameters-train-features.csv\u0026#34;) data.head() Pclass Sex Age SibSp Parch Fare 0 3 1 22.0 1 0 7.2500 1 1 0 38.0 1 0 71.2833 2 3 0 26.0 0 0 7.9250 3 1 0 35.0 1 0 53.1000 4 3 1 35.0 0 0 8.0500 data.describe(include=\u0026#39;all\u0026#39;) Pclass Sex Age SibSp Parch Fare count 891.000000 891.000000 891.000000 891.000000 891.000000 891.000000 mean 2.308642 0.647587 29.758889 0.523008 0.381594 32.204208 std 0.836071 0.477990 13.002570 1.102743 0.806057 49.693429 min 1.000000 0.000000 0.420000 0.000000 0.000000 0.000000 25% 2.000000 0.000000 22.000000 0.000000 0.000000 7.910400 50% 3.000000 1.000000 30.000000 0.000000 0.000000 14.454200 75% 3.000000 1.000000 35.000000 1.000000 0.000000 31.000000 max 3.000000 1.000000 80.000000 8.000000 6.000000 512.329200 from sdv.tabular import CTGAN model = CTGAN() model.fit(data) new_data = model.sample(200) new_data.head() Pclass Sex Age SibSp Parch Fare 0 3 1 31.40 1 1 24.7142 1 3 1 63.44 1 1 0.6418 2 3 0 32.73 0 0 2.8117 3 1 0 27.53 0 0 40.4747 4 2 1 46.31 2 1 104.2955 new_data.describe(include=\u0026#39;all\u0026#39;) Pclass Sex Age SibSp Parch Fare count 200.000000 200.000000 200.000000 200.000000 200.000000 200.000000 mean 2.605000 0.535000 37.052600 0.740000 0.450000 32.947807 std 0.715215 0.500025 15.763543 1.212332 0.692748 56.156945 min 1.000000 0.000000 2.870000 0.000000 0.000000 0.000000 25% 2.000000 0.000000 27.217500 0.000000 0.000000 12.590775 50% 3.000000 1.000000 33.100000 0.000000 0.000000 18.435050 75% 3.000000 1.000000 46.692500 1.000000 1.000000 24.083125 max 3.000000 1.000000 80.000000 6.000000 2.000000 380.969200 from sdv.evaluation import evaluate evaluate(new_data, data) 0.5513349938501996 model = CTGAN( epochs=500, batch_size=100, generator_dim=(256, 256, 256), discriminator_dim=(256, 256, 256) ) model.fit(data) new_data = model.sample(200) new_data.head() Pclass Sex Age SibSp Parch Fare 0 3 1 30.69 0 0 13.5556 1 2 0 0.42 1 0 33.1753 2 3 1 30.86 0 0 34.4404 3 1 1 11.07 0 0 29.0068 4 2 0 54.40 0 0 36.6160 new_data.describe(include=\u0026#39;all\u0026#39;) Pclass Sex Age SibSp Parch Fare count 200.000000 200.000000 200.000000 200.000000 200.000000 200.000000 mean 2.350000 0.540000 36.034900 0.365000 0.395000 34.128625 std 0.699964 0.499648 16.254839 0.809352 0.762813 42.191675 min 1.000000 0.000000 0.420000 0.000000 0.000000 2.339800 25% 2.000000 0.000000 28.737500 0.000000 0.000000 10.917300 50% 2.000000 1.000000 31.645000 0.000000 0.000000 19.874400 75% 3.000000 1.000000 49.312500 1.000000 0.000000 35.050750 max 3.000000 1.000000 80.000000 5.000000 2.000000 269.388000 evaluate(new_data, data) 0.6065965175235917 ","permalink":"/synthetic-data-with-sdv-and-ctgan.html","tags":null,"title":"Synthetic data with SDV and CTGAN"},{"categories":null,"contents":" import pandas as pd data = pd.read_csv(\u0026#34;data/svm-hyperparameters-train-features.csv\u0026#34;) data.head() Pclass Sex Age SibSp Parch Fare 0 3 1 22.000 1 0 7.250 1 1 0 38.000 1 0 71.283 2 3 0 26.000 0 0 7.925 3 1 0 35.000 1 0 53.100 4 3 1 35.000 0 0 8.050 data.describe(include=\u0026#34;all\u0026#34;) Pclass Sex Age SibSp Parch Fare count 891.000 891.000 891.000 891.000 891.000 891.000 mean 2.309 0.648 29.759 0.523 0.382 32.204 std 0.836 0.478 13.003 1.103 0.806 49.693 min 1.000 0.000 0.420 0.000 0.000 0.000 25% 2.000 0.000 22.000 0.000 0.000 7.910 50% 3.000 1.000 30.000 0.000 0.000 14.454 75% 3.000 1.000 35.000 1.000 0.000 31.000 max 3.000 1.000 80.000 8.000 6.000 512.329 from sdv.tabular import GaussianCopula model = GaussianCopula() model.fit(data) N_SAMPLES = 1000 new_df = model.sample(N_SAMPLES) new_df.head() Pclass Sex Age SibSp Parch Fare 0 3 1 12.780 1 1 2.070 1 2 1 44.930 0 0 95.806 2 2 1 21.980 1 0 72.020 3 2 0 30.120 2 1 84.968 4 2 1 23.480 1 1 37.170 new_df.describe() Pclass Sex Age SibSp Parch Fare count 1000.000 1000.000 1000.000 1000.000 1000.000 1000.000 mean 2.127 0.595 30.168 0.972 0.671 46.234 std 0.697 0.491 13.224 0.786 0.666 35.294 min 1.000 0.000 0.620 0.000 0.000 0.180 25% 2.000 0.000 20.693 0.000 0.000 16.959 50% 2.000 1.000 29.795 1.000 1.000 38.102 75% 3.000 1.000 39.163 1.000 1.000 67.841 max 3.000 1.000 77.000 4.000 3.000 166.853 \u0026lt;ggplot: (338236528)\u0026gt; \u0026lt;ggplot: (338395612)\u0026gt; \u0026lt;ggplot: (338577035)\u0026gt; \u0026lt;ggplot: (338383996)\u0026gt; model = GaussianCopula( field_transformers={ \u0026#39;Pclass\u0026#39;: \u0026#39;categorical\u0026#39;, \u0026#39;Sex\u0026#39;: \u0026#39;categorical\u0026#39;, \u0026#39;Age\u0026#39;: \u0026#39;float\u0026#39;, \u0026#39;SibSp\u0026#39;: \u0026#39;boolean\u0026#39;, \u0026#39;Parch\u0026#39;: \u0026#39;integer\u0026#39;, \u0026#39;Fare\u0026#39;: \u0026#39;float\u0026#39; } ) model.fit(data) new_df = model.sample(N_SAMPLES) new_df.head() Pclass Sex Age SibSp Parch Fare 0 3 0 24.330 0 1 24.759 1 1 1 58.070 0 0 22.957 2 2 0 23.700 0 0 15.298 3 1 1 30.200 0 0 19.092 4 1 1 51.370 0 0 99.106 \u0026lt;ggplot: (338395206)\u0026gt; \u0026lt;ggplot: (338261950)\u0026gt; \u0026lt;ggplot: (338558748)\u0026gt; \u0026lt;ggplot: (338314674)\u0026gt; data.Fare.describe() count 891.000 mean 32.204 std 49.693 min 0.000 25% 7.910 50% 14.454 75% 31.000 max 512.329 Name: Fare, dtype: float64 distributions = model.get_distributions() distributions {'Pclass.value': 'copulas.univariate.truncated_gaussian.TruncatedGaussian', 'Sex.value': 'copulas.univariate.truncated_gaussian.TruncatedGaussian', 'Age.value': 'copulas.univariate.truncated_gaussian.TruncatedGaussian', 'SibSp.value': 'copulas.univariate.truncated_gaussian.TruncatedGaussian', 'Parch.value': 'copulas.univariate.truncated_gaussian.TruncatedGaussian', 'Fare.value': 'copulas.univariate.truncated_gaussian.TruncatedGaussian'} model = GaussianCopula( field_transformers={ \u0026#39;Pclass\u0026#39;: \u0026#39;categorical\u0026#39;, \u0026#39;Sex\u0026#39;: \u0026#39;categorical\u0026#39;, \u0026#39;Age\u0026#39;: \u0026#39;float\u0026#39;, \u0026#39;SibSp\u0026#39;: \u0026#39;boolean\u0026#39;, \u0026#39;Parch\u0026#39;: \u0026#39;integer\u0026#39;, \u0026#39;Fare\u0026#39;: \u0026#39;float\u0026#39; }, field_distributions={ \u0026#39;Fare\u0026#39;: \u0026#39;truncated_gaussian\u0026#39; } ) model.fit(data) new_df = model.sample(N_SAMPLES) new_df.Fare.describe() count 1000.000 mean 47.928 std 36.876 min 0.044 25% 18.738 50% 40.028 75% 69.421 max 195.331 Name: Fare, dtype: float64 \u0026lt;ggplot: (338496543)\u0026gt; ","permalink":"/synthetic-data-with-svd-and-gaussian-copulas.html","tags":null,"title":"Synthetic data with SVD and Gaussian copulas"},{"categories":null,"contents":"(Based on Rasmus Bth\u0026rsquo;s post)\nA scaled $t$ distribution, with $\\mu$ mean, $s$ scale and $\\nu$ degrees of freedom, can be simulated\nfrom a mixture of Normals with $\\mu$ mean and precisions following a Gamma distribution:\n$$ \\begin{aligned} y \u0026amp;\\sim \\mathcal{N}\\left(\\mu,\\sigma\\right) \\\\ \\sigma^2 \u0026amp;\\sim \\mathcal{IG}\\left(\\frac{\\nu}{2},s^2\\frac{\\nu}{2}\\right) \\end{aligned} $$\nSince I\u0026rsquo;ve recently pickep up again the crystal-gsl in my spare time, I\u0026rsquo;ve decided to replicate the previously mentioned post using a Crystal one-liner.\nTo simulate 10,000 samples from $t_2\\left(0,3\\right)$ using the mixture, we can then write:\nsamples = (0..10000).map { |x| Normal.sample 0.0, 1.0/Math.sqrt(Gamma.sample 1.0, 9.0) } We can see the mixture distribution (histogram) converging nicely to the $(t_2(0,3)$ (red):\n","permalink":"/t-as-mixture-of-normals.html","tags":null,"title":"t as mixture of Normals"},{"categories":null,"contents":"Gradient-less counterfactuals.\n","permalink":"/thompson-sampling.html","tags":null,"title":"Thompson sampling"},{"categories":null,"contents":"Nomenclature Consider:\na set of contexts $\\mathcal{X}$ a set of actions $\\mathcal{A}$ and rewards in $\\mathbb{R}$ Rationale Definition For each iteration $t$:\nA \u0026ldquo;player\u0026rdquo; obtains a context$x\\in \\mathcal{X}$ Plays an action$a\\in \\mathcal{A}$ Receives a reward$r\\in \\mathcal{R}$ This rewards is distributed according to the context and the resulting action The player\u0026rsquo;s goal is to execute actions that maximize the cumulative rewards. Implementation The implementation will focus on these concepts:\na likelihood function $P(r|\\theta ,a,x)$ a set $\\Theta$ of parameters $\\theta$ of the distribution of $r$ a prior distribution $P(\\theta )$ on these parameters past observations triplets $\\mathcal{D}={(x;a;r)}$ a posterior distribution $P(\\theta |{\\mathcal {D}})\\propto P({\\mathcal {D}}|\\theta )P(\\theta )$, where $P({\\mathcal {D}}|\\theta )$ is the likelihood function. Thompson sampling consists in playing the action $a^{\\ast }\\in {\\mathcal {A}}$ according to the probability that it maximizes the expected reward, i.e.action $a^{\\ast }$ is chosen with probability\n$$ \\int \\mathbb {I} \\left[\\mathbb {E} (r|a^{\\ast },x,\\theta )=\\max _{a\u0026rsquo;}\\mathbb {E} (r|a\u0026rsquo;,x,\\theta )\\right]P(\\theta |{\\mathcal {D}})d\\theta , $$\nwhere $\\mathbb {I}$ is the indicator function.\nIn practice, the rule is implemented by sampling. In each round, parameters $\\theta^\\ast$ are sampled from the posterior $P(\\theta |{\\mathcal {D}})$, and an action $a^{\\ast }$ chosen that maximizes ${\\mathbb {E}}[r|\\theta ^{\\ast },a^{\\ast },x]$, i.e. the expected reward given the sampled parameters, the action, and the current context. Conceptually, this means that the player instantiates their beliefs randomly in each round according to the posterior distribution, and then acts optimally according to them. In most practical applications, it is computationally onerous to maintain and sample from a posterior distribution over models. As such, Thompson sampling is often used in conjunction with approximate sampling techniques.\nExample N_TRIALS = 2000 N_ARMS = 16 N_FEATURES = 5 BEST_ARMS = [3, 7, 9, 15] We now define a function to generate context vectors for all arms for each of the trial. We need:\nn_trials, number of trials ($N_T$) n_arms, number of arms per trial ($N_A$) n_features, number of feature per context vector ($N_f$) This function will return a matrix of size $N_{T} \\times N_{A} \\times N_{f}$\ndef make_design_matrix(n_trials: int, n_arms: int, n_features: int) -\u0026gt; np.ndarray: available_arms = np.arange(n_arms) X = np.array([[np.random.uniform(0, 1, size = n_features) for _ in np.arange(n_arms)] for _ in np.arange(n_trials)]) return X X = make_design_matrix(n_trials=N_TRIALS, n_arms=N_ARMS, n_features=N_FEATURES) This will have the shape\nThe following function will generate the true $\\Theta = {\\theta_1,\\dots,\\theta_n}$ for testing purposes. We provide:\n$N_A$, number of arms (n_arms) $N_f$, number of features for the context vector (n_features) best_arms, arms in which we should give some bias values (for good) bias, value to be added to the best arms A matrix of size $N_{A} \\times N_{f}$, each value is a random value with $\\mu = 0$ and standard deviation of $\\frac{1}{4}$. However, for the best arms, we will add the bias.\ndef make_theta(n_arms: int, n_features: int, best_arms, bias = 1): true_theta = np.array( [np.random.normal(size=n_features, scale=1.0/4.0) for _ in range(n_arms)]) true_theta[best_arms] += bias return true_theta true_theta = make_theta( n_arms=N_ARMS, n_features=N_FEATURES, best_arms=BEST_ARMS) A function is also available to generate rewards. It creates rewards for each arm, given a context.\nWe provide:\n$a$, this is the arm index ($0\\leq a \\leq N_{A}-1$) x, is the context that we are observing for the arm index (arm) $\\theta$, is the theta (true or predicted) that are are using to estimate the reward for each arm (theta) scale_noise, we may need to add some random noise ($\\mu=0$ and standard deviation as scale_noise) This will return the estimated score for the arm (with the arm index and the context observed corresponding to the given theta).\ndef generate_reward(arm, x, theta, scale_noise = 1.0/10.0): signal = theta[arm].dot(x) noise = np.random.normal(scale=scale_noise) return signal + noise random_payoffs = np.array( [generate_reward( arm=np.random.choice(N_ARMS), x=X[t, np.random.choice(N_ARMS)], theta=true_theta) for t in range(N_TRIALS)]) # Defining oracle (best payoffs based on the true_theta) oracles = np.array( [np.max( [generate_reward( arm=arm, x=X[t, arm], theta=true_theta) for arm in range(N_ARMS)]) for t in range(N_TRIALS)]) We also create a function to generate the cumulative regret over time.\nWe provide:\npayoffs, an array of $T$ payoffs (for $T$ number of trials) oracles, an array of best values for $T$ trials (oracles) And we get an array of the cumulative sum over time (of size $T$).\ndef make_regret(payoffs: np.ndarray, oracles: np.ndarray) -\u0026gt; np.ndarray: return np.cumsum(oracles - payoffs) payoffs = [ [generate_reward( arm=arm, x=X[t, arm], theta=true_theta) for arm in np.arange(N_ARMS)] for t in np.arange(N_TRIALS)] ave_rewards = np.mean(payoffs, axis=0) The actual sampling The method to perform the actual sampling is next. We provide:\n$\\delta$ (delta), with $0 \u0026lt; \\delta \u0026lt; 1$. With probability $1 - \\delta$, linear thompson sampling satisfies the theoretical regret bound. $R$, with $R \\geq 0$. Assume that the residual $ri(t) - bi(t)^T \\hat{\\mu}$ is R-sub-gaussian. In this case, $R^2$ represents the variance for residuals of the linear model $bi(t)^T$. $\\epsilon$ (epsilon), with $0 \u0026lt; \\epsilon \u0026lt; 1$ A parameter used by the Thompson Sampling algorithm. If the total trials $T$ is known, we can choose $\\epsilon = \\frac{1}{\\ln{T}}$. delta=0.5 R = 0.01 epsilon=0.5 We use r_payoffs to store the payoff for each trial (the payoff for the selected arm based on the true_theta). As such, we initialise a zero array of size n_trials.\nr_payoffs = np.zeros(N_TRIALS) v = R * np.sqrt(24 / epsilon * N_FEATURES * np.log(1 / delta)) Model initialisation:\nB = np.identity(N_FEATURES) mu_hat = np.zeros(shape=(N_FEATURES, 1)) f = np.zeros(shape=(N_FEATURES,1)) for t in range(N_TRIALS): context = X[t] mu_tilde = np.random.multivariate_normal(mu_hat.flat, v**2 * np.linalg.inv(B))[..., np.newaxis] score_array = context.dot(mu_tilde) chosen_arm = np.argmax(score_array) context_t = context[chosen_arm] reward = generate_reward(arm=chosen_arm, x=context_t, theta=true_theta) r_payoffs[t] = reward context_t = np.reshape(context_t, (-1, 1)) B += context_t.dot(context_t.T) f += reward*context_t mu_hat = np.linalg.inv(B).dot(f) ","permalink":"/thompson-sampling.html","tags":null,"title":"Thompson sampling"},{"categories":null,"contents":"Peaks and troughs Let\u0026rsquo;s start by creating a random walk.\nimport numpy as np import pandas as pd N = 10000 step_set = [-1, 0, 1] origin = np.zeros((1, 1)) step_shape = (N, 1) steps = np.random.choice(a=step_set, size=step_shape) path = np.concatenate([origin, steps]).cumsum(0) df = pd.DataFrame(path, columns =[\u0026#39;y\u0026#39;]) df.head() import matplotlib.pyplot as plt from matplotlib.pyplot import figure figure(figsize=(14, 7), dpi=80) plt.plot(df[\u0026#39;y\u0026#39;]) plt.show() from scipy.signal import find_peaks subset = df.head(100) peaks = find_peaks(subset[\u0026#34;y\u0026#34;]) troughs = find_peaks(-subset[\u0026#34;y\u0026#34;]) peaks ax = subset.plot(y=\u0026#34;y\u0026#34;, figsize=(14, 7)) subset.reset_index().iloc[peaks[0]].plot.scatter(x=\u0026#34;index\u0026#34;, y=\u0026#34;y\u0026#34;, ax=ax, color=\u0026#34;green\u0026#34;) subset.reset_index().iloc[troughs[0]].plot.scatter(x=\u0026#34;index\u0026#34;, y=\u0026#34;y\u0026#34;, ax=ax, color=\u0026#34;red\u0026#34;) Autocorrelation Pandas provides an autocorrelation1 plot function.\nfigure(figsize=(14, 7), dpi=80) pd.plotting.autocorrelation_plot(df[\u0026#34;y\u0026#34;]) Differencing Calculating the difference between $x_t$ and $x_{t-1}$.\nstationary = df[\u0026#39;y\u0026#39;].diff() figure(figsize=(14, 7), dpi=80) plt.plot(stationary) plt.show() https://pandas.pydata.org/docs/reference/api/pandas.plotting.autocorrelation_plot.html\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"/time-series-analysis.html","tags":null,"title":"Time-series analysis"},{"categories":null,"contents":"","permalink":"/transformation-functions.html","tags":null,"title":"Transformation functions"},{"categories":null,"contents":"Summary A collection of notes on typography.\nMonospaced fonts Currently I am favouring the Iosevka font for monospaced.\nThe Monoid font has a post to explain some design decisions1. https://medium.com/larsenwork-andreas-larsen/class-based-contextual-positioning-in-monospaced-fonts-cb6b8b9ffe6f\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"/typography.html","tags":null,"title":"Typography"},{"categories":null,"contents":"","permalink":"/unfairness-detection.html","tags":null,"title":"Unfairness detection"},{"categories":null,"contents":"Copy paste Press v to select characters, or uppercase V to select whole lines y to copy Press P to paste before the cursor, or p to paste after ","permalink":"/vim-keys.html","tags":null,"title":"Vim keys"},{"categories":null,"contents":"Download a file saveFile: function() { const data = JSON.stringify(this.myData) const blob = new Blob([data], {type: \u0026#39;text/plain\u0026#39;}) const e = document.createEvent(\u0026#39;MouseEvents\u0026#39;), a = document.createElement(\u0026#39;a\u0026#39;); a.download = \u0026#34;myData.json\u0026#34;; a.href = window.URL.createObjectURL(blob); a.dataset.downloadurl = [\u0026#39;text/json\u0026#39;, a.download, a.href].join(\u0026#39;:\u0026#39;); e.initEvent(\u0026#39;click\u0026#39;, true, false, window, 0, 0, 0, 0, 0, false, false, false, false, 0, null); a.dispatchEvent(e); } ","permalink":"/vue.html","tags":null,"title":"Vue"},{"categories":null,"contents":"Backup External media Since I switch OSes frequently1 having a minimum-hassle filesystem for my external drives would be very convenient. exFAT2 seems to me like the best solution. In fact, exFAT is the default FS for SD cards and USB flash drives with more than 32Gb. So perhaps, if have a large-ish USB pen or an external drive, chances are you don\u0026rsquo;t even need to reformat it.\nSoftware My software of choice for backups at the moment is Kopia3.\nWorkflow Optimise PNGs To optimise an entired folder of PNGs, you could use optipng. Install it on macOS using\n$ brew install optipng and then apply it recusively with\n$ find . -name \u0026#34;*.png\u0026#34; -exec optipng -o7 {} \\; I macOS and GNU/Linux (specifically Fedora and Ubuntu) daily, Windows seldomly.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nhttps://en.wikipedia.org/wiki/ExFAT\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nhttps://kopia.io/\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"/workflow.html","tags":null,"title":"Workflow"},{"categories":null,"contents":"Introduction XGBoost1 is a popular regularizing gradient boosting framework.\nInstallation In most systems, installing XGBoost can be done simply by using pip\n$ pip install xgboost Example Training XGBoost with the credit-bias dataset.\nimport pandas as pd data = pd.read_csv(\u0026#34;../data/credit-bias-train.zip\u0026#34;) data.head() NewCreditCustomer Amount Interest LoanDuration Education NrOfDependants EmploymentDurationCurrentEmployer IncomeFromPrincipalEmployer IncomeFromPension IncomeFromFamilyAllowance ... Mortgage Other Owner Owner_with_encumbrance Tenant Entrepreneur Fully Partially Retiree Self_employed 0 False 2125.0 20.97 60 4.0 0.0 6.0 0.0 301.0 0.0 ... 0 0 1 0 0 0 0 0 1 0 1 False 3000.0 17.12 60 5.0 0.0 6.0 900.0 0.0 0.0 ... 0 0 1 0 0 1 0 0 0 0 2 True 9100.0 13.67 60 4.0 1.0 3.0 600.0 0.0 0.0 ... 1 0 0 0 0 1 0 0 0 0 3 True 635.0 42.66 60 2.0 0.0 1.0 745.0 0.0 0.0 ... 0 0 0 0 1 0 1 0 0 0 4 False 5000.0 24.52 60 4.0 1.0 5.0 1000.0 0.0 0.0 ... 0 0 0 0 0 0 1 0 0 0 5 rows  40 columns\nX_df = data.drop(\u0026#39;PaidLoan\u0026#39;, axis=1) y_df = data[\u0026#39;PaidLoan\u0026#39;] y_df.describe() count 58003 unique 2 top True freq 29219 Name: PaidLoan, dtype: object from sklearn.model_selection import train_test_split train_x, test_x, train_y, test_y = train_test_split(X_df, y_df, test_size=0.25, random_state=42) Hyperparameter estimation Runs a grid search to find the tuning parameters that maxisimise the area under the curve (AUC). train_x is the training data frame with loan details and train_y is the default target column for training. The method returns the best parameters and corresponding AUC score.\nThe objective parameter2 specifies the learning task and the corresponding learning objective. Possible values include:\nObjective function reg:squarederror, regression with squared loss. reg:squaredlogerror, regression with squared log loss reg:logistic, logistic regression reg:pseudohubererror, regression with Pseudo Huber loss, a twice differentiable alternative to absolute loss. binary:logistic, logistic regression for binary classification, output probability binary:logitraw, logistic regression for binary classification, output score before logistic transformation binary:hinge, hinge loss for binary classification. This makes predictions of 0 or 1, rather than producing probabilities. count:poisson, poisson regression for count data, output mean of Poisson distribution survival:cox, Cox regression for right censored survival time data (negative values are considered right censored). survival:aft, Accelerated failure time model for censored survival time data. See Survival Analysis with Accelerated Failure Time for details. aft_loss_distribution, Probability Density Function used by survival:aft objective and aft-nloglik metric. multi:softmax, set XGBoost to do multiclass classification using the softmax objective, you also need to set num_class (number of classes) multi:softprob, same as softmax, but output a vector of ndata * nclass, which can be further reshaped to ndata * nclass matrix. The result contains predicted probability of each data point belonging to each class. rank:pairwise, Use LambdaMART to perform pairwise ranking where the pairwise loss is minimized rank:ndcg, Use LambdaMART to perform list-wise ranking where Normalized Discounted Cumulative Gain (NDCG) is maximized rank:map, Use LambdaMART to perform list-wise ranking where Mean Average Precision (MAP) is maximized reg:gamma, gamma regression with log-link. Output is a mean of gamma distribution. It might be useful, e.g., for modeling insurance claims severity, or for any outcome that might be gamma-distributed. reg:tweedie, Tweedie regression with log-link. It might be useful, e.g., for modeling total loss in insurance, or for any outcome that might be Tweedie-distributed. Weight balance scale_pos_weight (default 1) controls the balance of positive and negative weights, useful for unbalanced classes. A typical value to consider is sum(negative instances) / sum(positive instances).\nfrom sklearn.model_selection import GridSearchCV from xgboost.sklearn import XGBClassifier from typing import Tuple def find_best_xgboost_model(train_x: pd.DataFrame, train_y: pd.Series) -\u0026gt; Tuple[dict, float]: scale_pos_weight = (len(train_y) - train_y.sum()) / train_y.sum() param_test = { \u0026#39;max_depth\u0026#39;: [1, 2, 4, 8], \u0026#39;learning_rate\u0026#39;: [0.05, 0.06, 0.07], \u0026#39;n_estimators\u0026#39;: [10, 50, 100] } gsearch = GridSearchCV(estimator=XGBClassifier( use_label_encoder=False, objective=\u0026#39;binary:logistic\u0026#39;, scale_pos_weight=scale_pos_weight, tree_method = \u0026#34;hist\u0026#34;, seed=27), param_grid=param_test, scoring=\u0026#39;roc_auc\u0026#39;, n_jobs=-1, cv=8) gsearch.fit(train_x, train_y) return gsearch.best_params_, gsearch.best_score_ best_params, best_score = find_best_xgboost_model(train_x, train_y) Using the xgboost model parameters, it predicts the probabilities of defaulting.\nbest_params_, best tuning parameters train_x, training dataframe with loan details train_y, default target column for training test_x, testing dataframe with loan details test_y, default target column for testing The result is a series of probabilities whether loan entry will default or not and corresponding model\u0026rsquo;s AUC score\nfrom sklearn.metrics import roc_auc_score def xgboost_predict(best_params_: dict, train_x: pd.DataFrame, train_y: pd.Series, test_x: pd.DataFrame, test_y: pd.Series) -\u0026gt; Tuple[list, float]: scale_pos_weight = (len(train_y) - train_y.sum()) / train_y.sum() xgb_model = XGBClassifier(objective=\u0026#39;binary:logistic\u0026#39;, scale_pos_weight=scale_pos_weight, seed=27, max_depth=best_params_[\u0026#39;max_depth\u0026#39;], learning_rate=best_params_[\u0026#39;learning_rate\u0026#39;], n_estimators=best_params_[\u0026#39;n_estimators\u0026#39;] ) xgb_model.fit(train_x, train_y) predicted_probabilities_ = xgb_model.predict_proba(test_x)[:, 1] auc_ = roc_auc_score(test_y, predicted_probabilities_) return predicted_probabilities_, auc_ predicted_probabilities, auc = xgboost_predict(best_params, train_x, train_y, test_x, test_y) print(\u0026#34;AUC: {}\u0026#34;.format(auc)) AUC: 0.7356799122465589 Filters the original loan dataframe to just include the loans from the test dataframe and then it adds the predicted probabilities.\nloans_df_, original loan dataframe test_index, indices from the test dataframes predicted_probabilities_, the probabilities forecasted by the XGBoost model Returns the loans dataframe with predictions\nimport numpy as np def prepare_test_with_predictions(loans_df_: pd.DataFrame, test_index: pd.Index, predicted_probabilities_: np.array)\\ -\u0026gt;pd.DataFrame: loan_test_df = loans_df_.loc[test_index] loan_test_df[\u0026#39;predicted_probabilities\u0026#39;] = predicted_probabilities_ return loan_test_df loans_with_predictions_df = prepare_test_with_predictions(data, test_x.index, predicted_probabilities) loans_with_predictions_df.head() NewCreditCustomer Amount Interest LoanDuration Education NrOfDependants EmploymentDurationCurrentEmployer IncomeFromPrincipalEmployer IncomeFromPension IncomeFromFamilyAllowance ... Other Owner Owner_with_encumbrance Tenant Entrepreneur Fully Partially Retiree Self_employed predicted_probabilities 30299 False 530.0 10.68 36 4.0 NaN 5.0 0.0 0.0 0.0 ... 0 0 0 1 0 0 0 0 0 0.641520 34126 False 530.0 21.57 24 4.0 NaN 1.0 0.0 0.0 0.0 ... 0 0 0 0 0 0 0 0 0 0.770486 11200 False 2300.0 15.62 36 4.0 0.0 6.0 1159.0 0.0 0.0 ... 0 1 0 0 0 1 0 0 0 0.748680 25133 True 530.0 27.36 36 4.0 NaN 6.0 0.0 0.0 0.0 ... 0 0 1 0 0 0 0 0 0 0.469619 42758 True 4250.0 18.94 60 4.0 NaN 1.0 0.0 0.0 0.0 ... 0 0 0 0 0 0 0 0 0 0.527547 5 rows  41 columns\nVisualisation import seaborn as sns sns.histplot(loans_with_predictions_df[\u0026#39;predicted_probabilities\u0026#39;], stat=\u0026#39;density\u0026#39;) \u0026lt;AxesSubplot:xlabel='predicted_probabilities', ylabel='Density'\u0026gt; ROC and AUC Based on actuals and predicted values3, it calculates their false positive rate (fpr), the true positive rate (tpr). It also returns the corresponding thresholds used as well as the value for the area under the curve.\nactuals, series of actual values indicating whether the loan defaulted or not predicted_probabilities, series of predicted probabilities of the loan defaulting Return a unique series of false and true positive rates with corresponding series of thresholds and value for total area under the curve.\nfrom sklearn.metrics import roc_curve, auc def get_roc_auc_data(actuals: pd.Series, predicted_probabilities: pd.Series) -\u0026gt; Tuple[np.array, np.array, np.array, float]: fpr, tpr, thresholds = roc_curve(actuals, predicted_probabilities, pos_label=1) auc_score = auc(fpr, tpr) return fpr, tpr, thresholds, auc_score fpr, tpr, thresholds, auc_score = get_roc_auc_data(loans_with_predictions_df[\u0026#39;PaidLoan\u0026#39;], loans_with_predictions_df[\u0026#39;predicted_probabilities\u0026#39;]) sns.histplot(fpr) \u0026lt;AxesSubplot:ylabel='Count'\u0026gt; https://github.com/dmlc/xgboost\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nhttps://xgboost.readthedocs.io/en/stable/parameter.html#learning-task-parameters\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nSee ROC.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"/xgboost.html","tags":null,"title":"XGBoost"},{"categories":null,"contents":"Functions Default arguments Example of a function with default arguments in zsh\nfunction e() { if [ \u0026#34;$1\u0026#34; != \u0026#34;\u0026#34; ] then subl $1 else subl . fi } File name without the extension If you want the full path without the extension:\n$ myfile=/path/to/story.txt $ echo ${myfile:r} /path/to/story $ myfile=story.txt $ echo ${myfile:r} story If you want just the file name minus the path:\n$ myfile=/path/to/story.txt $ echo ${myfile:t} story.txt Check this out you can combine those two symbols!\n$ myfile=/path/to/story.txt $ echo ${myfile:t:r} story ","permalink":"/zsh.html","tags":null,"title":"zsh"}]