[{"categories":null,"contents":"I\u0026rsquo;ve been a long-time user of Poetry for my Python projects, and it\u0026rsquo;s been a welcome change. However, my recent exploration into Hatch has sparked my interest.\nPoetry simplifies dependency management with its unified pyproject.toml, but Hatch excels in scenarios requiring complex workflows. A common personal use-case is Docker multi-stage builds. Hatch, with its conventional requirements.txt and setup.py, offers more granular control, making complex configurations easier.\nHatch also aligns closely with the existing Python ecosystem due to its use of traditional setup files, linking old with new workflows, ensuring a better integration.\nFor instance, if using a container image manifest such as\n# Use a base Python image FROM python:3.9-slim as base # Set up a working directory WORKDIR /app # Copy requirements and install dependencies COPY requirements.txt . RUN pip install -r requirements.txt # Copy the rest of the application COPY . . # Other Docker configurations... Whereas with Poetry, you might need to install it within the Docker image and use poetry export to generate a requirements.txt equivalent, with Hatch, since it supports the traditional requirements.txt, integration with multi-stage builds can be simpler.\n","permalink":"/blog/2023-09-03.html","tags":null,"title":"Hatch Poetry"},{"categories":null,"contents":"Exception Bubbling in PythonOne aspect of Java that occasionally nudges at me is its explicit approach to exception handling. Java requires developers to either handle exceptions via try-catch blocks or declare them in method signatures. While it does enforce robustness, it sometimes feels a bit too constrained, especially when compared to the flexible nature of Python.\nRecently, I crafted a solution in Python for k8sutils. Instead of the usual explicit exception handling or modifying method signatures, I created a Python decorator - akin to annotations in Java - that substitutes an exception for another without altering the underlying code. Here\u0026rsquo;s what it looks like:\nimport functools import subprocess def rethrow(exception_type=Exception): \u0026#34;\u0026#34;\u0026#34;Rethrow a CalledProcessError as the specified exception type.\u0026#34;\u0026#34;\u0026#34; def decorator(func): @functools.wraps(func) def wrapper(*args, **kwargs): try: return func(*args, **kwargs) except subprocess.CalledProcessError as e: raise exception_type(f\u0026#34;Command failed: {e.cmd}. Error: {e.output}\u0026#34;) from e return wrapper return decorator Using this decorator, it becomes straightforward to alter the exception being thrown:\n@rethrow(ValueError) def get(namespace=None): \u0026#34;\u0026#34;\u0026#34;Get all deployments in the cluster.\u0026#34;\u0026#34;\u0026#34; cmd = \u0026#34;kubectl get deployments -o json\u0026#34; if namespace: cmd += f\u0026#34; -n {namespace}\u0026#34; result = subprocess.run(cmd, shell=True, check=True, capture_output=True, text=True) deployments = json.loads(result.stdout) return deployments The @rethrow(ValueError) decorator automatically translates a CalledProcessError to a ValueError without the need to change the method\u0026rsquo;s code.\nFor another example:\n@rethrow(RuntimeError) def delete_deployment(deployment_name): \u0026#34;\u0026#34;\u0026#34;Delete a specific deployment.\u0026#34;\u0026#34;\u0026#34; cmd = f\u0026#34;kubectl delete deployment {deployment_name}\u0026#34; subprocess.run(cmd, shell=True, check=True) Here, instead of bubbling up the generic CalledProcessError, any error encountered will raise a RuntimeError.\n","permalink":"/blog/2023-09-02.html","tags":null,"title":"Exception Bubbling in Python"},{"categories":null,"contents":"This is a caption for Drawing 2.\n","permalink":"/draw/form-and-function-in-flat-planes.html","tags":null,"title":"Form and Function in Flat Planes"},{"categories":null,"contents":"This is a caption for Drawing 1.\n","permalink":"/draw/harmonic-interference-three-colors.html","tags":null,"title":"Harmonic Interference (Three Colors)"},{"categories":null,"contents":"This is a caption for Drawing 1.\n","permalink":"/draw/mid-century-monoliths.html","tags":null,"title":"Mid-Century Monoliths"},{"categories":null,"contents":"This is a caption for Drawing 1.\n","permalink":"/draw/nocturnal-ripples.html","tags":null,"title":"Nocturnal Ripples"},{"categories":null,"contents":"I\u0026rsquo;ve been using a minimalist blog setup for some time now.\nI was having something of a framework fatigue after switching between a few static site generators. Each new generator I decided to try implied usually either learning a new programming language (Python, Ruby, Go) to perform basic setup and a new template engine syntax1. Typically I wasn\u0026rsquo;t using the vast majority of the features available for each generator. And finally, most of the generators I tried over the years rely on heavy configuration if I want to maintain the site organisation and look.\nI\u0026rsquo;ve discussed with some friends and colleagues why it\u0026rsquo;s my opinion that a plain HTML blog is still superior to other solutions (such as Markdown coupled with some generator framework). I\u0026rsquo;ll leave my arguments to a future post.\nHowever, I am still using some form of a generator. The blog writing process at the moment is the following:\nI write the content of the post to an HTML fragment (no HEAD, for instance). All files are HTML and in the same folder. I have a shell script to walk through the files in the input folder and add a common header, footer and process all code blocks with syntax highlighting. Save the \u0026ldquo;processed\u0026rdquo; files to an output folder Upload (currently to Github to be served via Github pages). The HTML fragments are minimal, for instance:\n\u0026lt;h1\u0026gt;A post-modern title\u0026lt;/h1\u0026gt; \u0026lt;p\u0026gt;Yes, this could be an entire blog post.\u0026lt;/p\u0026gt; The point is, where do we draw the line on what a static generator is? For this post, I won\u0026rsquo;t consider a loose collection of specialised scripts to be a static generator. There is no configuration, no convention, no theming ability 2. You can argue that this is what many generators do, but I think that\u0026rsquo;s beyond the scope of this short post.\nSince my static blog is straightforward, with minimal markup, why not create something equally simple for RSS generation? To do so, I\u0026rsquo;ve decided to go the way of \u0026ldquo;handcrafted\u0026rdquo; HTML.\nHowever, I was accustomed to a static site generator to generate some goodies, such as syndication feeds automatically.\nI\u0026rsquo;ve decided to add an RSS feed to the site, using minimal dependencies (only shell scripting and a couple of universal user-land tools such as grep and cat). This approach has the added benefit that it is applicable to expose other types of data as an RSS feed, such as server and periodic job logs.\nWe start by adding the feed header to the index.xml:\n\u0026lt;?xml version=\u0026#34;1.0\u0026#34; encoding=\u0026#34;utf-8\u0026#34;?\u0026gt; \u0026lt;rss version=\u0026#34;2.0\u0026#34;\u0026gt; \u0026lt;channel\u0026gt; \u0026lt;title\u0026gt;Rui Vieira\u0026#39;s blog\u0026lt;/title\u0026gt; \u0026lt;description\u0026gt;Rui Vieira\u0026#39;s personal blog\u0026lt;/description\u0026gt; \u0026lt;link\u0026gt;https://ruivieira.dev/\u0026lt;/link\u0026gt; \u0026lt;lastBuildDate\u0026gt;$(date)\u0026lt;/lastBuildDate\u0026gt; EOF The RSS 2.0 specification3 is quite simple in terms of the minimum requirements for a valid feed. The mandatory \u0026ldquo;header\u0026rdquo; fields are:\ntitle, the name of the channel. link, the URL to the HTML website corresponding to the channel. description, phrase or sentence describing the channel. In terms of feed items, according to the specification, at least one of title or description must be present, and all remaining elements are optional.\nWe use the following in this feed:\ntitle, the title of the item. link, the URL of the item. pubDate indicates when the item was published. pubDate needs to conform with RFC 822.\n[!INFO] Just as interesting tidbit, RFC 822 (which defines Internet Text Message formats) is one of the core email RFCs. It predates [https://en.wikipedia.org/wiki/ISO_8601]ISO 8601 by six years (1982) and it\u0026rsquo;s itself based on 1977\u0026rsquo;s [https://tools.ietf.org/html/rfc733]RFC 733.\nWe then loop over all the input files to build the RSS entries.\nFILES=input/*.html for FILE in $FILES do FILENAME=\u0026#34;${FILE##*/}\u0026#34; FILENAME=\u0026#34;${FILENAME%.*}\u0026#34; # extract title ... # write entry to index.xml done Using BashFirst, extract the title. The actual title is not inside the \u0026lt;title\u0026gt; tag, but on the first header \u0026lt;h1\u0026gt;.\ncat output/nb-estimation.html |\\ grep -E \u0026#34;\u0026lt;h1.*\u0026gt;(.*?)\u0026lt;/h1\u0026gt;\u0026#34; |\\ sed \u0026#39;s/.*\u0026lt;h1.*\u0026gt;\\(.*\\)\u0026lt;\\/h1\u0026gt;.*/\\1/\u0026#39; The first produces:\n\u0026lt;div id=\u0026#34;main\u0026#34;\u0026gt; \u0026lt;h1 id=\u0026#34;negative-binomial-estimation\u0026#34;\u0026gt;Negative Binomial estimation\u0026lt;/h1\u0026gt; \u0026lt;/div\u0026gt; While the second produces:\nNegative Binomial estimations Now, what happens if we have more than one \u0026lt;h1\u0026gt; header? UNIX pipelines to the rescue. We simple retrieve the first line of the matching grep, by inserting a head -1.\nTo get the modified date of $FILE we can use:\ndate -r $FILE.html The final RSS feed build is:\ncat \u0026gt;output/index.xml \u0026lt;\u0026lt;EOF \u0026lt;?xml version=\u0026#34;1.0\u0026#34; encoding=\u0026#34;utf-8\u0026#34;?\u0026gt; \u0026lt;rss version=\u0026#34;2.0\u0026#34;\u0026gt; \u0026lt;channel\u0026gt; \u0026lt;title\u0026gt;Rui Vieira\u0026#39;s blog\u0026lt;/title\u0026gt; \u0026lt;description\u0026gt;Rui Vieira\u0026#39;s personal blog\u0026lt;/description\u0026gt; \u0026lt;link\u0026gt;https://ruivieira.dev/\u0026lt;/link\u0026gt; \u0026lt;lastBuildDate\u0026gt;$(date)\u0026lt;/lastBuildDate\u0026gt; EOF FILES=input/*.html for FILE in $FILES do FILENAME=\u0026#34;${FILE##*/}\u0026#34; FILENAME=\u0026#34;${FILENAME%.*}\u0026#34; TITLE=$(cat output/$FILENAME.html | grep -E \u0026#34;\u0026lt;h1.*\u0026gt;(.*?)\u0026lt;/h1\u0026gt;\u0026#34; | head -1 | sed \u0026#39;s/.*\u0026lt;h1.*\u0026gt;\\(.*\\)\u0026lt;\\/h1\u0026gt;.*/\\1/\u0026#39;) cat \u0026gt;\u0026gt;output/index.xml \u0026lt;\u0026lt;EOF \u0026lt;item\u0026gt; \u0026lt;title\u0026gt;$TITLE\u0026lt;/title\u0026gt; \u0026lt;link\u0026gt;https://ruivieira.dev/$FILENAME.html\u0026lt;/link\u0026gt; \u0026lt;pubDate\u0026gt;$(date -r output/$FILENAME.html)\u0026lt;/pubDate\u0026gt; \u0026lt;/item\u0026gt; EOF done cat \u0026gt;\u0026gt;output/index.xml \u0026lt;\u0026lt;EOF \u0026lt;/channel\u0026gt; \u0026lt;/rss\u0026gt; EOF Using PythonAnother possibility is to use a specialised tool to extract an RSS item from an HTML file. To do so, we need to parse the necessary data and replace the extraction part of the loop. This is, after all, along the lines of the UNIX philosophy4: create specialised tools with a focus on modularity and reusability.\nTo do, we create a simple script called post_title.py. It uses the Beautiful Soup library, which you can install using:\n$ pip install beautifulsoup4 The script reads an HTML file, extract the title and return:\nfrom bs4 import BeautifulSoup import sys with open(sys.argv[1], \u0026#39;r\u0026#39;) as file: data = file.read() soup = BeautifulSoup(data, features=\u0026#34;html.parser\u0026#34;) print(soup.h1.string) This script can now be used to replace the title extraction:\ncat \u0026gt;output/index.xml \u0026lt;\u0026lt;EOF \u0026lt;?xml version=\u0026#34;1.0\u0026#34; encoding=\u0026#34;utf-8\u0026#34;?\u0026gt; \u0026lt;rss version=\u0026#34;2.0\u0026#34;\u0026gt; \u0026lt;channel\u0026gt; \u0026lt;title\u0026gt;Rui Vieira\u0026#39;s blog\u0026lt;/title\u0026gt; \u0026lt;description\u0026gt;Rui Vieira\u0026#39;s personal blog\u0026lt;/description\u0026gt; \u0026lt;link\u0026gt;https://ruivieira.dev/\u0026lt;/link\u0026gt; \u0026lt;lastBuildDate\u0026gt;$(date)\u0026lt;/lastBuildDate\u0026gt; EOF FILES=input/*.html for FILE in $FILES do FILENAME=\u0026#34;${FILE##*/}\u0026#34; FILENAME=\u0026#34;${FILENAME%.*}\u0026#34; cat \u0026gt;\u0026gt;output/index.xml \u0026lt;\u0026lt;EOF \u0026lt;item\u0026gt; \u0026lt;title\u0026gt;$(post_title.py $FILENAME.html)\u0026lt;/title\u0026gt; \u0026lt;link\u0026gt;https://ruivieira.dev/$FILENAME.html\u0026lt;/link\u0026gt; \u0026lt;pubDate\u0026gt;$(date -r output/$FILENAME.html)\u0026lt;/pubDate\u0026gt; \u0026lt;/item\u0026gt; EOF done cat \u0026gt;\u0026gt;output/index.xml \u0026lt;\u0026lt;EOF \u0026lt;/channel\u0026gt; \u0026lt;/rss\u0026gt; EOF The reason why the whole RSS feed is not generated in Python is to have the title extraction as a \u0026ldquo;function\u0026rdquo; which can map to whichever logic the shell script is using.\nHope this could be useful to you. Happy coding!\nAs it turns out \u0026hellip; I reverted to using a static site generator. More information can be found in the page site details.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nApart from plain CSS theming, that is.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nhttps://validator.w3.org/feed/docs/rss2.html\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nSee Unix philosophy.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"/semi-handcrafted-rss.html","tags":null,"title":"(Semi) handcrafted RSS"},{"categories":null,"contents":" The original page seems to have disappeared from the Internet1 (see: link rot), so the original contents are kept here.\nWhen making this website, I wanted a simple, reasonable way to make it look good on most displays. Not counting any minimization techniques, the following 58 bytes worked well for me:\nmain { max-width: 38rem; padding: 2rem; margin: auto; } Let\u0026rsquo;s break this down.\nmax-width: 38rem It appears that the default font size for most browsers is 16px, so 38rem is 608px. Supporting 600px displays at a minimum seems reasonable.\npadding: 2rem If the display\u0026rsquo;s width goes under 38rem, then this padding keeps things looking pretty good until around 256px. While this may seem optional, it actually hits two birds with one stone - the padding also provides sorely-needed top and bottom whitespace.\nmargin: auto This is really all that is needed to center the page, because main is a block element under semantic html5.\nA key insight: it took me a surprising number of iterations to arrive at this point. Perhaps that speaks to the fact that I know nothing about \u0026ldquo;modern\u0026rdquo; web development, or, as i\u0026rsquo;m more inclined to believe, just how hard it is to keep it simple in a world of complication.\n[!Update] Following some discussion (see footer), I\u0026rsquo;ve since changed the padding to 1.5rem for a happier compromise between mobile and desktop displays.\n[!Update 2] The ch unit was brought to my attention here, and I quite like it! I\u0026rsquo;ve since changed to 70ch / 2ch, which looks nearly the same with 2 less bytes, except that the padding is a little bit smaller (a good thing for mobile).\nA cached version can still be seen, hopefully, at http://web.archive.org/web/20210318102514/https://jrl.ninja/etc/1/\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"/58-bytes-of-css-to-look-great-nearly-everywhere.html","tags":null,"title":"58 bytes of CSS to look great nearly everywhere"},{"categories":null,"contents":"Recently, I\u0026rsquo;ve been following with interest the development of the Crystal language.\nCrystal is a statically typed language with a syntax resembling Ruby\u0026rsquo;s. The main features which drawn me to it were its simple boilerplate-free syntax (which is ideal for quick prototyping), tied with the ability to compile directly to native code along with a dead simple way of creating bindings to existing C code.\nThese features make it quite attractive, in my opinion, for scientific computing. To test it against more popular languages, I\u0026rsquo;ve decided to run the Gibbs sampling examples created in Darren Wilkinson\u0026rsquo;s blog.\nI recommend reading this post, and in fact, if you are interested in Mathematics and scientific computing in general, I strongly recommend you follow the blog.\nAs explained in the linked post, I will make a Gibbs sampler for\n$$ f\\left(x,y\\right)=kx^2\\exp\\left\\lbrace-xy^2-y^2+2y-4x\\right\\rbrace $$\nwith\n$$ \\begin{aligned} x|y \u0026amp;\\sim Ga\\left(3,y^2+4\\right) \\\\ y|x \u0026amp;\\sim N\\left(\\frac{1}{1+x},\\frac{1}{2\\left(1+x\\right)}\\right) \\end{aligned} $$\nThe original examples were ran again, without any code alterations. I\u0026rsquo;ve just added the Crystal version.\nThis implementation uses a very simple wrapper I wrote to the famous GNU Scientific Library (GSL).\nrequire \u0026#34;../libs/gsl/statistics.cr\u0026#34; require \u0026#34;math\u0026#34; def gibbs(n : Int = 50000, thin : Int = 1000) x = 0.0 y = 0.0 puts \u0026#34;Iter x y\u0026#34; (0..n).each do |i| (0..thin).each do |j| x = Statistics::Gamma.sample(3.0, y\\*y+4.0) y = Statistics::Normal.sample(1.0/(x+1.0), 1.0/Math.sqrt(2.0\\*x+2.0)) end puts \u0026#34;#{i} #{x} #{y}\u0026#34; end end gibbs (As you can see, the Crystal code is quite similar to the Python one).\nTo make sure it\u0026rsquo;s a fair comparison, I ran it in compiled (and optimised mode build using\n$ crystal build gibbs.cr --release $ time ./gibbs \u0026gt; gibbs_crystal.csv Looking at the results, you can see that they are consistent with the other implementations:\nThe timings for each of the different versions1 were\nLanguage Time (s) R 364.8 Python 144.0 Scala 9.896 Crystal 5.171 C 5.038 So there you have it. A Ruby-like language which can easily compete with C performance-wise.\nI sincerely hope that Crystal gets some traction in the scientific community.\nThat of course won\u0026rsquo;t depend solely on its merits but rather on an active community along with a strong library ecosystem.\nThis is lacking at the moment, simply because it is relatively new language with the specs and standard library still being finalised.\nRan in a 1.7 GHz Intel Core i7 Macbook Air.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"/a-gibbs-sampler-in-crystal.html","tags":null,"title":"A Gibbs Sampler in Crystal"},{"categories":null,"contents":"Recently when discussing the Crystal language and specifically the Gibbs sample blog post with a colleague, he mentioned that the Python benchmark numbers looked a bit off and not consistent with his experience of numerical programming in Python.\nTo recall, the numbers were:\nLanguage Time(s) R 364.8 Python 144.0 Scala 9.896 Crystal 5.171 C 5.038 To have a better understanding of what is happening, I\u0026rsquo;ve decided to profile and benchmark that code (running on Python 3.6).\nThe code is the following:\nimport random, math def gibbs(N=50000, thin=1000): x = 0 y = 0 print(\u0026#34;Iter x y\u0026#34;) for i in range(N): for j in range(thin): x = random.gammavariate(3, 1.0 / (y y + 4)) y = random.gauss(1.0 / (x + 1), 1.0 / math.sqrt(2 x + 2)) print(i,x,y) Profiling this code with cProfile gives the following results:\nName Call count Time (ms) Percentage gammavariate 50000000 141267 52.1% gauss 50000000 65689 24.2% \u0026lt;built-in method math.log\u0026gt; 116628436 18825 6.9% \u0026lt;method 'random' of '_random.Random' objects\u0026gt; 170239973 17155 6.3% \u0026lt;built-in method math.sqrt\u0026gt; 125000000 12352 4.6% \u0026lt;built-in method math.exp\u0026gt; 60119980 7276 2.7% \u0026lt;built-in method math.cos\u0026gt; 25000000 3338 1.2% \u0026lt;built-in method math.sin\u0026gt; 25000000 3336 1.2% \u0026lt;built-in method builtins.print\u0026gt; 50001 1030 0.4% gibbs.py 1 271396 100.0% The results look different than the original ones on account of being performed on a different machine. However, we will just look into the relative code performance between different implementations and whether the code itself has room for optimisation.\nSurprisingly, the console I/O took a much smaller proportion of the execution time than I expected (0.4%).\nOn the other hand, as expected, the bulk of the execution time is spent on the gammavariate and gauss methods.\nThese methods, however, are provided by the Python\u0026rsquo;s standard library random, which underneath makes heavy usage of C code (mainly by usage of the random() function).\nFor the second run of the code, I\u0026rsquo;ve decided to use numpy to sample from the Gamma and Normal distributions. The new code, gibbs_np.py, is provided below.\nimport numpy as np import math def gibbs(N=50000, thin=1000): x = 0 y = 0 print(\u0026#34;Iter x y\u0026#34;) for i in range(N): for j in range(thin): x = np.random.gamma(3, 1.0 / (y y + 4)) y = np.random.normal(1.0 / (x + 1), 1.0 / math.sqrt(2 x + 2)) print(i,x,y) if __name__ == \u0026#34;main\u0026#34;: gibbs() We can see from the plots below that the results from both modules are identical.\nThe profiling results for the numpy version were:\nName Call count Time (ms) Percentage \u0026lt;method 'gamma' of 'mtrand.RandomState' objects\u0026gt; 50000000 121211 45.8% \u0026lt;method 'normal' of 'mtrand.RandomState' objects\u0026gt; 50000000 83092 31.4% \u0026lt;built-in method math.sqrt\u0026gt; 50000000 6127 2.3% \u0026lt;built-in method builtins.print\u0026gt; 50001 920 0.3% gibbs_np.py 1 264420 100.0% A few interesting results from this benchmark were the fact that using numpy or random didn\u0026rsquo;t make much difference overall (264.4 and 271.3 seconds, respectively).\nThis is despite the fact that, apparently, the Gamma sampling seems to perform better in numpy but the Normal sampling seems to be faster in the random library.\nYou will notice that we\u0026rsquo;ve still used Python\u0026rsquo;s built-in math.sqrt since it is known that for scalar usage it out-performs numpy\u0026rsquo;s equivalent.\nUnfortunately, in my view, we are just witnessing a fact of life: Python is not the best language for number crunching.\nSince the bulk of the computational time, as we\u0026rsquo;ve seen, is due to the sampling of the Normal and Gamma distributions, it is clear that in our code there is little room for optimisation except the sampling methods themselves.\nA few possible solutions would be to:\nConvert the code to Cython Use FFI to call a highly optimised native library which provides Gamma and Normal distributions (such as GSL) Nevertheless, personally I still find Python a great language for quick prototyping of algorithms and with an excellent scientific computing libraries ecosystem. Keep on Pythoning.\n","permalink":"/a-simple-python-benchmark-exercise.html","tags":null,"title":"A simple Python benchmark exercise"},{"categories":null,"contents":"In this blog post I would like to talk a little bit about recommendation engines in general and how to build a streaming recommendation engine on top of Apache Spark.\nI will start by introducing the concept of collaborative filterig, and focus in two variants: batch and streaming Alternating Least Squares (ALS). I will look at the principles of a streaming distributed recommendation engine on Spark and finally, I\u0026rsquo;ll talk about practical issues when using these methods.\nRecommendation enginesSo what are \u0026ldquo;recommendation engines\u0026rdquo;?\nRecommendation engines are a popular method to match users, products and historical data on user behaviour.\nCollaborative filteringIn the majority of cases, we assume there\u0026rsquo;s a unique mapping between a user $x$, a product $y$ and rating $\\mathsf{R}_{x,y}$.\n$$ \\left(x,y\\right) \\mapsto \\mathsf{R}_{x,y} $$\nThe \u0026ldquo;collaborative\u0026rdquo; aspect refers to the fact that we are using collective information from a group of users and \u0026ldquo;filtering\u0026rdquo; is simply a synonym for \u0026ldquo;prediction\u0026rdquo;.\nSo, we use collaborative filtering quite frequently in our daily life and it really seems like common sense.\nThe main principle is that if a group of people tend to collectively have similar tastes, it is more likely that they agree on an unknown product.\nLet\u0026rsquo;s imagine that you have a number of friends with whom you share a very similar musical taste, let\u0026rsquo;s call it A and another group, B, compared to which you have very different musical tastes.\nIf group A and group B both recommend you a new album which they regard highly, which one would you pick?\nYou will probably pick the album from group A, right? So that\u0026rsquo;s collaborative filtering in a nutshell.\nBonus question\nWhat if an album is considered really bad by group B? Does it mean you\u0026rsquo;ll like it?\nIt\u0026rsquo;s difficult to tell. Because group A has relevance to you, it\u0026rsquo;s easy to match. Because B is too dissimilar, a low rating is not very informative.\nAlternating Least Squares (ALS)One of the most popular collaborative filtering methods is Alternating Least Squares (ALS).\nIn ALS we assume that the available rating data can be represented in a sparse matrix form, that is, we will assume a sequential ordering of both users and products. Each entry of the matrix will then represent the rating for a unique pair of user and products.\nIf we then consider ratings data as a matrix, let\u0026rsquo;s call it $\\mathsf{R}$, the user and product ids will represent coordinates in a ratings matrix and the actual rating will be the value for that particular entry. To keep the notation consistent with the above we simply call the entry $(x,y)$ as $\\mathsf{R}_{x,y}$. This will look something like the matrix represented in the figure below.\nThe idea behind ALS is to factorise the ratings matrix $\\mathsf{R}_{x,y}$ into two matrices $\\mathsf{U}$ and $\\mathsf{P}$, which in turn, when multiplied back, will return an approximation of the original ratings matrix, that is:\n$$ \\mathsf{R} \\approx \\hat{\\mathsf{R}} = \\mathsf{U}^T \\mathsf{P} $$\nTo \u0026ldquo;predict\u0026rdquo; a missing rating for a user $x$ and product $y$, we can simply multiply two vectors, namely the $x$ row from the user latent factors and the $y$ column from the product latent factors, $\\hat{\\mathsf{R}}_{x,y}$, that is:\n$$ \\hat{\\mathsf{R}}_{x,y} = \\mathsf{U}_x^T \\mathsf{P}_y $$\nThere are several ways to tackle this factorisation problem and we will cover two of them in here. We will first look at a batch method, which aims at factorising using the whole of the ratings matrix and a stochastic gradient descent method, which uses a single observation at a time.\nBatch ALSThis factorisation is performed by first defining an (objective) loss function (here called $\\ell$).\nA general form is represented below where, as before, $\\mathsf{R} _{x,y}$ is the true rating and $\\hat{\\mathsf{R}} _{x,y}$ is the predicted rating, calculated as seen previously. The remaining terms are simply regularisation terms to help prevent overfitting.\n$$ \\ell = \\sum c _{x,y} \\left(\\mathsf{R} _{x,y} - \\underbrace{\\mathsf{U}_x^T \\mathsf{P} _y} _{\\hat{\\mathsf{R}} _{x,t}}\\right)^2 + \\lambda\\left(\\left\\lVert \\mathsf{U} \\right\\rVert^2 + \\left\\lVert \\mathsf{P} \\right\\rVert^2\\right) $$\nThe value of $(c_{x,y})$ constitutes a penalisation function and will depend on whether we are considering explicit or implicit feedback. If we consider the known ratings as our training dataset $\\mathcal{T}$, then, in the case of explicit feedback we have\n$$ c_{x,y} = \\begin{cases} 0,\\qquad\\text{if}\\ \\left(x,y\\right) \\notin \\mathcal{T} \\\\ 1,\\qquad\\text{if}\\ \\left(x,y\\right) \\in \\mathcal{T} \\end{cases} $$\nConstraining our loss function to only include known ratings. The implicit feedback case is different (and a possible future topic) and for the remainder of this post we will only consider the explicit feedback case. Given the above, we can then simplify our loss function, in the explicit feedback case, to\n$$ \\ell = \\sum _{x,y \\in \\mathcal{T}} \\left(\\mathsf{R} _{x,y} -\\hat{\\mathsf{R}} _{x,y}\\right)^2 + \\lambda\\left(\\left\\lVert \\mathsf{U} \\right\\rVert^2 + \\left\\lVert \\mathsf{P} \\right\\rVert^2\\right) $$\nMinimizing $\\ell$ is however an NP-hard problem, due to its non-convexity. However, if we treat $\\mathsf{U}$ as constant, then $\\ell$ is a convex in relation to $\\mathsf{P}$ and if we treat $\\mathsf{P}$ as constant, $\\ell$ is convex in relation to $\\mathsf{U}$. We can then alternate between fixing $\\mathsf{U}$ and $\\mathsf{P}$, changing the values such that the loss function $\\ell$ (above) is minimized. This procedure is then repeated until we reach convergence.\nThe way that ALS works is, in simplified terms, to find the factors $\\mathsf{U}$ and $\\mathsf{P}$, which when multiplied together provide an approximation of our ratings matrix $\\mathsf{R}$, as we\u0026rsquo;ve seen previously.\nOnce we have the factors $\\mathsf{U}$ and $\\mathsf{P}$, we can then predict the missing values in $\\mathsf{R}$ by using the approximation $\\hat{\\mathsf{R}}$.\nIt is clear that in a real world scenario we would have many missing ratings, simply due to the assumption that no user rates all products (if they did, the case for a recommendation engine will be significantly weaker). ALS is designed to deal with sparse matrices and to fill the blanks using predicted values. After factorization, our approximated ratings matrix will look something like this:\nAs mentioned previously, the first step is then to minimise the loss function. In this case we take the partial derivatives and set them to zero and fortunately this has a closed form solution. We get a system of linear equations which we can easily implement. The system will correspond to the solution of\n$$ \\frac{\\partial \\ell}{\\partial \\mathsf{U}_x}=0, \\qquad \\frac{\\partial \\ell}{\\partial \\mathsf{P}_y}=0. $$\nWe start by solving the user latent factor minimisation using:\n$$ \\begin{aligned} \\frac{1}{2}\\frac{\\partial \\ell}{\\partial \\mathsf{U} _x}\u0026amp;=0 \\\\\n\\frac{1}{2}\\frac{\\partial}{\\partial \\mathsf{U} _ x} \\sum _ {x,y \\in \\mathcal{T}} \\left(\\mathsf{R} _ {x,y} - \\mathsf{U} _ x^T \\mathsf{P } _ y\\right)^2 + \\lambda\\left(\\left\\lVert \\mathsf{U} \\right\\rVert^2 + \\left\\lVert \\mathsf{P} \\right\\rVert^2\\right)\u0026amp;=0 \\\\\n-\\sum _{x,y \\in \\mathcal{T}} \\left(\\mathsf{R} _ {x,y} - \\mathsf{U} _x^T \\mathsf{P}_y\\right)\\mathsf{P} _y^T + \\lambda \\mathsf{U}\\ _ x^T\u0026amp;=0\\\\\n-\\left(\\mathsf{R} _ x -\\mathsf{U} _ x^T \\mathsf{P}^T\\right)\\mathsf{P} + \\lambda \\mathsf{U} _x^T\u0026amp;=0\\\\\n\\mathsf{U} _ x^T\\left(\\mathsf{P}^T \\mathsf{P} + \\lambda \\boldsymbol{\\mathsf{I}}\\right) \u0026amp;= \\mathsf{R} _ x \\mathsf{P} \\\\\n\\mathsf{U} _ x^T \u0026amp;= \\mathsf{R} _ x \\mathsf{P} \\left(\\mathsf{P}^T \\mathsf{P} + \\lambda \\boldsymbol{\\mathsf{I}}\\right)^{-1}. \\end{aligned} $$\nSimilarly, we can solve for the product latent factor by using:\n$$ \\begin{aligned} \\frac{1}{2}\\frac{\\partial \\ell}{\\partial \\mathsf{P} _y}\u0026amp;=0 \\\\ -\\sum _{x,y \\in \\mathcal{T}} \\left(\\mathsf{R} _{x,y} - \\mathsf{P} _y^T \\mathsf{U} _x\\right)\\mathsf{U}_x^T + \\lambda \\mathsf{P} _y^T\u0026amp;=0\\\\ -\\left(\\mathsf{R}_y - \\mathsf{P} _y^T \\mathsf{U}^T\\right)\\mathsf{U} + \\lambda \\mathsf{P} _y^T\u0026amp;=0\\\\ \\mathsf{P} _y^T\\left(\\mathsf{U}^T \\mathsf{U} + \\lambda \\boldsymbol{\\mathsf{I}}\\right) \u0026amp;= \\mathsf{R} _y \\mathsf{U} \\\\ \\mathsf{P} _y^T \u0026amp;= \\mathsf{R} _y \\mathsf{U} \\left(\\mathsf{U}^T \\mathsf{U} + \\lambda \\boldsymbol{\\mathsf{I}}\\right)^{-1}. \\end{aligned} $$\nWe can then calculate each factor iteratively, by fixing the other one and solving the estimator. While this process is alternated, an error measure (usually the Root Mean Squared Error), or $RMSE$ is calculated (as below) between the rating matrix approximation given by the latent factors and the ratings which we have, $\\mathcal{T}$. This method is guaranteed to converge and when we consider out approximation to be good enough, or after a set number of iterations we can then stop the refinement.\n$$ RMSE = \\sqrt{\\frac{1}{n}\\sum _{x,y \\in \\mathcal{T}}\\lvert \\hat{\\mathsf{R}} _{x,y} - \\mathsf{R} _{x,y}\\rvert} $$\nAfter the latent factors are estimated, we can then use them to try to recreate the original ratings matrix with the approximation as we\u0026rsquo;ve seen. The missing ratings in the original matrix will now be filled by values which minimize the least squares recursion and these are taken as the ratings \u0026ldquo;predictions\u0026rdquo;.\nTo illustrate the working of ALS, let\u0026rsquo;s assume we have a very quirky shop that only ever sells 300 products and has exactly 300 customers. On top of that, users are allowed to use 8 bit number to rate the products. We will also assume in this unusual shop that every user has rated every product.\nNow we\u0026rsquo;re humans, and we visualise patterns in colour more easily than in numbers. We will assign a palette to the ratings, so that each rating corresponds to a colour.\nI think you know where this is going \u0026hellip; we make up this final ratings matrix so now we can visualise the ALS progress.\nSo how do we perform this factorisation? The initial step is to fill the latent factors $\\mathsf{U}$ and $\\mathsf{P}$) with random values. Since at this point, we assume we don\u0026rsquo;t have any ratings, having random factors will lead to an initial random guess of the ratings matrix.\nWe then proceed to calculate each factor matrix, as we\u0026rsquo;ve seen, by calculating one using the estimator while keeping the other one constant and then alternating. We can see by the movie below that at each iteration the approximation to the original ratings gets better, stabilising after a few steps.\nThis is to be expected, in this case, since this would be the simplest implementation of ALS: a batch ALS on a single machine where we know all the ratings.\nThere should have been a video here but your browser does not seem to support it. So a fair question that arises is: why can\u0026rsquo;t we update this model and perform recommendations in a streaming fashion using this method?\nAfter all, if users add product ratings, we can simply update the predictions by recalculating the factors!\nThe problem is that when a new rating is added, or when new users and new products are added, we need to recalculate the entirety of the $\\mathsf{U}$ and $\\mathsf{P}$ matrices, and to do so, we need to have access to all of the data, $\\mathsf{R}$.\nStreaming ALSIdeally, we want a method that would allow us to update $\\mathsf{U}$ and $\\mathsf{P}$ using one observation, $\\mathsf{R}_{x,y}$ at a time\nIt turns out that the Stochastic Gradient Descent (or SGD)1 method allows us to do precisely that. We\u0026rsquo;ll look at the specific variant of SGD we\u0026rsquo;ve used which is called Bias-Stochastic Gradient Descent (B-SGD).\nIt is important to keep in mind, under a certain point of view, both methods aim at the same thing.\nThey both try to factorise the ratings matrix as latent factors, which would then be used to perform predictions. The main differences are of course, how the data is used (batch or one observation at the time) and how the factorisation is calculated.\nIn the SGD case we use the concept of biases in both users and items. The bias is a measure of how consistently a product is rated by different users. The bias of rating $(x,y)$, that is the rating given by user $x$ to product $y$, can be calculated as the sum of $\\mu$, an overall average rating and the observed deviations of user $x$, which we call $b_x$, and the observed deviations of product $y$, called $b_y$, that is:\n$$ b_{x,y} = \\mu + b_x + b_y $$\nThis bias information is now incorporated in the rating prediction. We can see that the SGD prediction is simply the batch prediction plus the corresponding bias term\n$$ \\hat{\\mathsf{R}} _{x,y} = b _{x,y} + \\underbrace{\\mathsf{U}^T _x \\times \\mathsf{P} _y} _{batch} $$\nIf we take the loss function definition for the batch method (and still considering the explicit feedback case), we can then replace the predicted rating formulation with our new one. We have, as before, some regularisation terms, but now also include a new regularisation term for the bias components,\nbut we don\u0026rsquo;t need to go into that.\n$$ \\ell _{SGD} = \\sum _{x,y \\in \\mathcal{T}} \\left(\\mathsf{R} _{x,y} - b _{x,y} - \\hat{\\mathsf{R}} _{x,y}\\right)^2 + \\lambda\\left(\\left\\lVert \\mathsf{U} \\right\\rVert^2 + \\left\\lVert \\mathsf{P} \\right\\rVert^2 + b _x^2 + b _y^2\\right) $$\nSince calculating the full gradient is computationally very expensive, we calculate it for a single observation. As we can see, the SGD method allows us to update the user and product specific bias as well as a single user and product latent factor row given a single rating.\nProvided we have a single rating, the rating of user $x$ for product $y$, we can update the biases as well as the latent vectors for user $x$ and for product $y$, that is, we no longer need to update the entire matrices $\\mathsf{U}$ and $\\mathsf{P}$, while still maintaining a convergence property.\nProvided with a learning rate $\\gamma$ and defining our prediction error as\n$$ \\epsilon _ {x,y}=\\mathsf{R} _{x,y}-\\hat{\\mathsf{R}} _{x,y}, $$\nthe biases and latent factors can now be updated in the opposite direction of the calculated gradient, proportionally to the learning rate, such that\n$$ \\begin{aligned} b _x \u0026amp;\\leftarrow b _x + \\gamma \\left(\\epsilon _{x,y}-\\lambda _x b _x\\right) \\\\ b _y \u0026amp;\\leftarrow b _y + \\gamma \\left(\\epsilon _{x,y}-\\lambda _y b _y\\right) \\\\ \\mathsf{U} _x \u0026amp;\\leftarrow \\mathsf{U} _x + \\gamma \\left(\\epsilon _{x,y}\\mathsf{P} _y - \\lambda^\\prime _x \\mathsf{U} _x\\right) \\\\ \\mathsf{P} _y \u0026amp;\\leftarrow \\mathsf{P} _y + \\gamma \\left(\\epsilon _{x,y}\\mathsf{U} _x - \\lambda^\\prime _y \\mathsf{P} _y\\right) \\end{aligned} $$\nSo the practical difference, in terms of streaming data is evident now. Given that, in both methods, the objective is to estimate the latent factors, given the ratings: with batch ALS, whenever we get a new rating, we need to fully recalculate the factors iteratively until we reach convergence. Conversely, with an SGD based factorisation, whenever we have a new rating, we can simply estimate the relevant row and column in the latent factors, by calculating the gradients and adjusting its values.\nNext we show the previous manufactured ratings matrix being factorised using B-SGD. We now simply recalculate the biases and a single latent factor vector, one observation at the time. We can see that, as expected, the convergence is slower (we are using a single observation at each step) but in the end, it produces a similar result.\nThere should have been a video here but your browser does not seem to support it. Now, this works fine for a single machine implementing streaming ALS. But we are interested in scaling this to something larger than this example so we will use a distributed implementation of ALS. And this is were it can start to get tricky. As it is the case with distributed algorithms, there are some pitfalls which we need to avoid in order to have a performant implementation. We will look at a few of these by looking at the Apache Spark\u0026rsquo;s and its default ALS implementation.\nApache SparkAs probably most of you are familiar with, Spark is an Apache community project which aims at providing a modern platform for distributed computations. Spark provides several core data structures, such as RDDs (Resilient Distributed Datasets), Dataframes and Datasets.\nThe RDD is an immutable, distributed typed collection of objects. The RDD is partitioned across the cluster. This allows the spark operations, such as function mapping, to be applied to each subset of the RDD in parallel at each partition.\nFor the streaming ALS application we will use RDDs to implement the algorithm. Spark\u0026rsquo;s MLlib provides a collaborative filtering implementation based on the distributed batch ALS which we\u0026rsquo;ve covered previously. The API is quite simple and to train a model we need:\nval model = ALS.train(ratings, rank, iterations, lambda) case class Rating(int user, int product, double rating) val ratings: RDD[Rating] val rank: int val iterations: int val lambda: Double An RDD containing the ratings. The RDD has elements of the class Rating, which is basically a wrapper around a tuple of user id, product id and rating. This RDD corresponds to all the entries in our ratings matrix used previously. The rank which corresponds to the number of elements in our latent factor vectors (this would be the number of columns or rows in our $\\mathsf{U}$ and $\\mathsf{P}$ matrices). A stopping criteria in terms of iterations for the ALS. And finally, we set the lambda parameter, a regularisation parameter, which we\u0026rsquo;ve shown to be a part of the loss function\u0026rsquo;s regularisation. Since we have the data, the question is then how to choose the parameters. A typical method is to split the original ratings data into two random sets, one for training and one for validation. We then proceed to train the model to several different parameters, usually according to a grid search, and calculate some error measure between the predicted and validation ratings, choosing the parameters which minimise the error.\nOnce the model is trained, we get a MatrixFactorizationModel instance, which is basically a wrapper for the latent factors as RDDs.\nval model = ALS.train(ratings, rank, iterations, lambda) model: MatrixFactorizationModel class MatrixFactorizationModel { val userFeatures: RDD[(Int, Array[Double])] val productFeatures: RDD[(Int, Array[Double])] } One we have the trained model, we can now perform predictions.\nStreaming dataWe now want to build a streaming recommender system. For this scenario, we will assume that the observations take the form of a Spark\u0026rsquo;s Discretised Stream or DStream. With DStreams we consume the stream as mini-batches of RDDs over a certain interval window.\nWe can for instance, use the first mini-batch to initialise the model and the following batches to continuously train the model.\nOne immediate advantage of using observations as a stream is that we no longer need to keep the entirety of the data in memory or read it from storage. If we consider the batch implementation with a very large dataset if we had a single new observation and wanted to retrain the model, we would have to, for instance, read several million ratings from a database. With a streaming variant we can use that single observation to update the latent factors.\nWe will try to recreate Spark\u0026rsquo;s batch ALS API by allowing model training using a ratings RDD, however this time, we consume each RDD from the stream mini-batch and will incrementally train the model as observations trickle in.\nFirst, we start by establishing the quantities and data structures needed to implement streaming ALS.\nWe\u0026rsquo;ve seen in the previous slides that the recursions for the gradient calculation take the following form, here presented in pseudo-code:\nuserBias += gamma * (error - lambda * userBias) userFeature(i) += gamma * (error prodFeature(j) - lambda * userFeature(i)) We create a Factor class to encapsulate the features and the corresponding bias.\nWe recall that in the Spark ALS implementation, the features were stored in RDDs typed as a tuple of (id, Array), where now we have an equivalent form of (id, Factor) which allows us to capture the bias. I\u0026rsquo;ll now provide a quick overview of the steps required to go from the initial ratings stream to the trained model, in terms of Spark\u0026rsquo;s RDD operations.\nSimilarly to Spark\u0026rsquo;s ALS, we can assume that the model data will be in the form of Rating\u0026rsquo;s RDDs. These, as we\u0026rsquo;ve seen, will correspond to a mini-batch of ratings from our data stream. We first need to create the initial user and product latent factors for the observed data and we would start by creating two separate RDDs from the data, one keyed by user id, the other by product id.\nFor each entry of those new ~RDDs~ (the user and product indexed ones) we will now generate a random vector of features. This can be done by simply filling a vector of size ~rank~ with random uniform values, but as you will recall, we now also have a bias associated with each entry, which will initially also be set to a random value.\nWe now join the incoming ratings, with the generated user factors (using the user id as the key) getting a resulting ~RDD~ consisting of product ids, user ids, ratings and user factors (and the same thing for products and product factors).\nFinally, we have these two joint ~RDDs~ which have all the necessary quantities needed to calculate the partial gradient in each element. Recalling how to calculate a predicted rating in streaming ALS, we need the global bias, the user and product bias and the corresponding user and product latent vectors.\nThis is straightforward to calculate for each element as we can see from the pseudo-code.\nHere the ~dot~ function is simply a function to calculate the dot product treating the two factor arrays as vectors.\n$$ \\hat{\\mathsf{R}}_{x,y}=\\mu + b_x + b_y + \\mathsf{U}_x^T \\times \\mathsf{P}_y $$\nprediction = dot(userFactors.features, itemFactors.features) + userFactors.bias + itemFactors.bias + bias Given the prediction, the error is also straightforward to calculate, since the real rating is also included in this RDD.\n$$ \\epsilon _{x,y} = \\mathsf{R} _{x,y} - \\hat{\\mathsf{R}} _{x,y} $$\neps = rating - prediction And now, since we have the error, we can also easily calculate the update term for the user and product features. As mentioned previously, ~gamma~ and ~lambda~ are known model parameters which we pick ourselves when instantiating the streaming ALS algorithm.\n$$ \\gamma \\left(\\epsilon_{x,y}\\mathsf{U}_x-\\lambda^{\\prime}\\mathsf{P}_y\\right) $$\n(0 until rank).map { i =\u0026gt; gamma * (eps * userFactors.features(i) - lambda * itemFactors.features(i)) } Finally, we update the user and product biases given the model parameters and the previous bias.\n$$ \\gamma \\left(\\epsilon_{x,y}-\\lambda_y b_y\\right) $$\ngamma * (eps - lambda * itemFactors.bias) These calculated gradients can now be mapped to a new ~RDD~ which we will use to update the final biases and latent factors.\nThe last step is to split the gradients according to user and product, and finally, when in possession of all the individual gradients, we reduce them into latent factors by performing an aggregated sum for each user and product.\nSo these steps define the entirety of the streaming ALS operation. For each observation window, we calculate the latent factors ~RDD~, and on the following window we update these factors, given the current observations.\nWe\u0026rsquo;ve covered the initialisation case, that is, we assumed the case where the model is not initialised and we received the first mini-batch of ratings. If, on the following window, we receive ratings for previously unseen user or products, the procedure is exactly the same, that is, we generate random factors and update as described.\nNow I\u0026rsquo;ll just quickly cover the case where we get some ratings from users or for some product we\u0026rsquo;ve already seen. For this new set of observations, we proceed exactly as previously, that is, we split the data into separate RDDs, each one containing the ratings but keyed by user and product id. I\u0026rsquo;ll assume that we get a mixture of completely new data, that is, unseen user and products and some ratings for previously seen users and products shown in red.\nThe difference now is that, instead of assigning random factors and biases to each entry of these ~RDDs~, we perform a full outer join between them and the current latent factors.\nThe strategy is then to keep the matching existing latent factor and create random features and biases just for the user and product entries we haven\u0026rsquo;t seen before.\nNow that we are in possession of this joint ~RDD~, we can apply exactly the same steps as previously to update the factors and repeat these steps for all future incoming observations, allowing us to continuously update the model.\nIt is now easy to see that, in the limit situation where we only have one new rating, we would now only have to update a single entry of the latent factors ~RDD~, in contrast with the batch method, where the entirety of the factors would be used in the ALS step.\nLet\u0026rsquo;s look at some results comparing the streaming implementation with the Spark\u0026rsquo;s batch implementation.\nThe dataset we have chosen to use in these tests is one of the MovieLens\u0026rsquo; datasets. These dataset are a widely used data in recommendation engine research. They are managed by the Lens corporation and are freely available for non-commercial applications.\nThese datasets come in several variants, namely a small variant, useful for a quick algorithm prototyping and testing, and a full variant, with approximately 26 million ratings (from 45,000 movies and 270,000 users), useful for a more comprehensive testing and benchmark.\nThe data is available as a set of Comma Separated Value files, each containing different variables, but we are mainly interested in the ~ratings~ file which contains four variables: a unique user and movie id, represented as integers, a rating represented by a value from 0 to 5 with steps of 0.5 and a timestamp for when the movie was rated by this user.\nFirst, we\u0026rsquo;ll start by training a batch ALS model using the MovieLens data. We assume that we already have the observations as an ~RDD~ of ratings and simply split the data into 80% for training and 20% for validation.\nval split: Array[RDD[Rating]] = ratings.randomSplit(0.8, 0.2) val model = ALS.train(split(0), rank, iter, lambda) Here, we won\u0026rsquo;t show the steps to determine the best parameters for this dataset, were we performed a simple parameter grid search over a number of possible candidates. The Spark ALS API is quite simple and to train the model we simply pass the training ~RDD~ and the parameters.\nWe can now use the remaining 20% of the observations to calculate the RMSE between the model predictions and the actual ratings.\nWe now can persist the validation ~RDD~, so we can use the exact same one for the streaming ALS run.\nval predictions: RDD[Rating] = model .predict(split(1).map { x =\u0026gt;(x.user, x.product)) } val pairs = predictions .map(x =\u0026gt; ((x.user, x.product), x.rating)) .join( split(1) .map(x =\u0026gt; ((x.user, x.product), x.rating)) .values val RMSE = math.sqrt( pairs.map(x =\u0026gt; math.pow(x._1 - x._2, 2)).mean()) In order to test the streaming version, we first need to define a data source. We start with the original MovieLens data and remove all the ratings from the validation observations.\nWe then create a simulated stream of observations using Kafka, with an interval of 5 seconds and with 1000 observations in each mini-batch. These are arbitrary numbers, chosen just for practical reasons. We could have, for instance, a single observation in each mini-batch.\nIt is not guaranteed that the best parameters (namely ~rank~ and ~lambda~) chosen for the batch version are the best for the streaming implementation, however we\u0026rsquo;ve decided to use the same ones. For each mini-batch we then incrementally train the model and calculate the RMSE up to that point. Given the actual ratings in the validation set and the model\u0026rsquo;s prediction, the RMSE calculation is the same as in the batch version. And looking at the results, we can see that with each mini-batch (of 1000 observations), the RMSE from the streaming version (in blue) is edging towards the batch value (plotted as the horizontal dashed line).\nCaveatsHowever, streaming ALS has pitfalls which we have to take into account.\nCold startAn issue, which is shared with batch ALS, is usually called the cold start problem. This refers to initial point in a recommender engine where we have too few observations to make meaningful predictions. As we now know, when having a small number of ratings, since our latent factors are initialised to random numbers, most of our predicted ratings will also be random.\nAlthough this is not an exclusive problem to the streaming implementation, we might be tempted, since the system is suited for realtime recommendations, to immediately start serving predictions. It might be wise to exercise caution and train the model offline with a larger dataset or at least perform some model diagnostics to check how sensible our predictions are.\nHyperparameter estimationAnother challenge we encounter is hyperparameter estimation.\nIn the batch ALS case, we can perform a grid search for instance and estimate the hyperparameters. If, after some time, we find ourselves with a new ratings or even new product and users, we can simply repeat this procedure using the totality of the data. As an example, if in batch ALS at any point we wish to estimate the model with a different rank, this would be perfectly acceptable.\nIn the streaming case, we can\u0026rsquo;t do that. When we have a new batch of observations, we assume that previous ones were discarded since they are already incorporated in the latent factors. If they weren\u0026rsquo;t and we keep all the observations in the stream, we might as well use batch ALS.\nA solution is to perform a grid search in parallel from the start and prune the least performant models as time progresses. This has the disadvantage of being expensive in terms of resources, since we have to keep several models simultaneously and again, we have the cold start problem surfacing.\nThis means that we have no guarantee that the best parameters for a initial batch with few observations will still be the best further on.\nPerformanceAlso, there are some performance considerations. As we\u0026rsquo;ve seen, we implement some operations which can be costly in a Spark setting. We have several join operations which can lead to a considerable amount of data shuffling between partitions.\nCare must be taken into choosing an appropriate partitioning strategy to minimise data shuffling.\nSpark\u0026rsquo;s implementation of batch ALS uses a specific method called blocked ALS, which computes outgoing and ingoing links between user and products vectors and then partitioning them in blocks in order to minimise data transfer between nodes.\nAlso, to make predictions we might have to try and perform random access to the latent factors ~RDDs~. This also can be quite inefficient since we are using ~lookup~ methods.\nIf you want to get straight setting up your own distributed recommendation engine, I highly suggest you start with Spark\u0026rsquo;s builtin solution. I would highly recommended looking at the ~jiminy~ project (part of the radanalytics.io community), a micro-service oriented complete recommendation engine, ready to deploy on OpenShift.\nThe engine is split into services such as a predictor and a modeler, along with a front-end and tools to simplify tasks (such as using the MovieLens data) and it\u0026rsquo;s a great way to look at how to put a modern recommender engine together and also a great code read.\nVinagre, J., Jorge, A. M., \u0026amp; Gama, J. (2014). Fast incremental matrix factorization for recommendation with positive-only feedback. In , International Conference on User Modeling, Adaptation, and Personalization (pp. 459470).\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"/a-streaming-als-implementation.html","tags":null,"title":"A streaming ALS implementation"},{"categories":null,"contents":"EnvironmentsCreateTo create a new environment foo use\n$ conda create --name foo And to activate it use\n$ conda activate foo ","permalink":"/anaconda.html","tags":null,"title":"Anaconda"},{"categories":null,"contents":"Ansible notes.\nInstallationDebian/Ubuntu$ sudo apt update $ sudo apt install software-properties-common $ sudo add-apt-repository --yes --update ppa:ansible/ansible $ sudo apt install ansible ReferenceInstalling packagesTo install a remote deb package with Ansible we can use, for instance\n- name: Install Wezterm (Ubuntu) apt: deb: https://github.com/wez/wezterm/releases/download/20211205-192649-672c1cc1/wezterm-20211205-192649-672c1cc1.Ubuntu20.04.deb ","permalink":"/ansible.html","tags":null,"title":"Ansible"},{"categories":null,"contents":"A common introductory problem in Bayesian changepoint detection is the record of UK coal mining disasters from 1851 to 1962. More information can be found in Carlin, Gelfand and Smith (1992).\nAs we can see from the plot below, the number of yearly disasters ranges from 0 to 6 and we will assume that at some point within this time range a change in the accident rate has occured.\nThe number of yearly disasters can be modelled as a Poisson with a unknown rate depending on the changepoint $k$:\n$$ y_t \\sim \\text{Po}\\left(\\rho\\right),\\qquad \\rho = \\begin{cases} \\mu, \u0026amp; \\text{if}\\ t=1,2,\\dots,k \\\\ \\lambda, \u0026amp; \\text{if}\\ t = k +1, k + 2, \\dots,m \\end{cases} $$\nOur objective is to estimate in which year the change occurs (the changepoint $k$) and the accident rate before ($\\mu$) and after ($\\lambda$) the changepoint amounting to the parameter set $\\Phi = \\left\\lbrace\\mu,\\lambda,k\\right\\rbrace$.\nWe will use Crystal (with crystal-gsl) to perform the estimation.\nWe start by placing independent priors on the parameters:\n$k \\sim \\mathcal{U}\\left(0, m\\right)$ $\\mu \\sim \\mathcal{G}\\left(a_1, b_1\\right)$ $\\lambda \\sim \\mathcal{G}\\left(a_2, b_2\\right)$ For the remainder we\u0026rsquo;ll set $a_1=a_2=0.5$, $c_1=c_2=0$ and $d_1=d_2=1$.\nThe joint posterior of $\\Phi$ is then:\n$$ \\pi\\left(\\Phi|Y\\right) \\propto p\\left(Y|\\Phi\\right) \\pi\\left(k\\right) \\pi\\left(\\mu\\right) \\pi\\left(\\lambda\\right), $$\nwhere the likelihood is\n$$ \\begin{aligned} p\\left(Y|\\Phi\\right) \u0026amp;= \\prod_{i=1}^{k} p\\left(y_i|\\mu,k\\right) \\prod_{i=k+1}^{m} p\\left(y_i|\\lambda,k\\right) \\\\ \u0026amp;= \\prod_{i=1}^{k} \\frac{\\mu^{y_i}e^{-\\mu}}{y_i!} \\prod_{i=k+1}^{m} \\frac{\\lambda^{y_i}e^{-\\lambda}}{y_i!}. \\end{aligned} $$\nAs such, the full joint posterior can be written as:\n$$ \\begin{aligned} \\pi\\left(\\Phi|Y\\right) \u0026amp;\\propto \\prod_{i=1}^{k} \\frac{\\mu^{y_i}e^{-\\mu}}{y_i!} \\prod_{i=k+1}^{m} \\frac{\\lambda^{y_i}e^{-\\lambda}}{y_i!} \\left(\\mu^{a_1-1} e^{-\\mu b_1}\\right) \\left(\\lambda^{a_2-1} e^{-\\lambda b_2}\\right) \\frac{1}{m} \\\\ \u0026amp;= \\mu^{a_1 + \\sum_{1}^{k}y_i - 1}e^{-\\mu\\left(k+b_1\\right)} \\lambda^{a_2 + \\sum_{k+1}^{m}y_i - 1}e^{-\\lambda\\left(m-k+b_2\\right)} \\end{aligned}. $$\nIt follows that the full conditionals are, for $\\mu$:\n$$ \\begin{aligned} \\pi\\left(\\mu|\\lambda,k,Y\\right) \u0026amp;\\propto \\mu^{a_1 + \\sum_{i=1}^{k}y_i-1}e^{-\\mu\\left(k+b_1\\right)} \\\\ \u0026amp;= \\mathcal{G}\\left(a_1+\\sum_{i=1}^{k}y_i, k + b_1\\right) \\end{aligned} $$\nWe can define the $\\mu$ update as:\ndef mu_update(data : Array(Int), k : Int, b1 : Float64) : Float64 Gamma.sample(0.5 + data[0..k].sum, k + b1) end The full conditional for $\\lambda$ is:\n$$ \\begin{aligned} \\pi\\left(\\lambda|\\mu,k,Y\\right) \u0026amp;\\propto \\lambda^{a_2 + \\sum_{i=k+1}^{m}y_i-1}e^{-\\lambda\\left(m-k+b_2\\right)} \\\\ \u0026amp;= \\mathcal{G}\\left(a_2+\\sum_{i=k+1}^{m}y_i, m - k + b_2\\right), \\end{aligned} $$\nwhich we implement as:\ndef lambda_update(data : Array(Int), k : Int, b2 : Float64) : Float64 Gamma.sample(0.5 + data[(k+1)..M].sum, M - k + b2) end The next step is to take\n$$ \\begin{aligned} b_1 \u0026amp;\\sim \\mathcal{G}\\left(a_1 + c_1,\\mu + d_1\\right) \\\\ b_2 \u0026amp;\\sim \\mathcal{G}\\left(a_2 + c_2,\\lambda + d_2\\right), \\end{aligned} $$\nwhich we will implement as:\ndef b1_update(mu : Float64) : Float64 Gamma.sample(0.5, mu + 1.0) end def b2_update(lambda : Float64) : Float64 Gamma.sample(0.5, lambda + 1.0) end And finally we choose the next year, $k$, according to\n$$ p\\left(k|Y,\\Phi\\right)=\\frac{L\\left(Y|\\Phi\\right)}{\\sum_{k^{\\prime}} L\\left(Y|\\Phi^{\\prime}\\right)} $$\nwhere\n$$ L\\left(Y|\\Phi\\right) = e^{\\left(\\lambda-\\mu\\right)k}\\left(\\frac{\\mu}{\\lambda}\\right)^{\\sum_i^k y_i} $$\nimplemented as\ndef l(data : Array(Int), k : Int, lambda : Float64, mu : Float64) : Float64 Math::E**((lambda - mu)*k) * (mu / lambda)**(data[0..k].sum) end So, let\u0026rsquo;s start by writing our initials conditions:\niterations = 100000 b1 = 1.0 b2 = 1.0 M = data.size # number of data points # parameter storage mus = Array(Float64).new(iterations, 0.0) lambdas = Array(Float64).new(iterations, 0.0) ks = Array(Int32).new(iterations, 0) We can then cast the priors:\nmus[0] = Gamma.sample(0.5, b1) lambdas[0] = Gamma.sample(0.5, b2) ks[0] = Random.new.rand(M) And define the main body of our Gibbs sampler:\n(1...iterations).map { |i| k = ks[i-1] mus[i] = mu_update(data, k, b1) lambdas[i] = lambda_update(data, k, b2) b1 = b1_update(mus[i]) b2 = b2_update(lambdas[i]) ks[i] = Multinomial.sample((0...M).map { |kk| l(data, kk, lambdas[i], mus[i]) }) } Looking at the results, we see that the mean value of $k$ is 38.761, which seems\nto indicate that the change in accident rates occurred somewhere near $1850+38.761\\approx 1889$.\nWe can visually check this by looking at the graph below. Also plotted are the density for the accident rates before ($\\mu$) and after ($\\lambda$) the change.\nOf course, one the main advantages of implementing the solution in Crystal is not only the boilerplate-free code, but the execution speed.\nCompared to an equivalent implementation in ~R~ the Crystal code executed roughly 17 times faster.\nLanguage Time (s) R 58.678 Crystal 3.587 ","permalink":"/bayesian-estimation-of-changepoints.html","tags":null,"title":"Bayesian estimation of changepoints"},{"categories":null,"contents":"ArXivist A bot which periodically toots a paper published on ArXiV. The main Mastodon page of the bot can be found at https://botsin.space/@arxivstats. The current queue can be found at https://w6118k.deta.dev/\nMesozoic A bot that publishes updates to Mastodon.\n","permalink":"/bots.html","tags":null,"title":"Bots"},{"categories":null,"contents":" The major guidelines of the Brutalist web design 1 are:\nContent is readable on all reasonable screens and devices This guideline is followed by this site. The vast majority of the pages work with all major browsers, implement a responsive design. They also work with Javascript disabled as described in site details. It even works with unreasonable browsers, screens and devices, such as Internet Explorer 6 (2001) and NCA Mosaic 2 (1993).\nIt also works with the Kristall2 lightweight browser, a browser for HTTP(S)/Gemini/Gopher without JS/CSS/WASM support.\nThis is achieved, generally, by using the following:\nUse of headings and subheadings to help break up content and make it easier to scan and read. Lists to organize information Examples and illustrations to helpt clarify complex concepts. Consistent formatting. This directly related to other points, sucn as uttons, yperlinks, ecorations, etc. Only hyperlinks and buttons respond to clicks This guideline is followed by this site. Buttons and hyperlinks (textual and TOC links) are the only way to navigate content (except for standard browser navigation, i.e. back button)\nHyperlinks are underlined and buttons look like buttons Not really followed. Hyperlinks are highlighted, rather than underlined.\n This guideline is now mostly followed by this site. Hyperlinks look like hyperlink and buttons look like buttons, though.\nThe back button works as expected This guideline is followed by this site. This site does not implement routers, history hijacking, etc. Hyperlinks and browser navigation are the only way to get around.\nView content by scrolling Vertical scrolling, specifically.\nDecoration when needed and no unrelated content This guideline is followed by this site. Decoration is judicious and mostly through colour. No splash images too.\nPerformance is a feature Performance is an important concern.\nAn area of work is image optimisation, since this the heaviest part of the site Javascript is minimal and the goal is to decrease dependence on it, not increase. https://brutalist-web.design/\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nhttps://github.com/MasterQ32/kristall\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"/brutalist-web-design.html","tags":null,"title":"Brutalist web design"},{"categories":null,"contents":" Life on the CLILife on the CLI is a journey of exploration and experimentation, where we are free to push the boundaries of what is possible and to create something truly extraordinary. It is a world of endless potential and boundless possibility, where every day brings new challenges and opportunities.\nEssentials CLI tools, so useful\nEffortlessly perform tasks\nTime-saving, essential\n\u0026mdash; \u0026ldquo;CLI tools, a necessity\u0026rdquo; Haiku\nEmacs Mosh1, the mobile shell https://mosh.org/\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"/cli.html","tags":null,"title":"CLI"},{"categories":null,"contents":"Notes on Clojure.\nReferenceConcatenating strings(require \u0026#39;[clojure.string :as string]) (string/join [\u0026#34;foo\u0026#34; \u0026#34;bar\u0026#34;]) List files recursivelyClojureTo list files recursively in Clojure1\n(file-seq \u0026#34;/etc\u0026#34;) BabashkaIf using Babashka, the following works:\n(file-seq (clojure.java.io/file \u0026#34;/etc\u0026#34;)) Filter by extension(filter #(.endsWith (.toString %) \u0026#34;.conf\u0026#34;) (file-seq \u0026#34;/etc\u0026#34;)) Get home directory(def home (System/getProperty \u0026#34;user.home\u0026#34;)) Filter collectionUse the syntax (filter predicate collection). For instance, to filter a collection of strings that end with bar, do:\n(def strings [\u0026#34;Foobar\u0026#34; \u0026#34;Barfoo\u0026#34; \u0026#34;Foobaz\u0026#34; \u0026#34;Barbar\u0026#34;]) (filter (fn [s] (clojure.string/ends-with? s \u0026#34;bar\u0026#34;)) strings) Compare with the Java version.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"/clojure.html","tags":null,"title":"Clojure"},{"categories":null,"contents":"Trying coconut!\n\u0026#34;hello, world!\u0026#34; |\u0026gt; print hello, world! def binexp(x) = 2**x 5 |\u0026gt; binexp |\u0026gt; print 32 def last_two(_ + [a, b]): return a, b def xydict_to_xytuple({\u0026#34;x\u0026#34;:x is int, \u0026#34;y\u0026#34;:y is int}): return x, y range(5) |\u0026gt; last_two |\u0026gt; print {\u0026#34;x\u0026#34;:1, \u0026#34;y\u0026#34;:2} |\u0026gt; xydict_to_xytuple |\u0026gt; print (3, 4) (1, 2) x = range(100) square = (-\u0026gt; _*_) ","permalink":"/coconut.html","tags":null,"title":"Coconut"},{"categories":null,"contents":"To prototype and test almost any application some type of input data is needed. Getting the right data can be difficult for several reasons, including strict licenses, a considerable amount of data engineering to shape the data to our requirements and the setup of dedicated data producers. Additionally, in modern applications, we are often interested in realtime/streaming and distributed processing of data with platforms such as Apache Kafka and Apache Spark and deployment in a cloud environment like OpenShift1 with tools such as oshinko.\nSimulating data is not trivial, since we might want to capture complex characteristic to evaluate our algorithms in conditions similar to the real world. In this post I\u0026rsquo;ll introduce a tool, timeseries-mock, which allows for a simple, containerised deployment of a data simulator along with some of the theory behind the data generation.\nState-space modelsA common way of modelling these patterns is to use state-space models (SSM). SSMs can be divided into a model and a observation structure.\n$$ Y_t|\\theta_t,\\Phi \\sim f\\left(y_t|\\theta_t,\\Phi_t\\right) \\ \\theta_t|\\theta_{t-1},\\Phi_t \\sim g\\left(\\theta_t|\\theta_{t-1},\\Phi_t\\right). $$\nIt is clear from the above that the state possesses a Markovian nature. The state at time $t$, $\\theta_t$ will on depend on the previous value, $\\theta_{t-1}$ and an observation at time $t$, $y_t$ will only depend on the current state, $\\theta_t$, that is:\n$$ p\\left(\\theta_{t}|\\theta_{0:t-1},y_{0:t-1}\\right)=p\\left(\\theta_{t}|\\theta_{t-1}\\right) \\ p\\left(\\theta_{t-1}|\\theta_{t:T},y_{t:T}\\right)=p\\left(\\theta_{t-1}|\\theta_{t}\\right) \\ p\\left(y_{t}|\\theta_{0:t},y_{0:t-1}\\right)=p\\left(y_{t}|\\theta_{t}\\right). $$\nIn this post we will focus on a specific instance of SSMs, namely Dynamic Generalised Linear Models (DGLMs). If you want a deeper theoretical analysis of DGLMs I strongly recommend Mike West and Jeff Harrison\u0026rsquo;s \u0026ldquo;Bayesian Forecasting and Dynamic Models\u0026rdquo; (1997). In DGLMs, the observation follows a distribution from the exponential family, $E\\left(\\cdot\\right)$ such, that\n$$ Y_t|\\theta_t,\\Phi \\sim E\\left(\\eta_t,\\Phi\\right) \\ \\eta_t|\\theta_t = L\\left(\\mathsf{F}^T \\theta_t\\right) $$\nwhere $L\\left(\\cdot\\right)$ is the linear predictor and the state evolves according to a multivariate normal (MVN) distribution:\n$$ \\theta_t \\sim \\mathcal{N}\\left(\\theta_t;\\mathsf{G}\\theta_{t-1},\\mathsf{W}\\right) $$\nStructureThe fundamental way in which timeseries-mock works is by specifying the underlying structure and observational model in a YAML configuration file. In the following sections we will look at the options available in terms of structural and observational components and look at how to represent them. As we\u0026rsquo;ve seen from (5), the structure will allows us to define the underlying patterns of the state evolution $\\lbrace \\theta_1, \\theta_2, \\dots, \\theta_t\\rbrace$. One of the advantages of DGLMs is the ability to compose several simpler components into a single complex structure. We will then look at some of these \u0026ldquo;fundamental\u0026rdquo; components.\nMeanAn underlying mean component will represent a random walk scalar state which can be specified in the configuration file by\nstructure: - type: mean start: 0.0 noise: 1.5 In this case start will correspond the mean of the state prior, $m_0$, and noise will correspond to the prior\u0026rsquo;s variance, $\\tau^2$, that is\n$$ \\theta_0 \\sim \\mathcal{N}\\left(m_0, \\tau^2\\right). $$\nIn the figure below we can see the above configuration for, respectively, a higher and lower value of noise.\nSeasonalitySeasonality is represented by Fourier components. A Fourier component can be completely specified by providing the period, start, noise and harmonics. The start and noise parameters are analogous to the mean components we saw previously. The period parameter refers to how long does it take for the cyclical pattern to repeat. This is done relatively to your time-point interval, such that\n$$ P = p_{\\text{fourier}}\\times p_{\\text{stream}}. $$\nThat is, if your stream\u0026rsquo;s rate is one observation every 100 milliseconds, $p_{\\text{stream}}=0.1$, and the harmonic\u0026rsquo;s period is 2000, $p_{\\text{fourier}}=1000$, then the seasonal component will repeat every $200$ seconds. The configuration example\nstructure: - type: season period: 200 harmonics: 5 start: 0.0 noise: 0.7 will create a sequence of state vectors $\\boldsymbol{\\theta}_{0:T}$ with five components, such that:\n$$ \\boldsymbol{\\theta}t = \\lbrace\\theta{1,t},\\dots,\\theta_{5,t}\\rbrace. $$\nIn this example, period refers to the number of time-points for each cycle\u0026rsquo;s repetition and harmonics to the number of Fourier harmonics used. \u0026ldquo;Simpler\u0026rdquo; cyclic patterns usually require less harmonics. In the figure below we show on the lowest and highest frequency harmonics, on the left and right respectively.\nAR-pAn AR($p$) (Auto-Regressive) component can be specified using the directives:\nstructure: - type: arma start: 0.0 coefficients: 0.1,0.3,0.15 noise: 0.5 In the above example we would be creating an AR(3) component, with respective coefficients $\\phi=\\lbrace 0.1,0.3,0.15 \\rbrace$. These coefficients will take part of the state model as\n$$ \\mathsf{G} = \\begin{bmatrix} \\phi_1 \u0026amp; \\phi_2 \u0026amp; \\dots \u0026amp; \\phi_{p-1} \u0026amp; \\phi_p \\ 1 \u0026amp; 0 \u0026amp; \\dots \u0026amp; 0 \u0026amp; 0 \\ \\vdots \u0026amp; \u0026amp; \\ddots \u0026amp; \\vdots \u0026amp; \\vdots \\ 0 \u0026amp; \\dots \u0026amp; 0 \u0026amp; 1 \u0026amp; 0 \\end{bmatrix} $$\nIn the following plots we show respectively the first and second component of the AR(3) state vector.\nComposingStructural composition of DGLM structures amounts to the individual composition of the state covariance matrix and state/observational evolution matrices such that:\n$$ \\mathsf{F}^T = \\begin{bmatrix}\\mathsf{F}_1 \u0026amp; \\mathsf{F}_2 \u0026amp; \\dots \\mathsf{F}_i\\end{bmatrix}^T \\\\ \\mathsf{G} = \\text{blockdiag}\\left(\\mathsf{G}_1, \\mathsf{G}_2, \\dots, \\mathsf{G}_i\\right) \\\\ \\mathsf{W} = \\text{blockdiag}\\left(\\mathsf{W}_1, \\mathsf{W}_2, \\dots, \\mathsf{W}_i\\right) $$\nTo express the composition of structures in the YAML configuration, we simply enumerate the separate components under the structure key. As as example, to compose the previous mean and seasonal components, we would simply write:\nstructure: - type: mean start: 0.0 noise: 1.5 - type: season period: 200 harmonics: 5 start: 0.0 noise: 0.7 This would create a structure containing both an underlying mean and a seasonal component.\nObservationsAs we have seen from (3) that an observational model can be coupled with a structure to complete the DGLM specification. In the following sections we will look at some example observational models and in which situations they might be useful.\nContinuousContinuous observations are useful to model real valued data such as stock prices, temperature readings, etc. This can be achieved by specifying the observational component as a Gaussian distribution such that:\n$$ Y_t|\\Phi \\sim \\mathcal{N}\\left(y_t|\\eta_t, \\mathsf{W}\\right). $$\nobservations: - type: continuous noise: 1.5 The following plot shows the coupling of the structure used in the mean section with the continuous (example above) observational model.\nDiscreteDiscrete observations, sometimes referred as \u0026ldquo;count data\u0026rdquo;, can be used to model integer quantities. This can be achieved by using a Poisson distribution in the observational model, such that:\n$$ Y_t|\\Phi \\sim \\text{Po}\\left(y_t|\\eta_t\\right) $$\nAn example configuration would be:\nobservations: - type: discrete In this case we will use the previous ARMA(3) structure example and couple it with a discrete observational model. The result is shown in the plot below:\nCategoricalIn the categorical case, we model the observations according to a binomial distribution, such that\n$$ Y_t \\sim \\text{Bin}\\left(y_t|\\eta_t,r\\right), $$\nwhere $r$ represents the number of categories. A typical example would be the case where $r=1$ which would represent a binary outcome (0 or 1). The following configuration implements this very case:\nobservations: - type: categorical categories: 1 We can see in the plot below (states on the left, observations on the right) a realisation of this stream, when using the previous seasonal example structure.\nOften, when simulating a data stream, we might be interested in the category labels themselves, rather than a numerical value. The generator allows to pass directly a list of labels and output the labelled observations. Let\u0026rsquo;s assume we wanted to generate a stream of random DNA nucleotides (C, T, A and G). The generator allows to pass the labels directly and output the direct mapping between observations and label, that is:\n$$ y_t: \\lbrace 0, 1, 2, 3 \\rbrace \\mapsto \\lbrace\\text{C, T, A, G}\\rbrace $$\nobservations: - type: categorical values: C,T,A,G Using the same seasonal structure and observation model as above, the output would then be:\nComposite modelIn a real-world scenario we are interested in simulating multivariate data and that comprises of different observational models. For instance, combining observation components from categorical, continuous, etc.\nThe approach taken for multivariate composite models, is that the structures are composed as seen previously into a single one and the resulting state vector is then \u0026ldquo;collapsed\u0026rdquo; into a vector on natural parameters, $\\eta_t$ which are then used to sample the individual observation components.\n$$ \\theta_t = \\lbrace\\underbrace{\\theta_{1}, \\theta_{2}, \\theta_{3}}{\\eta_1}, \\underbrace{\\theta{4}, \\theta_{5}, \\theta_{6}}_{\\eta_2}\\rbrace \\ y = f(\\eta_t) \\ = \\lbrace f_1(\\eta_1), f_2(\\eta_2)\\rbrace $$\nThe model composition can be expressed by grouping the different structures and observations under a compose key:\ncompose: - structure: # component 1 - type: mean start: 0.0 noise: 0.5 - observations: - type: continuous noise: 0.5 - structure: # component 2 - type: mean start: 5.0 noise: 3.7 - observations: - type: continuous noise: 1.5 ExamplesWe will look at two separate examples, one that creates a stream of simulated stock prices and one that generates a fake HTTP log. We assume we want to simulate a stream of per-day stock prices for 3 different companies, each with different characteristics. In this case, we will model the following:\nCompany A\u0026rsquo;s stocks start at quite a high value ($700) and are quite stable throughout time Company B\u0026rsquo;s stocks start slightly lower than A ($500) and are quite stable in the long run, but show heavy fluctuation from day to day Company C\u0026rsquo;s stocks start at $600 are very unpredictable Since we will be using per-day data we won\u0026rsquo;t be streaming this in realtime! We can map each daily observation to a second in our stream so we will specify a period=1. All stocks will exhibit a small monthly effect (period=30), which will be indicated by a noise=0.01 and a yearly effect (period=365) with a noise=2.5.\nThe resulting configuration will be:\ncompose: - structure: # company A - type: mean start: 700 noise: 0.01 # low structural variance - type: season period: 30 # monthly seasonality noise: 0.1 - type: season period: 365. # yearly seasonality noise: 1.7 - observations: - type: continuous noise: 0.05 # low observational variance - structure: # company B - type: mean start: 500 noise: 0.01 # low structural variance - type: season period: 30 # monthly seasonality noise: 0.7 - type: season period: 365. # yearly seasonality noise: 3.7 - observations: - type: continuous noise: 3.00 # higher observational variance - structure: # company C - type: mean start: 600 noise: 3.0 # higher structural variance - type: season period: 30 # monthly seasonality noise: 0.1 - type: season period: 365. # yearly seasonality noise: 0.25 - observations: - type: continuous noise: 4.0 # higher observational variance A realisation of this stream looks like the figure below.\nTo generate the fake HTTP log we will make the following assumptions:\nWe will have a request type (GET, POST, PUT) which will vary following a random walk A set of visited pages which, for illustration purposes, will be limited to (/site/page.htm, /site/index.htm and /internal/example.htm). We also want that the URLs visited follow a seasonal pattern. An IP address in the IPv4 format (i.e. 0-255.0-255.0-255.0-255) It is clear that for all variables the appropriate observational model is the categorical one. For the request type and the visited page we can pass directly the category name in the configuration file and for the IP we simply need four categorical observations with $r=255$.\nIf the underlying structure is the same, a useful shortcut to specify several observation component is the replicate key. In this particular example to generate four 0-255 numbers with an underlying mean as the structure, we simple use:\n- replicate: 4 structure: - type: mean start: 0.0 noise: 2.1 observations: type: categorical categories: 255 The full configuration for the HTTP log simulation could then be something like this:\ncompose: - structure: - type: mean start: 0.0 noise: 0.01 observations: type: categorical values: GET,POST,PUT - structure: - type: mean start: 0.0 noise: 0.01 - type: season start: 1.0 period: 15 noise: 0.2 observations: type: categorical values: /site/page.htm,/site/index.htm,/internal/example.htm - replicate: 4 structure: - type: mean start: 0.0 noise: 2.1 observations: type: categorical categories: 255 [\u0026#34;PUT\u0026#34;, \u0026#34;/internal/example.htm\u0026#34;, 171, 158, 59, 89] [\u0026#34;GET\u0026#34;, \u0026#34;/internal/example.htm\u0026#34;, 171, 253, 71, 146] [\u0026#34;PUT\u0026#34;, \u0026#34;/internal/example.htm\u0026#34;, 224, 252, 9, 156] [\u0026#34;POST\u0026#34;, \u0026#34;/site/index.htm\u0026#34;, 143, 253, 6, 126] [\u0026#34;POST\u0026#34;, \u0026#34;/site/page.htm\u0026#34;, 238, 254, 2, 48] [\u0026#34;GET\u0026#34;, \u0026#34;/site/page.htm\u0026#34;, 228, 252, 52, 126] [\u0026#34;POST\u0026#34;, \u0026#34;/internal/example.htm\u0026#34;, 229, 234, 103, 233] [\u0026#34;GET\u0026#34;, \u0026#34;/internal/example.htm\u0026#34;, 185, 221, 109, 195] ... Setting up the generatorAs I have mentioned in the beginning of this post, we want to fit the data simulation solution into a cloud computing workflow. To illustrate this we will use the OpenShift platform which allows for the deployment of containerised applications. A typical setup for a streaming data processing application would be as illustrated in the figure below. We have several sources connected to a message broker, such as Apache Kafka in this case. Data might be partitioned into \u0026ldquo;topics\u0026rdquo; which are then consumed by different applications, each performing data processing, either independently or in a distributed manner.\nAn advantage of ~timeseries-mock~ would then be to replace the \u0026ldquo;real\u0026rdquo; data sources with a highly configurable simulator either for the prototyping or initial testing phase. If we consider our previous example of the \u0026ldquo;fake\u0026rdquo; HTTP log generation, an application for Web log analytics could be prototyped and tested with simulated log data very quickly, without being blocked by the lack of suitable real data. Since the data is consumed by proxy via the message broker\u0026rsquo;s topics, we could later on replace the simulator with real data sources seamlessly without an impact on any of the applications. To setup the generator (and assuming Kafka and your consumer application are already running on OpenShift) we only need to perform two steps:\nWrite the data specifications in a YAML configuration Use the s2i to deploy the simulator The s2i functionality of OpenShift allows to create deployment ready images by simply pointing to a source code location. In this case we could simply write:\n$ oc new-app centos/python-36-centos7~https://github.com/ruivieira/timeseries-mock \\ -e KAFKA_BROKERS=kafka:9092 \\ -e KAFKA_TOPIC=example \\ -e CONF=examples/mean_continuous.yml \\ --name=emitter In this case, we would deploy a simulator generating data according to the specifications in mean_continuous.yml. This data will be sent to the topic example of a Kafka broker running on port 9092.\nThe stream will be ready to consume and message payload will a stream of serialised JSON strings. In the case of the simulated HTTP log this would be:\n{ name: \u0026#34;HTTP log\u0026#34; values: [\u0026#34;GET\u0026#34;, \u0026#34;/internal/example.htm\u0026#34;, 185, 221, 109, 195] } name - the name given to this stream in the configuration file values - a single observation for this stream After consuming the data it is straight-forward to do any post-processing if needed. For instance, the values above could be easily transformed into a standard Apache Web server log line.\nI hope you found this tool useful, simple to use and configure. Some future work includes adding more observations distributions beyond the exponential family and the ability to directly add transformation rules to the generated observations. If you have any suggestions, use cases (or found an issue!), please let me know in the repository.\nIf you have any comments please let me know on Mastodon (or Twitter).\nHappy coding!\nhttps://www.openshift.com/\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"/containerised-streaming-data-generation-using-state-space-models.html","tags":null,"title":"Containerised Streaming Data Generation using State-Space Models"},{"categories":null,"contents":"Docker is a containerization and manager tool. Docker is a kind of isolated space where an application runs by using system resources. An alternative to Docker is Podman.\nTipsExtract image locallyTo extract a container\u0026rsquo;s image locally (e.g. using Docker) the following can be used:\ndocker export $CONTAINER_NAME \u0026gt; output.tar Alternatively, and using the $IMAGE_ID we can do:\ndocker save $IMAGE_ID$ \u0026gt; output.tar ","permalink":"/containers.html","tags":null,"title":"Containers"},{"categories":null,"contents":"Main documentation is available here.\nSetupRequirementsFor the purpose of these instructions we will assume the following are installed:\nPython 3.9.0 virtualenv A new venv can be created with virtualenv env1 and activated with source venv/bin/activate. Once the environment is active we can install the cookiecutter package using pip install cookiecutter.\nThe create of the cookiecutter project can be done with\n$ cookiecutter https://github.com/drivendata/cookiecutter-data-science For the remainder of this text we will call the of the project you\u0026rsquo;ve just created as $PROJ.\nMore details at Python.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"/cookiecutter-data-science.html","tags":null,"title":"Cookiecutter data science"},{"categories":null,"contents":"SimilarityLet\u0026rsquo;s create two datasets, $\\mu_1$ and $\\mu_2$ such that\n$$ \\mu_i = {x_1,\\dots,x_n} \\sim {\\mathcal{U}_1(-1,1),\\dots,\\mathcal{U}_n(-1,1)} $$\nWe will use $N=100$ observations for a vector sized $n=10$.\nimport numpy as np import pandas as pd import scipy.stats as stats from scipy.spatial.distance import squareform n = 10 N = 100 np.random.seed(0) mu_1 = np.random.normal(loc=0, scale=1, size=(N, n)) mu_2 = np.random.normal(loc=0, scale=1, size=(N, n)) We now add some noise, $\\epsilon=0.6$, to $\\mu_2$ such that\n$$ \\mu_2 = \\epsilon \\mu_2 + (1-\\epsilon)*\\mu_1 $$\nepsilon = 0.6 mu_2 = epsilon*mu_2 + (1-epsilon)*mu_1 We use Pandas to calculate the correlation matrix:\nC1 = pd.DataFrame(mu_1).corr() C2 = pd.DataFrame(mu_2).corr() And we plot the correlation matrices.\nimport seaborn as sns import matplotlib.pyplot as plt f,axes = plt.subplots(1,2, figsize=(10,5)) sns.set_style(\u0026#34;white\u0026#34;) for ix, m in enumerate([C1,C2]): sns.heatmap(m, cmap=\u0026#34;RdBu_r\u0026#34;, center=0, vmin=-1, vmax=1, ax=axes[ix], square=True, cbar_kws={\u0026#34;shrink\u0026#34;: .5}, xticklabels=True) axes[ix].set(title=f\u0026#34;$C_{ix+1}$\u0026#34;) Spearman correlationCalculate similarity using Spearman correlation between the top triangle of the covariance matrices $C_1$ and $C_2$.\nindices = np.triu_indices(C1.shape[0], k=1) print(C1[indices]) ","permalink":"/correlation-matrix.html","tags":null,"title":"Correlation matrix"},{"categories":null,"contents":"Building counterfactually fair modelsDataTo evaluate counterfactual fairness we will be using the \u0026ldquo;law school\u0026rdquo; dataset1.\nThe Law School Admission Council conducted a survey across 163 law schools in the United States. It contains information on 21,790 law students such as their entrance exam scores (LSAT), their grade-point average (GPA) collected prior to law school, and their first year average grade (FYA). Given this data, a school may wish to predict if an applicant will have a high FYA. The school would also like to make sure these predictions are not biased by an individuals race and sex. However, the LSAT, GPA, and FYA scores, may be biased due to social factors.\nWe start by importing the data into a [Pandas]] DataFrame.\nimport warnings warnings.filterwarnings(\u0026#34;ignore\u0026#34;) import pandas as pd df = pd.read_csv(\u0026#34;data/law_data.csv\u0026#34;, index_col=0) df.head() Pre-processingWe now pre-process the data. We start by creating categorical \u0026ldquo;dummy\u0026rdquo; variables according to the race variable.\ndf = pd.get_dummies(df, columns=[\u0026#34;race\u0026#34;], prefix=\u0026#34;\u0026#34;, prefix_sep=\u0026#34;\u0026#34;) df.iloc[:, : 7].head() df.iloc[:, 7 :].head() We also want to expand the sex variable into male / female categorical variables and remove the original.\ndf[\u0026#34;male\u0026#34;] = df[\u0026#34;sex\u0026#34;].map(lambda x: 1 if x == 2 else 0) df[\u0026#34;female\u0026#34;] = df[\u0026#34;sex\u0026#34;].map(lambda x: 1 if x == 1 else 0) df = df.drop(axis=1, columns=[\u0026#34;sex\u0026#34;]) df.iloc[:, 0:7].head() df.iloc[:, 7:].head() We will also convert the entrance exam scores (LSAT) to a discrete variable.\ndf[\u0026#34;LSAT\u0026#34;] = df[\u0026#34;LSAT\u0026#34;].astype(int) df.iloc[:, :6].head() df.iloc[:, 6:].head() Protected attributesCounterfactual fairness enforces that a distribution over possible predictions for an individual should remain unchanged in a world where an individuals protected attributes $A$ had been different in a causal sense. Let\u0026rsquo;s start by defining the /protected attributes/. Obvious candidates are the different categorical variables for ethnicity (Asian, White, Black, etc) and gender (male, female).\nA = [ \u0026#34;Amerindian\u0026#34;, \u0026#34;Asian\u0026#34;, \u0026#34;Black\u0026#34;, \u0026#34;Hispanic\u0026#34;, \u0026#34;Mexican\u0026#34;, \u0026#34;Other\u0026#34;, \u0026#34;Puertorican\u0026#34;, \u0026#34;White\u0026#34;, \u0026#34;male\u0026#34;, \u0026#34;female\u0026#34;, ] Training and testing subsetsWe will now divide the dataset into training and testing subsets. We will use the same ratio as in 2, that is 20%.\nfrom sklearn.model_selection import train_test_split df_train, df_test = train_test_split(df, random_state=23, test_size=0.2); ModelsUnfair modelAs detailed in 2, the concept of counterfactual fairness holds under three levels of assumptions of increasing strength.\nThe first of such levels is Level 1, where $\\hat{Y}$ is built using only the observable non-descendants of $A$. This only requires partial causal ordering and no further causal assumptions, but in many problems there will be few, if any, observables which are not descendants of protected demographic factors.\nFor this dataset, since LSAT, GPA, and FYA are all biased by ethnicity and gender, we cannot use any observed features to construct a Level 1 counterfactually fair predictor as described in Level 1.\nInstead (and in order to compare the performance with Level 2 and 3 models) we will build two unfair baselines.\nA Full model, which will be trained with the totality of the variables An Unaware model (FTU), which will be trained will all the variables, except the protected attributes $A$. Let\u0026rsquo;s proceed with calculating the Full model.\nFull modelAs mentioned previously, the full model will be a simple linear regression in order to predict ZFYA using all of the variables.\nfrom sklearn.linear_model import LinearRegression linreg_unfair = LinearRegression() The inputs will then be the totality of the variabes (protected variables $A$, as well as UGPA and LSAT).\nimport numpy as np X = np.hstack( ( df_train[A], np.array(df_train[\u0026#34;UGPA\u0026#34;]).reshape(-1, 1), np.array(df_train[\u0026#34;LSAT\u0026#34;]).reshape(-1, 1), ) ) print(X) As for our target, we are trying to predict ~ZFYA~ (first year average grade).\ny = df_train[\u0026#34;ZFYA\u0026#34;] y[:10] We fit the model:\nlinreg_unfair = linreg_unfair.fit(X, y) And perform some predictions on the test subset.\nX_test = np.hstack( ( df_test[A], np.array(df_test[\u0026#34;UGPA\u0026#34;]).reshape(-1, 1), np.array(df_test[\u0026#34;LSAT\u0026#34;]).reshape(-1, 1), ) ) X_test predictions_unfair = linreg_unfair.predict(X_test) predictions_unfair We will also calculate the /unfair model/ score for future use.\nscore_unfair = linreg_unfair.score(X_test, df_test[\u0026#34;ZFYA\u0026#34;]) print(score_unfair) from sklearn.metrics import mean_squared_error RMSE_unfair = np.sqrt(mean_squared_error(df_test[\u0026#34;ZFYA\u0026#34;], predictions_unfair)) print(RMSE_unfair) Fairness through unawareness (FTU)As also mentioned in 2, the second baseline we will use is an Unaware model (FTU), which will be trained will all the variables, except the protected attributes $A$.\nlinreg_ftu = LinearRegression() We will create the inputs as previously, but without using the protected attributes, $A$.\nX_ftu = np.hstack( ( np.array(df_train[\u0026#34;UGPA\u0026#34;]).reshape(-1, 1), np.array(df_train[\u0026#34;LSAT\u0026#34;]).reshape(-1, 1), ) ) X_ftu And we fit the model:\nlinreg_ftu = linreg_ftu.fit(X_ftu, y) Again, let\u0026rsquo;s perform some predictions on the test subset.\nX_ftu_test = np.hstack( (np.array(df_test[\u0026#34;UGPA\u0026#34;]).reshape(-1, 1), np.array(df_test[\u0026#34;LSAT\u0026#34;]).reshape(-1, 1)) ) X_ftu_test predictions_ftu = linreg_ftu.predict(X_ftu_test) predictions_ftu As previously, let\u0026rsquo;s calculate this model\u0026rsquo;s score.\nftu_score = linreg_ftu.score(X_ftu_test, df_test[\u0026#34;ZFYA\u0026#34;]) print(ftu_score) RMSE_ftu = np.sqrt(mean_squared_error(df_test[\u0026#34;ZFYA\u0026#34;], predictions_ftu)) print(RMSE_ftu) Latent variable modelStill according to 2, a Level 2 approach will model latent fair variables which are parents of observed variables.\nIf we consider a predictor parameterised by $\\theta$, such as:\n$$ \\hat{Y} \\equiv g_\\theta (U, X_{\\nsucc A}) $$\nwith $X_{\\nsucc A} \\subseteq X$ are non-descendants of $A$. Assuming a loss function $l(.,.)$ and training data $\\mathcal{D}\\equiv{(A^{(i), X^{(i)}, Y^{(i)}})}$, for $i=1,2\\dots,n$, the empirical loss is defined as\n$$ L(\\theta)\\equiv \\sum_{i=1}^n \\mathbb{E}[l(y^{(i)},g_\\theta(U^{(i)}, x^{(i)}_{\\nsucc A}))]/n $$\nwhich has to be minimised in order to $\\theta$. Each $n$ expectation is with respect to random variable $U^{(i)}$ such that\n$$ U^{(i)}\\sim P_{\\mathcal{M}}(U|x^{(i)}, a^{(i)}) $$\nwhere $P_{\\mathcal{M}}(U|x,a)$ is the conditional distribution of the background variables as given by a causal model $M$ that is available by assumption.\nIf this expectation cannot be calculated analytically, Markov chain Monte Carlo (MCMC) can be used to approximate it as in the following algorithm.\nWe will follow the model specified in the original paper, where the latent variable considered is $K$, which represents a student\u0026rsquo;s knowledge. $K$ will affect GPA, LSAT and the outcome, FYA. The model can be defined by:\n$$ \\begin{aligned} GPA \u0026amp;\\sim \\mathcal{N}(GPA_0 + w_{GPA}^KK + w_{GPA}^RR + w_{GPA}^SS, \\sigma_{GPA}) \\ LSAT \u0026amp;\\sim \\text{Po}(\\exp(LSAT_0 + w_{LSAT}^KK + w_{LSAT}^RR + w_L^SS)) \\ FYA \u0026amp;\\sim \\mathcal{N}(w_{FYA}^KK + w_{FYA}^RR + w_{FYA}^SS, 1) \\ K \u0026amp;\\sim \\mathcal{N}(0,1) \\end{aligned} $$\nThe priors used will be:\n$$ \\begin{aligned} GPA_0 \u0026amp;\\sim \\mathcal{N}(0, 1) \\ LSAT_0 \u0026amp;\\sim \\mathcal{N}(0, 1) \\ GPA_0 \u0026amp;\\sim \\mathcal{N}(0, 1) \\end{aligned} $$\nimport pymc3 as pm K = len(A) def MCMC(data, samples=1000): N = len(data) a = np.array(data[A]) model = pm.Model() with model: # Priors k = pm.Normal(\u0026#34;k\u0026#34;, mu=0, sigma=1, shape=(1, N)) gpa0 = pm.Normal(\u0026#34;gpa0\u0026#34;, mu=0, sigma=1) lsat0 = pm.Normal(\u0026#34;lsat0\u0026#34;, mu=0, sigma=1) w_k_gpa = pm.Normal(\u0026#34;w_k_gpa\u0026#34;, mu=0, sigma=1) w_k_lsat = pm.Normal(\u0026#34;w_k_lsat\u0026#34;, mu=0, sigma=1) w_k_zfya = pm.Normal(\u0026#34;w_k_zfya\u0026#34;, mu=0, sigma=1) w_a_gpa = pm.Normal(\u0026#34;w_a_gpa\u0026#34;, mu=np.zeros(K), sigma=np.ones(K), shape=K) w_a_lsat = pm.Normal(\u0026#34;w_a_lsat\u0026#34;, mu=np.zeros(K), sigma=np.ones(K), shape=K) w_a_zfya = pm.Normal(\u0026#34;w_a_zfya\u0026#34;, mu=np.zeros(K), sigma=np.ones(K), shape=K) sigma_gpa_2 = pm.InverseGamma(\u0026#34;sigma_gpa_2\u0026#34;, alpha=1, beta=1) mu = gpa0 + (w_k_gpa * k) + pm.math.dot(a, w_a_gpa) # Observed data gpa = pm.Normal( \u0026#34;gpa\u0026#34;, mu=mu, sigma=pm.math.sqrt(sigma_gpa_2), observed=list(data[\u0026#34;UGPA\u0026#34;]), shape=(1, N), ) lsat = pm.Poisson( \u0026#34;lsat\u0026#34;, pm.math.exp(lsat0 + w_k_lsat * k + pm.math.dot(a, w_a_lsat)), observed=list(data[\u0026#34;LSAT\u0026#34;]), shape=(1, N), ) zfya = pm.Normal( \u0026#34;zfya\u0026#34;, mu=w_k_zfya * k + pm.math.dot(a, w_a_zfya), sigma=1, observed=list(data[\u0026#34;ZFYA\u0026#34;]), shape=(1, N), ) step = pm.Metropolis() trace = pm.sample(samples, step, progressbar = False) return trace train_estimates = MCMC(df_train) Let\u0026rsquo;s plot a single trace for $k^{(i)}$.\nimport matplotlib.pyplot as plt import seaborn as sns from plotutils import * # Thin the samples before plotting k_trace = train_estimates[\u0026#34;k\u0026#34;][:, 0].reshape(-1, 1)[0::100] plt.subplot(1, 2, 1) plt.hist(k_trace, color=colours[0], bins=100) plt.subplot(1, 2, 2) plt.scatter(range(len(k_trace)), k_trace, s=1, c=colours[0]) plt.show() train_k = np.mean(train_estimates[\u0026#34;k\u0026#34;], axis=0).reshape(-1, 1) train_k We can now estimate $k$ using the test data:\ntest_map_estimates = MCMC(df_test) test_k = np.mean(test_map_estimates[\u0026#34;k\u0026#34;], axis=0).reshape(-1, 1) test_k We now build the Level 2 predictor, using $k$ as the input.\nlinreg_latent = LinearRegression() linreg_latent = linreg_latent.fit(train_k, df_train[\u0026#34;ZFYA\u0026#34;]) predictions_latent = linreg_latent.predict(test_k) predictions_latent latent_score = linreg_latent.score(test_k, df_test[\u0026#34;ZFYA\u0026#34;]) print(latent_score) RMSE_latent = np.sqrt(mean_squared_error(df_test[\u0026#34;ZFYA\u0026#34;], predictions_latent)) print(RMSE_latent) Additive error modelFinally, in Level 3, we model GPA, LSAT, and FYA as continuous variables with additive error terms independent of race and sex3.\nThis corresponds to\n$$ \\begin{aligned} GPA \u0026amp;= b_G + w^R_{GPA}R + w^S_{GPA}S + \\epsilon_{GPA}, \\epsilon_{GPA} \\sim p(\\epsilon_{GPA}) \\ LSAT \u0026amp;= b_L + w^R_{LSAT}R + w^S_{LSAT}S + \\epsilon_{LSAT}, \\epsilon_{LSAT} \\sim p(\\epsilon_{LSAT}) \\ FYA \u0026amp;= b_{FYA} + w^R_{FYA}R + w^S_{FYA}S + \\epsilon_{FYA} , \\epsilon_{FYA} \\sim p(\\epsilon_{FYA}) \\end{aligned} $$\nWe estimate the error terms $\\epsilon_{GPA}, \\epsilon_{LSAT}$ by first fitting two models that each use race and sex to individually predict GPA and LSAT. We then compute the residuals of each model (e.g., $\\epsilon_{GPA} =GPA\\hat{Y}{GPA}(R, S)$). We use these residual estimates of $\\epsilon{GPA}, \\epsilon_{LSAT}$ to predict $FYA$. In 2 this is called Fair Add.\nSince the process is similar for the individual predictions for GPA and LSAT, we will write a method to avoid repetion.\ndef calculate_epsilon(data, var_name, protected_attr): X = data[protected_attr] y = data[var_name] linreg = LinearRegression() linreg = linreg.fit(X, y) predictions = linreg.predict(X) return data[var_name] - predictions Let\u0026rsquo;s apply it to each variable, individually. First we calculate $\\epsilon_{GPA}$:\nepsilons_gpa = calculate_epsilon(df, \u0026#34;UGPA\u0026#34;, A) epsilons_gpa Next, we calculate $\\epsilon_{LSAT}$:\nepsilons_LSAT = calculate_epsilon(df, \u0026#34;LSAT\u0026#34;, A) epsilons_LSAT Let\u0026rsquo;s visualise the $\\epsilon$ distribution quickly:\nimport matplotlib.pyplot as plt import seaborn as sns plt.subplot(1, 2, 1) plt.hist(epsilons_gpa, color=colours[0], bins=100) plt.title(\u0026#34;$\\epsilon_{GPA}$\u0026#34;) plt.xlabel(\u0026#34;$\\epsilon_{GPA}$\u0026#34;) plt.subplot(1, 2, 2) plt.hist(epsilons_LSAT, color=colours[1], bins=100) plt.title(\u0026#34;$\\epsilon_{LSAT}$\u0026#34;) plt.xlabel(\u0026#34;$\\epsilon_{LSAT}$\u0026#34;) plt.show() We finally use the calculated $\\epsilon$ to train a model in order to predict FYA. We start by getting the subset of the $\\epsilon$ which match the training indices.\nX = np.hstack( ( np.array(epsilons_gpa[df_train.index]).reshape(-1, 1), np.array(epsilons_LSAT[df_train.index]).reshape(-1, 1), ) ) X linreg_fair_add = LinearRegression() linreg_fair_add = linreg_fair_add.fit( X, df_train[\u0026#34;ZFYA\u0026#34;], ) We now use this model to calculate the predictions\nX_test = np.hstack( ( np.array(epsilons_gpa[df_test.index]).reshape(-1, 1), np.array(epsilons_LSAT[df_test.index]).reshape(-1, 1), ) ) predictions_fair_add = linreg_fair_add.predict(X_test) predictions_fair_add And as previously, we calculate the model\u0026rsquo;s score:\nfair_add_score = linreg_fair_add.score(X_test, df_test[\u0026#34;ZFYA\u0026#34;]) print(fair_add_score) RMSE_fair_add = np.sqrt(mean_squared_error(df_test[\u0026#34;ZFYA\u0026#34;], predictions_fair_add)) print(RMSE_fair_add) ComparisonThe scores, so far, are:\nprint(f\u0026#34;Unfair score:\\t{score_unfair}\u0026#34;) print(f\u0026#34;FTU score:\\t{ftu_score}\u0026#34;) print(f\u0026#34;L2 score:\\t{latent_score}\u0026#34;) print(f\u0026#34;Fair add score:\\t{fair_add_score}\u0026#34;) print(f\u0026#34;Unfair RMSE:\\t{RMSE_unfair}\u0026#34;) print(f\u0026#34;FTU RMSE:\\t{RMSE_ftu}\u0026#34;) print(f\u0026#34;L2 RMSE:\\t{RMSE_latent}\u0026#34;) print(f\u0026#34;Fair add RMSE:\\t{RMSE_fair_add}\u0026#34;) Measuring counterfactual fairnessFirst, we will measure two quantities, the Statistical Parity Difference (SPD)4 and Disparate impact (DI)5.\nStatistical Parity Difference / Disparate Impactfrom fairlearn.metrics import demographic_parity_difference, demographic_parity_ratio\nparities = [] impacts = [] for a in A: parity = demographic_parity_difference(df_train[\u0026#34;ZFYA\u0026#34;], df_train[\u0026#34;ZFYA\u0026#34;], sensitive_features = df_train[a]) di = demographic_parity_ratio(df_train[\u0026#34;ZFYA\u0026#34;], df_train[\u0026#34;ZFYA\u0026#34;], sensitive_features = df_train[a]) parities.append(parity) impacts.append(di) df_parities = pd.DataFrame({\u0026#39;protected\u0026#39;:A,\u0026#39;parity\u0026#39;:parities,\u0026#39;impact\u0026#39;:impacts}) import matplotlib.pyplot as plt from plotutils import * fig = plt.figure() ax = fig.add_subplot(111) ax2 = ax.twinx() fig.suptitle(\u0026#39;Statistical Parity Difference and Disparate Impact\u0026#39;) width = 0.4 df_parities.plot(x =\u0026#39;protected\u0026#39;, y = \u0026#39;parity\u0026#39;, kind = \u0026#39;bar\u0026#39;, ax = ax, width = width, position=1, color=colours[0], legend=False) df_parities.plot(x =\u0026#39;protected\u0026#39;, y = \u0026#39;impact\u0026#39;, kind = \u0026#39;bar\u0026#39;, ax = ax2, width = width, position = 0, color = colours[1], legend = False) ax.axhline(y = 0.1, linestyle = \u0026#39;dashed\u0026#39;, alpha = 0.7, color = colours[0]) ax2.axhline(y = 0.55, linestyle = \u0026#39;dashed\u0026#39;, alpha = 0.7, color = colours[1]) patches, labels = ax.get_legend_handles_labels() ax.legend(patches, [\u0026#39;Stat Parity Diff\u0026#39;], loc = \u0026#39;upper left\u0026#39;) patches, labels = ax2.get_legend_handles_labels() ax2.legend(patches, [\u0026#39;Disparate Impact\u0026#39;], loc = \u0026#39;upper right\u0026#39;) labels = [item.get_text() for item in ax.get_xticklabels()] for i in range(len(A)): labels[i] = A[i] ax.set_xticklabels(labels) ax.set_xlabel(\u0026#39;Protected Features\u0026#39;) ax.set_ylabel(\u0026#39;Statistical Parity Difference\u0026#39;) ax2.set_ylabel(\u0026#39;Disparate Impact\u0026#39;) plt.show() Finding sensitive featuresTypically a $SPD \u0026gt; 0.1$ and a $DI \u0026lt; 0.9$ might indicate discrimination on those features. All protected attributes fail the SPD test and, in our dataset, we have two features (~Hispanic~ and ~Mexican~) which clearly fail the DI test.\nfor a in [\u0026#34;Mexican\u0026#34;, \u0026#34;Hispanic\u0026#34;]: spd = demographic_parity_difference(y_true=df_train[\u0026#34;ZFYA\u0026#34;], y_pred=df_train[\u0026#34;ZFYA\u0026#34;], sensitive_features = df_train[a]) print(f\u0026#34;SPD({a}) = {spd}\u0026#34;) di = demographic_parity_ratio(y_true=df_train[\u0026#34;ZFYA\u0026#34;], y_pred=df_train[\u0026#34;ZFYA\u0026#34;], sensitive_features = df_train[a]) print(f\u0026#34;DI({a}) = {di}\u0026#34;) McIntyre, Frank, and Michael Simkovic. \u0026ldquo;Are law degrees as valuable to minorities?.\u0026rdquo; International Review of Law and Economics 53 (2018): 23-37.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nKusner, Matt J., Joshua Loftus, Chris Russell, and Ricardo Silva. \u0026ldquo;Counterfactual fairness.\u0026rdquo; In Advances in neural information processing systems, pp. 4066-4076. 2017.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\u0026#160;\u0026#x21a9;\u0026#xfe0e;\u0026#160;\u0026#x21a9;\u0026#xfe0e;\u0026#160;\u0026#x21a9;\u0026#xfe0e;\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nThat may in turn be correlated with one-another.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nSee {ref}fairness:demographic-parity-difference.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nSee {ref}fairness:disparate-impact.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"/counterfactual-fairness.html","tags":null,"title":"Counterfactual Fairness"},{"categories":null,"contents":"Here we will look at how to build a counterfactually fair model, as detailed in Counterfactual Fairness, specifically the \u0026ldquo;Fair Add\u0026rdquo; model.\nThis implementation will rely mostly on Apache Commons Math1 linear regression implementations, namely the Ordinary Least Squares (OLS) regression2. We start then by adding the relevant Maven dependencies:\n\u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.apache.commons\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;commons-math3\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;3.6.1\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt; Data will be passed as a RealMatrix3. This matrix will have dimensions $N\\times f$, where $N$ is the number of observations and $f$ is the number of features.\nWe can instatiate the model using\n// RealMatrix data = ... final CounterfactuallyFairModel model = new CounterfactuallyFairModel(data); We will then need the following information:\nThe protected attributes indices, protectedIndices The variable indices, variableIndices The target variable index, targetIndex Assuming that we have the same variables as in the counterfactual fairness example, let\u0026rsquo;s say that the protected attributes have in the data matrix, column numbers 5, 6, 7, 8, 9, 10, 11, 12, 13, 14 and the model variables (LSAT and UGPA) have indices 1, 0 and the target (ZFYA) has index 2. We then calculate the counterfactually fair model using:\nmodel.calculate(new int[]{5, 6, 7, 8, 9, 10, 11, 12, 13, 14}, new int[]{1, 0}, 2); The calculate method performs the following:\npublic void calculate(int[] protectedIndices, int[] variableIndices, int targetIndex) { final RealMatrix residuals = new Array2DRowRealMatrix(this.data.getRowDimension(), variableIndices.length); for (int i = 0; i \u0026lt; variableIndices.length; i++) { final int index = variableIndices[i]; final RealVector varResidual = this.calculateEpsilon(protectedIndices, index); residuals.setColumn(i, varResidual.toArray()); } // predict target from residuals final OLSMultipleLinearRegression regression = new OLSMultipleLinearRegression(); regression.newSampleData(this.data.getColumn(targetIndex), residuals.getData()); } As in counterfactual Fairness , we calculate a regression model to predict each of the variable (LSAT and UGPA) using the protected variables. The resulting residuals, $\\epsilon_{LSAT}$ and $\\epsilon_{UGPA}$ will in turn be used to calculate another regression model in order to predict the target variable ZFYA.\nThe residuals are calculated using the calculateEpsilon method, which consists of:\npublic RealVector calculateEpsilon(int[] protectedIndices, int targetIndex) { int[] protectedRows = new int[this.data.getRowDimension()]; Arrays.setAll(protectedRows, i -\u0026gt; i); final RealMatrix _x = this.data.getSubMatrix(protectedRows, protectedIndices); final RealVector _y = this.data.getSubMatrix(protectedRows, new int[]{targetIndex}).getColumnVector(0); final OLSMultipleLinearRegression regression = new OLSMultipleLinearRegression(); regression.newSampleData(_y.toArray(), _x.getData()); return new ArrayRealVector(regression.estimateResiduals()); } Which simply calculates a regression model for the variables using the protected attributes and returning a RealVector with the residual $\\epsilon$.\nhttps://commons.apache.org/proper/commons-math/.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nhttps://commons.apache.org/proper/commons-math/javadocs/api-3.6/org/apache/commons/math3/stat/regression/OLSMultipleLinearRegression.html\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nhttps://commons.apache.org/proper/commons-math/javadocs/api-3.6/org/apache/commons/math3/linear/RealMatrix.html\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"/counterfactual-fairness-in-java.html","tags":null,"title":"Counterfactual Fairness in Java"},{"categories":null,"contents":"A special type of Explainability.\nDesiderataAccording to Verma et al 1 the counterfactual desiderata is:\nValidity Actionability Sparsity Data manifold closeness Causality Amortised inference ValidityWe assume that a counterfactual is valid if it solves the optimisation as states in Wachter et al2. If we defined the loss function as\n$$ L(x,x^{\\prime},y^{\\prime},\\lambda)=\\lambda\\times(\\hat{f}(x^{\\prime})y^{\\prime})^2+d(x,x^{\\prime}), $$\nwe can define the counterfactual as\n$$ \\arg \\underset{x^{\\prime}}{\\min}\\underset{\\lambda}{\\max} \\lambda\\times(\\hat{f}(x^{\\prime})y^{\\prime})^2+d(x,x^{\\prime}) $$\nwhere:\n$x \\in \\mathcal{X}$ is the original data point $x^{\\prime} \\in \\mathcal{X}$ is the counterfactual $y^{\\prime} \\in \\mathcal{Y}$ is the desired label $d$ is a distance metric to measure the distance between $x$ and $x^{\\prime}$. this could be a L1 or L2 distance, a quadratic distance, etc. ActionabilityStill according to 2, actionability refers to the ability of a counterfactual method to separate between mutable and immutable features. Immutable, and additionally legally protected features, shouldn\u0026rsquo;t be changed by a counterfactual implementation. Formally, if we defined our set of mutable (or actionable) features as $\\mathcal{A}$, we have\n$$ \\arg \\underset{x^{\\prime} \\in \\mathcal{A}}{\\min}\\underset{\\lambda}{\\max} \\lambda\\times(\\hat{f}(x^{\\prime})y^{\\prime})^2+d(x,x^{\\prime}) $$\nSparsityAccording to 2 Shorter counterfactuals are easier to understand and an effective counterfactual implementation should change the least amount of features as possible. If a sparsity penalty term is added to our definition\n$$ g(x^{\\prime}-x) $$\nwhich increases the more features are changed and could be a L0 or [Distance metrics#Manhattan distance L1|L1]] metric, for instance. We can then define the counterfactual as\n$$ \\arg \\underset{x^{\\prime} \\in \\mathcal{A}}{\\min}\\underset{\\lambda}{\\max} \\lambda\\times(\\hat{f}(x^{\\prime})y^{\\prime})^2+d(x,x^{\\prime})+g(x^{\\prime}-x) $$\nData manifold closenessStill according to 2, data manifold closeness is the property which guarantees that the counterfactual will be as close to the training data as possible. This can translate into a more \u0026ldquo;realistic\u0026rdquo; counterfactual, since it is possible that the counterfactual would take extreme or never seen before values in order to satisfy the previous conditions. Formally, we can write a penalty term for the adherence to the training data manifold, $\\mathcal{X}$ as $l(x^{\\prime};\\mathcal{X})$ and the define the counterfactual as\n$$ \\arg \\underset{x^{\\prime} \\in \\mathcal{A}}{\\min}\\underset{\\lambda}{\\max} \\lambda\\times(\\hat{f}(x^{\\prime})y^{\\prime})^2+d(x,x^{\\prime})+g(x^{\\prime}-x)+l(x^{\\prime};\\mathcal{X}) $$\nCausalityCausality refers to the property where feature changes will impact dependent features. That is, we no longer assume that all features are independent. This implies that the counterfactual method needs to mantain the causal relations between features.\nAmortised inferenceAmortised inference refers to the property of a counterfactual search to provide multiple counterfactuals for a single data point.\nAlternative methodsConstraint solversAn alternative method to find counterfactuals is to use constraint solvers. This is explored more in-depth in Counterfactuals with Constraint Solvers.\nResources FACE: Feasible and Actionable Counterfactual Explanations Verma, Sahil, John Dickerson, and Keegan Hines. \u0026ldquo;Counterfactual Explanations for Machine Learning: A Review.\u0026rdquo; arXiv preprint arXiv:2010.10596 (2020).\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nWachter, Sandra, Brent Mittelstadt, and Chris Russell. \u0026ldquo;Counterfactual explanations without opening the black box: Automated decisions and the GDPR.\u0026rdquo; Harv. JL \u0026amp; Tech. 31 (2017): 841.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\u0026#160;\u0026#x21a9;\u0026#xfe0e;\u0026#160;\u0026#x21a9;\u0026#xfe0e;\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"/counterfactuals.html","tags":null,"title":"Counterfactuals"},{"categories":null,"contents":"ScoringAn implementation on how to calculate counterfactuals with Constraint Solvers (namely OptaPlanner) is available here.\nThis implementation satisfies several criteria of the counterfactuals.\nThe penalisation score is represented with a BendableBigDecimalScore 1, having three \u0026ldquo;hard\u0026rdquo; levels and two \u0026ldquo;soft\u0026rdquo; levels.\nThe first hard level component, 1, penalises the score according to the distance between the prediction, $y^{\\prime}$ for the currently proposed solution, $x^{\\prime}$ and the original prediction $y$, that is this our $(\\hat{f}(x^{\\prime})-y^{\\prime})^2$. This corresponds to the counterfactual\u0026rsquo;s validity.\nThe actionability is score with 2. This component penalises the score according to number of immutable features which were changed in the counterfactual.\nA confidence score component, 3 is use to, optionally, impose a minimum confidence threshold to the counterfactual\u0026rsquo;s associated prediction, $x^{\\prime}$.\nFinally, the feature distance, 4, penalises the score according to the feature distance. This is the representation of\n$$ d(x, x^{\\prime}). $$\nIn the concrete implementation linked above, the distance, $d$, chosen is a Manhattan (or $L^1$) distance calculated feature-wise. This also corresponds to the validity property.\nImplementation Entities are defined by classes such as Integer, Categorical, Boolean or Float, as shown in 5. Each of the features, shown in 6, is created as an instance of one of these entities. For instance, feature1 would be of type Integer and feature2 would be of type Categorical, etc.\nThe original data point, $x$ is represented by this set of features (6).\nA planning solution (PlanningSolution), illustrated in 7 will produce candidate solutions (shown in 8)\nFor each solution, we propose a new set of features ($x^{\\prime}$) as a counterfactual candidate. For instance, ~solution A~ in 8.\nIn the following section we will look at how each component is calculated. We will refer to each \u0026ldquo;hard\u0026rdquo; level component as $H_1, H_2$ and $H_3$ and the \u0026ldquo;soft\u0026rdquo; component as $S_1$. The overal score consists, then, of $S={H_1, H_2, H_3, S_1 }$\nPrediction distanceThe first component of the score, 1 is established by sending the proposed counterfactual $x^{\\prime}$, 8 to a predictive model, 9 and calculating the distance between the desired outcome, $y^{\\prime}$ and the model\u0026rsquo;s prediction. This is done component wise, for each feature of the output. That is, for a prediction with $N$ features, we calculate\n$$ H_1=\\left(\\sum_i^Nf(x^{\\prime}_i) - y^{\\prime}_i\\right)^2 $$\nToleranceFor numerical features, the above score ($H_1$) will cause the counterfactual to be invalid, unless the distance between the outcomes and proposed values is exactly zero.\nWe can solve this problem by introducing a \u0026ldquo;tolerance\u0026rdquo; adjustment, which allows proposed values to be accepted if they are \u0026ldquo;close enough\u0026rdquo; to the goal.\nTo make the tolerance scale-invariant and unit-less we can use a relative change and set the distance to zero, if smaller than the threshold $t$, that is\n$$ d = \\begin{cases} 0,\u0026amp;\\qquad\\text{if},\\frac{\\vert f(x\u0026rsquo;_i)-y\u0026rsquo;_i\\vert}{\\max(\\vert f(x\u0026rsquo;_i)\\vert,\\vert y\u0026rsquo;_i\\vert)} \u0026lt; t \\\\ \\vert f(x\u0026rsquo;_i)-y\u0026rsquo;_i\\vert,\u0026amp;\\qquad\\text{otherwise} \\end{cases} $$\nand compare to the threshold $t$. As an example, for a goal $y\u0026rsquo;_i=3$ and a threshold of $t=0.01$, around the goal we would have the distance as in the figure below:\nThis would however fail for the edge case where $y^{\\prime}_i=0$ as we can see below:\nTo solve this, we can introduce a special case for $y^{\\prime}_i=0$, such that:\n$$ d_{g=0} = \\begin{cases} 0,\u0026amp;\\qquad \\text{if}\\ |f(x^{\\prime}_i)|\u0026lt; t\\\\ \\lVert f(x^{\\prime}_i) - y^{\\prime}_i \\rVert,\u0026amp;\\qquad\\text{otherwise} \\end{cases} $$\nSo that we have now the desired behaviour at $y^{\\prime}_i=0$:\nGower distanceAn alternative metric for the outcome distance (and mixed variables in general) is the site/Machine learning/Gower distance.\nActionability scoreFor the second component, the actionability score, 2. We calculate the number of features for the protected set $\\mathcal{A}$, which have a different value from the original. That is, assuming we have a certain number of protectd features $M$, such that $\\mathcal{A}={A_1,A_2,\\dots,A_M}$, we calculate:\n$$ H_2 = \\sum_{a \\in \\mathcal{A}} \\mathbb{1}(x_a \\neq x^{\\prime}_a), $$\nConfidence scoreFor each feature $i$, if we have a prediction confidence, $p_i(f(x^{\\prime}))$, we calculate the number of features which have a confidence below a certain predefined threshold, $P_i$. If the threshold is not defined, this component will always be zero and not influence the counterfactual selection. Assuming we have defined a threshold for all $N$ features, $P = {P_1, P_2, \\dots, P_N}$ we calculate this score as\n$$ H_3 = \\sum_i^N \\mathbb{1} \\left( p_i \\left( f(x^{\\prime}) \u0026lt; P_i \\right) \\right) $$\nFeature distanceConsidering that each datapoint $x$ consists of different $N$ features, such that $x=\\left(f_1,\\dots,f_n\\right)$ and that each feature might be numerical or categorical2, we calculate the distance between a datapoint $x$ and a potential counterfactual $x^{\\prime}$:\n$$ d\\left(x,x^{\\prime}\\right)=\\sum_{i=1}^Nd^{\\prime}\\left(x_i,x_i^{\\prime}\\right) $$ $$ d^{\\prime}\\left(x_i,x_i^{\\prime}\\right)= \\begin{cases} \\left(x_i-x_i^{\\prime}\\right)^2,\\quad\\text{if}\\ x_i,x_i^{\\prime}\\in\\mathbb{N} \\lor x_i,x_i^{\\prime}\\in\\mathbb{R}\\ 1-\\delta_{x,x^{\\prime}},\\quad\\text{if}\\ x_i,x_i^{\\prime}\\ \\text{categorical} \\end{cases} $$\nSince in many scenarios we might not have access to the training data, the above distance are not normalised. In the event that we do have access to training data, then we can use the standard deviation ($SD$) to normalise the features. The $SD$ can be calculated as:\n$$ SD=\\sqrt{\\frac{1}{N}\\sum_{i=1}^N\\left(x_i-\\bar{x}\\right)^2} $$\nso that, in this case, we scale the numerical features with\n$$ \\bar{d}^{\\prime}\\left(x_i,x_i^{\\prime}\\right)= \\frac{\\left(x_i-x_i^{\\prime}\\right)^2}{SD}. $$\nSearchingTo search for a counterfactual, we start by specifying a search domain for each feature. This will include:\nAn upper and lower bounds for numerical features, respectively $\\mathcal{D}_l, \\mathcal{D}_u$ A set of categories for categorical features, $\\mathcal{C}$ $\\mathcal{B}={0,1}$ for the specific case of boolean/binary values Typically these values would be either established by someone with domain knowledge, or by values that might reflect our expectation for the actual counterfactual (for instance, an ~age~ would have realistic values).\nThe algorithm used for the search is Tabu search3 (Glover, 1989).\nBendableBigDecimalScore documentation\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nHere we are considering binary and boolean values as categorical.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\ncite:glover1989tabu Glover, F. (1989). Tabu searchpart i. ORSA Journal on computing, 1(3), 190206.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"/counterfactuals-with-constraint-solvers.html","tags":null,"title":"Counterfactuals with Constraint Solvers"},{"categories":null,"contents":"Deno1 is a Typescript and Javascript runtime.\nInstallationFedoraTo install Deno on Fedora, first download the installation file:\ncurl -fsSL https://deno.land/x/install/install.sh | sh And then add the following to your shell\u0026rsquo;s profile (e.g. ~/.bashrc):\nexport DENO_INSTALL=\u0026#34;/home/$USER/.deno\u0026#34; export PATH=\u0026#34;$DENO_INSTALL/bin:$PATH\u0026#34; Topis Deno types https://deno.land/\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"/deno.html","tags":null,"title":"Deno"},{"categories":null,"contents":"Union typesfunction add(a: any, b: any) { if (typeof a === \u0026#39;number\u0026#39; \u0026amp;\u0026amp; typeof b === \u0026#39;number\u0026#39;) { return a + b; } if (typeof a === \u0026#39;string\u0026#39; \u0026amp;\u0026amp; typeof b === \u0026#39;string\u0026#39;) { return a.concat(b); } throw new Error(\u0026#39;Parameters must be numbers or strings\u0026#39;); } return add(true, false); function add(a: number | string, b: number | string) { if (typeof a === \u0026#39;number\u0026#39; \u0026amp;\u0026amp; typeof b === \u0026#39;number\u0026#39;) { return a + b; } if (typeof a === \u0026#39;string\u0026#39; \u0026amp;\u0026amp; typeof b === \u0026#39;string\u0026#39;) { return a.concat(b); } throw new Error(\u0026#39;Parameters must be numbers or strings\u0026#39;); } return add(\u0026#39;a\u0026#39;, \u0026#39;b\u0026#39;); ","permalink":"/deno-types.html","tags":null,"title":"Deno types"},{"categories":null,"contents":" Digital gardens are places where information grows. It is a collection of notes and ideas that are organized in a way that allows for exploration and discovery. It is meant to be a living, growing repository of knowledge and ideas, rather than a static document.\nChallengesA digital garden presents a few challenges.\nChronological is the wrong metaphor, but how to capture time and sequence?. A digital garden can contain a large amount of information, and it can be difficult to decide how to organize and structure it in a way that is intuitive and easy for others to navigate.\nFinding the right tools: There are many different tools and platforms that can be used to create a digital garden, and it can be challenging to find the one that best fits your needs and workflow.\nKeeping the garden up to date: A digital garden is meant to be a living, growing repository of knowledge and ideas, which means that it requires ongoing maintenance and updates. It can be time-consuming to keep the garden up to date and to ensure that the information it contains is accurate and current.\nBalancing depth and breadth: A digital garden can contain a wide range of topics and ideas, and it can be challenging to find the right balance between providing enough detail to be useful and not overwhelming the reader with too much information.\nPromoting and sharing the garden: A digital garden is only valuable if it is used and appreciated by others. It can be challenging to promote and share a digital garden and to get others to engage with and contribute to it.\nChronological is the wrong metaphor, but how to capture time and sequence?In a digital garden, the organization of information is not necessarily tied to a specific timeline or sequence. Instead, the focus is on creating connections and relationships between ideas, rather than presenting them in a linear fashion.\nOne way to capture the sense of time and sequence in a digital garden is to use tags or labels to indicate when a particular note or idea was created or updated. This allows for some level of temporal organization, while still allowing for flexibility and the ability to create connections between ideas that may not necessarily be related to a specific point in time.\nAnother option is to use a visual representation of time, such as a timeline or a calendar, to show the evolution of ideas or concepts over a specific period of time. This can help to give context and provide a sense of progression, while still allowing for the non-linear organization of information that is characteristic of a digital garden.\nAs an example, have a Git commit hash as well as a recent changes history. Each page already display a commit hash to give it a context as well as a recently updated section in the index.\nHowever, using commit hashes to capture the sequence of events might not be the most intuitive or user-friendly approach. Git commit hashes are typically long strings of letters and numbers, and they are not necessarily easy for people to remember or understand.\nThe RSS1 also provides a temporal source of truth.\nNetworkingDigital gardens nurture the creation of complex networks from simple concept associations. Quoting Tim Berners-Lee\u0026rsquo;s2 \u0026ldquo;Weaving the Web\u0026rdquo;3 (2000):\nOne of the beautiful things about physics is its ongoing quest to find simple rules that describe the behavior of very small, simple objects. Once found, these rules can often be scaled up to describe the behavior of monumental systems in the real world. []\nIf the rules governing hypertext links between servers and browsers stayed simple, then our web of a few documents could grow to a global web. The art was to define the few basic, common rules of protocol that would allow one computer to talk to another, in such a way that when all computers everywhere did it, the system would thrive, not break down.\nRSS 2.0 file.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nhttps://en.wikipedia.org/wiki/Tim_Berners-Lee\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nFischetti, Mark., Berners-Lee, Tim., Fischetti, Mark., Berners-Lee, Tim. Weaving the Web: The Original Design and Ultimate Destiny of the World Wide Web by Its Inventor. N.p.: Paw Prints, 2008.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"/digital-garden.html","tags":null,"title":"Digital Garden"},{"categories":null,"contents":"L-p metricsManhattan distance (L1)Given two vectors $p$ and $q$, such that\n$$ \\begin{aligned} p \u0026amp;= \\left(p_1, p_2, \\dots,p_n\\right) \\ q \u0026amp;= \\left(q_1, q_2, \\dots,q_n\\right) \\end{aligned} $$\nwe define the Manhattan distance as:\n$$ d_1(p, q) = |p - q|1 = \\sum{i=1}^n |p_i-q_i| $$\nEuclidean distance (L2)In general, for points given by Cartesian coordinates in $n$-dimensional Euclidean space, the distance is\n$$ d(p,q)=\\sqrt {(p_{1}-q_{1})^{2}+(p_{2}-q_{2})^{2}+\\dots +(p_{i}-q_{i})^{2}+\\dots +(p_{n}-q_{n})^{2}} $$\nCluster distancesWithin-cluster sum of squares (WCSS)Given a set of observations ($x_1, x_2,\\dots,x_n$), where each observation is a $d$-dimensional real vector, $k$-means clustering aims to partition the $n$ observations into $k$ ($\\leq n$) sets $S=\\lbrace S_1, S_2, \\dots, S_k\\rbrace$ so as to minimize the within-cluster sum of squares (WCSS) (i.e. variance). Formally, the objective is to find:\n$$ {\\underset {\\mathbf {S} }{\\operatorname {arg,min} }}\\sum {i=1}^{k}\\sum {\\mathbf {x} \\in S{i}}\\left|\\mathbf {x} -{\\boldsymbol {\\mu }}{i}\\right|^{2}={\\underset {\\mathbf {S} }{\\operatorname {arg,min} }}\\sum {i=1}^{k}|S{i}|\\operatorname {Var} S_{i} $$\nwhere $\\mu_i$ is the mean of points in $S_i$. This is equivalent to minimizing the pairwise squared deviations of points in the same cluster:\n$$ {\\displaystyle {\\underset {\\mathbf {S} }{\\operatorname {arg,min} }}\\sum {i=1}^{k},{\\frac {1}{2|S{i}|}},\\sum {\\mathbf {x} ,\\mathbf {y} \\in S{i}}\\left|\\mathbf {x} -\\mathbf {y} \\right|^{2}} $$\nThe equivalence can be deduced from identity\n$${\\displaystyle \\sum {\\mathbf {x} \\in S{i}}\\left|\\mathbf {x} -{\\boldsymbol {\\mu }}{i}\\right|^{2}=\\sum {\\mathbf {x} \\neq \\mathbf {y} \\in S{i}}(\\mathbf {x} -{\\boldsymbol {\\mu }}{i})({\\boldsymbol {\\mu }}_{i}-\\mathbf {y} )}. $$\nBecause the total variance is constant, this is equivalent to maximizing the sum of squared deviations between points in different clusters (between-cluster sum of squares, BCSS) which follows from the law of total variance.\nDunn indexA full explanation is available at Dunn index.\nGower distanceA full explanation with examples is available at site/Machine learning/Gower distance.\n","permalink":"/distance-metrics.html","tags":null,"title":"Distance metrics"},{"categories":null,"contents":"Architecture Algorithm Input: Dataset $\\mathcal{D}_{train}$, Instance $x$, lenght of explanation $\\mathcal{K}$ Initialise $\\mathcal{Y} \\leftarrow {}$ Initialise cluster for $i$ /in/ $1,\\dots,N$ do $C_i \\leftarrow {i}$ end Initialise clusters to merge $\\mathcal{S} \\leftarrow$ for $i$ /in/ $1\\dots N$ while /no more clusters are available for merging/ do Pick two most similar cluster with minimum distance $d$: $(j,k) \\leftarrow \\arg\\min_{d(j,k)} \\in \\mathcal{S}$ Create new cluster $C_l \\leftarrow C_j \\bigcup C_k$ Mark $j$ and $k$ unavailable to merge if $C_l \\neq i$ in $1\\dots N$ then Mark $l$ as available, $\\mathcal{S} \\leftarrow \\mathcal{S} \\bigcup {l}$ end foreach $i \\in \\mathcal{S}$ do Update similarity matrix by computing distance $d(i, l)$ end end while $i$ /in/ $1,\\dots,n$ do $d(\\mathbf{x}i, \\mathbf{x})=\\sqrt{(x{i1}-x_1)^2+\\dots+(x_{im}-x_m)^2}$ end $ind \\leftarrow$ Find indices for the $k$ smallest distance $d(\\mathbf{x}_i, \\mathbf{x})$ $\\hat{y} \\leftarrow$ Get majority label for $x \\in ind$ $n^s \\leftarrow$ Filter $\\mathcal{D}_{train}$ based on $\\hat{y}$ foreach $i$ /in/ $1, \\dots, n$ do $\\mathcal{Y} \\leftarrow $ Pairwise distance of each instance in cluster $n^s$ with the original instance $x$ end $\\omega \\leftarrow$ LinearRegression$(n^s, \\mathcal{Y}, \\mathcal{K})$ return $\\omega$ ","permalink":"/dlime.html","tags":null,"title":"DLIME"},{"categories":null,"contents":"Configuration is at the same time the most fun and biggest challenge of any Emacs installation. I think it\u0026rsquo;s a good time to recall Anthony Bourdains thoughts about mise en place.\nMise-en-place is the religion of all good line cooks. Do not mess with a line cooks meez  meaning his setup, his carefully arranged supplies of sea salt, rough-cracked pepper, softened butter, cooking oil, wine, backups, and so on. As a cook, your station, and its condition, its state of readiness, is an extension of your nervous system The universe is in order when your station is set up the way you like it: you know where to find everything with your eyes closed, everything you need during the course of the shift is at the ready at arms reach, your defenses are deployed. If you let your mise-en-place run down, get dirty and disorganized, youll quickly find yourself spinning in place and calling for backup. I worked with a chef who used to step behind the line to a dirty cooks station in the middle of a rush to explain why the offending cook was falling behind. Hed press his palm down on the cutting board, which was littered with peppercorns, spattered sauce, bits of parsley, bread crumbs and the usual flotsam and jetsam that accumulates quickly on a station if not constantly wiped away with a moist side towel. You see this? hed inquire, raising his palm so that the cook could see the bits of dirt and scraps sticking to his chefs palm. Thats what the inside of your head looks like now.\n Anthony Bourdain, from Kitchen Confidential.\nThis an annotated version of my DOOM Emacs configuration. The source files can be found on Github or Sourcehut.\nInstallationThe most basic installation of Doom Emacs consists of:\n$ git clone --depth 1 https://github.com/doomemacs/doomemacs ~/.emacs.d $ ~/.emacs.d/bin/doom install After any configuration changes you should run the sync command with\n$ ~/.emacs.d/bin/doom sync Any upgrade to the doom Emacs packages can be done using the upgrade command.\n$ ~/.emacs.d/bin/doom upgrade LanguagesPythonvirtualenvIn order to use Python\u0026rsquo;s virtualenv, the virtualenvwrapper.el1 module is used. This is done by adding to packages.el\n(package! virtualenvwrapper) The only configuration I use for this library is setting my virtualenv root location in config.el\n(use-package! virtualenvwrapper) (after! virtualenvwrapper (setq venv-location \u0026#34;~/.virtualenvs/\u0026#34;) ) From a Python project the virtual environment can be selected by using M-x venv-workon.\nBlack formatterTo allow formatting of Python blocks in org-mode and elsewhere, add python-black 2 to packages.el.\n(package! python-black) Then we configure it with\n(use-package! python-black :after python :hook (python-mode . python-black-on-save-mode-enable-dwim)) We should install black-machiatto 3 to allow formatting of partial regions.\nGoThe following modules must be enabled in init.el:\n(go +lsp) in the lang section lsp in the tools section snippets in the editor section gopls should be installed.\nTemplatesTo enable support of Go templates, install the lang/web packages by adding to init.el.\n(web +html) This will install the web-mode package4 To specify the sepcific template engine as Go M-x web-mode-set-engine to go.\nRunning doom sync will finish the setup.\nPikchrSupport for Pikchr5 is added via the pikchr-mode package. By adding\n(package! pikchr-mode) It is necessary to install the pikchr binary according to the instructions in Pikchr. That page also contains examples.\nxonshI also use a basic mode6 to support xonsh7 scripts. The package can be installed with\n;; packages.el (package! xonsh-mode) and invoked with\n;; config.el (use-package! xonsh-mode) org-modeorg-agendaTo disable completed items to show up in agenda views, the following is set in config.el\n(setq org-agenda-skip-scheduled-if-done t) (setq org-agenda-skip-deadline-if-done t) Key bindingsFor Doom Emacs we add some additional key-bindings to the org-mode specifically. These bindings go under the local org leader key with the u prefix (which in my case is ) SPC m u. They are\nSPC m u a, archive an org-agenda item using #org-archive-subtree-default r, get a random note using #org-randomnote For more information on how to set doom Emacs keybings, check the elevant section.\nPretty bulletsThe default Doom Emacs org-mode bullets are *. To get \u0026ldquo;pretty\u0026rdquo; bullets you need to add the +pretty flag to init.el:\n(org +pretty ) ; organize your plain life in plain text Optionally, you can specify the bullet\u0026rsquo;s hierarchy glyphs with\n(setq org-superstar-headline-bullets-list \u0026#39;(\u0026#34;\u0026#34; \u0026#34;\u0026#34; \u0026#34;\u0026#34; \u0026#34;\u0026#34; \u0026#34;\u0026#34;) ) UIResolution dependent fontsI have to work with a variety of monitor resolutions, from a 720p pocket book to a 2160p iMac. The fonts usually look tiny in my larger screen so I\u0026rsquo;ve added support for resolution dependent font sizes in my config.el.\nFor instance\n(when (window-system) ;; only apply to GUI frames ;; Adjust font size dependent of the screen resolution (when (\u0026gt; (x-display-pixel-width) 3000) (setq doom-font (font-spec :family \u0026#34;Ubuntu Mono\u0026#34; :size 26 :weight \u0026#39;regular))) (when (\u0026lt; (x-display-pixel-width) 3000) (setq doom-font (font-spec :family \u0026#34;Ubuntu Mono\u0026#34; :size 14 :weight \u0026#39;regular))) ) Would set a larger font size for monitors with a horizontal resolution higher than 3000 pixels.\nPrettify symbolsSince Emacs 24.48 there is a builtin prettify-symbols-mode. It can be customized by changing prettify-symbols-alist. These strings will be replace by our selection, typically an unicode symbol. In this configuration we use the following:\n(defun my/pretty-symbols () (setq prettify-symbols-alist \u0026#39;((\u0026#34;#+begin_src python\u0026#34; . \u0026#34;\u0026#34;) (\u0026#34;#+begin_src elisp\u0026#34; . \u0026#34;\u0026#34;) (\u0026#34;#+begin_src jupyter-python\u0026#34; . \u0026#34;\u0026#34;) (\u0026#34;#+end_src\u0026#34; . \u0026#34;\u0026#34;) (\u0026#34;#+results:\u0026#34; . \u0026#34;\u0026#34;) (\u0026#34;#+RESULTS:\u0026#34; . \u0026#34;\u0026#34;)))) This will, for instance, replace the beginning of Python org-babel~ blocks with the single symbol. To register the prettify list with each mode we use\n(add-hook \u0026#39;org-mode-hook \u0026#39;my/pretty-symbols) Or we can register the prettify-symbols-mode as a global mode\n(global-prettify-symbols-mode +1) companycompany is great, but it can get in the way when using on org-mode buffers. To disable add the following:\n(after! org ;; disable auto-complete in org-mode buffers (remove-hook \u0026#39;org-mode-hook #\u0026#39;auto-fill-mode) ;; disable company too (setq company-global-modes \u0026#39;(not org-mode)) ;; ... ) BeaconBeacon highlights the current cursor line after major movements. Especially useful for HDPi screens. Add it on packages.el:\n(package! beacon) And enable the global minor-mode on config.el with:\n;; global beacon minor-mode (use-package! beacon) (after! beacon (beacon-mode 1)) FocusThis mode relies on the Focus package that dims regions not on \u0026hellip; focus. Since the package is on MELPA, it can be installed by adding to packages.el:\n(package! focus) Next, require it from config.el\n(use-package! focus) And call it by setting the mode with M-x focus-mode.\nNavigationTreemacstreemacs9 is a great file navigation explorer for Emacs. However, since it has its own iternal concept of project, it needs an external helper to be able to synchronise with other project management tools, such as projectile.\nTo be able to synchronise treemacs and projectile the treemacs-projectile module must be used. It can be activated using\n(use-package treemacs-projectile :after (treemacs projectile)) (after! (treemacs projectile) (treemacs-project-follow-mode 1)) This guarantees that when moving to a buffer of a different projectile project, the treemacs tree will reflect that.\nPersonally, I\u0026rsquo;m not a fan of doom Emacs\u0026rsquo; and treemacs\u0026rsquo; default behaviour of use variable pitch fonts. If you want, for consistency, to set treemacs to use a monospaced font, set the following on your config.el:\n(setq doom-themes-treemacs-enable-variable-pitch nil) DirvishDirvish offers a suitable replacement/enhancement for dired with features such as improved UI and image preview. The only requirement for dirvish is the ls alternative exa It is available from MELPA which means you can add it to packages.el with\n(package! dirvish) and then enable it on config.el with\n(use-package! dirvish) ToolsvtermAn interesting talk on the advantages of using vterm as the default Emacs terminal emulator can be found can found at EmacsConf 2021 \u0026ldquo;A Tour of vterm\u0026rdquo;. To use vterm as the Emacs shell, the respective section in init.el should be selected:\n(doom! :term ;;eshell ; the elisp shell that works everywhere ;;shell ; simple shell REPL for Emacs ;;term ; basic terminal emulator for Emacs vterm ; the best terminal emulation in Emacs ) We also need to install libvterm, with in macOS can be done with\nbrew install libvterm and in Linux with\n$ sudo apt-get install -y libvterm-dev # Ubuntu $ sudo dnf -y install libvterm # Fedora deadgrepFor full-text search, deadgrep10 is used, which leverages ripgrep. To install ripgrep on Linux, run\n$ sudo apt install ripgrep # Ubuntu $ sudo dnf install ripgrep # Fedora Defining key bindings(map! :leader (:prefix-map (\u0026#34;f\u0026#34; . \u0026#34;foo\u0026#34;) (:prefix (\u0026#34;b\u0026#34; . \u0026#34;bar\u0026#34;) :desc \u0026#34;Baz\u0026#34; \u0026#34;b\u0026#34; #\u0026#39;foo-bar-baz :desc \u0026#34;Qux\u0026#34; \u0026#34;q\u0026#34; #\u0026#39;foo-bar-qux))) This will define SPC-f-b-b to invoke foo-bar-baz and SPC-f-b-q to invoke foo-bar-qux.\nDynamic modulesThe best place to put dynamic modules in Doom Emacs is inside the actual .doom.d folder. This allows to load modules from Doom\u0026rsquo;s \u0026ldquo;private dir\u0026rdquo;. For instance, for a module called my_module and sub-directory named modules we would create\nmkdir ~/.doom.d/modules and then add to config.el\n(add-to-list \u0026#39;load-path (expand-file-name \u0026#34;modules/my_module\u0026#34; doom-private-dir)) (def-package! my_module :commands (mymodule-foo mymodule-bar)) https://github.com/porterjamesj/virtualenvwrapper.el\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nhttps://github.com/emacs-vault/emacs-python-black\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nhttps://github.com/wbolster/black-macchiato\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nhttps://web-mode.org/\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nhttps://pikchr.org/\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nhttps://github.com/seanfarley/xonsh-mode\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nhttps://xon.sh/\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nhttps://www.emacswiki.org/emacs/PrettySymbol\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nhttps://github.com/Alexander-Miller/treemacs\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nhttps://github.com/Wilfred/deadgrep\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"/doom-emacs.html","tags":null,"title":"DOOM Emacs"},{"categories":null,"contents":"A page on Drools.\n","permalink":"/drools.html","tags":null,"title":"Drools"},{"categories":null,"contents":"There are several ways to measure the robustness of a clustering algorithm. Three commonly used metrics are the Dunn index, Davis-Bouldin index, Silhoutte index and Calinski-Harabasz index.\nBut before we start, let\u0026rsquo;s introduce some concepts.\nWe are interested in clustering algorithms for a dataset $\\mathcal{D}$ with $N$ elements in a $n$-dimensional real space, that is:\n$$ \\mathcal{D} = {x_1, x_2, \\ldots, x_N} \\in \\mathbb{R}^p $$\nThe clustering algorithm will create a set $C$ of $K$ distinct disjoint groups from $\\mathcal{D}$ $C={c_1, c_2, \\ldots, c_k}$, such that:\n$$ \\cup_{c_k\\in C}c_k=\\mathcal{D} \\ c_k \\cap c_l \\neq \\emptyset \\forall k\\neq l $$\nEach group (or cluster) $c_k$, will have a centroid, $\\bar{c}_k$, which is the mean vector of its elements such that:\n$$ \\bar{c}k=\\frac{1}{|c_k|}\\sum{x_i \\in c_k}x_i $$\nWe will also make use of the dataset\u0026rsquo;s mean vector, $\\bar{\\mathcal{D}}$, defined as:\n$$ \\bar{\\mathcal{D}}=\\frac{1}{N}\\sum_{x_i \\in X}x_i $$\nDunn indexThe Dunn index aims at quantifying the compactness and variance of the clustering. A cluster is considered compact if there is small variance between members of the cluster. This can be calculated using $\\Delta(c_k)$, where\n$$ \\Delta(c_k) = \\max_{x_i, x_j \\in c_k}{d_e(x_i, x_j)} $$\nand $d_e$ is the Euclidian distance defined as:\n$$ d_e=\\sqrt{\\sum_{j=1}^p (x_{ij}-x_{kj})^2}. $$\nA cluster is considered well separated if the cluster are far-apart. This can quantified using\n$$ \\delta(c_k, c_l) = \\min_{x_i \\in c_k}\\min_{x_j\\in c_l}{d_e(x_i, x_j)}. $$\nGiven these quantities, the Dunn index for a set of clusters $C$, $DI(C)$, is then defined by:\n$$ DI(C)=\\frac{\\min_{c_k \\in C}{\\delta(c_k, c_l)}}{\\max_{c_k\\in C}\\Delta(c_k)} $$\nA higher Dunn Index will indicate compact, well-separated clusters, while a lower index will indicate less compact or less well-separated clusters.\nWe can now try to calculate the metric for the dataset we\u0026rsquo;ve created previously. Let\u0026rsquo;s simulate some data and apply the Dunn index from scratch. First, we will create a compact and well-separated dataset using the make_blobs method in scikit-learn. We will create a dataset of $\\mathbb{R}^2$ data (for easier plotting), with three clusters.\nfrom sklearn.datasets import make_blobs X, y = make_blobs(n_samples=1000, centers=3, n_features=2, random_state=23) import pandas as pd from plotnine import * from plotnine.data import * from plotutils import * data = pd.DataFrame(X, columns=[\u0026#34;x1\u0026#34;, \u0026#34;x2\u0026#34;]) data[\u0026#34;y\u0026#34;] = y data[\u0026#34;y\u0026#34;] = data.y.astype(\u0026#39;category\u0026#39;) We now cluster the data and we will have, as expected three distinct clusters, plotted below.\nfrom sklearn import cluster k_means = cluster.KMeans(n_clusters=3) k_means.fit(data) y_pred = k_means.predict(data) prediction = pd.concat([data, pd.DataFrame(y_pred, columns=[\u0026#39;pred\u0026#39;])], axis = 1) clus0 = prediction.loc[prediction.pred == 0] clus1 = prediction.loc[prediction.pred == 1] clus2 = prediction.loc[prediction.pred == 2] k_list = [clus0.values, clus1.values,clus2.values] Let\u0026rsquo;s focus now on two of these cluster, let\u0026rsquo;s call them $c_k$ and $c_l$.\nck = k_list[0] cl = k_list[1] We know we have to calculate the distance between the points in $c_k$ and $c_l$. We know that the len(ck)=len(cl)=333 we create\nimport numpy as np values = np.ones([len(ck), len(cl)]) values array([[1., 1., 1., ..., 1., 1., 1.], [1., 1., 1., ..., 1., 1., 1.], [1., 1., 1., ..., 1., 1., 1.], ..., [1., 1., 1., ..., 1., 1., 1.], [1., 1., 1., ..., 1., 1., 1.], [1., 1., 1., ..., 1., 1., 1.]]) For each pair of points, we then get the norm of $x_i-x_j$. For instance, for $i=0\\in c_k$ and $i=1\\in c_l$, we would have:\nvalues[0, 1] = np.linalg.norm(ck[0]-cl[1]) print(ck[0], cl[1]) print(values[0, 1]) [-5.37039106 3.47555168 2. 0. ] [ 5.46312794 -3.08938807 1. 1. ] 12.746119711608184 The calculation of $\\delta(c_k, c_l)$ between two clusters $c_k$ and $c_l$ will be defined as follows:\nimport numpy as np def (ck, cl): values = np.ones([len(ck), len(cl)]) for i in range(0, len(ck)): for j in range(0, len(cl)): values[i, j] = np.linalg.norm(ck[i]-cl[j]) return np.min(values) So, for our two clusters above, $\\delta(c_k, c_l)$ will be:\n(ck, cl) 8.13474311744193 Within a single cluster $c_k$, we can calculate $\\Delta(c_k)$ similarly as:\ndef (ci): values = np.zeros([len(ci), len(ci)]) for i in range(0, len(ci)): for j in range(0, len(ci)): values[i, j] = np.linalg.norm(ci[i]-ci[j]) return np.max(values) So, for instance, for our $c_k$ and $c_l$ we would have:\nprint((ck)) print((cl)) 6.726025773561468 6.173844284636552 We can now define the Dunn index as\ndef dunn(k_list): s = np.ones([len(k_list), len(k_list)]) s = np.zeros([len(k_list), 1]) l_range = list(range(0, len(k_list))) for k in l_range: for l in (l_range[0:k]+l_range[k+1:]): s[k, l] = (k_list[k], k_list[l]) s[k] = (k_list[k]) di = np.min(s)/np.max(s) return di and calculate the Dunn index for our clustered values list as\ndunn(k_list) 0.14867620697065728 Intuitively, we can expect a dataset with less well-defined clusters to have a lower Dunn index. Let\u0026rsquo;s try it. We first generate the new dataset.\nX2, y2 = make_blobs(n_samples=1000, centers=3, n_features=2, cluster_std=10.0, random_state=24) df = pd.DataFrame(X2, columns=[\u0026#39;A\u0026#39;, \u0026#39;B\u0026#39;]) k_means = cluster.KMeans(n_clusters=3) k_means.fit(df) #K-means training y_pred = k_means.predict(df) prediction = pd.concat([df,pd.DataFrame(y_pred, columns=[\u0026#39;pred\u0026#39;])], axis = 1) prediction[\u0026#34;pred\u0026#34;] = prediction.pred.astype(\u0026#39;category\u0026#39;) clus0 = prediction.loc[prediction.pred == 0] clus1 = prediction.loc[prediction.pred == 1] clus2 = prediction.loc[prediction.pred == 2] k_list = [clus0.values, clus1.values,clus2.values] dunn(k_list) 0.019563892388205984 Calinski-Harabasz indexThe Calinski-Harabasz index1 (also known as the variance ratio criterion) is a measure of the quality of a clustering algorithm. It is commonly used to evaluate the results of a clustering technique and to compare the performance of different clustering algorithms.\nThe index is calculated by dividing the between-cluster variance by the within-cluster variance. A higher Calinski-Harabasz index indicates a better separation of the clusters and a better overall clustering result.\nFor the following example we will use Scikit-learn\u0026rsquo;s implementation2 of the Calinski-Harabasz index.\nIf we apply it to the previous well-defined cluster data, X, y:\nfrom sklearn.cluster import KMeans from sklearn.datasets import make_blobs from sklearn.metrics import calinski_harabasz_score # Calculate the Calinski-Harabasz index score = calinski_harabasz_score(X, y) print(\u0026#34;Calinski-Harabasz index:\u0026#34;, score) Calinski-Harabasz index: 12403.892218876248 While applying it to the less well-defined cluster will return a lower index:\nscore = calinski_harabasz_score(X2, y2) print(\u0026#34;Calinski-Harabasz index:\u0026#34;, score) Calinski-Harabasz index: 135.29288069299935 Caliski, Tadeusz, and Jerzy Harabasz. \u0026ldquo;A dendrite method for cluster analysis.\u0026rdquo; Communications in Statistics-theory and Methods 3, no. 1 (1974): 1-27.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nhttps://scikit-learn.org/stable/modules/generated/sklearn.metrics.calinski_harabasz_score.html\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"/dunn-index.html","tags":null,"title":"Dunn index"},{"categories":null,"contents":"Elisp is the programming language used to program and configure Emacs.\nSome Elisp topics:\nElisp snippets ","permalink":"/elisp.html","tags":null,"title":"Elisp"},{"categories":null,"contents":"SnippetsExecute command in shell buffercomint-send-string is the function we\u0026rsquo;re looking for.1\nIt takes a PROCESS and a STRING. You can get the process from the shell buffer, and conveniently the shell function returns the buffer, so you can streamline it all into something like:\n(defun my-server () \u0026#34;SSH to my.server.com in `shell\u0026#39; buffer.\u0026#34; (interactive) (comint-send-string (get-buffer-process (shell)) \u0026#34;ssh my.server.com\\n\u0026#34;)) Where the (shell) call will take care of creating the shell buffer and/or process if necessary.2\nFree variablesIn some situations, for instance when setting a variable in a DOOM Emacs like\n(setq my/variable \u0026#34;value\u0026#34;) You might get the warning\nWarning: assignment to free variable `er/try-expand-list\u0026#39; This is because set and setq do not declare lexical variables3. The solution is to use4\n(defvar my/variable \u0026#34;value\u0026#34;) Lists to arraysList of listsTo convert a list of lists, e.g.\n(defvar mylist ((\u0026#34;1\u0026#34; \u0026#34;a\u0026#34;) (\u0026#34;2\u0026#34; \u0026#34;b\u0026#34;) (\u0026#34;3\u0026#34; \u0026#34;c\u0026#34;))) The method to convert a single element from the list is\n(require \u0026#39;cl) (coerce (nth 0 mylist) \u0026#39;vector) ;; [\u0026#34;1\u0026#34; \u0026#34;a\u0026#34;] We now just need to map this for all the list\n(mapcar (lambda (arg) (coerce arg \u0026#39;vector)) mylist) Lists conversionTo convert a vector to list:\n(coerce [1 2 3] \u0026#39;list) ;; (1 2 3) and to convert a list to vector:\n(coerce \u0026#39;(1 2 3) \u0026#39;vector) ;; [1 2 3] ReferencedefcustomYou can specify variables using defcustom so that you and others can then use Emacss customize feature to set their values. (You cannot use customize to write function definitions; but you can write defuns in your .emacs file. Indeed, you can write any Lisp expression in your .emacs~ file.)5\nFootnotesSQLiteEmacs provides functionality to interact with SQLite databases. For these examples we will use the emacSQL package. To open a database file /tmp/foo.sqlite we issue:\n(defvar db (emacsql-sqlite \u0026#34;~/tmp/foo.sqlite\u0026#34;)) We can then issue SELECT statements with\n;; Query the database for results: (emacsql db [:select [name id] :from people :where (\u0026gt; salary 62000)]) ;; =\u0026gt; ((\u0026#34;Susan\u0026#34; 1001)) shell is built on top of the comint library.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nn.b. if there\u0026rsquo;s an existing one, shell will re-use that.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nOnly let does.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nBut not quote the symbol.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nThe docs for defcustom: https://www.gnu.org/software/emacs/manual/html_node/eintr/defcustom.html\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"/elisp-snippets.html","tags":null,"title":"Elisp snippets"},{"categories":null,"contents":"This page is a collection of Emacs related notes. The From Scratch to Emacs, The Adventurous Configurator\u0026rsquo;s Handbook page contains a step-by-step guide to configure Emacs from scratch.\nNotes on EmacsMy current flavour/distribution of Emacs is DOOM Emacs spacemacs1 DOOM Emacs2. Yes, DOOM Emacs is my preferred configuration framework at the moment. I have used Spacemacs for a long time, but the performance increase by switching to DOOM Emacs is simply to great to be ignored. Recently, I have also migrated to the Emacs 28 branch which includes Elisp native compilation. It is a thing of beauty in terms of speed.\nThis page refers to broad Emacs base configuration. For specific Emacs recipes check the Emacs cookbook.\nInstallationmacOSHomebrewMany people swear by emacs-mac as the gold standard for macOS Emacs distributions. To install it, using homebrew simply run\n$ brew tap railwaycat/emacsmacport $ brew install --cask emacs-mac-spacemacs-icon Alternatively, you can choose whatever icon you want from here. A good alternative is for instance --with-emacs-big-sur-icon. On Linux you can use my con, if you wish.\nHowever, emacs-mac is, at the time of writing, targetting Emacs 27. If you want to use Emacs 29 (and don\u0026rsquo;t want to build it from source) a good option is emacs-plus. To install it use:\n$ brew install emacs-plus@28 \\ --with-native-comp \\ --with-modern-doom3-icon \\ --with-mailutils \\ --with-imagemagick Another option is to use jimeh\u0026rsquo;s Emacs builds3. These include nightly builds of Emacs 28 with native compilation enabled. To install it, simply run\nbrew install --cask emacs-app or, if want the latest nightly build from Emacs\u0026rsquo; master branch:\nbrew install --cask emacs-app-nightly MacPortsRecently I\u0026rsquo;ve been using MacPorts4 more often than Homebrew. MacPorts also includes an Emacs5 recipe. I tend to use the simple Emacs 28 text-only one:\n$ sudo port install emacs FedoraAt the time of writing6, Fedora only supports, officially, Emacs 27. If you want to try Emacs 28 (and the native compilation feature) you need to install it using\n$ sudo dnf copr enable deathwish/emacs-pgtk-nativecomp and then (if Emacs is already present)\n$ sudo dnf upgrade emacs otherwise run\n$ sudo dnf install emacs UbuntuFor Ubuntu (and other Linux distros, including Fedora) an option is to use the Snap store. The available Snap can install Emacs 28 with native compilation enabled. To use it, simply run\n$ sudo snap install emacs --edge --classic From sourceIn December 2021 the pgtk branch of Emacs was merged into master7. Among other features this adds full Wayland support and better font rendering for the Emacs GUI. This version can be compiled from source with just a few simple steps.\nDependenciesTo build this version in Linux (I\u0026rsquo;m assuming Ubuntu and variants) you will need the following dependencies:\nautoconf build-essential, provides GCC, make, libc, etc. libgtk-3-dev, Gnome dependencies libgnutls28-dev, provides libgnutls28 libtiff5-dev, libgif-dev, libjpeg-dev, libpng-dev and libxpm-dev provide image support libncurses-dev, provides terminal support texinfo, for Info documentation libxml2-dev for XML support I also wanted to include Emacs\u0026rsquo; native JSON support and Elisp native compilation, so will additionally need:\nlibjansson4, libjansson-dev, provides native JSON support libgccjit0, libgccjit-11-dev, gcc-11 and g++-118 A one-liner to install all dependencies is:\n$ sudo apt update $ sudo apt install build-essential libgtk-3-dev libgnutls28-dev \\ libtiff5-dev libgif-dev libjpeg-dev libpng-dev libxpm-dev \\ libncurses-dev texinfo \\ libxml2-dev \\ jansson4 libjansson-dev \\ libgccjit0 libgccjit-11-dev gcc-11 g++-11 If building on Fedora, the dependency names will naturally be slightly different. In this case we would use\n$ sudo dnf update $ sudo dnf install @development-tools autoconf \\ gtk3-devel gnutls-devel \\ libtiff-devel giflib-devel libjpeg-devel libpng-devel libXpm-devel \\ ncurses-devel texinfo \\ libxml2-devel \\ jansson jansson-devel \\ libgccjit libgccjit-devel BuildingWe can then clone the Emacs repo and run autogen. The steps from now on should be the same for most GNU/Linux distributions.\n$ git clone git://git.sv.gnu.org/emacs.git $ cd emacs $ ./autogen.sh To use native Elisp compilation we will use gcc-118 so before starting the build run:\n$ export CC=/usr/bin/gcc-11 CXX=/usr/bin/gcc-11 On Fedora, if are sure you have gcc 11, simply export\n$ export CC=/usr/bin/gcc CXX=/usr/bin/gcc Emacs supports --with-xxx to enable support for feature xxx, which in our case will be:\n--with-pgtk, native GTK support --with-json, native JSON support --with-native-compilation, native compilation support --with-xml2, XML support --with-modules, support for dynamic modules --with-mailutils, support for mail-utils So we can run the configure script with\n$ ./configure \\ --with-native-compilation \\ --with-json \\ --with-pgtk \\ --with-xml2 \\ --with-modules \\ --with-mailutils We can now start the build and installation (you can choose the approriate number for your machine for the parallel run, in this case 8):\n$ make -j8 $ sudo make install If everything completes successfully you should have Emacs 29 available in /usr/local/bin\n$ emacs --version GNU Emacs 29.0.50 Copyright (C) 2021 Free Software Foundation, Inc. GNU Emacs comes with ABSOLUTELY NO WARRANTY. You may redistribute copies of GNU Emacs under the terms of the GNU General Public License. For more information about these matters, see the file named COPYING. Configuration Annotated DOOM Emacs config CompletionI recently move from helm9 to ivy. The main reason is that apparently helm development has stalled.\nThe important stuff?IconI\u0026rsquo;ve also made a vaporwave Emacs icon which you can find in [https://github.com/ruivieira/emacs-vaporwave-icon]here.\nCursorA cursor should blink, in my opinion. For a history of the blinking cursor see The Forgotten History of the Blinking Cursor. You can instruct Emacs to blink it with the appropriate mode10.\n(blink-cursor-mode 1) ThemesAn excellent resource for Emacs themes is Peach MELPA. Some selected theme are below.\nHere is a list of good themes for Emacs:\nzenburn ample alect cyberpunk gruvbox The theme that I\u0026rsquo;m currently using is spacemacs-light. It works really well the new org-mode styling. A more detailed rundown of the specific theming can be found at DOOM Emacs.\nElisp The main page for Elisp You can check my solutions for the Euler project in Elisp: Project Euler in Elisp Command lineRunning org filesDynamic modulesSince Emacs 25.1, Emacs can use Dynamic Modules.\nhttps://www.spacemacs.org/\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nhttps://github.com/hlissner/doom-emacs\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nhttps://github.com/jimeh/emacs-builds\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nhttps://www.macports.org/\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nhttps://ports.macports.org/port/emacs/\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nFedora 32/33.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nhttps://mail.gnu.org/archive/html/emacs-devel/2021-12/msg01732.html\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nThis also works with gcc-10.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nThe main Helm maintainer is Thierry Volpiatto.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nhttps://www.emacswiki.org/emacs/NonBlinkingCursor\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"/emacs.html","tags":null,"title":"Emacs"},{"categories":null,"contents":"PackagesLoad local packagesTo load an Emacs package locally, you\u0026rsquo;ll need to do a few things. Here\u0026rsquo;s a step-by-step guide:\nInstall the package manually: First, you\u0026rsquo;ll need to have the package files somewhere on your system. You may download it from a source or create it yourself. Let\u0026rsquo;s say you have a package named mypackage in a directory at ~/emacs/mypackage.\nAdd the package directory to your load path: Emacs uses a variable called load-path to determine where to look for packages to load. You can add your package\u0026rsquo;s directory to this path by adding the following line to your Emacs configuration file (.emacs or init.el):\n(add-to-list \u0026#39;load-path \u0026#34;~/emacs/mypackage\u0026#34;) Note that Emacs does not expand the ~ character in paths, so you\u0026rsquo;ll need to use the full path to your package directory or use the expand-file-name function to expand the path.\nRequire the package: Once the directory is in your load path, you can require the package in your Emacs configuration file: (require \u0026#39;mypackage) This tells Emacs to load the mypackage package when it starts up.\nNote: The package\u0026rsquo;s main file should be named mypackage.el, and there should be a (provide 'mypackage) expression at the end of this file for the require function to work.\nThis approach is for simple, single-file packages. More complex packages that have a structure and come with an -autoloads.el file will require a different approach. You might need to use the built-in package manager (package.el) or a third-party package manager like straight.el or use-package to handle autoloading and dependencies properly.\nOrdering JSON by key valueRun json-pretty-print-buffer-ordered[^spacemacslayer]\n[^spacemacslayer] If using spacemacs, it requires the Javascript layer\nProjectsCreating a projectile projectTo create a projectile project either use a valid Git repository or create the special .projectile file in root.\nFuzzy searchDo a SPC p f, but make sure the projectile project is created.\nSplit screenThe standard Emacs keys are valid here (even for Spacemacs).\nSPC w h Indent a block of textSelect the block with V and then press Ctrl-x TAB. Then use h or l to move it along\n","permalink":"/emacs-cookbook.html","tags":null,"title":"Emacs cookbook"},{"categories":null,"contents":"An Emacs package for Quarkus.\n","permalink":"/emacs-quarkus.html","tags":null,"title":"Emacs Quarkus"},{"categories":null,"contents":" Root Mean Squared Error R-squared ","permalink":"/error-metrics.html","tags":null,"title":"Error metrics"},{"categories":null,"contents":"Topics Counterfactuals Counterfactuals with Constraint Solvers LIME and the deterministic version, DLIME Resources TrustyAI Explainability Toolkit1 pre-print, https://arxiv.org/abs/2104.12717 A nice presentation on AI/ML explainability: https://explainml-tutorial.github.io/neurips20 Software omnixai Literature Kakogeorgiou, Ioannis, and Konstantinos Karantzalos. \u0026ldquo;Evaluating Explainable Artificial Intelligence Methods for Multi-label Deep Learning Classification Tasks in Remote Sensing.\u0026rdquo; arXiv preprint arXiv:2104.01375 (2021). Geada, Rob, Tommaso Teofili, Rui Vieira, Rebecca Whitworth and Daniele Zonca. TrustyAI Explainability Toolkit. (2021).\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"/explainability.html","tags":null,"title":"Explainability"},{"categories":null,"contents":"Summary","permalink":"/extending-junit.html","tags":null,"title":"Extending JUnit"},{"categories":null,"contents":"Machine Learning fairness is directly related to almost all fields where Machine Learning can be applied:\nAutonomous machines Job application workflow Predictive models for the justice system Online shopping recommendation systems etc. Many of the causes in ML unfairness or bias can be tracked to the original training data. Some common causes include:\nSkewed observations Tainted observations Limited features Sample size disparity Proxies Some algortihms discussed in these pages:\nCounterfactual Fairness (also how to create counterfactually fair models in Java) Group fairnessGroup fairness metrics are measures that assess the fairness of a decision-making process or outcome for different groups within a population. These metrics are used to evaluate the fairness of systems or policies that have an impact on various groups, such as race, gender, age, or other characteristics. Group fairness metrics can help identify potential biases in decision-making processes and ensure that outcomes are just and equitable for all individuals.\nSome common types of group fairness metrics include:\nStatistical Parity: This metric assesses whether the proportion of positive outcomes (e.g. being approved for a loan) is the same for all groups. Demographic parity: This metric assesses whether the probability of a positive outcome is the same for all groups. Equal opportunity: This metric assesses whether the probability of a positive outcome is the same for individuals from different groups who have the same qualifications or characteristics. Equalized odds: This metric assesses whether the true positive rate and false positive rate are the same for all groups. Predictive parity: This metric assesses whether the error rates for different groups are the same, given the same predicted probability of a positive outcome. It is important to note that group fairness metrics are not a substitute for addressing the root causes of inequality, but they can help identify and mitigate potential biases in decision-making processes.\nStatistical ParityThere are several different types of statistical parity metrics that can be used to assess the fairness of a decision-making process or outcome for different groups within a population. Some common types of statistical parity metrics include:\nGroup Statistical Parity: This metric assesses whether the proportion of positive outcomes (e.g. being approved for a loan) is the same for all groups. Statistical parity difference (SPD), measures the difference between the proportion of positive outcomes for two groups. Disparate Impact Ratio (DIR), measures the ratio between the proportion of positive outcomes for two groups. Subgroup statistical parity: This metric assesses whether the proportion of positive outcomes is the same for subgroups within a larger group. For example, this could be used to assess the fairness of a hiring process for men and women within a particular job category. Individual statistical parity: This metric assesses whether the probability of a positive outcome is the same for all individuals, regardless of their group membership. Pairwise statistical parity: This metric assesses whether the probability of a positive outcome is the same for all pairs of groups. For example, this could be used to compare the probability of a positive outcome for men and women, as well as for men and people of other gender identities. It is important to note that no single statistical parity metric is a perfect measure of fairness, and different metrics may be more or less appropriate depending on the specific context and goals of the evaluation. It may also be helpful to use a combination of different statistical parity metrics in order to get a more comprehensive understanding of the fairness of a decision-making process or outcome.\nGroup Statistical ParityStatistical Parity Difference (SPD) and Group Statistical Parity are two different group fairness metrics that are used to assess the fairness of a decision-making process or outcome for different groups within a population.\nGroup statistical parity measures whether the proportion of positive outcomes (e.g. being approved for a loan) is the same for all groups. For example, if the proportion of race A applicants who are approved for a loan is 50%, and the proportion of race B applicants who are approved is also 50%, then the loan approval process could be considered fair according to this metric.\nStatistical parity differenceStatistical parity difference (SPD), on the other hand, measures the difference between the proportion of positive outcomes for two groups. It is often used to assess the fairness of a decision-making process or outcome where there are two groups of interest, such as men and women or people of different racial groups. SPD is calculated as the difference between the proportion of positive outcomes for one group and the proportion of positive outcomes for the other group.\nOne key difference between group statistical parity and SPD is that group statistical parity assesses fairness for all groups within a population, while SPD is specifically designed to compare the fairness of two groups. Group statistical parity is also based on proportions, while SPD is based on the difference between proportions.\nFor example, consider a credit approval process where 60% of white applicants are approved and 50% of Black applicants are approved. According to group statistical parity, this process would not be considered fair, as the proportion of approved applicants is not the same for both groups. However, according to SPD, the difference between the proportions of approved applicants for the two groups is 10%, which may be considered acceptable depending on the specific context and goals of the evaluation.\nThe formal definiton of SPD is\n$$ SPD=p(\\hat{y}=1|\\mathcal{D}_u)-p(\\hat{y}=1|\\mathcal{D}_p), $$\nwhere $\\hat{y}=1$ is the favourable outcome and $\\mathcal{D}_u, \\mathcal{D}_p$ are respectively the privileged and unprivileged group data.\nDisparate Impact RatioDisparate Impact Ratio (DIR) is specifically a ratio-based statistical parity metric, as it measures the ratio of the probability of a positive outcome for one group to the probability of a positive outcome for another group. It is often used to assess the fairness of a decision-making process or outcome where there are two groups of interest, such as men and women or people of different racial groups.\nThe formal definition of DIR is\n$$ DIR=\\frac{p(\\hat{y}=1|\\mathcal{D}_u)}{p(\\hat{y}=1|\\mathcal{D}_p)}. $$\n","permalink":"/fairness-in-machine-learning.html","tags":null,"title":"Fairness in Machine Learning"},{"categories":null,"contents":"TechniquesThe most common tecnhiques for feature scaling are normalisation and standardisation. For the examples, we will use the reference dataframe\nimport pandas as pd import numpy as np import matplotlib.pyplot as plt df = pd.DataFrame({\u0026#39;x\u0026#39;: np.random.rand(100)*10.0, \u0026#39;y\u0026#39;: np.random.rand(100)*2.0}) print(df) x y 0 7.338272 0.963962 1 9.282307 0.799143 2 2.505291 0.664340 3 3.212283 0.137100 4 4.370920 0.383998 .. ... ... 95 1.454787 0.773893 96 3.847065 1.478079 97 4.198221 0.308595 98 9.986268 0.298912 99 4.940190 0.916740 [100 rows x 2 columns] Min-Max scalerA common scaler which transforms the original space between $[A, B]$ to another space $[A^{\\prime}, B^{\\prime}]$. Typically, $[A^{\\prime}, B^{\\prime}]=[0, 1]$. The transformation is:\n$$ x^{\\prime}=\\frac{x-x_{min}}{x_{max}-x_{min}} $$\nThe Min-Max scaler works best when no normality is assumed and it is very sensitive to outliers.\nExamplefrom sklearn.preprocessing import MinMaxScaler scaler = MinMaxScaler() df_scaled = pd.DataFrame(scaler.fit_transform(df), columns=[\u0026#39;x\u0026#39;,\u0026#39;y\u0026#39;]) ","permalink":"/feature-scaling.html","tags":null,"title":"Feature scaling"},{"categories":null,"contents":" The Fedora1 OS A Linux distribution UpgradingTo upgrade a Fedora distribution (to, say, version 36), run:\n$ sudo dnf upgrade --refresh $ sudo dnf system-upgrade download --releasever=36 After the download of new packages is finished, run:\nsudo dnf system-upgrade reboot Build toolsRun\nsudo dnf install @development-tools https://docs.fedoraproject.org/en-US/project/\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"/fedora.html","tags":null,"title":"Fedora"},{"categories":null,"contents":"A page on Flask things.\n","permalink":"/flask.html","tags":null,"title":"Flask"},{"categories":null,"contents":"Eternal questionsWhy does Pepperoni curl in Pizza? As the pizza bakes, the edge of the pepperoni curls upwards, forming a distinct lip. Once exposed like this, the lip cooks faster than the base, which is insulated by the cheese and crust, and thus crisps and renders its fat faster.\n","permalink":"/food.html","tags":null,"title":"Food"},{"categories":null,"contents":" Emacs, the world-class, extensible, customisable, free/libre text editor, has stood the test of time and continues to be a powerful tool for both developers and writers alike. Its reputation for complexity belies its potential: it is an undiscovered country, a treasure chest filled with gold, jewels, and priceless artifacts. Embarking on a journey with Emacs is much like setting sail on the high seas of productivity, and in this guide, we aim to help you navigate these waters with ease.\nThis guide is a treasure map, of sorts, to help you chart your course in the vast ocean of Emacs functionalities. We will help you equip your Emacs ship with an arsenal of powerful tools and configurations to bolster your productivity, simplify your workflow, and make your voyage smoother. From the helm to the cannon, from the crow\u0026rsquo;s nest to the rigging, you\u0026rsquo;ll find that your Emacs ship will be ready for whatever storm the high seas might throw at you.\nWhether you\u0026rsquo;re a seasoned Emacs sailor or a landlubber who\u0026rsquo;s only just beginning to learn the ropes, this guide is intended to help you tailor your Emacs experience to your needs and preferences. By the end of our journey together, you\u0026rsquo;ll have transformed your Emacs from a humble sailboat to a powerful, fully-equipped galleon, ready to tackle the toughest coding or writing challenges you might face.\nSo, hoist the Jolly Roger, set your course, and let\u0026rsquo;s set sail on the open sea of Emacs optimisation!\nChapter 1: Sailing the SeasPackages: The Mariner\u0026rsquo;s Charts and ToolsOur maiden voyage begins by setting our charts and readying our tools. Just as a ship needs maps to navigate the high seas, so do we need the MELPA1 repository, our trove of packages to steer Emacs.\nAdd the following markings to your captain\u0026rsquo;s log (init.el), to invite MELPA onboard and ensure our tools are always shiny and ready. For every new dawn we sail, our package list shall be refreshed.\n(require \u0026#39;package) (setq package-archives \u0026#39;((\u0026#34;melpa\u0026#34; . \u0026#34;https://melpa.org/packages/\u0026#34;) (\u0026#34;org\u0026#34; . \u0026#34;http://orgmode.org/elpa/\u0026#34;) (\u0026#34;gnu\u0026#34; . \u0026#34;http://elpa.gnu.org/packages/\u0026#34;))) (package-initialize) (unless package-archive-contents (package-refresh-contents)) Evil Mode: Hoisting the Jolly Roger of EditingOur ship now sails under the black flag of \u0026ldquo;Evil mode\u0026rdquo;2, an homage to the legendary seafarers of Vim, allowing us to navigate our text with precision and ease. Heave the following ropes into your init.el logbook to hoist the black flag.\n(require \u0026#39;package) (setq package-archives \u0026#39;((\u0026#34;melpa\u0026#34; . \u0026#34;https://melpa.org/packages/\u0026#34;) (\u0026#34;org\u0026#34; . \u0026#34;http://orgmode.org/elpa/\u0026#34;) (\u0026#34;gnu\u0026#34; . \u0026#34;http://elpa.gnu.org/packages/\u0026#34;))) (package-initialize) (unless package-archive-contents (package-refresh-contents)) (unless (package-installed-p \u0026#39;evil) (package-install \u0026#39;evil)) (require \u0026#39;evil) (evil-mode 1) Window Size: Calibrating the SpyglassOur ship\u0026rsquo;s porthole is our window into the vast sea of code. Currently, it\u0026rsquo;s too small, like a miser\u0026rsquo;s eyepiece. We can widen our view with the following enchantment:\n(require \u0026#39;package) (setq package-archives \u0026#39;((\u0026#34;melpa\u0026#34; . \u0026#34;https://melpa.org/packages/\u0026#34;) (\u0026#34;org\u0026#34; . \u0026#34;http://orgmode.org/elpa/\u0026#34;) (\u0026#34;gnu\u0026#34; . \u0026#34;http://elpa.gnu.org/packages/\u0026#34;))) (package-initialize) (unless package-archive-contents (package-refresh-contents)) (unless (package-installed-p \u0026#39;evil) (package-install \u0026#39;evil)) (require \u0026#39;evil) (evil-mode 1) (add-to-list \u0026#39;default-frame-alist \u0026#39;(height . 40)) (add-to-list \u0026#39;default-frame-alist \u0026#39;(width . 90)) For the sake of our journey, we won\u0026rsquo;t veer too far off course, but remember, the horizon is yours to command.\nFonts: Inking the Captain\u0026rsquo;s QuillOur final touch for this chapter of the voyage, we shall imbue the ship with a touch of elegance, by changing the ship\u0026rsquo;s lettering, the font of all frames. Here, we choose the scribe\u0026rsquo;s favorite, \u0026ldquo;Fira Code\u0026rdquo;.\n(require \u0026#39;package) (setq package-archives \u0026#39;((\u0026#34;melpa\u0026#34; . \u0026#34;https://melpa.org/packages/\u0026#34;) (\u0026#34;org\u0026#34; . \u0026#34;http://orgmode.org/elpa/\u0026#34;) (\u0026#34;gnu\u0026#34; . \u0026#34;http://elpa.gnu.org/packages/\u0026#34;))) (package-initialize) (unless package-archive-contents (package-refresh-contents)) (unless ( package-installed-p \u0026#39;evil) (package-install \u0026#39;evil)) (require \u0026#39;evil) (evil-mode 1) (add-to-list \u0026#39;default-frame-alist \u0026#39;(height . 40)) (add-to-list \u0026#39;default-frame-alist \u0026#39;(width . 90)) (set-frame-font \u0026#34;Fira Code-12\u0026#34; nil t) Tool bar and menu bar: The Ship\u0026rsquo;s Helm and MainsailKeep your helm clean by removing unnecessary clutter such as the default tool bar and menu bar. This can be achieved with a couple of simple commands.\n(require \u0026#39;package) (setq package-archives \u0026#39;((\u0026#34;melpa\u0026#34; . \u0026#34;https://melpa.org/packages/\u0026#34;) (\u0026#34;org\u0026#34; . \u0026#34;http://orgmode.org/elpa/\u0026#34;) (\u0026#34;gnu\u0026#34; . \u0026#34;http://elpa.gnu.org/packages/\u0026#34;))) (package-initialize) (unless package-archive-contents (package-refresh-contents)) (unless (package-installed-p \u0026#39;evil) (package-install \u0026#39;evil)) (require \u0026#39;evil) (evil-mode 1) (add-to-list \u0026#39;default-frame-alist \u0026#39;(height . 40)) (add-to-list \u0026#39;default-frame-alist \u0026#39;(width . 90)) (set-frame-font \u0026#34;Fira Code-12\u0026#34; nil t) ;; Remove the menu bar and tool bar (tool-bar-mode -1) (menu-bar-mode -1) Projectile: The Cannon of Project NavigationElevate your project management skills by utilising Projectile3. This tool not only helps in navigating projects but can also be customised to use dired4 for opening files in a project. By default, Projectile uses whatever method it thinks is most appropriate for opening files in a project, but we want to use dired. With this configuration, whenever you switch to a project, Projectile will open the project\u0026rsquo;s root in dired.\n;; ... same as before ;; Projectile (unless (package-installed-p \u0026#39;projectile) (package-install \u0026#39;projectile)) (require \u0026#39;projectile) (projectile-mode +1) (setq projectile-switch-project-action \u0026#39;projectile-dired) Mini-frame: The Crow\u0026rsquo;s Nest of DisplayObserve your Emacs environment from a higher perspective with the mini-frame. Install it and place it at the top for an optimal viewing experience.\n;; Mini-frame (unless (package-installed-p \u0026#39;mini-frame) (package-install \u0026#39;mini-frame)) (require \u0026#39;mini-frame) (mini-frame-mode 1) (setq mini-frame-show-parameters \u0026#39;((top . 0) (width . 0.7) (left . 0.5))) Ivy: The Rigging of CompletionIvy5 is your personal assistant for completion in Emacs. Enhance its functionalities by integrating ivy-posframe6 which displays Ivy in a child frame. Here we\u0026rsquo;ll install both packages and configure them.\n;; Ivy support (unless (package-installed-p \u0026#39;ivy) (package-install \u0026#39;ivy)) (unless (package-installed-p \u0026#39;ivy-posframe) (package-install \u0026#39;ivy-posframe)) (require \u0026#39;ivy) (ivy-mode 1) (require \u0026#39;ivy-posframe) (setq ivy-display-function #\u0026#39;ivy-posframe-display-at-frame-center) (setq ivy-posframe-display-functions-alist \u0026#39;((t . ivy-posframe-display-at-frame-center))) (ivy-posframe-mode 1) Doom modeline: The Jolly Roger of Status LinesNow we install an improved modeline, which is the Doom modeline7.\n;; Doom modeline (unless (package-installed-p \u0026#39;doom-modeline) (package-install \u0026#39;doom-modeline)) (require \u0026#39;doom-modeline) (doom-modeline-mode 1) Startup projects: The Voyages BeginningsAlways keep your projects within reach by displaying a list of recent projects every time Emacs starts. To do this, we tell it to remember your recent projects. The alien indexing method will cause Projectile to use external utilities like git to provide a speed boost. The ivy completion system will use Ivy for project selection. A function creates a new buffer called *Projectile Projects*, erases any existing content in it, inserts a list of the projects known to Projectile, and displays the buffer.\nThe new \u0026ldquo;Projectile\u0026rdquo; section will look like this:\n;; Projectile (unless (package-installed-p \u0026#39;projectile) (package-install \u0026#39;projectile)) (require \u0026#39;projectile) ;;; Enable Projectile and configure it to remember recent projects (projectile-mode +1) (setq projectile-switch-project-action \u0026#39;projectile-dired) (setq projectile-indexing-method \u0026#39;alien) (setq projectile-completion-system \u0026#39;ivy) ;;; Create a function to display recent projects on startup (defun display-projectile-list-on-startup () (let ((buf (get-buffer-create \u0026#34;*Projectile Projects*\u0026#34;))) (with-current-buffer buf (erase-buffer) (dolist (project (projectile-relevant-known-projects)) (insert project \u0026#34;\\n\u0026#34;))) (switch-to-buffer buf))) The page is still quite bare-bones, but we\u0026rsquo;ll add more features as we go along.\nMarkdown: The Sea Chart of FormattingSince we\u0026rsquo;re writing this page in Markdown, let\u0026rsquo;s go ahead and install a Markdown mode8. We\u0026rsquo;ll install a Markdown major mode, a TOC generation package, and a formatting package. We also need to tell Emacs to use the Markdown mode for files with the .md extension.\n;; Markdown (dolist (package \u0026#39;(markdown-mode markdown-toc markdownfmt)) (unless (package-installed-p package) (package-install package))) (require \u0026#39;markdown-mode) (add-to-list \u0026#39;auto-mode-alist \u0026#39;(\u0026#34;\\\\.md\\\\\u0026#39;\u0026#34; . markdown-mode)) But \u0026hellip; wait. The code blocks are ugly and do not have syntax highlighting. To fix this, we first enable global font lock mode, which will enable syntax highlighting for all major modes. Then we tell Emacs to use the markdown-fontify-code-blocks-natively function to highlight code blocks. markdown-mode should automatically use syntax highlighting for code blocks, but it depends on the major mode associated with the language used in the code block. We also have to make sure the major mode for that language is installed. For example, if you have a Python code block, you would need python-mode (actually, this also comes with Emacs).\n;;; Enable font-lock globally (syntax highlighting for code blocks) (global-font-lock-mode 1) ;; ... (setq markdown-fontify-code-blocks-natively t) Looks better now!\nScrolling conservatively: The Steady Course of ViewingBy default, Emacs scrolls by nearly \u0026ldquo;full screens\u0026rdquo; (the exact behavior is influenced by the scroll-step and scroll-conservatively variables).\nTo make Emacs scroll line by line, you can set the scroll-conservatively variable to a high value. You can add the following line to your init.el file:\n(setq scroll-conservatively 101) The number 101 is somewhat arbitrary, it just needs to be greater than 100. When scroll-conservatively is set to a value greater than 100, Emacs will always keep the cursor at least that many lines from the top and bottom of the window, effectively causing it to scroll one line at a time.\nSpeedbar: The Swift Current of ShortcutsOne of my favourite features in Doom Emacs is the speedbar. Doom Emacs uses a package named which-key to provide this and it is a package that displays available keybindings in a popup.\nThis package is very handy to discover available keybindings and to learn new ones. which-key-mode makes it so that whenever you start a key sequence, and then wait for a short period of time, which-key will pop up a buffer with all possible completions of the key sequence. This sequence (sometimes called the \u0026ldquo;leader key\u0026rdquo;) will be set in the config as M-SPC.\nWe will also use general.el, a package that provides a more convenient way to define key bindings. The first keybindings we will define is the p for a \u0026ldquo;projects\u0026rdquo; section and f to find all files in a project.\n;; Speed bar (dolist (package \u0026#39;(which-key general)) (unless (package-installed-p package) (package-install package))) (require \u0026#39;which-key) (which-key-mode) (require \u0026#39;general) (general-create-definer my-leader-def :states \u0026#39;(normal visual insert emacs) :prefix \u0026#34;M-SPC\u0026#34; :non-normal-prefix \u0026#34;M-SPC\u0026#34;) (my-leader-def \u0026#34;p\u0026#34; \u0026#39;(:ignore t :which-key \u0026#34;projects\u0026#34;) \u0026#34;pf\u0026#34; \u0026#39;projectile-find-file) (setq which-key-idle-delay 0.5) ;; default 0.5 A Few Cosmetic Changes: Polishing the Captain\u0026rsquo;s QuartersWe will now do a few cosmetic changes. The theme we will use is modus-operandi9. This theme is bundled with Emacs 27, so we don\u0026rsquo;t need to install it. We will also change the font in the Markdown code blocks to the same as the main font. Additionally, we will add syntax highlighting in Markdown mode to wikilinks.\n;; Font name (defvar my-font-name \u0026#34;Fira Code\u0026#34;) (set-frame-font (concat my-font-name \u0026#34;-12\u0026#34;) nil t) (set-face-attribute \u0026#39;fixed-pitch nil :family my-font-name :height 120) ;;; ... (setq markdown-enable-wiki-links t) ;; Match Markdown font main one (set-face-attribute \u0026#39;markdown-code-face nil :family my-font-name) (set-face-attribute \u0026#39;markdown-pre-face nil :family my-font-name) ;;; ... ;; Modus operandi (load-theme \u0026#39;modus-operandi t) Buffer splitting: Dividing the Ships LogSomething I use quite offen is splitting the window vertically or horizontally. In addition, if I\u0026rsquo;m inside a project, I want the new window to be in the same project and show me a list of files so that I can easily open one of them. To do this, we define two functions, one for vertical splitting and one for horizontal splitting and assign to keys for the speedbar. The keys will be M-SPC b s r (Buffers -\u0026gt; split -\u0026gt; right) and M-SPC b s t (Buffers -\u0026gt; split -\u0026gt; top) for vertical and horizontal splitting, respectively.\n;; Buffer splitting (defun split-right-and-list-project-files () \u0026#34;Split the current window into two, left and right. The left one contains the current contents and the right one contains a projectile-list of all the files in the project.\u0026#34; (interactive) (split-window-right) (other-window 1) (projectile-find-file)) (defun split-below-and-list-project-files () \u0026#34;Split the current window into two, top and bottom. The top one contains the current contents and the bottom one contains a projectile-list of all the files in the project.\u0026#34; (interactive) (split-window-below) (other-window 1) (projectile-find-file)) (my-leader-def \u0026#34;b\u0026#34; \u0026#39;(:ignore t :which-key \u0026#34;Buffers\u0026#34;) \u0026#34;bs\u0026#34; \u0026#39;(:ignore t :which-key \u0026#34;Split\u0026#34;) \u0026#34;bsr\u0026#34; \u0026#39;(split-right-and-list-project-files :which-key \u0026#34;Right split\u0026#34;) \u0026#34;bst\u0026#34; \u0026#39;(split-below-and-list-project-files :which-key \u0026#34;Top split\u0026#34;)) Chapter 2: The Archipelago of LanguagesRust: The Ironclad IslandTo add support for Rust we use the rustic10 package. rustic provides a more comprehensive environment for Rust development essentially combinating several packages (rust-mode, cargo, flycheck, lsp-mode, etc.) providing syntax highlighting, formatting, linting, running Cargo commands and so on.\nWe also will use rustic-lsp-server with Rust Analyzer11 as the language server (altough RLS is also supported).\ncompany will be added for autocompletion and rustic-lsp-server\u0026rsquo;s inlay type hints will be enabled.\nrustic will have auto-formatting on save enabled.\n;; Rust (unless (package-installed-p \u0026#39;rustic) (package-install \u0026#39;rustic)) (require \u0026#39;rustic) ;; use rust-analyzer as the LSP server (setq rustic-lsp-server \u0026#39;rust-analyzer) ;;; Company (unless (package-installed-p \u0026#39;company) (package-install \u0026#39;company)) (require \u0026#39;company) ;; Enable company mode in all buffers (add-hook \u0026#39;after-init-hook \u0026#39;global-company-mode) ;; Set company backends for lsp-mode (push \u0026#39;company-capf company-backends) ;;; lsp-mode (unless (package-installed-p \u0026#39;lsp-mode) (package-install \u0026#39;lsp-mode)) (require \u0026#39;lsp-mode) ;; Enable lsp-mode in rustic-mode (add-hook \u0026#39;rustic-mode-hook #\u0026#39;lsp) ;; Use company-capf as the completion provider (add-hook \u0026#39;lsp-mode-hook (lambda () (set (make-local-variable \u0026#39;company-backends) \u0026#39;(company-capf)))) ;; Enable rustfmt on save (setq rustic-format-on-save t) ;; Enable inlay hints (setq lsp-rust-analyzer-display-inlay-hints t) (setq lsp-inlay-hints-mode t) We will also add keybindings for common Rust/Cargo commands, such a build, run, test, format, check and clippy.\n;;; Speedbar for Rust (my-leader-def :states \u0026#39;(normal visual emacs) :keymaps \u0026#39;override \u0026#34;l\u0026#34; \u0026#39;(:ignore t :which-key \u0026#34;Language\u0026#34;) \u0026#34;r\u0026#34; \u0026#39;(:ignore t :which-key \u0026#34;Rust\u0026#34;)) (my-leader-def :states \u0026#39;(normal visual emacs) :keymaps \u0026#39;override \u0026#34;l r b\u0026#34; \u0026#39;(rustic-cargo-build :which-key \u0026#34;Build\u0026#34;) \u0026#34;l r r\u0026#34; \u0026#39;(rustic-cargo-run :which-key \u0026#34;Run\u0026#34;) \u0026#34;l r t\u0026#34; \u0026#39;(rustic-cargo-test :which-key \u0026#34;Test\u0026#34;) \u0026#34;l r f\u0026#34; \u0026#39;(rustic-cargo-fmt :which-key \u0026#34;Format\u0026#34;) \u0026#34;l r c\u0026#34; \u0026#39;(rustic-cargo-check :which-key \u0026#34;Check\u0026#34;) \u0026#34;l r p\u0026#34; \u0026#39;(rustic-cargo-clippy :which-key \u0026#34;Clippy\u0026#34;)) Python: The Serpent\u0026rsquo;s AtollIn this section, we make Python programming in Emacs a breeze by integrating the Elpy12 package. Elpy adds a multitude of enhancements to the Python editing experience, including code navigation, automatic formatting, refactoring helpers, and more.\nFirst, we check if Elpy is installed and install it if necessary:\n(unless (package-installed-p \u0026#39;elpy) (package-install \u0026#39;elpy)) (require \u0026#39;elpy) Next, we enable Elpy for all Python buffers and configure IPython to be the default Python interpreter:\n(elpy-enable) (setq python-shell-interpreter \u0026#34;ipython\u0026#34; python-shell-interpreter-args \u0026#34;-i --simple-prompt\u0026#34;) We also specify the Python 3 interpreter for Elpy\u0026rsquo;s RPC and enable the Python Language Server Protocol (LSP) for Python mode:\n(setq elpy-rpc-python-command \u0026#34;python3\u0026#34;) (add-hook \u0026#39;python-mode-hook #\u0026#39;lsp-deferred) (setq lsp-python-ms-python-executable-cmd \u0026#34;python3\u0026#34;) Flycheck13, a modern syntax checking package, is then configured to replace Elpy\u0026rsquo;s default Flymake:\n(when (require \u0026#39;flycheck nil t) (setq elpy-modules (delq \u0026#39;elpy-module-flymake elpy-modules)) (add-hook \u0026#39;elpy-mode-hook \u0026#39;flycheck-mode)) Our scripting journey gets even more exciting as we set sail to the land of Virtual Environments. We define a function set-python-virtualenv that automatically sets the Python virtual environment based on the Projectile project root:\n(defun set-python-virtualenv () \u0026#34;Set `python-shell-virtualenv-root\u0026#39; based on projectile\u0026#39;s project root.\u0026#34; (interactive) ... ) (add-hook \u0026#39;python-mode-hook \u0026#39;set-python-virtualenv) Lastly, we bind the treasure map (which-key) with commonly used Python commands, helping us navigate the Python seas smoothly:\n(my-leader-def :states \u0026#39;(normal visual emacs) :keymaps \u0026#39;override \u0026#34;lp\u0026#34; \u0026#39;(:ignore t :which-key \u0026#34;Python\u0026#34;) \u0026#34;lpv\u0026#34; \u0026#39;(set-python-virtualenv :which-key \u0026#34;Set Virtualenv\u0026#34;) \u0026#34;lpr\u0026#34; \u0026#39;(run-python :which-key \u0026#34;Run Python REPL\u0026#34;) \u0026#34;lpf\u0026#34; \u0026#39;(python-format-buffer :which-key \u0026#34;Format buffer\u0026#34;) \u0026#34;lpd\u0026#34; \u0026#39;(elpy-goto-definition :which-key \u0026#34;Go to definition\u0026#34;)) This marks the end of our Python setup chapter. We have set up a powerful Python development environment in Emacs, complete with syntax checking, virtual environment support, and interactive Python commands, all just a few key presses away. With these tools at hand, your Python adventure in Emacs will be a sailing pleasure!\nGo: The Swift Current IsleAs our adventure in this programming environment continues, we delve into the rich ecosystem of Go, bringing out its full potential in Emacs.\nThe story begins with the installation of the go-mode14 package. If the package isn\u0026rsquo;t already part of the inventory (installed), our script takes the initiative to acquire it:\n(unless (package-installed-p \u0026#39;go-mode) (package-install \u0026#39;go-mode)) Following this, we require or load the go-mode package to prepare Emacs for Go development:\n(require \u0026#39;go-mode) The support crew, company mode, is called upon to enhance the user experience by providing autocompletion features globally:\n(add-hook \u0026#39;after-init-hook \u0026#39;global-company-mode) Next, Language Server Protocol (LSP) support is hooked into go-mode to offer a plethora of language features like linting, code navigation, and much more:\n(add-hook \u0026#39;go-mode-hook #\u0026#39;lsp-deferred) (setq lsp-gopls-server-path (expand-file-name \u0026#34;~/go/bin/gopls\u0026#34;)) (setq lsp-enable-snippet nil) ;; disable snippet support (setq lsp-gopls-staticcheck t) ;; enable gopls\u0026#39; staticcheck linter (setq lsp-eldoc-render-all t) ;; display documentation in minibuffer while typing To keep our code neat and imports organised, gofmt (set to goimports) is triggered before every save. Additional save hooks are also set up to automatically format the buffer and organise imports on saving a Go file:\n(setq gofmt-command \u0026#34;goimports\u0026#34;) (add-hook \u0026#39;before-save-hook \u0026#39;gofmt-before-save) (defun lsp-go-install-save-hooks () (add-hook \u0026#39;before-save-hook #\u0026#39;lsp-format-buffer t t) (add-hook \u0026#39;before-save-hook #\u0026#39;lsp-organize-imports t t)) (add-hook \u0026#39;go-mode-hook #\u0026#39;lsp-go-install-save-hooks) The adventure wouldn\u0026rsquo;t be complete without a handy map and compass. So, we set up key bindings (using which-key) under M-SPC l g for Go:\n(my-leader-def :states \u0026#39;(normal visual emacs) :keymaps \u0026#39;override \u0026#34;lg\u0026#34; \u0026#39;(:ignore t :which-key \u0026#34;Go\u0026#34;) \u0026#34;lgf\u0026#34; \u0026#39;(gofmt :which-key \u0026#34;Format code\u0026#34;) \u0026#34;lgt\u0026#34; \u0026#39;(compile :which-key \u0026#34;Run test\u0026#34;) \u0026#34;lgi\u0026#34; \u0026#39;(lsp-goto-implementation :which-key \u0026#34;Go to implementation\u0026#34;) \u0026#34;lgd\u0026#34; \u0026#39;(lsp-find-definition :which-key \u0026#34;Go to definition\u0026#34;) \u0026#34;lgr\u0026#34; \u0026#39;(lsp-find-references :which-key \u0026#34;Find references\u0026#34;) \u0026#34;lgh\u0026#34; \u0026#39;(lsp-describe-thing-at-point :which-key \u0026#34;Describe thing at point\u0026#34;)) This setup offers a more navigable and interactive environment, enabling you to journey through your Go code like an expert adventurer, efficiently uncovering the treasures of your project.\nYAML: Charting the Indentation Ocean ;; First, we ensure that the yaml-mode package is installed. If not, Emacs is instructed to download and install it. (unless (package-installed-p \u0026#39;yaml-mode) (package-refresh-contents) (package-install \u0026#39;yaml-mode)) With the indispensable yaml-mode installed and readied, our vessel for navigating the YAML seas, we now link it with every YAML file we encounter during our journey.\n(require \u0026#39;yaml-mode) (add-to-list \u0026#39;auto-mode-alist \u0026#39;(\u0026#34;\\\\.ya?ml\\\\\u0026#39;\u0026#34; . yaml-mode)) One of the quirks of our YAML sea is the importance of proper indentation. But worry not! We\u0026rsquo;ve equipped our ship with an auto-indenting feature. Whenever you press the enter key (C-m), a new line is created, and the cursor is automatically positioned with the correct indentation.\n(add-hook \u0026#39;yaml-mode-hook (lambda () (define-key yaml-mode-map \u0026#34;\\C-m\u0026#34; \u0026#39;newline-and-indent))) Our journey in the YAML sea often involves vast and complex structures. To make our journey manageable, we employ code folding, a valuable map that allows us to hide and reveal portions of our code as needed.\n(add-hook \u0026#39;yaml-mode-hook (lambda () (hs-minor-mode 1))) As we chart our course, we also enable company-mode to provide us with suggestions, turning our journey into a collective effort with a supportive crew.\n(add-hook \u0026#39;yaml-mode-hook \u0026#39;company-mode) We\u0026rsquo;re not alone on these seas. To keep us from straying off course, we enlist the help of a yamllint lookout in our crow\u0026rsquo;s nest, alerting us whenever we deviate from the ideal YAML structure. Before we embark, ensure you\u0026rsquo;ve enlisted yamllint by installing it with pip install yamllint.\n;; Add YAML lint. Must have `pip install yamllint` installed. (add-hook \u0026#39;yaml-mode-hook \u0026#39;flycheck-mode) Lastly, to aid our navigation through the layers of indentation, we make use of indent-guide. Like a dotted line in the ocean, it illuminates our path, making the structure of our YAML document clear and visible.\n(unless (package-installed-p \u0026#39;indent-guide) (package-install \u0026#39;indent-guide)) (require \u0026#39; indent-guide) (add-hook \u0026#39;yaml-mode-hook \u0026#39;indent-guide-mode) (set-face-foreground \u0026#39;indent-guide-face \u0026#34;lightgray\u0026#34;) (set-face-background \u0026#39;indent-guide-face nil) (setq indent-guide-char \u0026#34;:\u0026#34;) With these tools at our disposal, our journey through the YAML seas will be a breeze, and we\u0026rsquo;ll be able to navigate the waves of indentation with ease and confidence.\nChapter 3: In the Hold: The Craftsman\u0026rsquo;s KitNavigating the Git Galleon: Version Control on the High SeasOnce you have set sail on the vast seas of code, it is crucial to wield the power of version control to navigate your way through the ever-changing tides of development. Git, the legendary version control system, will become your trusted companion on this adventurous journey. In this chapter, we will equip ourselves with the tools and knowledge to master Git and unlock its hidden treasures.\nTo harness the power of Git within Emacs, we need to gather the necessary tools. Let\u0026rsquo;s start by installing the evil-collection package, which provides Evil keybindings for various Emacs modes:\n(unless (package-installed-p \u0026#39;evil-collection) (package-install \u0026#39;evil-collection)) (require \u0026#39;evil-collection) Evil Collection enhances your Emacs experience by bringing Evil keybindings to Magit, among other modes.\nThe heart of our Git journey lies within Magit15, the legendary Git interface for Emacs. We must install the magit package to summon its power:\n(unless (package-installed-p \u0026#39;magit) (package-install \u0026#39;magit)) Magit will serve as our compass, guiding us through the treacherous waters of version control.\nTo ensure that our Git knowledge permeates various programming realms, we will bind the magit-refresh-status command to our modes of choice. Let\u0026rsquo;s create a list of modes and add the hook to each one:\n(setq my-modes \u0026#39;(rust-mode-hook python-mode-hook go-mode-hook markdown-mode-hook yaml-mode-hook)) (mapc (lambda (mode) (add-hook mode (lambda () (add-hook \u0026#39;after-save-hook \u0026#39;magit-refresh-status)))) my-modes) By doing so, we guarantee that whenever we save a file in one of these modes, Magit will refresh its status, keeping us informed about the state of our repositories.\nOur quest wouldn\u0026rsquo;t be complete without discovering the hidden repositories lurking in the shadows. With the power of Projectile, we can uncover their secrets. Let\u0026rsquo;s map over the known projects, filter out those without a .git directory, and prepare our repository list:\n(setq magit-repo-dirs (mapcar (lambda (dir) (substring dir 0 -1)) (cl-remove-if-not (lambda (project) (unless (file-remote-p project) (file-directory-p (concat project \u0026#34;/.git/\u0026#34;)))) (projectile-relevant-known-projects)))) Now, we possess a list of repositories, each ready to be explored and conquered.\nWith our arsenal fully assembled, it\u0026rsquo;s time to unleash the power of Git with our custom keybindings. Using the my-leader-def macro, we can assign keybindings that will guide us through the Git seas:\n(my-leader-def :states \u0026#39;(normal visual emacs) :keymaps \u0026#39;override \u0026#34;g\u0026#34; \u0026#39;(:ignore t :which-key \u0026#34;Git\u0026#34;) \u0026#34;gs\u0026#34; \u0026#39;(magit-status :which-key \u0026#34;Show status\u0026#34;) \u0026#34;gb\u0026#34; \u0026#39;(magit-branch :which-key \u0026#34;Show all branches\u0026#34;) \u0026#34;gco\u0026#34; \u0026#39;(magit-checkout :which-key \u0026#34;Checkout branch\u0026#34;) \u0026#34;gr\u0026#34; \u0026#39;(magit-rebase :which-key \u0026#34;Rebase on a branch\u0026#34;) \u0026#34;gsq\u0026#34; \u0026#39;(magit-rebase-squash :which -key \u0026#34;Squash commits\u0026#34;) \u0026#34;gc\u0026#34; \u0026#39;(magit-commit :which-key \u0026#34;Commit\u0026#34;) \u0026#34;gp\u0026#34; \u0026#39;(magit-push-popup :which-key \u0026#34;Push\u0026#34;)) These keybindings will be your guiding stars as you navigate through Git\u0026rsquo;s vast expanse, allowing you to perform actions like displaying the repository status, managing branches, checking out branches, rebasing, squashing commits, committing changes, and pushing to remote repositories.\nSometimes, even in the face of adversity, we encounter unexpected challenges. In this case, the unbound suffix ':' error arises when trying to access Evil\u0026rsquo;s evil-ex prompt. Fear not, for we shall reclaim our territory:\n(with-eval-after-load \u0026#39;evil-maps (define-key evil-motion-state-map (kbd \u0026#34;:\u0026#34;) \u0026#39;evil-ex)) (defun my-magit-setup () (define-key evil-normal-state-local-map (kbd \u0026#34;:\u0026#34;) \u0026#39;evil-ex)) (add-hook \u0026#39;magit-status-mode-hook \u0026#39;my-magit-setup) With these configurations, we restore the full power of Evil\u0026rsquo;s evil-ex prompt, allowing us to issue commands fearlessly.\nTo ensure a smooth voyage, we must tame the unseen forces that could disrupt our Git journey. One such force is the \u0026ldquo;unbound suffix \u0026lt;mouse movement\u0026gt;\u0026rdquo; error. By advising the magit-key-mode-error-message function, we can suppress this error when it\u0026rsquo;s triggered by mouse movement:\n(defun ignore-mouse (ret) (let ((debug-on-message \u0026#34;unbound suffix \u0026lt;mouse movement\u0026gt;\u0026#34;)) (if (called-interactively-p \u0026#39;interactive) ret (ignore-errors (funcall ret))))) (advice-add \u0026#39;magit-key-mode-error-message :around \u0026#39;ignore-mouse) With this protection in place, you can navigate Magit buffers using your mouse without being interrupted by the \u0026ldquo;unbound suffix\u0026rdquo; error.\nCongratulations! With these Git tools at your disposal, you are now equipped to embark on your version control adventures. May you sail through the Git seas with confidence and mastery, leaving a trail of impeccable code in your wake.\nKubernetes: Commanding the Container FleetOnce upon a time, in the deep blue Emacs sea, there was a brave adventurer on a quest to tame the fearsome beast of Kubernetes. The tale begins with the collection of tools required for the daring voyage.\n(unless (package-installed-p \u0026#39;kubernetes) (package-install \u0026#39;kubernetes)) (require \u0026#39;kubernetes) (with-eval-after-load \u0026#39;kubernetes (unless (package-installed-p \u0026#39;kubernetes-evil) (package-refresh-contents) (package-install \u0026#39;kubernetes-evil))) Our mariner, with the trusty map of package-installed-p, first ensured the company of the mighty warrior, kubernetes. When found missing, package-install was invoked to summon the warrior. The brave kubernetes16 was then made to pledge allegiance to the mariner. A similar course of action was taken with the wily mercenary, kubernetes-evil, ensuring a formidable party for the adventure ahead.\n(setq kubernetes-parent-directories \u0026#39;(\u0026#34;k8s\u0026#34; \u0026#34;manifests\u0026#34; \u0026#34;config\u0026#34;)) With the team assembled, the mariner set out on the mighty vessel, seeking out the treacherous terrains named k8s, manifests, and config. The name of these lands was encrypted in a secret scroll labeled kubernetes-parent-directories.\n;; Function to check if a parent directory exists (defun kubernetes-enable-mode-if-parent-directory-exists () (let ((parent-dir (file-name-nondirectory (directory-file-name (file-name-directory buffer-file-name))))) (when (member parent-dir kubernetes-parent-directories) (kubernetes-mode)))) ;; Add a hook to enable kubernetes-mode based on parent directory (add-hook \u0026#39;yaml-mode-hook \u0026#39;kubernetes-enable-mode-if-parent-directory-exists) With a clever device - a hook paired with a function, our mariner ensured that upon sailing through the stormy YAML sea, the presence of one of the desired lands triggered the transformation of the ship, morphing it into a kubernetes-mode leviathan, ready to face any storm.\n(setq kubernetes-namespace \u0026#34;default\u0026#34;) As they ventured forth, the mariner christened their current realm as the default namespace - their home in the chaotic sea, their sanctuary amidst the turmoil.\n;; Function to deploy the current buffer in Kubernetes (defun kubernetes-deploy-buffer () \u0026#34;Deploy the currently opened buffer using kubectl apply -f.\u0026#34; (interactive) (let ((file (buffer-file-name))) (if (not (and file (file-exists-p file))) (message \u0026#34;Buffer is not visiting a file.\u0026#34;) (async-shell-command (concat \u0026#34;kubectl apply -f \u0026#34; file))))) Armed with a magical charm, the kubernetes-deploy-buffer, the mariner could summon their mighty forces into action at will. A mere invocation was enough to deploy the fleet on the battlefield.\n;; Function to list Kubernetes namespaces and switch to selected namespace (defun kubernetes-switch-namespace () \u0026#34;List available namespaces and switch to selected namespace.\u0026#34; (interactive) (let ((output-buffer (get-buffer-create \u0026#34;*kubernetes-namespace*\u0026#34;))) (with-current-buffer output-buffer (erase-buffer) (call-process \u0026#34;kubectl\u0026#34; nil t nil \u0026#34;get\u0026#34; \u0026#34;namespaces\u0026#34;)) (let ((namespace (completing-read \u0026#34;Switch to namespace: \u0026#34; (split-string (with-current-buffer output-buffer (buffer-string)) \u0026#34;\\n\u0026#34; t)))) (unless (string-empty-p namespace) (async-shell-command (concat \u0026#34;kubectl config set-context --current --namespace=\u0026#34; namespace)) (message \u0026#34;Switched to namespace: %s\u0026#34; namespace))))) The mariner also possessed the mystical power to switch their ship to any other namespace with the spell, kubernetes-switch-namespace. This allowed them to nimbly navigate through the whirlpools of the Kube-sea.\n(my-leader-def :states \u0026#39;(normal visual emacs) :keymaps \u0026#39;override \u0026#34;k\u0026#34; \u0026#39;(:ignore t :which-key \u0026#34;Kubernetes\u0026#34;) \u0026#34;kf\u0026#34; \u0026#39;(kubernetes-forward-resource :which-key \u0026#34;Forward resource\u0026#34;) \u0026#34;kb\u0026#34; \u0026#39;(kubernetes-backward-resource :which-key \u0026#34;Backward resource\u0026#34;) \u0026#34;kcr\u0026#34; \u0026#39;(kubernetes-create-resource :which-key \u0026#34;Create resource\u0026#34;) \u0026#34;kdel\u0026#34; \u0026#39;(kubernetes-delete-resource :which-key \u0026#34;Delete resource\u0026#34;) \u0026#34;ked\u0026#34; \u0026#39;(kubernetes-describe-resource :which-key \u0026#34;Describe resource\u0026#34;) \u0026#34;kex\u0026#34; \u0026#39;(kubernetes-exec-popup :which-key \u0026#34;Execute command in resource\u0026#34;) \u0026#34;kl\u0026#34; \u0026#39;(kubernetes-logs-popup :which-key \u0026#34;View resource logs\u0026#34;) \u0026#34;kp\u0026#34; \u0026#39;(kubernetes-pods :which-key \u0026#34;List pods\u0026#34;) \u0026#34;ks\u0026#34; \u0026#39;(kubernetes-set-namespace :which-key \u0026#34;Set namespace\u0026#34;) \u0026#34;kd\u0026#34; \u0026#39;(kubernetes-deploy-buffer :which-key \u0026#34;Deploy buffer\u0026#34;) \u0026#34;kn\u0026#34; \u0026#39;(kubernetes-switch-namespace :which-key \u0026#34;Switch namespace\u0026#34;) \u0026#34;ko\u0026#34; \u0026#39;(kubernetes-overview :which-key \u0026#34;Kubernetes overview\u0026#34;)) Lastly, a grand tapestry was woven, a guide to the whole adventure. Each spell, each charm, and each command were carefully etched onto this map, creating a comprehensive guide for our brave mariner.\nThis tale, fellow Emacs sailor, is one of thrill, exploration, and conquest, sailing the tumultuous Kubernetes sea. With tools and treasures abound, we traverse these waters, ready for what lies ahead. May your Emacs voyage be as fruitful and your adventures as exciting as our brave mariner\u0026rsquo;s journey.\nContainers: The Legend of the Layered VesselAhoy there, brave Emacs sailor! Let\u0026rsquo;s prepare for our new adventure in the stormy Docker seas, in search of legendary Containers, each akin to a floating island, ripe with opportunities and treasures!\n(unless (package-installed-p \u0026#39;docker) (package-install \u0026#39;docker)) (require \u0026#39;docker) (add-hook \u0026#39;docker-mode-hook (lambda () (docker-container-ls))) The journey begins in our homeland, ensuring that the courageous Docker is aboard, ready to guide us through treacherous tides. With the trusty docker-mode-hook, our ship is ever-ready to unfurl its sails, revealing a tableau of Docker containers, awaiting exploration.\n(defun docker-container-prompt (prompt) \u0026#34;Prompt for a Docker container name.\u0026#34; (completing-read prompt (docker-container-names))) (defun docker-container-names () \u0026#34;Retrieve a list of Docker container names.\u0026#34; (split-string (shell-command-to-string \u0026#34;docker ps -a --format \u0026#39;{{.Names}}\u0026#39;\u0026#34;) \u0026#34;\\n\u0026#34; t)) Our mariner conjured the spell docker-container-prompt that whispered the names of all Docker containers, calling upon them like sirens of the sea. An even more potent spell docker-container-names was forged to summon the entire list of container names, each echoing like a chant from the deep Docker sea.\n(defun docker-kill-prompt () \u0026#34;Prompt and kill a Docker container.\u0026#34; (interactive) (let ((container-name (docker-container-prompt \u0026#34;Kill container:\u0026#34;))) (docker-container-kill container-name))) (defun docker-restart-prompt () \u0026#34;Prompt and restart a Docker container.\u0026#34; (interactive) (let ((container-name (docker-container-prompt \u0026#34;Restart container:\u0026#34;))) (docker-container-restart container-name))) (defun docker-start-prompt () \u0026#34;Prompt and start a Docker container.\u0026#34; (interactive) (let ((container-name (docker-container-prompt \u0026#34;Start container:\u0026#34;))) (docker-container-start container-name))) (defun docker-stop-prompt () \u0026#34;Prompt and stop a Docker container.\u0026#34; (interactive) (let ((container-name (docker-container-prompt \u0026#34;Stop container:\u0026#34;))) (docker-container-stop container-name))) (defun docker-tag-prompt () \u0026#34;Prompt and tag a Docker container.\u0026#34; (interactive) (let ((container-name (docker-container-prompt \u0026#34;Tag container:\u0026#34;))) (docker-container-tag container-name))) The mariner\u0026rsquo;s toolkit contained many powerful charms, each more wondrous than the last. One could subdue an unruly container, the docker-kill-prompt. Another breathed life into a dormant one, the docker-start-prompt. If a container became sluggish, docker-restart-prompt was used to rejuvenate it, while docker-stop-prompt could coax an overly active one into submission. Lastly, the docker-tag-prompt was used to mark the containers with unique insignia, aiding the mariner in their journeys.\n(my-leader-def :states \u0026#39;(normal visual emacs) :keymaps \u0026#39;override \u0026#34;c\u0026#34; \u0026#39;(:ignore t :which-key \u0026#34;Containers\u0026#34;) \u0026#34;ci\u0026#34; \u0026#39;(docker-images :which-key \u0026#34;Images (New Buffer)\u0026#34;) \u0026#34;cl\u0026#34; \u0026#39;(docker-logs :which-key \u0026#34;Logs (New Buffer)\u0026#34;) \u0026#34;cn\u0026#34; \u0026#39;(docker-network :which-key \u0026#34;Network (New Buffer)\u0026#34;) \u0026#34;cp\u0026#34; \u0026#39;(docker-ps :which-key \u0026#34;PS (New Buffer)\u0026#34;) \u0026#34;ck\u0026#34; \u0026#39;(docker-kill-prompt :which-key \u0026#34;Kill (Prompt)\u0026#34;) \u0026#34;cr\u0026#34; \u0026#39;(docker-restart-prompt :which-key \u0026#34;Restart (Prompt)\u0026#34;) \u0026#34;cs\u0026#34; \u0026#39;(docker-start-prompt :which-key \u0026#34;Start (Prompt)\u0026#34;) \u0026#34;cS\u0026#34; \u0026#39;(docker-stop-prompt :which-key \u0026#34;Stop (Prompt)\u0026#34;) \u0026#34;ct\u0026#34; \u0026#39;(docker-tag-prompt :which-key \u0026#34;Tag (Prompt)\u0026#34;) \u0026#34;ca\u0026#34; \u0026#39;(docker-attach :which-key \u0026#34;Attach\u0026#34;) \u0026#34;cb\u0026#34; \u0026#39;(docker-build-buffer :which-key \u0026#34;Build Buffer\u0026#34;) \u0026#34;cc\u0026#34; \u0026#39;(docker-compose :which-key \u0026#34;Compose\u0026#34;) \u0026#34;ce\u0026#34; \u0026#39;(docker-exec :which-key \u0026#34;Exec\u0026#34;)) Then a grand guide was etched onto a gleaming shell, the \u0026lsquo;my-leader-def\u0026rsquo;. This was a guide to navigate the Docker sea with ease, with each container under the mariner\u0026rsquo;s command. Be it the birth of new containers or their demise, the mariner stood at the helm, orchestrating their symphony.\nSuch is our tale, dear sailor, of an epic adventure in the Docker sea. Now, it\u0026rsquo;s time for you to write your own legend in the annals of the Emacs sea.\nChapter 4. Taming the Kraken: An Adventure in Org-ModeAnchors Aweigh: Setting Up Org-ModeArrr, step aboard the mighty vessel of knowledge, the S.S. Org-Mode! This be not your regular ship, it be a floating archive of ye thoughts, a library of knowledge, a ship to navigate the vast ocean of ideas. Let\u0026rsquo;s hoist the sails!\n(unless (package-installed-p \u0026#39;org) (package-install \u0026#39;org)) We\u0026rsquo;re stocking up on our provisions, ye see. Without the trusty Org17, we\u0026rsquo;d be set adrift. Be it organising your thoughts like stowing cargo or keeping a captain\u0026rsquo;s log, it be essential.\n(defvar my-org-root-directory \u0026#34;~/notes/org/\u0026#34; \u0026#34;Root directory for Org mode files.\u0026#34;) (setq org-directory my-org-root-directory) In the heart of our ship, a chest - the root directory. A grand treasure map of sorts, guiding us to all our Org files.\n(defun my-org-open-index-file () \u0026#34;Open the index file for Org mode.\u0026#34; (interactive) (find-file (expand-file-name \u0026#34;index.org\u0026#34; my-org-root-directory))) Ahoy, the captain\u0026rsquo;s log! A simple incantation to unfurl it, my-org-open-index-file, and it\u0026rsquo;s open for us to pen our thoughts, musings, and tales.\n(setq-local face-remapping-alist \u0026#39;((default (:height 1.5) variable-pitch) (header-line (:height 4.0) variable-pitch) (org-document-title (:height 1.75) org-document-title) (org-code (:height 1.55) org-code) (org-verbatim (:height 1.55) org-verbatim) (org-block (:height 1.25) org-block) (org-block-begin-line (:height 0.7) org-block))) The captain likes his scrolls a particular way, see. A custom array, face-remapping-alist, sets the parchment just right, tuning the height of different parts like adjusting the tension in the rigging of a ship.\n(unless (package-installed-p \u0026#39;org-modern) (package-install \u0026#39;org-modern)) (add-hook \u0026#39;org-mode-hook #\u0026#39;org-modern-mode) Nothing like a bit of modern charm on the old Org-Mode. With \u0026lsquo;org-modern\u0026rsquo;, our ship stays seaworthy and trendy.\n(add-hook \u0026#39;org-mode-hook (lambda () (org-indent-mode))) The captain insists on keeping the ship\u0026rsquo;s log neat and tidy. Indentation be the order of the day!\n(setq ;; Edit settings org-auto-align-tags nil org-tags-column 0 org-catch-invisible-edits \u0026#39;show-and-error org-special-ctrl-a/e t org-insert-heading-respect-content t ;; Org styling, hide markup etc. org-hide-emphasis-markers t org-pretty-entities t org-ellipsis \u0026#34;\u0026#34; ;; Agenda styling org-agenda-tags-column 0 org-agenda-block-separator ? org-agenda-time-grid \u0026#39;((daily today require-timed) (800 1000 1200 1400 1600 1800 2000) \u0026#34;  \u0026#34; \u0026#34;\u0026#34;) org-agenda-current-time-string \u0026#34; now \u0026#34;) A flurry of captain\u0026rsquo;s orders followed, fine-tuning the look and feel of our ship\u0026rsquo;s logs - the visibility of emphasis markers, the style of ellipses, and more.\n(setq org-modern-indent t) (custom-set-faces \u0026#39;(org-document-title ((t (:inherit outline-1 :height 1.5))))) (custom-set-faces \u0026#39;(org-level-1 ((t (:inherit outline-1 :height 1.1))))) (setq org-todo-keywords \u0026#39;((sequence \u0026#34;TODO\u0026#34; \u0026#34;LATER\u0026#34; \u0026#34;|\u0026#34; \u0026#34;DONE\u0026#34;))) More customization followed, the captain being a detail-oriented fellow. Everything, from the inheritance of the document title to the task keywords, had to be shipshape.\n(global-org-modern-mode) The whole ship was adorned with the \u0026lsquo;org-modern\u0026rsquo; charm, setting the tone for our long voyage.\n(setq org-default-notes-file (expand-file-name \u0026#34;inbox.org\u0026#34; my-org-root-directory)) (setq org-capture-templates `((\u0026#34;c\u0026#34; \u0026#34;Capture\u0026#34; entry (file+headline org-default-notes-file \u0026#34;LATER\u0026#34;) \u0026#34;* LATER %?\\n\\n %i\\n\\n %a\u0026#34;))) The captain had an ingenious system for capturing fleeting ideas. Into the \u0026lsquo;inbox.org\u0026rsquo; they went, to be sorted and dealt with later.\n(my-leader-def :states \u0026#39;(normal visual emacs) :keymaps \u0026#39;override \u0026#34;o\u0026#34; \u0026#39;(:ignore t :which-key \u0026#34;Org Mode\u0026#34;) \u0026#34;oi\u0026#34; \u0026#39;(my-org-open-index-file :which-key \u0026#34;Open Index File\u0026#34;) \u0026#34;oc\u0026#34; \u0026#39;(org-capture :which-key \u0026#34;Capture\u0026#34;)) Lastly, the captain etched onto his map - the leader definition. It contained the quickest routes to key tasks: opening the index file and capturing thoughts. One could sail the sea of ideas with ease.\nSo, fellow sailor, that be our tale. The ship be prepared, the sails set. The vast sea of thoughts, ideas, and knowledge awaits us!\nhttps://melpa.org/\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nhttps://github.com/emacs-evil/evil\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nhttps://github.com/bbatsov/projectile\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nhttps://www.gnu.org/software/emacs/manual/html_node/emacs/Dired.html\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nhttps://github.com/abo-abo/swiper\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nhttps://github.com/tumashu/ivy-posframe\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nhttps://github.com/seagle0128/doom-modeline\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nhttps://github.com/jrblevin/markdown-mode\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nhttps://protesilaos.com/emacs/modus-themes\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nhttps://github.com/brotzeit/rustic\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nhttps://rust-analyzer.github.io/\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nhttps://github.com/jorgenschaefer/elpy\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nhttps://www.flycheck.org/en/latest/\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nhttps://github.com/dominikh/go-mode.el\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nhttps://magit.vc/\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nhttps://github.com/kubernetes-el/kubernetes-el\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nhttps://orgmode.org/\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"/from-scratch-to-emacs-the-adventurous-configurators-handbook.html","tags":null,"title":"From Scratch to Emacs, The Adventurous Configurator's Handbook"},{"categories":null,"contents":"Example taken from the official Scikit-learn documentation1.\nDataWe will start by generating a synthetic dataset. The true generative process is defined as $f(x)=x\\sin(x)$.\nimport numpy as np np.random.seed(42) X = np.linspace(start=0, stop=10, num=1_000).reshape(-1, 1) y = np.squeeze(X * np.sin(X)) We will use this dataset in the next experiment to illustrate how Gaussian Process regression is working.\nExample with noise-free targetIn this first example, we will use the true generative process without adding any noise. For training the Gaussian Process regression, we will only select few samples.\nrng = np.random.RandomState(1) training_indices = rng.choice(np.arange(y.size), size=6, replace=False) X_train, y_train = X[training_indices], y[training_indices] Now, we fit a Gaussian process on these few training data samples. We will use a radial basis function (RBF) kernel and a constant parameter to fit the amplitude.\nfrom sklearn.gaussian_process import GaussianProcessRegressor from sklearn.gaussian_process.kernels import RBF kernel = 1 * RBF(length_scale=1.0, length_scale_bounds=(1e-2, 1e2)) gaussian_process = GaussianProcessRegressor(kernel=kernel, n_restarts_optimizer=9) gaussian_process.fit(X_train, y_train) gaussian_process.kernel_ 5.02**2 * RBF(length_scale=1.43) After fitting our model, we see that the hyperparameters of the kernel have been optimized. Now, we will use our kernel to compute the mean prediction of the full dataset and plot the 95% confidence interval.\nWe see that for a prediction made on a data point close to the one from the training set, the 95% confidence has a small amplitude. Whenever a sample falls far from training data, our model\u0026rsquo;s prediction is less accurate and the model prediction is less precise (higher uncertainty).\nExample with noisy targetsWe can repeat a similar experiment adding an additional noise to the target this time. It will allow seeing the effect of the noise on the fitted model.\nWe add some random Gaussian noise to the target with an arbitrary standard deviation.\nnoise_std = 0.75 y_train_noisy = y_train + rng.normal(loc=0.0, scale=noise_std, size=y_train.shape) We create a similar Gaussian process model. In addition to the kernel, this time, we specify the parameter alpha which can be interpreted as the variance of a Gaussian noise.\ngaussian_process = GaussianProcessRegressor( kernel=kernel, alpha=noise_std**2, n_restarts_optimizer=9 ) gaussian_process.fit(X_train, y_train_noisy) mean_prediction, std_prediction = gaussian_process.predict(X, return_std=True) Let\u0026rsquo;s plot the mean prediction and the uncertainty region as before.\nThe noise affects the predictions close to the training samples: the predictive uncertainty near to the training samples is larger because we explicitly model a given level target noise independent of the input variable.\nhttps://scikit-learn.org/stable/auto_examples/gaussian_process/plot_gpr_noisy_targets.html\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"/gaussian-process-regression.html","tags":null,"title":"Gaussian Process Regression"},{"categories":null,"contents":"A recommended graphical Gemini browser is Lagrange or Amfora for a text-based/terminal browser.\nSyntaxGemini syntax is quite similar to Markdown\nSetupUsing the agate server\n","permalink":"/gemini.html","tags":null,"title":"Gemini"},{"categories":null,"contents":"IntroductionThis is a page containing the topics for Git1.\nGit cookbook https://git-scm.com/\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"/git.html","tags":null,"title":"Git"},{"categories":null,"contents":"Useful aliasesThis is a list of useful aliases I\u0026rsquo;ve found on the Internet1. To use them, simply add them to the [alias] section of ~/.gitconfig.\nSync with main branchThis command updates the local branch with the origin\u0026rsquo;s main.\n[alias] synced = \u0026#34;!git pull origin $(git mainbranch) --rebase\u0026#34; TreeTruncate historyTo truncate Git history, that is discard all commits before a specific one, you can do the following. Assume I have a commit with a certain hash, say abc123, and want to drop all commits before this one. Create a new orphan branch (name not important) called, say, truncated.\n$ git checkout --orphan truncated abc123 And then rebase master (or any other main branch) on top of truncated.\n$ git commit -m \u0026#34;Truncate history\u0026#34; $ git rebase --onto truncated abc123 master Your new branch truncated will be free of master history.\nHookspre-pushThe pre-push script is called by git push, when the push actually happens. If the exit status is 0, then the push will proceed, otherwise it will be stopped.\nThe script is supplied with the following arguments:\n$1 -- Name of the remote to which the push is being done (Ex: origin) $2 -- URL to which the push is being done (Ex: https://\u0026lt;host\u0026gt;:\u0026lt;port\u0026gt;/\u0026lt;username\u0026gt;/\u0026lt;project_name\u0026gt;.git) Information about the commits which are being pushed is supplied as lines to the standard input in the form:\n\u0026lt;local_ref\u0026gt; \u0026lt;local_sha1\u0026gt; \u0026lt;remote_ref\u0026gt; \u0026lt;remote_sha1\u0026gt; Sample values:\nlocal_ref = refs/heads/master local_sha1 = 68a07ee4f6af8271dc40caae6cc23f283122ed11 remote_ref = refs/heads/master remote_sha1 = efd4d512f34b11e3cf5c12433bbedd4b1532716f https://softwaredoug.com/blog/2022/11/09/idiot-proof-git-aliases.html\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"/git-cookbook.html","tags":null,"title":"Git cookbook"},{"categories":null,"contents":"Contributions ","permalink":"/github.html","tags":null,"title":"GitHub"},{"categories":null,"contents":"Notes on GitHub actions.\n","permalink":"/github-actions.html","tags":null,"title":"GitHub actions"},{"categories":null,"contents":"Some notes regarding the Go language. Some topics have graduated to their own page:\nGo cookbook Go resource bundling Go filesystem operations SetupFirst things first. How to install the Go language in different OSes.\nFedoraYou can either use dnf directly and simply run\n$ sudo dnf install golang-bin This might not install the latest and greatest. If you want to use the most recent version, download it directly from https://golang.org/doc/install#install. If applicable, delete any previous /usr/local/go directory with\n$ sudo rm -rf /usr/local/go Next, extract the archive file with\n$ sudo tar -C /usr/local -xzf /home/foo/tmp/go-$VERSION.linux-amd64.tar.gz And add /usr/local/go/bin to the $PATH.\nmacOSThe best way to perform an attended installation of Go in macOS is to simply download the installer from https://go.dev/dl/ and running it.\nLanguage designGo doesn\u0026rsquo;t have setsThe Go language, notoriously, does not have1 some common data structures like sets. There are two main reasons for that:\n+Go does not have generics2 Go relies on you writing your own data structures, generally Go lacks generics, which prevent writing a \u0026hellip; well, generic and efficient set implementation. Also, writing your own (non-generic) set with maps is quite straight-forward.2 The usual structure for a type T is map[T]bool, where the key is the element and the value is just a placeholder. For instance, for a int set, we can add elements:\ns := map[int]bool{1: true, 3: true} s[1] = true // already presenvar t s[2] = true // adds new element Some other techniques for maps replacing sets:\nSet unionset_1 := map[int]bool{1:true, 2:true, 3:false} set_2 := map[int]bool{1:false, 2:false, 4:false} set_union := map[int]bool{} for k, _ := range set_1{ set_union[k] = true } for k, _ := range set_2{ set_union[k] = true } fmt.Println(set_union) Set intersectionset_1 := map[int]bool{1:true, 2:true, 3:false} set_2 := map[int]bool{1:false, 2:false, 4:false} set_intersection := map[int]bool{} for k,_ := range set_1 { if set_2[k] { set_intersection[k] = true } } fmt.Println(set_intersection) To convert a (map) set to an array:\narray := make([]int, 0) set_1 := map[int]bool{1:true, 2:true, 3:false} for k := range set_1 { array = append(array, k) } fmt.Println(array) CIGitHubA potential workflow for GitHub is to use GitHub Actions for Go. An example workflow file, .github/workflows/test.yml, which runs go test (see Testing in Go) and go vet is:\non: [push, pull_request\\] name: Test jobs: test: strategy: matrix: go-version: [1.14.x, 1.15.x] os: [ubuntu-latest] runs-on: ${{ matrix.os }} steps: - name: Install Go uses: actions/setup-go@v2 with: go-version: ${{ matrix.go-version }} - name: Checkout code uses: actions/checkout@v2 - name: Test run: go test ./... - name: Vet run: go vet ./... ContainersMinimal exampleA minimal example of a Go container configuration for a web server running on port 8080:\n# Start from the latest golang base image FROM golang:latest # Add Maintainer Info LABEL maintainer=\u0026#34;Rui Vieira\u0026#34; # Set the Current Working Directory inside the container WORKDIR /app # Copy go mod and sum files COPY go.mod go.sum ./ # Download all dependencies. Dependencies will be cached if the go.mod and go.sum files are not changed RUN go mod download # Copy the source from the current directory to the Working Directory inside the container COPY . . # Build the Go app RUN go build -o main . # Expose port 8080 to the outside world EXPOSE 8080 # Command to run the executable CMD [\u0026#34;./main\u0026#34;] ReferenceConversionsHow to convert a string to byte array?The conversion is simple:\nb := []byte(\u0026#34;This is a string\u0026#34;) CollectionsSort map keys alphabeticallyIf a map contains string keys, i.e. var myMap map[string]T, we must sort the map keys independently. For instance:\nkeys := make([]string, 0) for k, _ := range myMap { keys = append(keys, k) } sort.Strings(keys) for _, k := range keys { fmt.Println(k, myMap[k]) } Check for elementIf we consider a collection, say, []string collection, the way to check for an element already present is, for instance:\nfunc existsIn(needle string, haystack []string) bool { for _, element := range haystack { if element == needle { return true } } return false } TemplatesCheck if variable emptyIn a Go template you check if a variable is empty by doing:\n{{if .Items}} \u0026lt;ul\u0026gt; {{range .Items}} \u0026lt;li\u0026gt;{{.Name}}\u0026lt;/li\u0026gt; {{end}} \u0026lt;/ul\u0026gt; {{end}} Looping over a mapLooping over the map var data map[string]bool in a Go template:\n{{range $index, $element := .}} {{$index}}: {{$element}} {{end}} ProcessesExecuting external processesExecuting an external process and directing input and output to Stdout and Stderr.\ncmd := exec.Command(\u0026#34;ls\u0026#34;, \u0026#34;-1ao\u0026#34;) cmd.Stdout = os.Stdout cmd.Stderr = os.Stderr err := cmd.Run() if err != nil { log.Fatalf(\u0026#34;cmd.Run() failed with %s\\\\n\u0026#34;, err) } Testing in GoPlace the tests in your place of choosing, but keep the package declaration. Test functions should be parameterised as (t *testing.T) and start with the prefix Test, for instance:\npackage main func TestFoo(t *testing.T) { value := Foo(5, 5) // ... assertions The test files themselves must have the suffix *_test.go. Call the tests with go test.\nOutputPrinting struct keysTo print a struct along with its keys, we can use the Printf switch as in the official documentation. That is,\nfmt.Printf(\u0026#34;%+v\\n\u0026#34;, myStruct) Date and timeCheck if a date is emptyIf a date is unassigned, the .IsZero() method can be used to check it\na := time.Time{} a.IsZero() // This will be true As of the time of writing, that is Go 1.15.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nGo indeed has generics starting with 1.18.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"/go.html","tags":null,"title":"Go"},{"categories":null,"contents":"Notes on Go filesystem operations.\nCopying filesGo does not have an utility method to copy files. We have to rely on writing our own implementation using the reading and writing functionality in other packages. As an example:\npackage main import ( \u0026#34;io\u0026#34; \u0026#34;log\u0026#34; \u0026#34;os\u0026#34; ) func main() { from, err := os.Open(\u0026#34;./foo.txt\u0026#34;) if err != nil { log.Fatal(err) } defer from.Close() to, err := os.OpenFile(\u0026#34;./bar.txt\u0026#34;, os.O_RDWR|os.O_CREATE, 0666) if err != nil { log.Fatal(err) } defer to.Close() _, err = io.Copy(to, from) if err != nil { log.Fatal(err) } } Path operationsBasepathTo get the basepath of a path string we use the Dir method:\nfilepath.Dir(\u0026#34;/etc/foo/file.txt\u0026#34;) // \u0026#34;/etc/foo\u0026#34; Check if directory existsif _, err := os.Stat(\u0026#34;/etc/foo/\u0026#34;); os.IsNotExist(err) { // do something because it does not exist } Create nested directoriesUse MkdirAll:\nos.MkdirAll(\u0026#34;/etc/long/nested/path/to/create\u0026#34;, os.ModePerm) ","permalink":"/go-filesystem-operations.html","tags":null,"title":"Go filesystem operations"},{"categories":null,"contents":"Notes on the installation and usage of pkger.\nInstallation done with\ngo get github.com/markbates/pkger/cmd/pkger pkger works by bundling the resources with a code-generated pkg.go. The configuration of assets to be bundled is done by reflection at compile time and not direct configuration. This is done by replacing standard Go file operations with pkger proxy ones, such as:\ntype Pkger interface { Parse(p string) (Path, error) Current() (here.Info, error) Info(p string) (here.Info, error) Create(name string) (File, error) MkdirAll(p string, perm os.FileMode) error Open(name string) (File, error) Stat(name string) (os.FileInfo, error) Walk(p string, wf filepath.WalkFunc) error Remove(name string) error RemoveAll(path string) error } type File interface { Close() error Info() here.Info Name() string Open(name string) (http.File, error) Path() Path Read(p []byte) (int, error) Readdir(count int) ([]os.FileInfo, error) Seek(offset int64, whence int) (int64, error) Stat() (os.FileInfo, error) Write(b []byte) (int, error) } ExampleBundling a Go template file.\ntmplFile, _ := pkger.Open(\u0026#34;/templates/page.tmpl\u0026#34;) tmplBytes, _ := ioutil.ReadAll(tmplFile) tmplString := string(tmplBytes) tpl, err := template.New(\u0026#34;page\u0026#34;).Parse(tmplString) _ = tpl.Execute(f, ...) The bundling is simply done by running\npkger and building as usual\ngo build ","permalink":"/go-resource-bundling.html","tags":null,"title":"Go resource bundling"},{"categories":null,"contents":"For features $x_i={x_{i1},\\dots,x_{ip}}$ and $x_j={x_{j1},\\dots,x_{jp}}$, the Gower similarity matrix 1 can be defined as\n$$ S_{\\text{Gower}}(x_i, x_j) = \\frac{\\sum_{k=1}^p s_{ijk}\\delta_{ijk}}{\\sum_{k=1}^p \\delta_{ijk}}. $$\nFor each feature $k=1,\\dots,p$ a score $s_{ijk}$ is calculated. A quantity $\\delta_{ijk}$ is also calculated having possible values ${0,1}$ depending on whether the variables $x_i$ and $x_j$ can be compared or not (/e.g./ if they have different types).\nA special case2 for when no missing values exist can be formulated as the mean of the Gower similarity scores, that is:\n$$ S_{\\text{Gower}}(x_i, x_j) = \\frac{\\sum_{k=1}^p s_{ijk}}{p}. $$\nThe score $s_{ijk}$ calculation will depend on the type of variable and below we will see some examples.\nThis similarity score will take values between $0 \\leq s_{ijk} \\leq 1$ with $0$ representing maximum similarity and $1$ no similarity.\nScoringNumerical variablesFor numerical variables the score can be calculated as\n$$ s_{ijk} = 1 - \\frac{|x_{ik}-x_{jk}|}{R_k}. $$\nThis is simply a L1 distance between the two values normalised by a quantity $R_k$. The quantity $R_k$ refers to the range of feature (population /or/ sample).\nCategorical variablesFor categorical variables we will use following score:\n$$ s_{ijk} = 1{x_{ik}=x_{jk}} $$\nThis score will be $1$ if the categories are the same and $0$ if they are not.\nIn reality the score $S_{\\text{Gower}}(x_i, x_j)$ will be a /similarity score/ taking values between $1$ (for equal points) and $0$ for extremely dissimilar points. In order to turn this value into a /distance metric/ we can convert it using (for instance)\n$$ d_{\\text{Gower}} = \\sqrt{1-S_{\\text{Gower}}}. $$\nThis will take values of $1$ for the furthest points and $0$ for the same points.\nExampleHere we will use the special case when no missing values exist. A test dataset can be:\nimport pandas as pd df = pd.DataFrame({ \u0026#34;Sex1\u0026#34;: [\u0026#39;M\u0026#39;, \u0026#39;M\u0026#39;, \u0026#39;F\u0026#39;, \u0026#39;F\u0026#39;, \u0026#39;F\u0026#39;, \u0026#39;M\u0026#39;, \u0026#39;M\u0026#39;, \u0026#39;F\u0026#39;, \u0026#39;F\u0026#39;, \u0026#39;F\u0026#39;], \u0026#34;Sex2\u0026#34;: [\u0026#39;M\u0026#39;, \u0026#39;M\u0026#39;, \u0026#39;F\u0026#39;, \u0026#39;F\u0026#39;, \u0026#39;F\u0026#39;, \u0026#39;F\u0026#39;, \u0026#39;F\u0026#39;, \u0026#39;M\u0026#39;, \u0026#39;M\u0026#39;, \u0026#39;M\u0026#39;], \u0026#34;Age1\u0026#34;: [15, 15, 15, 15, 15, 15, 15, 15, 15, 15], \u0026#34;Age2\u0026#34;: [15, 36, 58, 78, 100, 15, 36, 58, 78, 100] }) df Sex1 Sex2 Age1 Age2 0 M M 15 15 1 M M 15 36 2 F F 15 58 3 F F 15 78 4 F F 15 100 5 M F 15 15 6 M F 15 36 7 F M 15 58 8 F M 15 78 9 F M 15 100 For the numerical variable (age) we can define the range as $R_{\\text{age}}=\\left(\\max{\\text{age}}-\\min{\\text{age}}\\right)$.\n_fields = [\u0026#34;Age1\u0026#34;, \u0026#34;Age2\u0026#34;] age_min = df[_fields].min().min() age_max = df[_fields].max().max() R_age = age_max - age_min We can now calculate the score for each numerical field\ndef s_numeric(x1, x2, R): return 1 - abs(x1-x2)/R df[\u0026#39;s_age\u0026#39;] = df.apply(lambda x: s_numeric(x[\u0026#39;Age1\u0026#39;], x[\u0026#39;Age2\u0026#39;], R_age), axis=1) df Sex1 Sex2 Age1 Age2 s_age 0 M M 15 15 1.000000 1 M M 15 36 0.752941 2 F F 15 58 0.494118 3 F F 15 78 0.258824 4 F F 15 100 0.000000 5 M F 15 15 1.000000 6 M F 15 36 0.752941 7 F M 15 58 0.494118 8 F M 15 78 0.258824 9 F M 15 100 0.000000 For categorical variables we can define the following score function:\ndef s_categorical(x1, x2): return 1 if x1==x2 else 0 df[\u0026#39;s_sex\u0026#39;] = df.apply(lambda x: s_categorical(x[\u0026#39;Sex1\u0026#39;], x[\u0026#39;Sex2\u0026#39;]), axis=1) df Sex1 Sex2 Age1 Age2 s_age s_sex 0 M M 15 15 1.000000 1 1 M M 15 36 0.752941 1 2 F F 15 58 0.494118 1 3 F F 15 78 0.258824 1 4 F F 15 100 0.000000 1 5 M F 15 15 1.000000 0 6 M F 15 36 0.752941 0 7 F M 15 58 0.494118 0 8 F M 15 78 0.258824 0 9 F M 15 100 0.000000 0 We can now calculate the final score using\nimport math df[\u0026#39;s\u0026#39;] = df.apply(lambda x: (x[\u0026#39;s_age\u0026#39;] + x[\u0026#39;s_sex\u0026#39;])/2.0, axis=1) df[\u0026#39;d\u0026#39;] = df.apply(lambda x: math.sqrt(1.0 - x[\u0026#39;s\u0026#39;]), axis=1) df Sex1 Sex2 Age1 Age2 s_age s_sex s d 0 M M 15 15 1.000000 1 1.000000 0.000000 1 M M 15 36 0.752941 1 0.876471 0.351468 2 F F 15 58 0.494118 1 0.747059 0.502933 3 F F 15 78 0.258824 1 0.629412 0.608760 4 F F 15 100 0.000000 1 0.500000 0.707107 5 M F 15 15 1.000000 0 0.500000 0.707107 6 M F 15 36 0.752941 0 0.376471 0.789639 7 F M 15 58 0.494118 0 0.247059 0.867722 8 F M 15 78 0.258824 0 0.129412 0.933053 9 F M 15 100 0.000000 0 0.000000 1.000000 Range impactVarying boundsLet\u0026rsquo;s visualise how the choice of range can affect the scoring, if can set it arbitrarily. First let\u0026rsquo;s pick two random points, $x_1=(30, M)$ and $x_2=(35, F)$.\nWe will vary the bounds from a $15\\leq x_{min}\u0026lt;30$ and $35\u0026lt; x_{max} \\leq 100$.\ndef score(x1, x2, R): s_0 = 1-abs(x1[0]-x2[0])/R s_1 = 1 if x1[1]==x2[1] else 0 return (s_0 + s_1)/2.0 def distance(s): return math.sqrt(1.0-s) import numpy as np x1 = (30, \u0026#39;M\u0026#39;) x2 = (35, \u0026#39;F\u0026#39;) bmin = np.linspace(15, 30, num=1000) bmax = np.linspace(36, 100, num=1000) scores_min = [distance(score(x1, x2, 100-bm)) for bm in bmin] scores_max = [distance(score(x1, x2, bm-15)) for bm in bmax] Let\u0026rsquo;s try with more separated points\nx1 = (16, \u0026#39;M\u0026#39;) x2 = (90, \u0026#39;F\u0026#39;) bmin = np.linspace(15, 16, num=1000, endpoint=False) bmax = np.linspace(91, 100, num=1000) scores_min = [distance(score(x1, x2, 100-bm)) for bm in bmin] scores_max = [distance(score(x1, x2, bm-16)) for bm in bmax] Varying range directlyWe will now try to see how the distance between two point comparisons (very close, very far) changes when varying the range directly. We will choose two sets of points, $x_1=(1000, M), x_2=(1001, F)$ and $x_1=(500, M), x_2=(50000, F)$. The range will vary between\n$$ \\max(x_1, x_2)-\\min(x_1, x_2)\u0026lt;R\u0026lt;100000. $$\nWe are also interested on the weight the categorical variable will have on the final distance with varying bounds, so we will also calculate them for an alternative $x_2\u0026rsquo;=(1001, M)$ anf $x_2\u0026rsquo;=(50000, M)$.\nFor the first set of points we will have:\nx1 = (1000.0, \u0026#39;M\u0026#39;) x2 = (1001.0, \u0026#39;F\u0026#39;) MAX_RANGE = 100000 R = np.linspace(max(x1[0], x2[0])-min(x1[0], x2[0]), MAX_RANGE, num=100000) distances_M = [distance(score(x1, x2, i)) for i in R] distances_F = [distance(score(x1, (x2[0], \u0026#39;M\u0026#39;), i)) for i in R] And for far away points we will have:\nx1 = (500.0, \u0026#39;M\u0026#39;) x2 = (50000.0, \u0026#39;F\u0026#39;) MAX_RANGE = 100000 R = np.linspace(max(x1[0], x2[0])-min(x1[0], x2[0]), MAX_RANGE, num=100000) distances_M = [distance(score(x1, x2, i)) for i in R] distances_F = [distance(score(x1, (x2[0], \u0026#39;M\u0026#39;), i)) for i in R] Categorical impactPredictably, in the scenario where we calculate the mean of the Gower distances, for a point $x$ with $p$ features, $x=(x_{1},\\dots,x_{p})$, the contribution to the final distance of a categorial variable will be either $0$ or $1/p$, regardless of the range.\nMissing rangeFor the previous examples the range $R$ was available, but how to calculate the mixed distance when the numerical range is absent?\nA possible way is to use scale each feature using unit scaling:\n$$ f_u(x) = \\frac{x}{||x||} $$\nWe will visualise how a difference varying from $-1000 \\leq \\delta \\leq 1000$ varies with the $f_u(\\delta)$ transformation.\ndef f_unit(x): return np.exp(x)/(np.exp(x)+1.0) def ilogit(eta): return 1.0 - 1.0/(np.exp(eta)+1) delta = np.linspace(-10, 10, num=20000) transformed = [ilogit(abs(x)) for x in delta] cite:gower1971general Gower, J. C. (1971). A general coefficient of similarity and some of its properties. Biometrics, (), 857\u0026ndash;871.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nThis is for instance the case we deal with in the Counterfactuals with Constraint Solvers.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"/gower-distance.html","tags":null,"title":"Gower distance"},{"categories":null,"contents":"InstallationmacOSYou can install GPG on macOS using:\n$ brew install gpg WorkflowGetting existing keyTo get an existing key id use:\ngpg --list-secret-keys --keyid-format LONG The key id will be available at\ngpg --list-secret-keys --keyid-format LONG /home/foo/.gnupg/pubring.gpg ------------------------------ sec rsa2048/7FFFC09ACAC05FD0 2017-06-02 [SC] [expires: 2019-06-02] 5538B0F643277336BA7F0E457FFFC09ACAC05FD0 uid [ultimate] foo \u0026lt;foo@[example.com](http://realaddress.com)\u0026gt; ssb rsa2048/95E8A289DFE77A84 2017-06-02 [E] [expires: 2019-06-02] The key id we want, in this case is 7FFFC09ACAC05FD0.\nTo retrieve the public key use\ngpg --armor --export 7FFFC09ACAC05FD0 And the result will be\n-----BEGIN PGP PUBLIC KEY BLOCK----- ... The actual key ... -----END PGP PUBLIC KEY BLOCK----- TroubleshootingmacOSCan\u0026rsquo;t access keychain from UIIf a program cannot access the keychain from the UI, probably there\u0026rsquo;s some problem in prompting you for the passphrase. You can install, for instance pinentry to solve this. Install it with\n$ brew install pinentry-mac and then register pinentry as the passphrase input option:\n$ echo \u0026#34;pinentry-program /usr/local/bin/pinentry-mac\u0026#34; \u0026gt;\u0026gt; ~/.gnupg/gpg-agent.conf ","permalink":"/gpg.html","tags":null,"title":"GPG"},{"categories":null,"contents":"Missing headersOn macOS, if you find missing headers when installing polyglot packages, try:\n$ export CPATH=`xcrun --show-sdk-path`/usr/include ","permalink":"/graalvm.html","tags":null,"title":"GraalVM"},{"categories":null,"contents":"Page on Grad-CAM.\n","permalink":"/grad-cam.html","tags":null,"title":"Grad-CAM"},{"categories":null,"contents":" Hill-climbing optimisation ","permalink":"/gradient-free-optimisation.html","tags":null,"title":"Gradient-free optimisation"},{"categories":null,"contents":"I have been working in a new library called gulp which you can find on https://github.com/ruivieira/gulp.\nOn the project\u0026rsquo;s page there are some usage examples but I will try to summarise the main points here.\nThe purpose of this library is to facilitate the parallel development of R and Java code, using rJava as the bridge. Creating bindings in rJava is quite simple, the tricky part of the process (in my opinion) being the maintenance of the bindings (usually done by hand) when refactoring your code.\nAs an example, let\u0026rsquo;s assume you have the following Java class:\n@ExportClassReference(value=\u0026#34;test\u0026#34;) public class Test { // Java code } That you wish to call from R.\n","permalink":"/gulp.html","tags":null,"title":"gulp"},{"categories":null,"contents":"Global maximumLet\u0026rsquo;s try it with the function\n$$ f(x,y) = e^{-\\left(x^2+y^2\\right)} $$\nimport numpy as np import matplotlib.pyplot as plt from plotutils import * x = np.linspace(-2.0, 2.0, 1000) y = np.linspace(-2.0, 2.0, 1000) X, Y = np.meshgrid(x, y) Z = np.exp(-(X ** 2 + Y ** 2)) fig, ax = plt.subplots(1, 1) cp = ax.contourf(X, Y, Z, cmap=cmaps[1]) ax.set_title(\u0026#34;f(x,y)\u0026#34;) ax.set_xlabel(\u0026#34;x\u0026#34;) ax.set_ylabel(\u0026#34;y\u0026#34;) plt.show() from gradient_free_optimizers import HillClimbingOptimizer search_space = { \u0026#34;x\u0026#34;: x, \u0026#34;y\u0026#34;: y, } opt = HillClimbingOptimizer(search_space) def f(pos): x = pos[\u0026#34;x\u0026#34;] y = pos[\u0026#34;y\u0026#34;] z = np.exp(-(x**2 + y**2)) return z result = opt.search(f, n_iter=30000, verbosity=[\u0026#39;print_times\u0026#39;]) Evaluation time : 0.9117169380187988 sec [28.97 %] Optimization time : 2.235219955444336 sec [71.03 %] Iteration time : 3.1469368934631348 sec [9533.08 iter/sec] opt.best_para {'x': -0.002002002002001957, 'y': 0.002002002002002179} Local maximumLet\u0026rsquo;s try it with the function\n$$ f(x,y) = e^{-\\left(x^2+y^2\\right)}+2e^{-\\left((x-1.7)^2+(y-1.7)^2\\right)} $$\nx = np.linspace(-1.0, 3.0, 1000) y = np.linspace(-1.0, 3.0, 1000) X, Y = np.meshgrid(x, y) Z = np.exp(-(X**2 + Y**2))+2*np.exp(-((X-1.7)**2+(Y-1.7)**2)) fig,ax=plt.subplots(1,1) cp = ax.contourf(X, Y, Z, cmap=cmaps[1]) ax.set_title(\u0026#39;f(x,y)\u0026#39;) ax.set_xlabel(\u0026#39;x\u0026#39;) ax.set_ylabel(\u0026#39;y\u0026#39;) plt.show() opt = HillClimbingOptimizer(search_space) def f(pos): x = pos[\u0026#34;x\u0026#34;] y = pos[\u0026#34;y\u0026#34;] z = np.exp(-(x**2 + y**2))+2*np.exp(-((x-1.7)**2+(y-1.7)**2)) return z result = opt.search(f, n_iter=30000, verbosity=[\u0026#39;print_times\u0026#39;]) Evaluation time : 0.9092590808868408 sec [29.73 %] Optimization time : 2.148698091506958 sec [70.27 %] Iteration time : 3.057957172393799 sec [9810.47 iter/sec] opt.best_para {'x': 1.6956956956956954, 'y': 1.6956956956956954} ","permalink":"/hill-climbing-optimisation.html","tags":null,"title":"Hill-climbing optimisation"},{"categories":null,"contents":"This is a collection of the hot sauces I\u0026rsquo;ve tried.\nPiri piriNando\u0026rsquo;s XX Hot ","permalink":"/hot-sauces.html","tags":null,"title":"Hot Sauces"},{"categories":null,"contents":"HTML TricksMetersA meter element is availble natively for HTML.\n\u0026lt;label for=\u0026#34;value1\u0026#34;\u0026gt;Low\u0026lt;/label\u0026gt; \u0026lt;meter id=\u0026#34;value1\u0026#34; min=\u0026#34;0\u0026#34; max=\u0026#34;100\u0026#34; low=\u0026#34;30\u0026#34; high=\u0026#34;75\u0026#34; optimum=\u0026#34;80\u0026#34; value=\u0026#34;25\u0026#34;\u0026gt;\u0026lt;/meter\u0026gt; \u0026lt;label for=\u0026#34;value2\u0026#34;\u0026gt;Medium\u0026lt;/label\u0026gt; \u0026lt;meter id=\u0026#34;value2\u0026#34; min=\u0026#34;0\u0026#34; max=\u0026#34;100\u0026#34; low=\u0026#34;30\u0026#34; high=\u0026#34;75\u0026#34; optimum=\u0026#34;80\u0026#34; value=\u0026#34;50\u0026#34;\u0026gt;\u0026lt;/meter\u0026gt; \u0026lt;label for=\u0026#34;value3\u0026#34;\u0026gt;High\u0026lt;/label\u0026gt; \u0026lt;meter id=\u0026#34;value3\u0026#34; min=\u0026#34;0\u0026#34; max=\u0026#34;100\u0026#34; low=\u0026#34;30\u0026#34; high=\u0026#34;75\u0026#34; optimum=\u0026#34;80\u0026#34; value=\u0026#34;80\u0026#34;\u0026gt;\u0026lt;/meter\u0026gt; Low Medium High Ordered list startChange the starting point of an ordered list.\n\u0026lt;ol start=\u0026#34;11\u0026#34;\u0026gt; \u0026lt;li\u0026gt;Eleven\u0026lt;/li\u0026gt; \u0026lt;li\u0026gt;Twelve\u0026lt;/li\u0026gt; \u0026lt;li\u0026gt;Thirteen\u0026lt;/li\u0026gt; \u0026lt;li\u0026gt;Fourteen\u0026lt;/li\u0026gt; \u0026lt;/ol\u0026gt; Eleven Twelve Thirteen Fourteen HTML Native Search\u0026lt;div class=\u0026#34;wrapper\u0026#34;\u0026gt; \u0026lt;h1\u0026gt;HTML native search\u0026lt;/h1\u0026gt; \u0026lt;input list=\u0026#34;items\u0026#34;\u0026gt; \u0026lt;datalist id=\u0026#34;items\u0026#34;\u0026gt; \u0026lt;option value=\u0026#34;Rui\u0026#34;\u0026gt; \u0026lt;option value=\u0026#34;Vieira\u0026#34;\u0026gt; \u0026lt;option value=\u0026#34;ruivieira.dev\u0026#34;\u0026gt; \u0026lt;option value=\u0026#34;HTML\u0026#34;\u0026gt; \u0026lt;/datalist\u0026gt; \u0026lt;/div\u0026gt; Native HTML Search Fieldset element\u0026lt;form\u0026gt; \u0026lt;fieldset\u0026gt; \u0026lt;legend\u0026gt;Best editor\u0026lt;/legend\u0026gt; \u0026lt;input type=\u0026#34;radio\u0026#34; id=\u0026#34;emacs\u0026#34; name=\u0026#34;editor\u0026#34;\u0026gt; \u0026lt;label for=\u0026#34;emacs\u0026#34;\u0026gt;Emacs\u0026lt;/label\u0026gt;\u0026lt;br/\u0026gt; \u0026lt;input type=\u0026#34;radio\u0026#34; id=\u0026#34;vim\u0026#34; name=\u0026#34;editor\u0026#34;\u0026gt; \u0026lt;label for=\u0026#34;vim\u0026#34;\u0026gt;Vim\u0026lt;/label\u0026gt;\u0026lt;br/\u0026gt; \u0026lt;input type=\u0026#34;radio\u0026#34; id=\u0026#34;nano\u0026#34; name=\u0026#34;editor\u0026#34;\u0026gt; \u0026lt;label for=\u0026#34;nano\u0026#34;\u0026gt;nano\u0026lt;/label\u0026gt; \u0026lt;/fieldset\u0026gt; \u0026lt;/form\u0026gt; Best editor \u0026lt;input type=\u0026quot;radio\u0026quot; id=\u0026quot;emacs\u0026quot; name=\u0026quot;editor\u0026quot;\u0026gt; \u0026lt;label for=\u0026quot;emacs\u0026quot;\u0026gt;Emacs\u0026lt;/label\u0026gt;\u0026lt;br/\u0026gt; \u0026lt;input type=\u0026quot;radio\u0026quot; id=\u0026quot;vim\u0026quot; name=\u0026quot;editor\u0026quot;\u0026gt; \u0026lt;label for=\u0026quot;vim\u0026quot;\u0026gt;Vim\u0026lt;/label\u0026gt;\u0026lt;br/\u0026gt; \u0026lt;input type=\u0026quot;radio\u0026quot; id=\u0026quot;nano\u0026quot; name=\u0026quot;editor\u0026quot;\u0026gt; \u0026lt;label for=\u0026quot;nano\u0026quot;\u0026gt;nano\u0026lt;/label\u0026gt; Spellchecking\u0026lt;label for=\u0026#34;input1\u0026#34;\u0026gt;spellcheck=\u0026#34;true\u0026#34;\u0026lt;/label\u0026gt; \u0026lt;input type=\u0026#34;text\u0026#34; id=\u0026#34;input1\u0026#34; spellcheck=\u0026#34;true\u0026#34;\u0026gt; \u0026lt;label for=\u0026#34;input2\u0026#34;\u0026gt;spellcheck=\u0026#34;false\u0026#34;\u0026lt;/label\u0026gt; \u0026lt;input type=\u0026#34;text\u0026#34; id=\u0026#34;input2\u0026#34; spellcheck=\u0026#34;false\u0026#34;\u0026gt; spellcheck=\u0026ldquo;true\u0026rdquo; spellcheck=\u0026ldquo;false\u0026rdquo; Sliders\u0026lt;label for=\u0026#34;volume\u0026#34;\u0026gt;Volume (goes to 11): \u0026lt;/label\u0026gt; \u0026lt;input type=\u0026#34;range\u0026#34; id=\u0026#34;volume\u0026#34; name=\u0026#34;volume\u0026#34; min=\u0026#34;0\u0026#34; max=\u0026#34;11\u0026#34;\u0026gt; Volume (goes to 11): Details \u0026lt;details\u0026gt; \u0026lt;summary\u0026gt; Spoiler \u0026lt;/summary\u0026gt; \u0026lt;p\u0026gt; Keyser Sze is a myth. \u0026lt;/p\u0026gt; \u0026lt;/details\u0026gt; Spoiler \u0026lt;p\u0026gt; Keyser Sze is a myth. \u0026lt;/p\u0026gt; Mark\u0026lt;p\u0026gt;This is an \u0026lt;mark\u0026gt;example with highlight\u0026lt;/mark\u0026gt; since it\u0026#39;s so important\u0026lt;/p\u0026gt; This is an example with highlight since it's so important\n","permalink":"/html.html","tags":null,"title":"HTML"},{"categories":null,"contents":"TemplatesSpecial pagesBy default, the type for a piece of content is inherited from the the contents section. So, the file you create for content at content/posts/my-post.md automatically has a type of posts. However, you may want to keep my-post.md within that section because you want to rely on Hugos default behavior to render the page to yoursite.com/posts/my-post, but you want it to render according to a different layout. In this case, you can specify a type for the content that overrides the default behavior. Types are always singular.\nYou can then put specific layouts in a layout folder of the same name as the type (hence why it works with mylayout). You are telling Hugo that your About page, while living inside the root content section, is of the specific mylayout type. Further, you could even specify a layout that Hugo should use to render your About page:\n+++ date = \u0026#34;2017-04-17T11:01:21-04:00\u0026#34; draft = false title = \u0026#34;About\u0026#34; type = \u0026#34;mylayout\u0026#34; layout = \u0026#34;speciallayout\u0026#34; +++ In this example, Hugo will render the page according to what you create in ./themes/mytheme/layouts/mylayout/speciallayout.html. If you do not specify the layout, Hugo then looks for the the next layout in the lookup order.\nLooping with indicesTo loop a collection along with the index you can use:\n{{range $index, $element := .collection}} index:{{ $index }} name:{{ $collection.name }} {{ end }} ","permalink":"/hugo.html","tags":null,"title":"Hugo"},{"categories":null,"contents":"Stardate 96893.29.\nYou are the USS Euler\u0026rsquo;s Science Officer at a moment when the computer graphical displays and voice systems went down. You only have enough deuterium for a short travel and need to find the nearest star system. This is not a simple matter of looking at a chart. You have multiple dimensions in which you can travel. In a bid for galactic peace, the Federation mandated that both Emacs and Vim should be installed in all computers. You open your favourite editor and, fortunately, know exactly how to formulate the solution to your problem: a $d$-dimensional nearest neighbour algorithm.\nGiven a dataset $\\mathcal{D}$ of $n$ points in a space $X$ we want to be able to tell which are the closest point to a query point $q \\in X$, preferably in a way which is computationally cheaper than brute force methods (e.g. iterating through all of the points) which typically solve this problem in $\\mathcal{O}(dn)$ 1.\n$X$ could have $d$ dimensions (that is $\\mathcal{D} \\subset X : \\mathbb{R}^d$) and we define closest using2 Minkowski distance metrics, that is:\n$$ L_m = \\left(\\sum_{i=1}^d |p_i - q_i|^m\\right)^{\\frac{1}{m}},\\qquad p,q \\in X : \\mathbb{R}^d. $$\nA potential solution for this problem would be to use kd-trees, which for low dimension scenarios provide $\\mathcal{O}(\\log n)$ query times 3. However, as the number of dimensions increase (as quickly as $d\u0026gt;2$) the query times also increase as $2^d$.\nThe case can be made then for approximate nearest neighbour (NN) algorithms and that\u0026rsquo;s precisely what we will discuss here, namely the Balanced Box-Decomposition Tree (BBD, 1). The definition of approximate NN for a query point $q$ can be given as\n$$ \\text{dist}(p, q) \\leq (1+\\epsilon)\\text{dist}(p^{\\star},q),\\qquad \\epsilon \u0026gt; 0, $$\nwhere $p$ is the approximate NN and $p^{\\star}$ is the true NN. Let\u0026rsquo;s consider, for the sake of visualisation, a small two dimensional dataset $\\mathcal{D} \\to \\mathbb{R}^2$ as shown in Figure 1.\nFigure 1. A small test dataset in $\\mathbb{R}^2,n=7$.\nSpace decompositionBBD trees belong to the category of hierarchical space decomposition trees. In BBD trees, specifically, space is divided in $d$-dimensional rectangles and cells. Cells can either represent another $d$-dimensional rectangle or the intersection of two rectangles (one, the outer box fully enclosing the other, the inner box). Another important distinction of BBD trees is that rectangle\u0026rsquo;s size (in this context, the largest length in all of the $d$ dimensions) is bounded by a constant value. The space decomposition must follow an additional rule which is boxes must be sticky. If we consider a inner box $[x_{inner}, y_{inner}]$ contained in a outer box $[x_{outer}, y_{outer}]$, such that\n$$ [x_{inner}, y_{inner}] \\subseteq [x_{outer}, y_{outer}], $$\nthen, considering $w = y_{inner} - x_{inner}$, the box is considered sticky if either\n$$ \\begin{aligned} x_{inner}-x_{outer} = 0 \u0026amp;\\lor x_{inner}-x_{outer} \\nleq w \\\\ y_{outer}-y_{inner} = 0 \u0026amp;\\lor y_{outer}-y_{inner} \\nleq w. \\end{aligned} $$\nAn illustration of the stickiness concept can viewed in the diagram below.\nFigure 2. Visualisation of the \u0026ldquo;stickiness\u0026rdquo; criteria for $\\mathbb{R}^2$ rectangles.\nStickiness provides some important geometric properties to the space decomposition which will be discussed further on. The actual process of space decomposition will produce a tree of nodes, each with an associated $d$-dimensional rectangle enclosing a set of points. Each node will be further decomposed into children nodes, containing a region of space with a subset of the parent\u0026rsquo;s data points. If a node has no children it will be called a leaf node. The division process can occur either by means of:\na fair split, this is done by partitioning the space with an hyperplane, resulting in a low and high children nodes a shrink, splitting the box into a inner box (the inner child) and a outer box (the outer child). Figure 3. \u0026ldquo;Fair split\u0026rdquo; and \u0026ldquo;shrinking\u0026rdquo; division strategies example in $\\mathbb{R}^2$ with respective high/low and outer/inner children.\nThe initial node of the tree, the root node, will include all the dataset points, $\\mathcal{D}$. In the Figure 4 we can see a representation of the root node for the dataset presented above. We can see the node boundaries in dashed red lines as well as the node\u0026rsquo;s center, marked as $\\mu_{root}$.\nFigure 4. Associated cell for the BBD-tree root node for the example dataset. Node boundaries in red and node centre labelled as $\\mu_{root}$.\nThe actual method to calculate the division can either be based on the midpoint algorithm or the middle interval algorithm. The method used for these examples is the latter, for which more details can be found in 4. The next step is to divide the space according to the previously mentioned rules. As an example, we can see the root node\u0026rsquo;s respective children in Figure 5.\nFigure 5. BBD-tree root node\u0026rsquo;s lower (left) and upper (right) children. Node boundaries in red and centres labelled with a red cross.\nThis process is repeated until the child nodes are leaves and cannot be divided anymore. To better visualise the construction process it would be helpful to have a larger tree, so we will now consider still the 2-dimensional case, but now with a larger dataset (Figure 6), consisting of 2000 samples in total, each half from a bivariate Gaussian distribution:\n$$ \\begin{aligned} \\text{X}_1 \u0026amp;\\sim \\mathcal{N}([0,0], \\mathbf{I}) \\\\ \\text{X}_2 \u0026amp;\\sim \\mathcal{N}([3, 3], \\mathbf{I}). \\end{aligned} $$\nFigure 6. Larger example dataset in $\\mathbb{R}^2$ consisting of a realisation of $n=2000$ from two bivariate Gaussian distributions centred in $\\mu_1=(0,0)$ and $\\mu_2=(3,3)$ and with $\\Sigma=\\mathbf{I}$.\nWith this larger dataset, we have enough points to illustrate the tree node building. This time, we will start from the root node and always follow either the \u0026ldquo;lower\u0026rdquo; nodes or the \u0026ldquo;upper\u0026rdquo; nodes (as show in Figure 7). We can clearly see the cells getting smaller, until finally we have a single point included (i.e. a leaf node).\nFigure 7. BBD-tree node building process for the bivariate dataset. On the left we traverse the upper tree nodes and on the right the lower tree nodes.\nThis division process illustrates an important property of BBD-trees. Although other space decomposition algorithms (such as kd-trees) display a geometric reduction of number of points enclosed in each cell, methods such as the BBD-tree, which impose constraints on the cell\u0026rsquo;s size aspect ratio as stated before, display not only a geometric reduction in the number of points, but also in the cell\u0026rsquo;s size as well. The construction cost of a BBD-tree is $\\mathcal{O}(dn \\log n)$ and the tree itself will have $\\mathcal{O}(n)$ nodes and $\\mathcal{O}(\\log n)$ height.\nTree queryingNow that we have successfully constructed a BBD-tree, we want to actually find the (approximate) nearest neighbour of an arbitrary query point $q$ (Figure 8).\nFigure 8. Query point $q$ (red) for the bivariate dataset.\nThe first step consists in descending the tree in order to locate the smallest cell containing the query point $q$. This process is illustrated for the bivariate data in Figure 9.\nFigure 9. BBD-tree descent to locate the smallest cell containing $q$ (red).\nOnce the cell has been located, we proceed to enumerate all the leaf nodes contained by it and calculate our distance metric $L_2$ in this case) between the query point $q$ and the leaf nodes, eventually declaring the point with the smallest $L_2$ as the aproximate NN. BBD-trees provide strong guarantees that the ANN will be located within this cell and not in a neighbouring cell. In Figure 10 we zoomed in the smallest cell containing $q$ and show the associated calculated $L_2$ distance for each node.\nFigure 10. $L_2$ distance between leaf nodes and the query point $q$ inside the smallest cell containing $q$.\nAn important property of BBD-trees is that the tree structure does not need to be recalculated if we change either $\\epsilon$ or if we decide to use another $L_m$ distance metric 1. The query time for a point $q$ in a BBD-tree is $\\mathcal{O}(\\log n)$. For comparison, if you recall, the query time for a brute force method is typically $\\mathcal{O}(dn)$.\nFiltering and k-NNGreat. Now that you solved the USS Euler\u0026rsquo;s problem, you want to make a suggestion to the federation. Where to place several star-bases and how to divide the system\u0026rsquo;s coverage between them. An immediate generalisation of this method is easily applicable to the problem of clustering. Note that, at the moment, we are not concerned with determining the \u0026ldquo;best\u0026rdquo; clusters for our data5. Given a set of points $Z = {z_1, z_2, \\dots, z_n}$, we are concerned now in partitioning the data in clusters centred in each of the $Z$ points. A way of looking at this, is that we are building, for each point $z_n$ a Voronoi cell $V(z_n)$. This is achieved by a method called filtering. Filtering, in general terms, works by walking the tree with the list of candidate centres ($Z$) and pruning points from the candidate list as we move down. We will denote an arbitrary node as $n$, $z^{\\star}_w$ and $n_w$ respectively as the candidate and the node weight, $z^{\\star}_n$ and $n_n$ as the candidate and node count. The algorithm steps, as detailed in 6, are detailed below:\nFilter($n$, $Z$) { $C \\leftarrow n.cell$ if ($n$ is a leaf) { $z^{\\star} \\leftarrow$ the closest point in $Z$ to $n.point$ $z^{\\star}_w \\leftarrow z^{\\star}_w + n.point$ $z^{\\star}_n \\leftarrow z^{\\star}_n + 1\\qquad$ } *else { $z^{\\star} \\leftarrow$ the closest point in $Z$ to $C$\u0026rsquo;s midpoint for each ($z \\in Z \\setminus {z^{\\star}}$) { if ($z.isFarther(z^{\\star},C)$) { $Z \\leftarrow Z \\setminus {z}$ } if ($|Z|=1$) { $z^{\\star}_w \\leftarrow z^{\\star}_w + n_w$ $z^{\\star}_n \\leftarrow z^{\\star}_n + n_n$ } else { Filter($n_{left}, Z$) Filter($n_{right}, Z$) } } To illustrate the assignment of data points to the centres, we will consider the previous bivariate Gaussian data along with two centres, $z_1 = {0,0}$ and $z_2 = {3, 3}$. Figure 11 shows the process of splitting the dataset $\\mathcal{D}$ into two clusters, namely the subsets of data points closer to $z_1$ or $z_2$.\nFigure 11. Assignment of points in $\\mathcal{D}$ to $Z$. Data points coloured according to the assigned center. Lines represent the distance from the cells midpoint to $Z$.\nWe can see in Figure 12 the final cluster assignment of the data points. With a $\\mathbb{R}^2$ dataset and only two centres the organisation of points follows a simple perpendicular bisection of the segment connecting the centres, as expected.\nFigure 12. Final $\\mathcal{D}$ point assignment to clusters centred in $z_1$ and $z_2$.\nIn Figure 13 we can see more clearly the dataset clusters changing when center $z_1$ is moving around the plane. BBD-trees can play an important role in improving $k$-means performance, as described in 6.\nFigure 13. Dynamic assignment of points to a cluster using a BBD-tree.\nThis concludes a (short) introduction to BBD-trees, I hope you enjoyed it. If you have any comments or suggestions, please let me know at Mastodon.\nArya, S., Mount, D. M., Netanyahu, N. S., Silverman, R., \u0026amp; Wu, A. Y. (1998). An optimal algorithm for approximate nearest neighbor searching fixed dimensions. Journal of the ACM. https://doi.org/10.1145/293347.293348\u0026#160;\u0026#x21a9;\u0026#xfe0e;\u0026#160;\u0026#x21a9;\u0026#xfe0e;\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nThe $L_m$ distance may be pre-computed in this method to avoid recalculation for each query.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nFriedman, J. H., \u0026amp; Bentley, J. L. (1977). RA Finkel. An algorithm for finding best matches in logarithmic expected time. ACM Transactions on Mathematical Software, 3(3), 209-226.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nCallahan, P. B., \u0026amp; Kosaraju, S. R. (1995). A decomposition of multidimensional point sets with applications to k-nearest-neighbors and n-body potential fields. Journal of the ACM, 42(1), 67-90.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nThis would be a k-means problem. I intend to write a blog post on k-means clustering (and the role BBD-trees can play) in the future.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nKanungo, T., Mount, D. M., Netanyahu, N. S., Piatko, C. D., Silverman, R., \u0026amp; Wu, A. Y. (2002). An efficient k-means clustering algorithms: Analysis and implementation. IEEE Transactions on Pattern Analysis and Machine Intelligence, 24(7), 881892. https://doi.org/10.1109/TPAMI.2002.1017616\u0026#160;\u0026#x21a9;\u0026#xfe0e;\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"/introduction-to-balanced-box-decomposition-trees.html","tags":null,"title":"Introduction to Balanced Box-Decomposition Trees"},{"categories":null,"contents":"Isolation Forests (IFs), presented in Liu1 et. al (2012), are a popular algorithm used for outlier classification. In a very simplified way, the method consists of building an ensemble of Isolation Trees (ITs) for a given data set and observations are deemed anomalies if they have short adjusted average path lengths on the ITs.\nITs, which will be covered shortly, have several properties in common with a fundamental data structure: the Binary Search Tree (BSTs). In a very simplified way, BSTs are a special instance of tree structures where keys are kept in such an order that a node search is performed by iteratively (or recursively) choosing a left or right branch based on a quantitative comparison (e.g. lesser or greater). Node insertion is performed by doing a tree search, using the method described previously, until reaching an external node, where the new node will be inserted. This allows for efficient node searches since, on average, half the tree will not be visited. To illustrate this assume the values $x=[1, 10, 2, 4, 3, 5, 26, 9, 7, 54]$ and the respective insertion on a BST. The intermediate steps would then be as shown below.\nOne of the properties of BSTs is that, with randomly generated data, the path between the root node and the outliers will typically be shorter. We can see from the illustration below that, with our example data, the path length for (say) 7 is twice the length than for the suspicious value of 54. This property will play an important role in the IF algorithm, as we will see further on.\nIsolation TreesSince ITs are the fundamental component of IFs, we will start by describing their building process. We start by defining $t$ as the number of trees in the IF, $\\mathcal{D}$ as the training data (contained in an $n$-dimensional feature space, $\\mathcal{D} \\subset \\mathbb{R}^n$) and $\\psi$ as the subsampling size. The building of a IT consists then in recursively partitioning the data $\\mathcal{D}$ by sampling (without replacement) a subsample $\\mathcal{D}^{\\prime}$ of size $\\psi$. We then build an isolation tree $\\mathcal{T}^{\\prime}$ with this subsample (in order to later add it to the isolation forest $\\mathcal{F}$) and the process is repeated $t$ times.\nTo build an isolation tree $\\mathcal{T}^{\\prime}$ from the subsample we proceed as follows: if the data subsample $\\mathcal{D}^{\\prime}$ is indivisible, a tree is returned containing a single external node corresponding to the feature dimensions, $n$. If it can be divided, a series of steps must be performed. Namely, if we consider $Q = \\lbrace q_1,\\dots,q_n\\rbrace$ as the list of features in $\\mathcal{D}^{\\prime}$, we select a random feature $q \\in Q$ and a random split point $p$ such that\n$$ \\min(q) \u0026lt; p \u0026lt; \\max(q), \\qquad q \\in Q. $$\nBased on the cut-off point $p$, we filter the features into a BSTs left and right nodes according to\n$$ \\mathcal{D}_l := \\lbrace \\mathcal{D}^{\\prime} : q \\in Q, q\u0026lt;p\\rbrace \\ \\mathcal{D}_r := \\lbrace \\mathcal{D}^{\\prime} : q \\in Q, q \\geq p\\rbrace, $$\nand return an internal node having an isolation tree with left and right nodes as respectively $\\mathcal{D}_l$ and $\\mathcal{D}_r$.\nTo illustrate this (and the general method of identifying anomalies in a two dimensional feature space, $x\\in\\mathbb{R}^2$) we will look at some simulated data and its processing. We start by simulating two clusters of data from a multivariate normal distribution, one centred in $x_a=[-10, 10]$ and another centred in $x_b=[10, 10]$, with a variance of $\\Sigma=\\text{diag}(2, 2)$, that is\n$$ X_a \\sim \\mathcal{N}\\left([-10, -10], \\text{diag}(2, 2)\\right) \\ X_b \\sim \\mathcal{N}\\left([10, 10], \\text{diag}(2, 2)\\right). $$\nThe particular realisation of this simulation looks like this:\nBelow we illustrate the building of a single IT (given the data), illustrating the feature split point $p$ and respective division of the feature list into left or right IT nodes. The process is conducted recursively until the feature list is no longer divisible. As mentioned previously, this process, the creation of an IT, is repeated $t$ times in order to create the IF.\nThere should have been a video here but your browser does not seem to support it. In order to perform anomaly detection (e.g. observation scoring) we will then use the IT equivalent of the BST unsuccessful search heuristics. An external node termination in an IT is equivalent to a BST unsuccessful search. Given an observation $x$, our goal is then to calculate the score for this observation, given our defined subsampling size, that is, $s(x,\\psi)$.\nThis technique amounts to partitioning the feature space randomly until feature points are isolated. Intuitively, points in high density regions will need more partitioning steps, whereas anomalies (by definition away from high density regions) will need fewer splits. Since the building of the ITs is performed in a randomised fashion and using a subsample of the data, this density predictor can be average over a number of ITs, the Isolation Forest.\nIntuitively, this could be done by calculating the average path length for our $\\mathcal{T}n, n=1,\\dots,t$ ITs, $\\overline{h}(x)$. However, as pointed in Liu1 et. al (2012), a problem with calculating this is that maximum possible height of each $\\mathcal{T}_n$ grows as $\\mathcal{O}(\\log(\\psi))$. To compare $h(x)$ given different subsampling sizes, a normalisation factor, $c(\\psi)$ must be established. This can be calculated by\n$$ c(\\psi) = \\begin{cases} 2H(\\psi-1)-2\\frac{\\psi-1}{n},\\text{if}\\ \\psi \u0026gt;2,\\ 1, \\text{if}\\ \\psi=2,\\ 0, \\text{otherwise}, \\end{cases} $$\nwhere $H(i)$ is the harmonic number estimated by $H(i)\\approx\\log(i) + e$.\nDenoting $h_{max}$ as the tree height limit and e as the current path length, initialised as $e=0$ we can then calculate $h(x)$ recursively as:\n$$ h(x,\\mathcal{T},h_{max},e) = \\begin{cases} h(x,\\mathcal{T}{n,left},h{max},e+1) \\text{if}\\ x_a \u0026lt; q_{\\mathcal{T}} \\ h(x,\\mathcal{T}{n,right},h{max},e+1) \\text{if}\\ x_a \\geq q_{\\mathcal{T}} \\ e+c(\\mathcal{T_{n,s}}) \\text{if}\\ \\mathcal{T} \\text{is a terminal node or}\\ e \\geq h_{max}. \\end{cases} $$\nGiven these quantities we can then, finally, calculate the anomaly score, $s$ as\n$$ s(x,\\psi) = 2^{-\\frac{\\text{E}[h(x)]}{c(\\psi)}} $$\nwith $\\text{E}[h(x)]$ being the average $h(x)$ for a collection of ITs.\nParameters As mentioned in Liu1 et. al (2012), the empirical subsampling size $\\psi=2^8$ is typically enough to perform anomaly detection in a wide range of data. Regarding the number of trees, $t$ no considerable accuracy gain is usually observed with $t\u0026gt;100$. In the plots below, we can see the score calculation for two point in our data, namely an outlier ($x_o=[3.10, -12.69])$ and a normal observation ($x_n=[8.65, 9.71]$) with a varying number of trees and $\\psi=2^8$ (left) and a varying subsample size and $t=100$ (right). We can see that the score value stabilised quite early on when using $\\psi=2^8$ and that very low subsampling sizes can lead to problems when classifying anomalies.\nNow that we know how to implement an IF algorithm and calculate an anomaly score, we will try to visualise the anomaly score distribution in the vicinity of the simulated data. To do so, we simply create a two dimensional lattice enclosing our data an iteratively calculate $s(., \\psi)$. The result is show below:\nThe above steps fully define a naive isolation forest algorithm, which when applied to the previously simulated data, result in 88% of the anomalies being correctly identified.\nThanks for reading! If you have any questions or comments, please let me know on Mastodon or Twitter.\nLiu, F. T., Ting, K. M., \u0026amp; Zhou, Z. (2012). Isolation-based anomaly detection. ACM Transactions on Knowledge Discovery from Data (TKDD), 6(1), 139.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\u0026#160;\u0026#x21a9;\u0026#xfe0e;\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"/introduction-to-isolation-forests.html","tags":null,"title":"Introduction to Isolation Forests"},{"categories":null,"contents":"SummaryThis page contains links to most Java topics. Java, created by James Gosling, among others.\nTools Java build systems Includes Maven JReleaser JUnit Concepts Java Completable Futures Java consumer Extending JUnit ReferenceGet user home directorySystem.getProperty(\u0026#34;user.home\u0026#34;); List files recursivelytry (Stream\u0026lt;Path\u0026gt; walk = Files.walk(Paths.get(input))) { List\u0026lt;String\u0026gt; result = walk.filter(Files::isRegularFile) .map(x -\u0026gt;x.toString()) .collect(Collectors.toList()); result.forEach(System.out::println); } catch (IOException e) { e.printStackTrace(); } In case we want the file subset with a specific extension, txt we can filter the stream with\nList\u0026lt;String\u0026gt; result = walk.filter(Files::isRegularFile) .filter(x -\u0026gt; x.toString().endsWith(\u0026#34;.txt\u0026#34;)) .map(x -\u0026gt; x.toString()) .collect(Collectors.toList()); Setting the logger levelFrom the CLI specify it with, for instance to set the logger level to info:\n$ mvn test -Dorg.slf4j.simpleLogger.defaultLogLevel=info ","permalink":"/java.html","tags":null,"title":"Java"},{"categories":null,"contents":"SummaryNotes on Java build systems.\nMaven ","permalink":"/java-build-systems.html","tags":null,"title":"Java build systems"},{"categories":null,"contents":"Running in parallelimport java.util.concurrent.CompletableFuture; import java.lang.InterruptedException; import java.util.concurrent.ExecutionException; public static void main(String[] args) throws InterruptedException, ExecutionException { CompletableFuture\u0026lt;String\u0026gt; future1 = CompletableFuture.supplyAsync(() -\u0026gt; \u0026#34;Hello\u0026#34;); CompletableFuture\u0026lt;String\u0026gt; future2 = CompletableFuture.supplyAsync(() -\u0026gt; \u0026#34;Beautiful\u0026#34;); CompletableFuture\u0026lt;String\u0026gt; future3 = CompletableFuture.supplyAsync(() -\u0026gt; \u0026#34;World\u0026#34;); CompletableFuture\u0026lt;Void\u0026gt; combinedFuture = CompletableFuture.allOf(future1, future2, future3); CompletableFuture\u0026lt;String\u0026gt; result = combinedFuture.thenApply(v -\u0026gt; future1.join() + future2.join() + future3.join()); System.out.println(result.get()); } Waiting for allLets assume we have a completable future, $f$. This future, in turn, create $N$ additional completable futures, $f_1, f_2, \\dots, f_N$. How can we set $f$ to complete only when /all/ $f_1, f_2, \\dots, f_N$ are also completed?\nThe answer is to use a combination of allOf1 with thenRun2. According to the documentation, allOf returns a new CompletableFuture that is completed when all of the given CompletableFutures complete. In turn, thenRun will execute the given action. Let\u0026rsquo;s look at an example:\nimport java.util.concurrent.CompletableFuture; import java.util.concurrent.ExecutorService; import java.util.concurrent.Executors; import java.lang.InterruptedException; import java.util.concurrent.ExecutionException; public static void main(String[] args) throws InterruptedException, ExecutionException { CompletableFuture\u0026lt;String\u0026gt; f = new CompletableFuture\u0026lt;\u0026gt;(); int N = 10; CompletableFuture\u0026lt;String\u0026gt; f1 = CompletableFuture.completedFuture(\u0026#34;f1\u0026#34;); CompletableFuture\u0026lt;String\u0026gt; f2 = CompletableFuture.completedFuture(\u0026#34;f2\u0026#34;); CompletableFuture\u0026lt;String\u0026gt; f3 = CompletableFuture.completedFuture(\u0026#34;f3\u0026#34;); ExecutorService executor = Executors.newSingleThreadExecutor(); executor.submit(() -\u0026gt; { CompletableFuture.allOf(f1, f2, f3).thenRun(() -\u0026gt; f.complete(\u0026#34;f1,f2,f3 completed.\\nProceed to finish f.\u0026#34;)); }); f.thenAccept(v -\u0026gt; { System.out.println(v); }); Thread.sleep(100); executor.shutdown(); } https://docs.oracle.com/javase/8/docs/api/java/util/concurrent/CompletableFuture.html#allOf-java.util.concurrent.CompletableFuture\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nhttps://docs.oracle.com/javase/8/docs/api/java/util/concurrent/CompletableFuture.html#thenRun-java.lang.Runnable\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"/java-completable-futures.html","tags":null,"title":"Java Completable Futures"},{"categories":null,"contents":"IntroductionApplyingIntroduced in Java 8, the Consumer interface aims at providing additional functional programming capabilities for Java.\nConsumer defined functions do not return any value and they consist mainly of two methods:\nvoid accept(T t); default Consumer\u0026lt;T\u0026gt; andThen(Consumer\u0026lt;? super T\u0026gt; after); Let\u0026rsquo;s look at an example:\nimport java.util.ArrayList; import java.util.function.Consumer; public static void main(String[] args) { Consumer\u0026lt;String\u0026gt; say = a -\u0026gt; System.out.println(\u0026#34;Hello, \u0026#34; + a + \u0026#34;!\u0026#34;); say.accept(\u0026#34;World\u0026#34;); } : Hello, World! A Consumer can be applied in a functional way, since applying a consumer is equivalent to applying the accept method. For instance:\nimport java.util.ArrayList; import java.util.List; import java.util.function.Consumer; public static void main(String[] args) { Consumer\u0026lt;String\u0026gt; say = a -\u0026gt; System.out.println(\u0026#34;Hello, \u0026#34; + a + \u0026#34;!\u0026#34;); List\u0026lt;String\u0026gt; musketeers = new ArrayList\u0026lt;String\u0026gt;(); musketeers.add(\u0026#34;D\u0026#39;Artagnan\u0026#34;); musketeers.add(\u0026#34;Athos\u0026#34;); musketeers.add(\u0026#34;Aramis\u0026#34;); musketeers.add(\u0026#34;Porthos\u0026#34;); musketeers.stream().forEach(say); } : Hello, D\u0026#39;Artagnan! : Hello, Athos! : Hello, Aramis! : Hello, Porthos! Consumer functions can also modify reference objects. For instance:\nimport java.util.ArrayList; import java.util.List; import java.util.function.Consumer; public static void main(String[] args) { List\u0026lt;Double\u0026gt; numbers = new ArrayList\u0026lt;Double\u0026gt;(); numbers.add(1d); numbers.add(2d); numbers.add(3d); Consumer\u0026lt;List\u0026lt;Double\u0026gt;\u0026gt; square = list -\u0026gt; { for (int i = 0; i \u0026lt; list.size(); i++) { double x = list.get(i); list.set(i, x*x); }; }; System.out.println(numbers); square.accept(numbers); System.out.println(numbers); } : [1.0, 2.0, 3.0] : [1.0, 4.0, 9.0] ComposingLet\u0026rsquo;s now look at how to create a chain of Consumers by composing them with the andThen method. Let\u0026rsquo;s first create a consumer which converts a string to uppercase in-place:\nimport java.util.ArrayList; import java.util.List; import java.util.function.Consumer; public static void main(String[] args) { Consumer\u0026lt;String\u0026gt; say = a -\u0026gt; System.out.println(\u0026#34;Hello, \u0026#34; + a + \u0026#34;!\u0026#34;); Consumer\u0026lt;List\u0026lt;String\u0026gt;\u0026gt; upperCaseConsumer = list -\u0026gt; { for(int i=0; i\u0026lt; list.size(); i++){ String value = list.get(i).toUpperCase(); list.set(i, value); } }; Consumer\u0026lt;List\u0026lt;String\u0026gt;\u0026gt; sayAll = list -\u0026gt; list.stream().forEach(say); } We will now create a chain by first applying upperCaseConsumer and the say to our list.\nimport java.util.ArrayList; import java.util.List; import java.util.function.Consumer; public static void main(String[] args) { Consumer\u0026lt;String\u0026gt; say = a -\u0026gt; System.out.println(\u0026#34;Hello, \u0026#34; + a + \u0026#34;!\u0026#34;); Consumer\u0026lt;List\u0026lt;String\u0026gt;\u0026gt; upperCaseConsumer = list -\u0026gt; { for(int i=0; i\u0026lt; list.size(); i++){ String value = list.get(i).toUpperCase(); list.set(i, value); } }; Consumer\u0026lt;List\u0026lt;String\u0026gt;\u0026gt; sayAll = list -\u0026gt; list.stream().forEach(say); upperCaseConsumer.andThen(sayAll).accept(musketeers); } ","permalink":"/java-consumer.html","tags":null,"title":"Java consumer"},{"categories":null,"contents":"Flat mappingNested mapsimport java.util.Map; import java.util.HashMap; public static void main(String[] args) { final Map\u0026lt;String, Object\u0026gt; client = new HashMap\u0026lt;\u0026gt;(); client.put(\u0026#34;Age\u0026#34;, 43); client.put(\u0026#34;Salary\u0026#34;, 1950); client.put(\u0026#34;Existing payments\u0026#34;, 100); final Map\u0026lt;String, Object\u0026gt; loan = new HashMap\u0026lt;\u0026gt;(); loan.put(\u0026#34;Duration\u0026#34;, 15); loan.put(\u0026#34;Installment\u0026#34;, 100); final Map\u0026lt;String, Object\u0026gt; contextVariables = new HashMap\u0026lt;\u0026gt;(); contextVariables.put(\u0026#34;Client\u0026#34;, client); contextVariables.put(\u0026#34;Loan\u0026#34;, loan); System.out.println(contextVariables); } : {Loan={Installment=100, Duration=15}, Client={Salary=1950, Existing payments=100, Age=43}} ","permalink":"/java-streams.html","tags":null,"title":"Java streams"},{"categories":null,"contents":"Notes on the JPype Python library.\nConversionNumpy arrays","permalink":"/jpype.html","tags":null,"title":"JPype"},{"categories":null,"contents":"Test locationsBy default JUnit will look for test following the glob pattern:\n**/Test*.java **/*Test.java **/*Tests.java **/*TestCase.java If you want to add different patterns or locations (as well as exclude them), this can be done via de surefire Maven configuration:\n\u0026lt;build\u0026gt; \u0026lt;plugins\u0026gt; \u0026lt;plugin\u0026gt; \u0026lt;groupId\u0026gt;org.apache.maven.plugins\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;maven-surefire-plugin\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;3.0.0-M7\u0026lt;/version\u0026gt; \u0026lt;configuration\u0026gt; \u0026lt;includes\u0026gt; \u0026lt;include\u0026gt;**/*Test.java\u0026lt;/include\u0026gt; \u0026lt;/includes\u0026gt; \u0026lt;excludes\u0026gt; \u0026lt;exclude\u0026gt;**/FooTest.java\u0026lt;/exclude\u0026gt; \u0026lt;exclude\u0026gt;**/NotReallyATest.java\u0026lt;/exclude\u0026gt; \u0026lt;/excludes\u0026gt; \u0026lt;/configuration\u0026gt; \u0026lt;/plugin\u0026gt; \u0026lt;/plugins\u0026gt; \u0026lt;/build\u0026gt; ","permalink":"/junit.html","tags":null,"title":"JUnit"},{"categories":null,"contents":"IntroductionK-means is still one of the fundamental clustering algorithms. It is used in such diverse fields as Natural Language Processing (NLP), social sciences and medical sciences.\nThe core idea behind K-means is that we want to group data in clusters. Data points will be assigned to a specific cluster depending on it\u0026rsquo;s distance to a cluster\u0026rsquo;s center, usually called the centroid.\nIt is important to note that typically, the mean distance to a centroid is used to partition the clusters, however, difference distances can be used and different pivot points. An example is the K-medoids clustering algorithm.\nWe will define the two main steps of a generic K-means clustering algorithm, namely the data assignement and the centroid update step.\nData assignementThe criteria to determine whether a point is closer to one centroid is typically an Euclidean distance ($L^2$) . If we consider a set of $n$ centroids $C$, such that\n$$ C = \\lbrace c_1, c_2, \\dots, c_n \\rbrace $$\nWe assign each data point in $\\mathcal{D}=\\lbrace x_1, x_2, \\dots, x_n \\rbrace$ to the nearest centroid according to its distance, such that\n$$ \\underset{c_i \\in C}{\\arg\\min} ; dist(c_i,x)^2 $$\nAs mentioned previously $dist(.)$ is typically the standard ($L^2$) Euclidean distance. We define the subset of points assigned to a centroid $i$ as $S_i$.\nCentroid update stepThis step corresponds to updating the centroids using the mean of add points assign to a cluster, $S_i$. That is\n$$ c_i=\\frac{1}{|S_i|}\\sum_{x_i \\in S_i} x_i $$\nPartitioningDifferent algorithms can be used for cluster partitioning, for instance:\nPAM CLARA CLARANS PAMTo illustrate the PAM partitioning method, we will use a synthetic dataset created along the guidelines in synthetic data generation.\nElbow methodIn order to use the \u0026ldquo;Elbow method\u0026rdquo; we calculate the Within-Cluster Sum of Squares (WCSS) for a varying number of clusters, $K$.\nimport numpy as np import matplotlib.pyplot as plt import pandas as pd import sklearn dataset = pd.read_csv(\u0026#39;../../data/mall-customers.zip\u0026#39;) X = dataset.iloc[:, [3, 4]].values from sklearn.cluster import KMeans wcss = [] for i in range(1, 11): kmeans = KMeans(n_clusters = i, init = \u0026#39;k-means++\u0026#39;, random_state = 42) kmeans.fit(X) wcss.append(kmeans.inertia_) from plotutils import * plt.plot(range(1, 11), wcss) plt.xlabel(\u0026#39;Number of clusters\u0026#39;) plt.ylabel(\u0026#39;WCSS\u0026#39;) plt.show() kmeans = KMeans(n_clusters = 5, init = \u0026#34;k-means++\u0026#34;, random_state = 42) y_kmeans = kmeans.fit_predict(X) y_kmeans array([2, 3, 2, 3, 2, 3, 2, 3, 2, 3, 2, 3, 2, 3, 2, 3, 2, 3, 2, 3, 2, 3, 2, 3, 2, 3, 2, 3, 2, 3, 2, 3, 2, 3, 2, 3, 2, 3, 2, 3, 2, 3, 2, 0, 2, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 1, 4, 0, 4, 1, 4, 1, 4, 0, 4, 1, 4, 1, 4, 1, 4, 1, 4, 0, 4, 1, 4, 1, 4, 1, 4, 1, 4, 1, 4, 1, 4, 1, 4, 1, 4, 1, 4, 1, 4, 1, 4, 1, 4, 1, 4, 1, 4, 1, 4, 1, 4, 1, 4, 1, 4, 1, 4, 1, 4, 1, 4, 1, 4, 1, 4, 1, 4, 1, 4, 1, 4, 1, 4, 1, 4], dtype=int32) ps = 30 plt.scatter(X[y_kmeans == 0, 0], X[y_kmeans == 0, 1], s = ps, c = colours[0], label = \u0026#39;Cluster1\u0026#39;) plt.scatter(X[y_kmeans == 1, 0], X[y_kmeans == 1, 1], s = ps, c = colours[1], label = \u0026#39;Cluster2\u0026#39;) plt.scatter(X[y_kmeans == 2, 0], X[y_kmeans == 2, 1], s = ps, c = colours[2], label = \u0026#39;Cluster3\u0026#39;) plt.scatter(X[y_kmeans == 3, 0], X[y_kmeans == 3, 1], s = ps, c = colours[3], label = \u0026#39;Cluster4\u0026#39;) plt.scatter(X[y_kmeans == 4, 0], X[y_kmeans == 4, 1], s = ps, c = colours[4], label = \u0026#39;Cluster5\u0026#39;) plt.scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 1], s = 100, c = \u0026#39;black\u0026#39;, label = \u0026#39;Centroids\u0026#39;) plt.xlabel(\u0026#39;Annual Income (k$)\u0026#39;) plt.ylabel(\u0026#39;Spending Score (1-100)\u0026#39;) plt.legend() plt.show() ","permalink":"/k-means-clustering.html","tags":null,"title":"K-means clustering"},{"categories":null,"contents":"OpenShiftInstallingTo install a k8s operator, define the subscription with, for instance opendatahub-operator.yaml:\napiVersion: operators.coreos.com/v1alpha1 kind: Subscription metadata: name: opendatahub-operator namespace: openshift-operators spec: channel: rolling installPlanApproval: Automatic name: opendatahub-operator source: community-operators sourceNamespace: openshift-marketplace and apply it with oc apply -f opendatahub-operator.yaml.\n","permalink":"/k8s-operators.html","tags":null,"title":"k8s operators"},{"categories":null,"contents":"Radial Basis Function (RBF)Given\n$$ K(X_1,X_2)=\\sigma^2\\exp\\left(-\\frac{||X_1-X_2||^2}{2\\ell^2}\\right) $$\n$\\sigma^2$ is the overall variance (where $\\sigma$ is also known as the amplitude) It determines the average distance of your function away from its mean. It can be interpreted as a scale factor. $\\ell$ the lengthscale. In general, you won\u0026rsquo;t be able to extrapolate more than $\\ell$ units away from your data. import numpy as np from sklearn.gaussian_process.kernels import RBF np.random.seed(42) xlim = (-4, 4) X = np.expand_dims(np.linspace(*xlim, num=75), 1) zero = np.array([0.]()) amplitude = 1.0 1 = (amplitude**2) * RBF(length_scale=1)(X) 2 = (amplitude**2) * RBF(length_scale=0.5)(X) amplitude = 0.5 3 = (amplitude**2) * RBF(length_scale=1)(X) ","permalink":"/kernel-functions.html","tags":null,"title":"Kernel functions"},{"categories":null,"contents":"These are the KinD notes. Kubernetes IN Docker.\nInstallationTo install on Linux:\ncurl -Lo ./kind https://kind.sigs.k8s.io/dl/v0.17.0/kind-linux-amd64 chmod +x ./kind sudo mv ./kind /usr/local/bin/kind Storage Create a directory on your laptop that will serve as the Persistent Volume (PV). Create a YAML file for the PV and PVC, specifying the path to the directory on your laptop as the source of the PV. Apply the YAML file to your KIND cluster to create the PV and PVC. In your pod specification, refer to the PVC by its name to mount the volume. Here is an example of a YAML file for the PV and PVC:\napiVersion: v1 kind: PersistentVolume metadata: name: mypv spec: capacity: storage: 1Gi accessModes: - ReadWriteOnce hostPath: path: /path/to/your/laptop/directory persistentVolumeReclaimPolicy: Retain --- apiVersion: v1 kind: PersistentVolumeClaim metadata: name: mypvc spec: accessModes: - ReadWriteOnce resources: requests: storage: 1Gi To apply the YAML file, run:\nkubectl apply -f pv-pvc.yaml In your pod specification, refer to the PVC by its name to mount the volume:\napiVersion: v1 kind: Pod metadata: name: mypod spec: containers: - name: mycontainer image: myimage volumeMounts: - name: mypv mountPath: /path/in/container volumes: - name: mypv persistentVolumeClaim: claimName: mypvc ","permalink":"/kind.html","tags":null,"title":"KinD"},{"categories":null,"contents":"IntroductionSome requirements to run KNative:\nMinikube kubectl Architecture ","permalink":"/knative.html","tags":null,"title":"KNative"},{"categories":null,"contents":"","permalink":"/Kompose.html","tags":null,"title":"Kompose"},{"categories":null,"contents":"IntroductionKourier page.\n","permalink":"/kourier.html","tags":null,"title":"Kourier"},{"categories":null,"contents":"Kubernetes (also known as K8s) is an open-source container orchestration system for automating the deployment, scaling, and management of containerised applications. It was originally developed by Google and is now maintained by the Cloud Native Computing Foundation (CNCF)1.\nArchitecture Master node: This is the central control plane of the Kubernetes cluster, responsible for managing the overall state of the system. The master node runs a number of components, including the API server, scheduler, and controller manager. Worker nodes: These are the machines that actually run the containers. A worker node runs a container runtime, such as Docker or containerd, as well as a kubelet process, which is responsible for communicating with the master node and launching containers on the node. Pods: A pod is the smallest deployable unit in Kubernetes. It is a logical host for one or more containers, and all containers in a pod are scheduled on the same node. Pods provide a shared context for the containers, allowing them to communicate with each other and access shared resources such as shared storage. Replication controller: A replication controller ensures that a specified number of replicas of a pod are running at any given time. If a pod fails, the replication controller will create a new one to replace it. Services: A service is a logical abstraction that represents a group of pods and defines how they should be accessed. Services allow you to access your application through a stable, load-balanced endpoint, rather than having to directly access individual pods. Deployments: A deployment is a higher-level abstraction that represents a desired state for your application. It specifies the number of replicas of a pod that should be running and manages the process of rolling out updates to your application. Volumes: A volume is a persistent storage mechanism that can be mounted into a pod. It allows containers to access data that is stored outside of the container itself, such as on a shared filesystem or in a cloud storage service. ustom resource definitions (CRDs): which permit you to define custom resources in Kubernetes. A custom resource is a resource that is not natively supported by the Kubernetes API, but that you can create and manage as one. ComponentsCustom Resource DefinitionsA Custom Resource Definiton (CRD) is an endpoint in the Kubernetes API that stores a collection of API objects. This abstraction permits an expanding the Kubernetes API with new resource definitions.\nFor example, you might want to create a custom resource to manage a specific type of resource in your application, such as a database or a message queue. With CRDs, you can define the custom resource and the associated API for managing it, and then use the Kubernetes API to create, delete, and update instances of that resource.\nCRDs are useful because they allow you to extend the Kubernetes API with custom resources that are specific to your application or environment. This can make it easier to manage your application within the Kubernetes ecosystem, as you can use the same tools and APIs to manage both native and custom resources.\nTo use custom resource definitions, you first need to create a definition of the custom resource in the form of a CustomResourceDefinition object. This object defines the properties of the custom resource, such as its name, scope, and version, as well as the schema for the resource\u0026rsquo;s data. Once you have created the definition, you can use the Kubernetes API to create and manage instances of the custom resource.\nServicesIngress serviceIn Kubernetes, an ingress2 is a collection of rules that allow inbound connections to reach the cluster services. It acts as a reverse proxy, routing traffic from the Internet to the appropriate service within the cluster.\nAn ingress service is a special type of service that exposes an HTTP or HTTPS endpoint for external traffic. It is typically used to provide a single entry point for all traffic to the cluster, allowing you to route traffic to different services based on the incoming request.\nTo create an ingress service, you first need to define an ingress resource. This resource specifies the rules for routing traffic to the appropriate service, such as the hostname or path that should be used to route traffic to a particular service.\nOnce you have defined the ingress resource, you can create an ingress controller to implement the ingress rules. An ingress controller is a piece of software that runs within the cluster and listens for incoming traffic, routing it to the appropriate service based on the rules defined in the ingress resource.\nOverall, an ingress service is a useful way to provide a single entry point for external traffic to your cluster, allowing you to easily route traffic to the appropriate service based on the incoming request.\nOperatorsFirst of all: what\u0026rsquo;s the difference between a controller and an operator? A controller is a piece of software that runs within the cluster and watches for changes to resources. When a change occurs, the controller takes some action to reconcile the current state of the resource with the desired state. An operator is a controller that is designed to manage a specific type of resource (a CustomResourceDefinition). For example, a database operator might be responsible for managing a database resource, while a message queue operator might be responsible for managing a message queue resource.\nOperator languagesMost operators are written in Go, but there are also operators written in other languages such as Python and Java. In theory, any language can be used to write an operator, as long as it can interact with the Kubernetes API which is mostly done over HTTP using Kubernetes OpenAPI client libraries.\nHowever, some languages are better suited for writing operators than others. For example, Go is a good choice because it is a compiled language that produces a single binary, which makes it easy to distribute and run. It also has good support for concurrency, which is important for operators that need to handle multiple requests at once.\nSome examples of languages with have good support for writing operators include:\nGo, using client-go3 and controller-runtime Rust, using kube-rs4 Python, using kubernetes-client5 Some operator frameworks also provide support for writing operators in other languages, such as:\nOperator Framework (Golang) Kopf (Python) Resources Writing Kubernetes operators ConceptsSelectorsA ==Kubernetes selector is a label that is used to select pods that match a certain criteria==. Selectors can be used to select pods based on their labels, annotations, or even the contents of their containers.\nSelectors are used in a variety of Kubernetes operations, such as:\nCreating a deployment. When you create a deployment, you can specify a selector to select the pods that should be created. Creating a service. When you create a service, you can specify a selector to select the pods that should be exposed by the service. Scaling a deployment. When you scale a deployment, you can specify a selector to select the pods that should be scaled. Scheduling a pod. When you schedule a pod, you can specify a selector to select the node that the pod should be scheduled on. Selectors are a powerful tool that can be used to control how Kubernetes manages your pods. More info on selectors:\nKubernetes Labels Expert Guide with 10 Best Practices MinikubeMain page on Minikube is here.\nToolsKubernetes tools page is here.\nArticles The Missing Kubernetes Type System https://www.cncf.io/\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nhttps://kubernetes.io/docs/concepts/services-networking/ingress/\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nhttps://github.com/kubernetes/client-go\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nhttps://github.com/kube-rs/kube\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nhttps://github.com/kubernetes-client/python\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"/kubernetes.html","tags":null,"title":"Kubernetes"},{"categories":null,"contents":"Notes on Kustomize1.\nInstallationkubectl is a Kustomize requirement. To install the binary version of Kustomize use\n$ curl -s \u0026#34;https://raw.githubusercontent.com/kubernetes-sigs/kustomize/master/hack/install_kustomize.sh\u0026#34; | bash ConfigMapsConfigMap GeneratorsCreating a ConfigMap from a kustomization.yaml:\nconfigMapGenerator: - name: some-cmap files: - ./blah.yaml options: disableNameSuffixHash: true This will create a ConfigMap named some-cmap from the file ./blah.yaml. It will also disable the hash suffix on the name of the ConfigMap.\nhttps://kustomize.io/\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"/kustomize.html","tags":null,"title":"Kustomize"},{"categories":null,"contents":" Last week, at the North East Functional Programming meet up, we were given a code Kata consisting of the Langton\u0026rsquo;s ant algorithm.\nI\u0026rsquo;ve had a go at Scala but decided later on to put a live version in this blog.\nI considered several implementation options, such as scala.js and Elm, but in the end decided to implement it in plain Javascript.\nAdd ant\n","permalink":"/langtons-ant.html","tags":null,"title":"Langton's ant"},{"categories":null,"contents":"Notes on LaTeX.\nImages side-by-sideUse the subfig package.\n\\documentclass[10pt,a4paper]{article} \\usepackage[demo]{graphicx} \\usepackage{subfig} \\begin{document} \\begin{figure}% \\centering \\subfloat[\\centering label 1]{{\\includegraphics[width=5cm]{img1} }}% \\qquad \\subfloat[\\centering label 2]{{\\includegraphics[width=5cm]{img2} }}% \\caption{2 Figures side by side}% \\label{fig:example}% \\end{figure} \\end{document} ","permalink":"/latex.html","tags":null,"title":"LaTeX"},{"categories":null,"contents":"Notes on Linux admin.\n","permalink":"/linux-admin.html","tags":null,"title":"Linux admin"},{"categories":null,"contents":"Notes on machine learning.\nData Synthetic Data Generation Using Synthetic data with SDV and Gaussian copulas, Synthetic data with SDV and CTGAN and Synthetic data with SDV and CopulaGAN. Explainabilty Explainability Drift Introduction to Concept Drift in Machine Learning Model drift Data drift Time-series Time-series analysis Streaming anomaly detection Clustering K-means clustering Fairness Fairness in Machine Learning Model fairness MetricsError metrics Distance metrics Performance Model performance metrics Transformations Feature scaling Optimisation Gradient descent Stochastic Gradient Descent Stochastic Gradient descent with momentum Mini-Batch Gradient Descent Adagrad RMSProp AdaDelta Adam Gradient-free optimisation RNN LSTM Statistics Statistics\nStreaming statistics\nThompson sampling\nStatistical dependence\nModel selection Cross-validation Kernel functionsKernels can be interpreted as \u0026ldquo;similarity functions\u0026rdquo;. Typically they are functions that take two $n$-dimensional points and produce a scalar. In the following sections we look at some well-known kernels and their typical applications.\nKernel functions Unsupervised methods Self-organising maps Supervised methodsMethods pertaining to Supervised learning.\nTechniques such as:\nRandom Forest Regression Gaussian Process Regression Frameworks Cookiecutter Data Science Scikit-learn Model serving Theory The difference between AI and Machine Learning ","permalink":"/machine-learning.html","tags":null,"title":"Machine learning"},{"categories":null,"contents":"My machine collection.\nMacBook Air A 2015 MacBook Air1 with a 1.6GHz dual-core Intel Core i5, 13-inch display, 512Gb SSD and 8Gb RAM.\nThis could well be my all-time favourite machine. Still going strong and with an almost faultless form factor. Light and I really enjoy the keyboard feel. My only complaint is not being a Retina display and the fans can get a bit loud when under heavy workloads.\nCurrently it is running on Fedora.\nMac mini A mid-2010s Mac mini2 with 2.66GHz Intel Core 2 Duo processor, 8Gb RAM and 500Gb ATA HD. It is currently running Fedora with a Keychron Q1 QMK V23 keyboard and a Samsung 1080p monitor.\nThis is a nice, silent, little machine for general purpose computing.\nMacBook Pro (2015) A work machine from 20154, the MacBook Pro Retina 15-inch. With 2.5GHz quad-core Intel Core i7 processor, 16Gb RAM and 512GB SSD. A nice work machine. This laptop has seen a lot of travelling. Although a very nice working machine, not the best to lug around due to its weight (almost 2Kg).\nMacBook Pro (2019) My current main working machine. A MacBook Pro (15-inch, 2019)5 with 2.6GHz 6-core Intel Core i7, 512Gb SSD and 32Gb RAM.\nAll in all a very good machine. Copes with heavy workloads quite happily (but noisily). It was my first non-MagSafe laptop.\nhttps://support.apple.com/kb/sp714?locale=en_GB\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nhttps://support.apple.com/kb/sp585\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nhttps://www.keychron.com/products/keychron-q1\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nhttps://support.apple.com/kb/sp719\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nhttps://support.apple.com/kb/SP794\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"/machines.html","tags":null,"title":"Machines"},{"categories":null,"contents":"SummaryNotes on the Maven build system.\nModesDaemonThis build mode relies on the Maven daemon, itself inspired by the Gradle daemon:\nThe Maven Daemon process does not reload classes unless they are changed.\nThe daemon is called with mvnd instead of mvn, for instance\n$ mvnd clean install The daemon uses multiple threads by default, with $N_{cores}-1$.\nArtifactsber JARsThere are many ways to build ber JARs, but we will talk about two, maven-assembly-plugin and maven-shade-plugin.\nAssemblyThe maven-assembly-plugin 1 adds all dependencies inside the final fat JAR and can be used by adding the following to your pom.xml:\n\u0026lt;plugin\u0026gt; \u0026lt;groupId\u0026gt;org.apache.maven.plugins\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;maven-assembly-plugin\u0026lt;/artifactId\u0026gt; \u0026lt;configuration\u0026gt; \u0026lt;descriptorRefs\u0026gt; \u0026lt;descriptorRef\u0026gt;jar-with-dependencies\u0026lt;/descriptorRef\u0026gt; \u0026lt;/descriptorRefs\u0026gt; \u0026lt;archive\u0026gt; \u0026lt;manifest\u0026gt; \u0026lt;mainClass\u0026gt;{your.package.main.class}\u0026lt;/mainClass\u0026gt; \u0026lt;/manifest\u0026gt; \u0026lt;/archive\u0026gt; \u0026lt;/configuration\u0026gt; \u0026lt;executions\u0026gt; \u0026lt;execution\u0026gt; \u0026lt;phase\u0026gt;package\u0026lt;/phase\u0026gt; \u0026lt;goals\u0026gt; \u0026lt;goal\u0026gt;single\u0026lt;/goal\u0026gt; \u0026lt;/goals\u0026gt; \u0026lt;/execution\u0026gt; \u0026lt;/executions\u0026gt; \u0026lt;/plugin\u0026gt; Maven plugins should be specified inside a \u0026lt;plugins\u0026gt;\u0026lt;/plugins\u0026gt; scope, which itself must be inside \u0026lt;build\u0026gt;\u0026lt;/build\u0026gt; scope. As you can see from execution phase, the ber JAR is built when calling mvn package.\nShadeThe maven-shade-plugin 2 also adds all dependencies inside the final fat JAR, but additionally executes shading. Add the following to your pom.xml:\n\u0026lt;plugin\u0026gt; \u0026lt;groupId\u0026gt;org.apache.maven.plugins\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;maven-shade-plugin\u0026lt;/artifactId\u0026gt; \u0026lt;configuration\u0026gt; \u0026lt;transformers\u0026gt; \u0026lt;transformer implementation=\u0026#34;org.apache.maven.plugins.shade.resource.ManifestResourceTransformer\u0026#34;\u0026gt; \u0026lt;mainClass\u0026gt;{your.package.main.class}\u0026lt;/mainClass\u0026gt; \u0026lt;/transformer\u0026gt; \u0026lt;/transformers\u0026gt; \u0026lt;/configuration\u0026gt; \u0026lt;executions\u0026gt; \u0026lt;execution\u0026gt; \u0026lt;phase\u0026gt;package\u0026lt;/phase\u0026gt; \u0026lt;goals\u0026gt; \u0026lt;goal\u0026gt;shade\u0026lt;/goal\u0026gt; \u0026lt;/goals\u0026gt; \u0026lt;/execution\u0026gt; \u0026lt;/executions\u0026gt; \u0026lt;/plugin\u0026gt; The shade plugin also allows to specify dependency inclusion/exclusion using a variety of specifiers. The following example from the official docs3 illustrates this:\n\u0026lt;project\u0026gt; ... \u0026lt;build\u0026gt; \u0026lt;plugins\u0026gt; \u0026lt;plugin\u0026gt; \u0026lt;groupId\u0026gt;org.apache.maven.plugins\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;maven-shade-plugin\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;3.3.0\u0026lt;/version\u0026gt; \u0026lt;executions\u0026gt; \u0026lt;execution\u0026gt; \u0026lt;phase\u0026gt;package\u0026lt;/phase\u0026gt; \u0026lt;goals\u0026gt; \u0026lt;goal\u0026gt;shade\u0026lt;/goal\u0026gt; \u0026lt;/goals\u0026gt; \u0026lt;configuration\u0026gt; \u0026lt;artifactSet\u0026gt; \u0026lt;excludes\u0026gt; \u0026lt;exclude\u0026gt;classworlds:classworlds\u0026lt;/exclude\u0026gt; \u0026lt;exclude\u0026gt;junit:junit\u0026lt;/exclude\u0026gt; \u0026lt;exclude\u0026gt;jmock:*\u0026lt;/exclude\u0026gt; \u0026lt;exclude\u0026gt;*:xml-apis\u0026lt;/exclude\u0026gt; \u0026lt;exclude\u0026gt;org.apache.maven:lib:tests\u0026lt;/exclude\u0026gt; \u0026lt;exclude\u0026gt;log4j:log4j:jar:\u0026lt;/exclude\u0026gt; \u0026lt;/excludes\u0026gt; \u0026lt;/artifactSet\u0026gt; \u0026lt;/configuration\u0026gt; \u0026lt;/execution\u0026gt; \u0026lt;/executions\u0026gt; \u0026lt;/plugin\u0026gt; \u0026lt;/plugins\u0026gt; \u0026lt;/build\u0026gt; ... \u0026lt;/project\u0026gt; DependenciesTreeThe dependency-tree plugin4 allows to display a project\u0026rsquo;s dependency tree. At it\u0026rsquo;s simplest running mvn dependency:tree will print a tree with the project\u0026rsquo;s depency hierarchy.\nTestingExcluding testsTo exclude test from Maven use the following notation5\n# Exclude specific class with (!) $ mvn test -Dtest=!ExcludedClass # Exclude specific method $ mvn test -Dtest=!ExcludedClass#excludedMethod # Exclude more than one method $ mvn test -Dtest=!ExcludedClass#excludedMethod+excludedMethod2 # Exclude a package with a wildcard (*) $ mvn test -Dtest=!dev.ruivieira.test.Excluded* https://maven.apache.org/plugins/maven-assembly-plugin/\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nhttps://maven.apache.org/plugins/maven-shade-plugin/\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nhttps://maven.apache.org/plugins/maven-shade-plugin/examples/includes-excludes.html\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nhttps://maven.apache.org/plugins/maven-dependency-plugin/tree-mojo.html\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nIf using zsh remember to escape !.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"/maven.html","tags":null,"title":"Maven"},{"categories":null,"contents":"It is said that patience is a virtue but the truth is that no one likes waiting (especially waiting around: this interesting article explores why people prefer walking 8 minutes to the airports baggage claim and having the bags ready rather than waiting the same amount of time entirely in the claim area).\nAnyone performing computationally heavy work, such as Monte Carlo methods, will know that these are usually computationally expensive algorithms which, even in modern hardware, can result in waiting times in the magnitude of hours, days and even weeks.\nThese long running times coupled with the fact that in certain cases it is not easy to accurately predict how long a certain number of iterations will take, usually leads to a tiresome behaviour of constantly checking for good (or bad) news.\nAlthough it is perfectly possible to specify that your simulation should stop after a certain amount of time (especially valid for very long simulations), this doesnt seem to be the standard practice.\nIn this post Ill detail my current setup for being notified exactly of when simulations are finished. To implement this setup, the following stack is required:\nA JDK Apache Maven1 A messaging service Pushbullet A smartphone, tablet, smartwatch (or any other internet enabled device) To start, we can create an account in Pushbullet, which will involve, in the simplest case, signing up using some authentication service such as Google.\nNext, we will install the client application (available for Android, iOS and most modern browsers after which we can enable notifications (at least in the Android client, Im not familiar with the iPhone version).\nSince my current work started as a plain Java project which in time evolved mainly to Scala, it consists of an unholy mixture of Maven as a build tool for Scala code.\nThis shouldn\u0026rsquo;t be a problem for other setups, but Ill just go through my specific setup (i.e. using Maven dependencies to a Scala project).\nTo implement communication between the code and the messaging service, we can use a simple library such as jpushbullet.\nThe library works well enough, although at the time of writing it only supports Pushbullets v1 API but not the newer v2 API.\nSince the project, unfortunately, is not in Maven central, you should build it from scratch. Fortunately, in a sensibly configured machine, this is trivial.\nIn the machine where you plan to perform the simulations, clone and build jpushbullet.\n$ git clone git@github.com:silk8192/jpushbullet.git $ mvn clean install Once the build is complete, you can add it as a local dependency in your projects pom.xml:\n\u0026lt;dependencies\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;com.shakethat.jpushbullet\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;jpushbullet\u0026lt;/artifactId\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;/dependencies\u0026gt; For the purpose of this example, lets assume that you have the following Object as the entry point of your simulation. The next step is to add a call to the Pushbullet service before the exit point. Please keep in mind that it is very bad practice to include your personal API key in your committed code. I strongly suggest you keep this information in a separate file (/e.g./ in ~resources~), read it at runtime and add it to ~.gitignore~.\nThat being said, place the messaging code as such:\npackage benchmarks import com.shakethat.jpushbullet.PushbulletClient\nobject MCMC { def main(args:Array[String]):Unit = { // Your MCMC code val client = new PushbulletClient(api\\_key) val devices = client.getDevices val title = \u0026#34;MCMC simulation finished\u0026#34; val body = \u0026#34;Some summary can be included\u0026#34; // n is the preferred device number client.sendNote(true, devices .getDevices .get(n) .getIden(), title, body) } } Usually, I would call this code via ~ssh~ into my main machine from Maven (using Scala Maven) as:\n$ nohup mvn scala:run -DmainClass=benchmarks.MCMC \u0026amp; Finally, when the simulation is completed, you will be notified in the client devices (you can select which ones by issuing separate sendNote calls) and include a result summary, as an example.\nMy current setup generates an R script from a template which is run by ~Rscript~ in order to produce a PDF report. However, be careful, since file quotas in Pushbullet are limited, so text notifications should be used without worry of going over the free usage tier.\nKeep in mind that there are other alternatives to jpushbullet, such as send-notification, a general notification library for Java for which the setup is quite similar.\nHope this was helpful.\nhttp://maven.apache.org\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"/mcmc-notifications.html","tags":null,"title":"MCMC notifications"},{"categories":null,"contents":"Recently I\u0026rsquo;ve been following (but not very closely, I admit) the development of the GraalVM project. The project has many interesting goals (such as Project Metropolis, increased JIT performance and others).\nHowever, having dabbled with projects such as Scala native and Kotlin native, one of the aspects of GraalVM that caught my attention was the SubstrateVM, which allegedly allows for a simple, straight-forward compilation of any Java bytecode into a native binary.\nI specifically wanted to compare the performance and memory consumption of simple scientific computing tasks when using the JVM and native executables. To do this, I picked two simple numerical simulations in the form of toy Gibbs samplers, in order to keep the cores busy for a while.\nBinomial-Beta caseThe first problem chosen was the one of sampling from a Beta-Binomial distribution where we have\n$$ X \\sim \\text{Binom}\\left(n,\\theta\\right) \\\\ \\theta \\sim \\text{B}\\left(a,b\\right). $$\nSince we know that\n$$ \\pi\\left(\\theta|x\\right) \\propto \\theta^x \\left(1-\\theta\\right)^{n-x}\\theta^{a-1}\\left(1-\\theta\\right)^{b-1}, $$\nWe calculate the joint density\n$$ p(x,\\theta) = \\begin{pmatrix} n \\\\ x \\end{pmatrix} \\theta^x \\left(1-\\theta\\right)^{n-x}\\frac{\\Gamma\\left(a+b\\right)}{\\Gamma(a)\\Gamma(b)}\\theta^{a-1}\\left(1-\\theta\\right)^{b-1} $$\nThe marginal distribution is a Binomial-Beta:\n$$ p\\left(x\\right)=\\begin{pmatrix} n \\\\ x \\end{pmatrix}\\frac{\\Gamma\\left(a+b\\right)}{\\Gamma(a)\\Gamma(b)}\\frac{\\Gamma\\left(a+b\\right)\\Gamma\\left(b+n-x\\right)}{\\Gamma\\left(a+b+n\\right)},\\qquad x=0,1,\\dots,n. $$\nThe code for this simulation is available here.\nThe project is setup so that Maven produces an assembly Jar file, since I\u0026rsquo;ve found that to be the easier artifact we can offer to the GraalVM\u0026rsquo;s native compiler. To enable assembly Jars we add the maven-assembly-plugin to pom.xml and specify a main class. The assembly can then be produced simply by executing\nmvn package An assembly Jar should be available in the target folder and named benchmark-gibbs-1.0-SNAPSHOT-jar-with-dependencies.jar. Both the Jar and the native executable allow to specify how many iterations the Gibbs sampler should run for (as well as the thinning factor). If nothing is specified, the default will be used, which is $50000$ iterations thinned by $100$.\nThis particular Gibbs sampler was implemented in two variants. One variant stores the samples draws of $x$ and $\\theta$ in arrays double[] while the other one simply calculates the Gibbs steps by using the previous value, that is $x_i=f(x_{i-1},\\theta_{i-1})$ and then discarding the previous values. The latter has a constant memory cost in $\\mathcal{O}(1)$ in terms of number of iterations, while the former clearly doesn\u0026rsquo;t.\nWe can them proceed with the first test, first benchmarking it under the JVM by running (for both sample history variants):\n$ /usr/bin/time -v java -jar target/benchmark-gibbs-1.0-SNAPSHOT-jar-with-dependencies.jar store 50000 100 $ /usr/bin/time -v java -jar target/benchmark-gibbs-1.0-SNAPSHOT-jar-with-dependencies.jar nostore 50000 100 (It is important to note that the time command is the executable under /usr/bin and not your shell\u0026rsquo;s builtin.) The next step is to build the native image using GraalVM\u0026rsquo;s compiler. This is also quite straight-forward and simply a matter of calling:\n$GRAALVM_BIN/native-image target/benchmark-gibbs-1.0-SNAPSHOT-jar-with-dependencies.jar where $GRAALVM_BIN is simply the location where you installed the GraalVM binaries. If the compilation is successful, you should see some information about the compilation steps, such as parsing, inlining, compiling and writing the image. Finally, if using the default, you should have a native executable available in your current directory. Again, the benchmark command is similar to the JVM step, that is:\n$ /usr/bin/time -v ./benchmark-gibbs-1.0-SNAPSHOT-jar-with-dependencies store 50000 100 $ /usr/bin/time -v ./benchmark-gibbs-1.0-SNAPSHOT-jar-with-dependencies nostore 50000 100 The results from the runs which saved the sampling history on both platforms (JVM and native) were consistent as we can see from the plots below:\nThe (peak) memory consumption and execution time for each version is presented in the table below:\nTime(s) Peak (Mb) JVM (no sample history) 110.09 320.913 native (no sample history) 130.52 273.747 JVM (sample history) 112.51 324.796 native (sample history) 130.62 274.239 Another bivariate caseThe second problem chosen is another bivariate model, a Gibbs sampler in this blog. The code is included in the same repositoty as the Beta-Binomial case and the setup for the benchmarks is similar. The only step needed to run this example is to change the main class in the assemply plugin section of the pom.xml from BinomialBeta to Bivariate. The benchmark results are in the table below:\nTime(s) Peak (Mb) JVM 106.92 176.541 native 121.29 273.383 Now, in this case, the results are much more interesting. The JVM version outperforms the native version in both execution time and memory consumption. I don\u0026rsquo;t have an explanation for this, but if you think you have (or have any other questions) please let me know on Mastodon or Twitter.\nThanks for reading!\n","permalink":"/mcmc-performance-on-substrate-vm.html","tags":null,"title":"MCMC performance on substrate VM"},{"categories":null,"contents":"IntroductionPage on Minikube1.\nInstallationFedoraTo install Minikube on Fedora use:\n$ curl -LO https://storage.googleapis.com/minikube/releases/latest/minikube-latest.x86_64.rpm $ sudo rpm -Uvh minikube-latest.x86_64.rpm SetupMost of the configuration settings can be set using the config subcommand.\nKubernetes versionTo stet the [Kubernetes cluster version (e.g 1.19) there are two options. Either permanently set it as a config option\n$ minikube config set kubernetes-version v1.19.0 Or specify it as a parameter when starting, using\n$ minikube start --kubernetes-version=v1.19.0 MemorySet the available amount of memory.\n$ minikube config set memory 8000 DeploymentFrom imageTo build an image use the minikube command\nminikube image build -t ruivieira/my-image . Image deployment?\nhttps://minikube.sigs.k8s.io/docs/\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"/minikube.html","tags":null,"title":"Minikube"},{"categories":null,"contents":" Unfairness detection Fairness metricsGroup fairnessModel fairness metrics are described in the fairness page.\n","permalink":"/model-fairness.html","tags":null,"title":"Model fairness"},{"categories":null,"contents":"Difference in Positive Proportions in Predicted Labels (DPPL)DPPL (Difference in Positive Proportions in Predicted Labels) is a metric used to evaluate the performance of machine learning models in imbalanced datasets. It measures the difference in the proportion of positive predictions made by the model between the minority class and the majority class.\nA low value of DPPL indicates that the model is able to make similar positive predictions for instances of the minority class and instances of the majority class, which is desirable in imbalanced datasets where it is important to ensure that the minority class is not overlooked.\nIt is a classification-specific metric, as it is only applicable to models that perform binary or multi-class classification. It assesses the model\u0026rsquo;s ability to make positive predictions for instances of the minority class similar to the instances of the majority class, which is important in imbalanced datasets where the minority class is often under-represented.\nDPPL is also a threshold-independent metric, which means that it does not depend on the specific threshold used to make binary predictions. This makes it useful in situations where the threshold used to make predictions may need to be adjusted depending on the specific use case.\nThe formula for the DPPL metric is as follows:\n$$ DPPL = \\vert p_{m} - p_M \\vert $$\nwhere:\n$p_m$ is the proportion of positive predictions made by the model for instances of the minority class\n$p_M$ is the proportion of positive predictions made by the model for instances of the majority class\nIt calculates the absolute difference between the proportion of positive predictions made by the model for the minority class and the majority class.\nimport pandas as pd import numpy as np # assume data is a pandas dataframe with the columns \u0026#39;label\u0026#39; and \u0026#39;prediction\u0026#39; # calculate the proportion of positive predictions for the minority class p_minority = data[data[\u0026#39;label\u0026#39;] == minority_class][\u0026#39;prediction\u0026#39;].mean() # calculate the proportion of positive predictions for the majority class p_majority = data[data[\u0026#39;label\u0026#39;] == majority_class][\u0026#39;prediction\u0026#39;].mean() # calculate DPPL DPPL = abs(p_minority - p_majority) print(\u0026#34;DPPL: \u0026#34;, DPPL) Model quality Model quality in machine learning can be measured using various metrics depending on the type of problem you are trying to solve. Here are some common metrics for evaluating model quality: Accuracy: This metric measures the percentage of correct predictions made by the model on the test data. Precision: This metric measures the proportion of true positive predictions out of all the positive predictions made by the model. Recall: This metric measures the proportion of true positive predictions out of all the actual positive instances in the test data. F1 score: This metric is the harmonic mean of precision and recall and is used when both precision and recall are important. Mean Squared Error (MSE): This metric is used to evaluate the performance of regression models. It measures the average of the squared differences between the predicted values and the actual values. Mean Absolute Error (MAE): This metric is used to evaluate the performance of regression models. It measures the average of the absolute differences between the predicted values and the actual values. Area Under the Receiver Operating Characteristic Curve (AUC-ROC): This metric is used to evaluate the performance of binary classification models. It measures the trade-off between the true positive rate and the false positive rate. Mean Average Precision (mAP): This metric is used to evaluate the performance of object detection and image segmentation models. It measures the average precision across all classes. When selecting a metric to measure model quality, consider the problem type, the desired outcome, and the specific needs of your project. It\u0026rsquo;s also important to keep in mind that a single metric may not be sufficient to evaluate the performance of a model, so it\u0026rsquo;s a good idea to consider multiple metrics when evaluating model quality. ","permalink":"/model-performance-metrics.html","tags":null,"title":"Model performance metrics"},{"categories":null,"contents":"Page on model serving.\nSeldon Serving models with Seldon ModelMesh ModelMesh ","permalink":"/model-serving.html","tags":null,"title":"Model serving"},{"categories":null,"contents":"This is originally at https://github.com/kserve/modelmesh-serving/blob/main/docs/quickstart.md\nGetting startedTo quickly get started using ModelMesh Serving, here is a brief guide.\nPrerequisites A Kubernetes cluster v 1.16+1 with cluster administrative privileges kubectl2 and kustomize3 (v3.2.0+) At least 4 vCPU and 8 GB memory4. For more details, please see the deployed components section. 1. Install ModelMesh ServingGet the latest releaseRELEASE=release-0.10 git clone -b $RELEASE --depth 1 --single-branch https://github.com/kserve/modelmesh-serving.git cd modelmesh-serving Run install scriptkubectl create namespace modelmesh-serving ./scripts/install.sh --namespace-scope-mode --namespace modelmesh-serving --quickstart This will install ModelMesh Serving in the modelmesh-serving namespace, along with an etcd and MinIO instances. Eventually after running this script, you should see a Successfully installed ModelMesh Serving! message.\n[!Note] These etcd and MinIO deployments are intended for development/experimentation and not for production.\nVerify installationCheck that the pods are running:\nkubectl get pods NAME READY STATUS RESTARTS AGE pod/etcd 1/1 Running 0 5m pod/minio 1/1 Running 0 5m pod/modelmesh-controller-547bfb64dc-mrgrq 1/1 Running 0 5m Check that the ServingRuntimes are available:\nkubectl get servingruntimes NAME DISABLED MODELTYPE CONTAINERS AGE mlserver-0.x sklearn mlserver 5m ovms-1.x openvino_ir ovms 5m torchserve-0.x pytorch-mar torchserve 5m triton-2.x tensorflow triton 5m ServingRuntimes are automatically provisioned based on the framework of the model deployed. Three ServingRuntimes are included with ModelMesh Serving by default. The current mappings for these are:\nServingRuntime Supported Frameworks mlserver-0.x sklearn, xgboost, lightgbm ovms-1.x openvino_ir, onnx torchserve-0.x pytorch-mar triton-2.x tensorflow, pytorch, onnx, tensorrt 2. Deploy a modelWith ModelMesh Serving now installed, try deploying a model using the KServe InferenceService CRD.\n[!Note] While both the KServe controller and ModelMesh controller will reconcile InferenceService resources, the ModelMesh controller will only handle those InferenceServices with the serving.kserve.io/deploymentMode: ModelMesh annotation. Otherwise, the KServe controller will handle reconciliation. Likewise, the KServe controller will not reconcile an InferenceService with the serving.kserve.io/deploymentMode: ModelMesh annotation, and will defer under the assumption that the ModelMesh controller will handle it.\nHere, we deploy an pages/site/Scikit-learn MNIST model which is served from the local MinIO container:\nkubectl apply -f - \u0026lt;\u0026lt;EOF apiVersion: serving.kserve.io/v1beta1 kind: InferenceService metadata: name: example-sklearn-isvc annotations: serving.kserve.io/deploymentMode: ModelMesh spec: predictor: model: modelFormat: name: sklearn storage: key: localMinIO path: sklearn/mnist-svm.joblib EOF Note: the above YAML uses the InferenceService predictor storage spec. You can also continue using the storageUri field in lieu of the storage spec:\nkubectl apply -f - \u0026lt;\u0026lt;EOF apiVersion: serving.kserve.io/v1beta1 kind: InferenceService metadata: name: example-sklearn-isvc annotations: serving.kserve.io/deploymentMode: ModelMesh serving.kserve.io/secretKey: localMinIO spec: predictor: model: modelFormat: name: sklearn storageUri: s3://modelmesh-example-models/sklearn/mnist-svm.joblib EOF After applying this InferenceService, you should see that it is likely not yet ready.\nkubectl get isvc NAME URL READY PREV LATEST PREVROLLEDOUTREVISION LATESTREADYREVISION AGE example-sklearn-isvc False 3s Eventually, you should see the ServingRuntime pods that will hold the SKLearn model become Running.\nkubectl get pods ... modelmesh-serving-mlserver-0.x-7db675f677-twrwd 3/3 Running 0 2m modelmesh-serving-mlserver-0.x-7db675f677-xvd8q 3/3 Running 0 2m Then, checking on the InferenceService again, you should see that the one we deployed is now ready with a provided URL:\nkubectl get isvc NAME URL READY PREV LATEST PREVROLLEDOUTREVISION LATESTREADYREVISION AGE example-sklearn-isvc grpc://modelmesh-serving.modelmesh-serving:8033 True 97s You can describe the InferenceService to get more status information:\nkubectl describe isvc example-sklearn-isvc Name: example-sklearn-isvc ... Status: Components: Predictor: Grpc URL: grpc://modelmesh-serving.modelmesh-serving:8033 Rest URL: http://modelmesh-serving.modelmesh-serving:8008 URL: grpc://modelmesh-serving.modelmesh-serving:8033 Conditions: Last Transition Time: 2022-07-18T18:01:54Z Status: True Type: PredictorReady Last Transition Time: 2022-07-18T18:01:54Z Status: True Type: Ready Model Status: Copies: Failed Copies: 0 Total Copies: 2 States: Active Model State: Loaded Target Model State: Transition Status: UpToDate URL: grpc://modelmesh-serving.modelmesh-serving:8033 ... 3. Perform an inference requestNow that a model is loaded and available, you can then perform inference. Currently, only gRPC inference requests are supported by ModelMesh, but REST support is enabled via a REST proxy container. By default, ModelMesh Serving uses a headless Service since a normal Service has issues load balancing gRPC requests. See more info here.\ngRPC requestTo test out gRPC inference requests, you can port-forward the headless service in a separate terminal window:\nkubectl port-forward --address 0.0.0.0 service/modelmesh-serving 8033 -n modelmesh-serving Then a gRPC client generated from the KServe notes grpc_predict_v2.proto file can be used with localhost:8033. A ready-to-use Python example of this can be found here.\nAlternatively, you can test inference with grpcurl. This can easily be installed with brew install grpcurl if on macOS.\nWith grpcurl, a request can be sent to the SKLearn MNIST model like the following. Make sure that the MODEL_NAME variable below is set to the name of your InferenceService.\nMODEL_NAME=example-sklearn-isvc grpcurl \\ -plaintext \\ -proto fvt/proto/kfs_inference_v2.proto \\ -d \u0026#39;{ \u0026#34;model_name\u0026#34;: \u0026#34;\u0026#39;\u0026#34;${MODEL_NAME}\u0026#34;\u0026#39;\u0026#34;, \u0026#34;inputs\u0026#34;: [{ \u0026#34;name\u0026#34;: \u0026#34;predict\u0026#34;, \u0026#34;shape\u0026#34;: [1, 64], \u0026#34;datatype\u0026#34;: \u0026#34;FP32\u0026#34;, \u0026#34;contents\u0026#34;: { \u0026#34;fp32_contents\u0026#34;: [0.0, 0.0, 1.0, 11.0, 14.0, 15.0, 3.0, 0.0, 0.0, 1.0, 13.0, 16.0, 12.0, 16.0, 8.0, 0.0, 0.0, 8.0, 16.0, 4.0, 6.0, 16.0, 5.0, 0.0, 0.0, 5.0, 15.0, 11.0, 13.0, 14.0, 0.0, 0.0, 0.0, 0.0, 2.0, 12.0, 16.0, 13.0, 0.0, 0.0, 0.0, 0.0, 0.0, 13.0, 16.0, 16.0, 6.0, 0.0, 0.0, 0.0, 0.0, 16.0, 16.0, 16.0, 7.0, 0.0, 0.0, 0.0, 0.0, 11.0, 13.0, 12.0, 1.0, 0.0] }}]}\u0026#39; \\ localhost:8033 \\ inference.GRPCInferenceService.ModelInfer This should give you output like the following:\n{ \u0026#34;modelName\u0026#34;: \u0026#34;example-sklearn-isvc__isvc-3642375d03\u0026#34;, \u0026#34;outputs\u0026#34;: [ { \u0026#34;name\u0026#34;: \u0026#34;predict\u0026#34;, \u0026#34;datatype\u0026#34;: \u0026#34;INT64\u0026#34;, \u0026#34;shape\u0026#34;: [\u0026#34;1\u0026#34;], \u0026#34;contents\u0026#34;: { \u0026#34;int64Contents\u0026#34;: [\u0026#34;8\u0026#34;] } } ] } REST request [!Note] The REST proxy is currently in an alpha state and may still have issues with certain usage scenarios.\nYou will need to port-forward a different port for REST.\nkubectl port-forward --address 0.0.0.0 service/modelmesh-serving 8008 -n modelmesh-serving With curl, a request can be sent to the SKLearn MNIST model like the following. Make sure that the MODEL_NAME variable below is set to the name of your InferenceService.\nMODEL_NAME=example-sklearn-isvc curl -X POST -k http://localhost:8008/v2/models/${MODEL_NAME}/infer -d \u0026#39;{\u0026#34;inputs\u0026#34;: [{ \u0026#34;name\u0026#34;: \u0026#34;predict\u0026#34;, \u0026#34;shape\u0026#34;: [1, 64], \u0026#34;datatype\u0026#34;: \u0026#34;FP32\u0026#34;, \u0026#34;data\u0026#34;: [0.0, 0.0, 1.0, 11.0, 14.0, 15.0, 3.0, 0.0, 0.0, 1.0, 13.0, 16.0, 12.0, 16.0, 8.0, 0.0, 0.0, 8.0, 16.0, 4.0, 6.0, 16.0, 5.0, 0.0, 0.0, 5.0, 15.0, 11.0, 13.0, 14.0, 0.0, 0.0, 0.0, 0.0, 2.0, 12.0, 16.0, 13.0, 0.0, 0.0, 0.0, 0.0, 0.0, 13.0, 16.0, 16.0, 6.0, 0.0, 0.0, 0.0, 0.0, 16.0, 16.0, 16.0, 7.0, 0.0, 0.0, 0.0, 0.0, 11.0, 13.0, 12.0, 1.0, 0.0]}]}\u0026#39; This should give you a response like the following:\n{ \u0026#34;model_name\u0026#34;: \u0026#34;example-sklearn-isvc__ksp-7702c1b55a\u0026#34;, \u0026#34;outputs\u0026#34;: [ { \u0026#34;name\u0026#34;: \u0026#34;predict\u0026#34;, \u0026#34;datatype\u0026#34;: \u0026#34;FP32\u0026#34;, \u0026#34;shape\u0026#34;: [1], \u0026#34;data\u0026#34;: [8] } ] } 4. (Optional) Deleting your ModelMesh Serving installationTo delete all ModelMesh Serving resources that were installed, run the following from the root of the project:\n./scripts/delete.sh --namespace modelmesh-serving Implementing a Custom Serving RuntimeModelMesh Serving serves different kinds of models via different Serving Runtime implementations. A Serving Runtime is one or more containers which:\nCan dynamically load and unload models from disk into memory on demand Exposes a gRPC service endpoint to serve inferencing requests for loaded models More specifically, the container(s) must:\nImplement the simple model management gRPC SPI which comprises RPC methods to load/unload models, report their size, and report the runtime\u0026rsquo;s total capacity Implement one or more other arbitrary gRPC services to serve inferencing requests for already-loaded models These gRPC services for (2) must all be served from the same server endpoint. The management service SPI may be served by that same endpoint or a different one. Each of these endpoints may listen on a localhost port, or a unix domain socket. For best performance, a domain socket is preferred for the inferencing endpoint, and the corresponding file should be created in an empty dir within one of the containers. This dir will become a mount in all of the runtime containers when they are run.\nModel server Management SPIBelow is a description of how to implement the mmesh.ModelRuntime gRPC service, specified in model-runtime.proto. Note that this is currently subject to change, but we will try to ensure that any changes are backwards-compatible or at least will require minimal change on the runtime side.\nModel sizingSo that ModelMesh Serving can decide when/where models should be loaded and unloaded, a given serving runtime implementation must communicate details of how much capacity it has to hold loaded models in memory, as well as how much each loaded model consumes.\nModel sizes are communicated in a few different ways:\nA rough \u0026ldquo;global\u0026rdquo; default/average size for all models must be provided in the defaultModelSizeInBytes field in the response to the runtimeStatus rpc method. This should be a very conservative estimate.\nA predicted size can optionally be provided by implementing the predictModelSize rpc method. This will be called prior to loadModel and if implemented should return immediately (for example it should not make remote calls which could be delayed).\nThe more precise size of an already-loaded model can be provided by either:\nIncluding it in the sizeInBytes field of the response to the loadModel rpc method Not setting in the loadModel response, and instead implementing the separate modelSize method to return the size. This will be called immediately after loadModel returns, and isn\u0026rsquo;t required to be implemented if the first option is used. The second of these last two options is preferred when a separate step is required to determine the size after the model has already been loaded. This is so that the model can start to be used for inferencing immediately, while the sizing operation is still in progress.\nCapacity is indicated once via the capacityInBytes field in the response to the runtimeStatus rpc method and assumed to be constant.\nIdeally, the value of capacityInBytes should be calculated dynamically as a function of your model server container\u0026rsquo;s allocated memory. One way to arrange this is via Kubernetes\u0026rsquo; Downward API - mapping the container\u0026rsquo;s requests.memory property to an environment variable. Of course some amount of fixed overhead should likely be subtracted from this value:\nenv: - name: MODEL_SERVER_MEM_REQ_BYTES valueFrom: resourceFieldRef: containerName: my-model-server resource: requests.memory runtimeStatusmessage RuntimeStatusRequest {} This is polled at the point that the main model-mesh container starts to check that the runtime is ready. You should return a response with status set to STARTING until the runtime is ready to accept other requests and load/serve models at which point status should be set to READY.\nThe other fields in the response only need to be set in the READY response (and will be ignored prior to that). Once READY is returned, no further calls will be made unless the model-mesh container unexpectedly restarts.\nCurrently, to ensure overall consistency of the system, it is required that runtimes purge any loaded/loading models when receiving a runtimeStatus call, and do not return READY until this is complete. Typically, it\u0026rsquo;s only called during initialization prior to any load/unloadModel calls and hence this \u0026ldquo;purge\u0026rdquo; will be a no-op. But runtimes should also handle the case where there are models loaded. It is likely that this requirement will be removed in a future update, but ModelMesh Serving will remain compatible with runtimes that still include the logic.\nmessage RuntimeStatusResponse { enum Status { STARTING = 0; READY = 1; FAILING = 2; //not used yet } Status status = 1; // memory capacity for static loaded models, in bytes uint64 capacityInBytes = 2; // maximum number of model loads that can be in-flight at the same time uint32 maxLoadingConcurrency = 3; // timeout for model loads in milliseconds uint32 modelLoadingTimeoutMs = 4; // conservative \u0026#34;default\u0026#34; model size, // such that \u0026#34;most\u0026#34; models are smaller than this uint64 defaultModelSizeInBytes = 5; // version string for this model server code string runtimeVersion = 6; message MethodInfo { // \u0026#34;path\u0026#34; of protobuf field numbers leading to the string // field within the request method corresponding to the // model name or id repeated uint32 idInjectionPath = 1; } // optional map of per-gRPC rpc method configuration // keys should be fully-qualified gRPC method name // (including package/service prefix) map\u0026lt;string,MethodInfo\u0026gt; methodInfos = 8; // EXPERIMENTAL - Set to true to enable the mode where // each loaded model reports a maximum inferencing // concurrency via the maxConcurrency field of // the LoadModelResponse message. Additional requests // are queued in the modelmesh framework. Turning this // on will also enable latency-based autoscaling for // the models, which attempts to minimize request // queueing time and requires no other configuration. bool limitModelConcurrency = 9; } loadModelmessage LoadModelRequest { string modelId = 1; string modelType = 2; string modelPath = 3; string modelKey = 4; } The runtime should load a model with name/id specified by the modelId field into memory ready for serving, from the path specified by the modelPath field. At this time, the modelType field value should be ignored.\nThe modelKey field will contain a JSON string with the following contents:\n{ \u0026#34;model_type\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;mytype\u0026#34;, \u0026#34;version\u0026#34;: \u0026#34;2\u0026#34; } } Where model_type corresponds to the modelFormat section from the originating InferenceSerivce predictor. Note that version is optional and may not be present. In future, additional attributes might be present in the outer json object so your implementation should ignore them gracefully.\nThe response shouldn\u0026rsquo;t be returned until the model has loaded successfully and is ready to use.\nmessage LoadModelResponse { // OPTIONAL - If nontrivial cost is involved in // determining the size, return 0 here and // do the sizing in the modelSize function uint64 sizeInBytes = 1; // EXPERIMENTAL - Applies only if limitModelConcurrency = true // was returned from runtimeStatus rpc. // See RuntimeStatusResponse.limitModelConcurrency for more detail uint32 maxConcurrency = 2; } unloadModelmessage UnloadModelRequest { string modelId = 1; } The runtime should unload the previously loaded (or failed) model specified by modelId, and not return a response until the unload is complete and corresponding resources have been freed. If the specified model is not found/loaded, the runtime should return immediately (without error).\nmessage UnloadModelResponse {} InferencingThe model runtime server can expose any number of protobuf-based gRPC services on the grpcDataEndpoint to use for inferencing requests. ModelMesh Serving is agnostic to specific service definitions (request/response message content), but for tensor-in/tensor-out based services it is recommended to conform to the KServe V2 dataplane API spec.\nA given model runtime server will be guaranteed to only receive model inferencing requests for models that had previously completed loading successfully (via a loadModel request), and to have not been unloaded since.\nThough generally agnostic to the specific API methods, ModelMesh Serving does need to be able to set/override the model name/id used in a given request. There are two options for obtaining the model name/id within the (which will correspond to the same id previously passed to loadModel):\nObtain from one of the mm-model-id or mm-model-id-bin gRPC metadata headers (latter required for non-ASCII UTF-8 ids). Precisely how this is done depends on the implementation language - see gRPC documentation for more information (TODO specific refs/examples here). Provide the location of a specific string field within your request protobuf message (per RPC method) which will be replaced with the target model id. This is done via the methodInfos map in the runtime\u0026rsquo;s response to the runtimeStatus RPC method. Each applicable inferencing method should have an entry whose idInjectionPath field is set to a list of field numbers corresponding to the heirarchy of nested messages within the request message, the last of which being the number of the string field to replace. For example, if the id is a string field in the top-level request message with number 1 (as is the case in the KServe V2 ModelInferRequest), this list would be set to just [1]. Option 2 is particularly applicable when integrating with an existing gRPC-based model server.\nDeploying a RuntimeEach Serving Runtime implementation is defined using the custom resource type ServingRuntime which defines information about the runtime such as which container images need to be loaded, and the local gRPC endpoints on which they will listen. When the resource is applied to the Kubernetes cluster, the model server will deploy the runtime specific containers which will then enable support for the corresponding model types.\nThe following is an example of a ServingRuntime custom resource\napiVersion: serving.kserve.io/v1alpha1 kind: ServingRuntime metadata: name: example-runtime spec: supportedModelFormats: - name: new-modelformat version: \u0026#34;1\u0026#34; autoSelect: true containers: - name: model-server image: samplemodelserver:latest multiModel: true grpcEndpoint: \u0026#34;port:8085\u0026#34; grpcDataEndpoint: \u0026#34;port:8090\u0026#34; In each entry of the supportedModelFormats list, autoSelect: true can optionally be specified to indicate that that the given ServingRuntime can be considered for automatic placement of InferenceServices with the corresponding model type/format if no runtime is explicitly specified. For example, if a user applies an InferenceService with predictor.model.modelFormat.name: new-modelformat and no runtime value, the above ServingRuntime will be used since it contains an \u0026ldquo;auto-selectable\u0026rdquo; supported model format that matches new-modelformat. If autoSelect were false or unspecified, the InferenceService would fail to load with the message \u0026ldquo;No ServingRuntime supports specified model type and/or protocol\u0026rdquo; unless the runtime example-runtime was specified directly in the YAML.\nRuntime container resource allocationsTODO more detail coming here\nIntegrating with existing model serversThe ability to specify multiple containers provides a nice way to integrate with existing model servers via an adapter pattern, as long as they provide the required capability of dynamically loading and unloading models.\nNote: In the above diagram, only the adapter and model server containers are explicitly specified in the ServingRuntime CR, the others are included automatically.\nThe built-in runtimes based on Nvidia\u0026rsquo;s Triton Inferencing Server and the Seldon MLServer, and their corresponding adapters serve as good examples of this and can be used as a reference.\nReferenceSpec AttributesAvailable attributes in the ServingRuntime spec:\nAttribute Description multiModel Whether this ServingRuntime is ModelMesh-compatible and intended for multi-model usage (as opposed to KServe single-model serving). disabled Disables this runtime containers List of containers associated with the runtime containers[ ].image The container image for the current container containers[ ].command Executable command found in the provided image containers[ ].args List of command line arguments as strings containers[ ].resources Kubernetes limits or requests containers[ ].imagePullPolicy The container image pull policy containers[ ].workingDir The working directory for current container grpcEndpoint The port for model management requests grpcDataEndpoint The port or unix socket for inferencing requests arriving to the model server over the gRPC protocol. May be set to the same value as grpcEndpoint supportedModelFormats List of model types supported by the current runtime supportedModelFormats[ ].name Name of the model type supportedModelFormats[ ].version Version of the model type. It is recommended to include only the major version here, for example \u0026ldquo;1\u0026rdquo; rather than \u0026ldquo;1.15.4\u0026rdquo; storageHelper.disabled Disables the storage helper nodeSelector Influence Kubernetes scheduling to assign pods to nodes affinity Influence Kubernetes scheduling to assign pods to nodes tolerations Allow pods to be scheduled onto nodes with matching taints replicas The number of replicas of the runtime to create. This overrides the podsPerRuntime configuration Endpoint formatsSeveral of the attributes (grpcEndpoint, grpcDataEndpoint) support either Unix Domain Sockets or TCP. The endpoint should be formatted as either port:\u0026lt;number\u0026gt; or unix:\u0026lt;path\u0026gt;. The provided container must be either listening on the specific TCP socket or at the provided path.\nWarning\nIf a unix domain socket is specified for both grpcEndpoint and grpcDataEndpoint then it must either be the same socket (identical path) or reside in the same directory.\nFull ExampleThe following example demonstrates all of the possible attributes that can be set in the model serving runtime spec:\napiVersion: serving.kserve.io/v1alpha1 kind: ServingRuntime metadata: name: example-runtime spec: supportedModelFormats: - name: my_model_format # name of the model version: \u0026#34;1\u0026#34; autoSelect: true containers: - args: - arg1 - arg2 command: - command - command2 env: - name: name value: value - name: fromSecret valueFrom: secretKeyRef: key: mykey image: image name: name resources: limits: memory: 200Mi imagePullPolicy: IfNotPresent workingDir: \u0026#34;/container/working/dir\u0026#34; multiModel: true disabled: false storageHelper: disabled: true grpcEndpoint: port:1234 # or unix:/path grpcDataEndpoint: port:1234 # or unix:/path # To influence pod scheduling, one or more of the following can be used nodeSelector: # https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#nodeselector kubernetes.io/arch: \u0026#34;amd64\u0026#34; affinity: # https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#affinity-and-anti-affinity nodeAffinity: requiredDuringSchedulingIgnoredDuringExecution: nodeSelectorTerms: - matchExpressions: - key: \u0026#34;kubernetes.io/arch\u0026#34; operator: In values: - \u0026#34;amd64\u0026#34; tolerations: # https://kubernetes.io/docs/concepts/scheduling-eviction/taint-and-toleration/ - key: \u0026#34;example-key\u0026#34; operator: \u0026#34;Exists\u0026#34; effect: \u0026#34;NoSchedule\u0026#34; Storage HelperStorage helper will download the model from the S3 bucket using the secret storage-config and place it in the local path. By default, storage helper is enabled in the serving runtime. Storage helper can be disabled by adding the configuration storageHelper.disabled set to true in serving runtime. If the storage helper is disabled, the custom runtime needs to handle access to and pulling model data from storage itself. Configuration can be passed to the runtime\u0026rsquo;s pods through environment variables.\nExampleConsider the custom runtime defined above with the following InferenceService:\napiVersion: serving.kserve.io/v1beta1 kind: InferenceService metadata: name: my-mnist-isvc annotations: serving.kserve.io/deploymentMode: ModelMesh spec: predictor: model: modelFormat: name: my_model_format version: \u0026#34;1\u0026#34; storage: key: my_storage path: my_models/mnist-model parameters: bucket: my_bucket If the storage helper is enabled, the model serving container will receive the below model metadata in the loadModel call where modelPath will contain the path of the model in the local file system.\n{ \u0026#34;modelId\u0026#34;: \u0026#34;my-mnist-isvc-\u0026lt;suffix\u0026gt;\u0026#34;, \u0026#34;modelType\u0026#34;: \u0026#34;my_model_format\u0026#34;, \u0026#34;modelPath\u0026#34;: \u0026#34;/models/my-mnist-isvc-\u0026lt;suffix\u0026gt;/\u0026#34;, \u0026#34;modelKey\u0026#34;: \u0026#34;\u0026lt;serialized metadata as JSON, see below\u0026gt;\u0026#34; } The following metadata for the InferenceService predictor is serialized to a string and embedded as the modelKey field:\n{ \u0026#34;bucket\u0026#34;: \u0026#34;my_bucket\u0026#34;, \u0026#34;disk_size_bytes\u0026#34;: 2415, \u0026#34;model_type\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;my_model_format\u0026#34;, \u0026#34;version\u0026#34;: \u0026#34;1\u0026#34; }, \u0026#34;storage_key\u0026#34;: \u0026#34;my_storage\u0026#34; } If the storage helper is disabled, the model serving container will receive the below model metadata in the loadModel call where modelPath is same as the path provided in the predictor storage spec.\n{ \u0026#34;modelId\u0026#34;: \u0026#34;my-mnist-isvc-\u0026lt;suffix\u0026gt;\u0026#34;, \u0026#34;modelType\u0026#34;: \u0026#34;my_model_format\u0026#34;, \u0026#34;modelPath\u0026#34;: \u0026#34;my_models/mnist-model\u0026#34;, \u0026#34;modelKey\u0026#34;: \u0026#34;\u0026lt;serialized metadata as JSON, see below\u0026gt;\u0026#34; } The following metadata for the InferenceService predictor is serialized to a string and embedded as the modelKey field:\n{ \u0026#34;bucket\u0026#34;: \u0026#34;my_bucket\u0026#34;, \u0026#34;model_type\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;my_model_format\u0026#34;, \u0026#34;version\u0026#34;: \u0026#34;1\u0026#34; }, \u0026#34;storage_key\u0026#34;: \u0026#34;my_storage\u0026#34; } InstallationPrerequisites Kubernetes cluster - A Kubernetes cluster is required. You will need cluster-admin authority in order to complete all of the prescribed steps.\nKubectl and Kustomize - The installation will occur via the terminal using kubectl and kustomize.\netcd - ModelMesh Serving requires an etcd server in order to coordinate internal state which can be either dedicated or shared. More on this later.\nS3-compatible object storage - Before models can be deployed, a remote S3-compatible datastore is needed from which to pull the model data. This could be for example an IBM Cloud Object Storage instance, or a locally running MinIO deployment. Note that this is not required to be in place prior to the initial installation.\nWe provide an install script to quickly run ModelMesh Serving with a provisioned etcd server. This may be useful for experimentation or development but should not be used in production.\nThe install script has a --quickstart option for setting up a self-contained ModelMesh Serving instance. This will deploy and configure local etcd and MinIO servers in the same Kubernetes namespace. Note that this is only for experimentation and/or development use - in particular the connections to these datastores are not secure and the etcd cluster is a single member which is not highly available. Use of --quickstart also configures the storage-config secret to be able to pull from the ModelMesh Serving example models bucket which contains the model data for the sample Predictors. For complete details on the manfiests applied with --quickstart see config/dependencies/quickstart.yaml.\nSetup the etcd connection informationIf the --quickstart install option is not being used, details of an existing etcd cluster must be specified prior to installation. Otherwise, please skip this step and proceed to Installation.\nCreate a file named etcd-config.json, populating the values based upon your etcd server. The same etcd server can be shared between environments and/or namespaces, but in this case the root_prefix must be set differently in each namespace\u0026rsquo;s respective secret. The complete json schema for this configuration is documented here.\n{ \u0026#34;endpoints\u0026#34;: \u0026#34;https://etcd-service-hostame:2379\u0026#34;, \u0026#34;userid\u0026#34;: \u0026#34;userid\u0026#34;, \u0026#34;password\u0026#34;: \u0026#34;password\u0026#34;, \u0026#34;root_prefix\u0026#34;: \u0026#34;unique-chroot-prefix\u0026#34; } Then create the secret using the file (note that the key name within the secret must be etcd_connection):\nkubectl create secret generic model-serving-etcd --from-file=etcd_connection=etcd-config.json A secret named model-serving-etcd will be created and passed to the controller.\nInstallationInstall the latest release of modelmesh-serving by first cloning the corresponding release branch:\nRELEASE=release-0.8 git clone -b $RELEASE --depth 1 --single-branch https://github.com/kserve/modelmesh-serving.git cd modelmesh-serving Run the script to install ModelMesh Serving CRDs, controller, and built-in runtimes into the specified Kubernetes namespaces, after reviewing the command line flags below.\nA Kubernetes --namespace is required, which must already exist. You must also have cluster-admin authority and cluster access must be configured prior to running the install script.\nA list of Kubernetes namespaces --user-namespaces is optional to enable user namespaces for ModelMesh Serving. The script will skip the namespaces which don\u0026rsquo;t already exist.\nThe --quickstart option can be specified to install and configure supporting datastores in the same namespace (etcd and MinIO) for experimental/development use. If this is not chosen, the namespace provided must have an Etcd secret named model-serving-etcd created which provides access to the Etcd cluster. See the instructions above on this step.\nkubectl create namespace modelmesh-serving ./scripts/install.sh --namespace modelmesh-serving --quickstart See the installation help below for detail:\n./scripts/install.sh --help usage: ./scripts/install.sh [flags] Flags: -n, --namespace (required) Kubernetes namespace to deploy ModelMesh Serving to. -p, --install-config-path Path to local model serve installation configs. Can be ModelMesh Serving tarfile or directory. -d, --delete Delete any existing instances of ModelMesh Serving in Kube namespace before running install, including CRDs, RBACs, controller, older CRD with serving.kserve.io api group name, etc. -u, --user-namespaces Kubernetes namespaces to enable for ModelMesh Serving --quickstart Install and configure required supporting datastores in the same namespace (etcd and MinIO) - for experimentation/development --fvt Install and configure required supporting datastores in the same namespace (etcd and MinIO) - for development with fvt enabled -dev, --dev-mode-logging Enable dev mode logging (stacktraces on warning and no sampling) --namespace-scope-mode Run ModelMesh Serving in namespace scope mode Installs ModelMesh Serving CRDs, controller, and built-in runtimes into specified Kubernetes namespaces. Expects cluster-admin authority and Kube cluster access to be configured prior to running. Also requires Etcd secret \u0026#39;model-serving-etcd\u0026#39; to be created in namespace already. You can optionally provide a local --install-config-path that points to a local ModelMesh Serving tar file or directory containing ModelMesh Serving configs to deploy. If not specified, the config directory from the root of the project will be used.\nYou can also optionally use --delete to delete any existing instances of ModelMesh Serving in the designated Kube namespace before running the install.\nThe installation will create a secret named storage-config if it does not already exist. If the --quickstart option was chosen, this will be populated with the connection details for the example models bucket in IBM Cloud Object Storage and the local MinIO; otherwise, it will be empty and ready for you to add your own entries.\nSetup additional namespacesTo enable additional namespaces for ModelMesh after the initial installation, you need to add a label named modelmesh-enabled, and optionally setup the storage secret storage-config and built-in runtimes, in the user namespaces.\nThe following command will add the label to \u0026ldquo;your_namespace\u0026rdquo;.\nkubectl label namespace your_namespace modelmesh-enabled=\u0026#34;true\u0026#34; --overwrite You can also run a script to setup multiple user namespaces. See the setup help below for detail:\n./scripts/setup_user_namespaces.sh --help Run this script to enable user namespaces for ModelMesh Serving, and optionally add the storage secret for example models and built-in serving runtimes to the target namespaces. usage: ./scripts/setup_user_namespaces.sh [flags] Flags: -u, --user-namespaces (required) Kubernetes user namespaces to enable for ModelMesh -c, --controller-namespace Kubernetes ModelMesh controller namespace, default is modelmesh-serving --create-storage-secret Create storage secret for example models --deploy-serving-runtimes Deploy built-in serving runtimes --dev-mode Run in development mode meaning the configs are local, not release based -h, --help Display this help The following command will setup two namespaces with the required label, optional storage secret, and built-in runtimes, so you can deploy sample predictors into any of them immediately.\n./scripts/setup_user_namespaces.sh -u \u0026#34;ns1 ns2\u0026#34; --create-storage-secret --deploy-serving-runtimes Delete the installationTo wipe out the ModelMesh Serving CRDs, controller, and built-in runtimes from the specified Kubernetes namespaces:\n./scripts/delete.sh --namespace modelmesh-serving (Optional) Delete the specified namespace modelmesh-serving\nkubectl delete ns modelmesh-serving To start a Minikube cluster with a specific Kubernetes version see this section.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nhttps://kubernetes.io/docs/tasks/tools/#kubectl\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nhttps://kubectl.docs.kubernetes.io/installation/kustomize/\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nTo start a Minikube cluster with specific memory and vCPU specs, see this section.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"/modelmesh.html","tags":null,"title":"ModelMesh"},{"categories":null,"contents":"Monotonic Cubic Spline interpolation (MCSI) is a popular and useful method which fits a smooth, continuous function through discrete data. MCSI has several applications in the field of computer vision and trajectory fitting. MCSI further guarantees monotonicity of the smoothed approximation, something which a cubic spline approximation alone cannot. In this post I\u0026rsquo;ll show how to implement the method developed by F. N. Fritsch and R. E. Carlson1 in the Rust2 programming language.\nRustWhy Rust? Definitely this is a type of solution so simple that it can be implemented in practically any programming language we can think of. However, I do find that the best way to get acquainted with a new language and its concepts is precisely to try to implement a simple and well-know solution. Although this post does not intend to be an introduction to the Rust language, some of the fundamentals will be presented as we go along.\nIdiomatic Rust Object-Oriented Programming (OOP) has several characteristics which differ significantly from \u0026ldquo;traditional\u0026rdquo; OOP languages. Rust achieves data and behaviour encapsulation by means of defining data structure blueprints (called struct) and then defining their behaviour though a concrete implementation (through impl). As an example, a simple \u0026ldquo;class\u0026rdquo; Foo would consist of:\nstruct Foo { } impl Foo { fn new() -\u0026gt; Foo { return Foo {}; } fn method(\u0026amp;mut self) {} fn static_method() {} } pub fn main() { let mut f = Foo::new(); f.method(); Foo::static_method(); } The \u0026ldquo;constructor\u0026rdquo; is defined typically as new(), but any \u0026ldquo;static\u0026rdquo; method which returns an initialised struct can be a constructor and \u0026ldquo;object\u0026rdquo; methods include the passing of the self instance not unlike languages such as Python. The \u0026amp;mut self refers to the control or exclusive access to self and it is not directly related to mut mutability control. These concepts touch on Rust\u0026rsquo;s borrowing and ownership model which, unfortunately, are way beyond the scope of this blog post. A nice introduction is provided by the \u0026ldquo;Rust programming book\u0026rdquo; available here. Our implementation aims at building a MCSI class MonotonicCubicSpline by splitting the algorithm into the slope calculation at construction time, a Hermite interpolation function and a partial application function generator. This will follow the general structure\npub struct MonotonicCubicSpline { m_x: Vec\u0026lt;f64\u0026gt;, m_y: Vec\u0026lt;f64\u0026gt;, m_m: Vec\u0026lt;f64\u0026gt; } impl MonotonicCubicSpline { pub fn new(x : \u0026amp;Vec\u0026lt;f64\u0026gt;, y : \u0026amp;Vec\u0026lt;f64\u0026gt;) -\u0026gt; MonotonicCubicSpline { // ... } pub fn hermite(point: f64, x : (f64, f64), y: (f64, f64), m: (f64, f64)) -\u0026gt; f64 { // ... } pub fn interpolate(\u0026amp;mut self, point : f64) -\u0026gt; f64 { // ... } fn partial(x: Vec\u0026lt;f64\u0026gt;, y: Vec\u0026lt;f64\u0026gt;) -\u0026gt; impl Fn(f64) -\u0026gt; f64 { // ... } } Vec is a vector, a typed growable collection available in Rust\u0026rsquo;s standard library with documentation available here.\nMonotonic Cubic SplinesMCSI hinges on the concept of cubic Hermite interpolators. The Hermite interpolation for the unit interval for a generic interval $(x_k,x_{k+1})$ is\n$$ p(x)=p_k h_{00}(t)+ h_{10}(t)(x_{k+1}-x_k)m_k + \\\\ h_{01}(t)p_{k+1} + h_{11}(t)(x_{k+1}-x_{k})m_{k+1}. $$\nThe $h_{\\star}$ functions are usually called the Hermite basis functions in the literature and here we will use the factorised forms of:\n$$ \\begin{aligned} h_{00}(t) \u0026amp;= (1+2t)(1-t)^2 \\\\ h_{10}(t) \u0026amp;= t(1-t)^2 \\\\ h_{01}(t) \u0026amp;= t^2 (3-2t) \\\\ h_{11}(t) \u0026amp;= t^2 (t-1). \\end{aligned} $$\nThis can be rewritten as\n$$ \\begin{aligned} p(x) = (p_k(1 + 2t) + \\Delta x_k m_k t)(1-t)(1-t) + \\\\ (p_{k+1} (3 -2t) + \\Delta x_k m_{k+1} (t-1))t^2 \\end{aligned} $$\nwhere\n$$ \\begin{aligned} \\Delta x_k \u0026amp;= x_{k+1} - x_k \\\\ t \u0026amp;= \\frac{x-x_k}{h}. \\end{aligned} $$\nThis associated Rust method is the above mentioned \u0026ldquo;static\u0026rdquo; MonotonicCubicSpline::hermite():\npub fn hermite(point: f64, x : (f64, f64), y: (f64, f64), m: (f64, f64)) -\u0026gt; f64 { let h = x.1 - x.0; let t = (point - x.0) / h; return (y.0 (1.0 + 2.0 t) + h m.0 t) (1.0 - t) (1.0 - t) + (y.1 (3.0 - 2.0 t) + h m.1 (t - 1.0)) t t; } where the tuples correspond to $x \\to (x_k, x_{k+1})$, $t \\to (y_k, y_{k+1})$ and $m \\to (m_k, m_{k+1})$\nFor a series of data points $(x_k, y_k)$ with $k=1,\\dots,n$ we then calculate the slopes of the secant lines between consecutive points, that is:\n$$ \\Delta_k = \\frac{\\Delta y_{k}}{\\Delta x_k},\\qquad \\text{for}\\ k=1,\\dots,n-1 $$\nwith $Delta y_k = y_{k+1}-y_k$ and $\\Delta x_k$ as defined previously.\nSince the data is represented by the vectors x : Vec\u0026lt;f64\u0026gt; and y : Vec\u0026lt;f64\u0026gt; we implement this in the \u0026ldquo;constructor\u0026rdquo;:\nlet mut secants = vec![0.0 ; n - 1]; let mut slopes = vec![0.0 ; n]; for i in 0..(n-1) { let dx = x[i + 1] - x[i]; let dy = y[i + 1] - y[i]; secants[i] = dy / dx; } The next step is to average the secants in order to get the tangents, such that\n$$ m_k = \\frac{\\Delta_{k-1}+\\Delta_k}{2},\\qquad \\text{for}\\ k=2,\\dots,n-1. $$\nThis is achieved by the code:\nslopes[0] = secants[0]; for i in 1..(n-1) { slopes[i] = (secants[i - 1] + secants[i]) * 0.5; } slopes[n - 1] = secants[n - 2]; By definition, we want to ensure monotonicity of the interpolated points, but to guarantee this we must avoid the interpolation spline to go too far from a certain radius of the control points. If we define $\\alpha_k$ and $\\beta_k$ as\n$$ \\begin{aligned} \\alpha_k \u0026amp;= \\frac{m_k}{\\Delta_k} \\\\ \\beta_k \u0026amp;= \\frac{m_{k+1}}{\\Delta_k}, \\end{aligned} $$\nto ensure the monotonicity of the interpolation we can impose the following constraint on the above quantities:\n$$ \\phi(\\alpha, \\beta) = \\alpha - \\frac{(2\\alpha+\\beta-3)^2}{3(\\alpha+\\beta-2)}\\geq 0, $$\nthat is\n$$ \\alpha + 2\\beta - 3 \\leq 0, \\text{or}\\ 2\\alpha+\\beta-3 \\leq 0 $$\nTypically the vector $(\\alpha_k, \\beta_k)$ is restricted to a circle of radius 3, that is\n$$ \\alpha^2_l + \\beta_k^2\u0026gt;9, $$\nand then setting\n$$ m_{k+1} = t\\beta_k\\Delta_k, $$\nwhere\n$$ \\begin{aligned} h \u0026amp;= \\sqrt{\\alpha^2_k + \\beta^2_k} \\\\ t \u0026amp;= \\frac{3}{h}. \\end{aligned} $$\nOne of the ways in which Rust implements polymorphism is through method dispatch. The f64 primitive provides a shorthand for the quantity $\\sqrt{\\alpha^2_k + \\beta^2_k}$ as $\\alpha.\\text{hypot}(\\beta)$. The relevant Rust code will then be:\nfor i in 0..(n-1) { if secants[i] == 0.0 { slopes[i] = 0.0; slopes[i + 1] = 0.0; } else { let alpha = slopes[i] / secants[i]; let beta = slopes[i + 1] / secants[i]; let h = alpha.hypot(beta); if h \u0026gt; 3.0 { let t = 3.0 / h; slopes[i] = t * alpha * secants[i]; slopes[i + 1] = t * beta * secants[i]; } } } We are now able to define a \u0026ldquo;smooth function\u0026rdquo; generator using MCSI. We generate a smooth function $g(.)$ given a set of $(x_k, y_k)$ points, such that\n$$ f(x_k, y_k, p) \\to g(p). $$\nPartial applicationBefore anything, it is important to recall the difference between partial application and currying, since the two are (incorrectly) used interchangeably quite often. Function currying allows to factor functions with multiple arguments into a chain of single-argument functions, that is\n$$ f(x, y, z) = h(x)(y)(z) $$\nThe concept is prevalent in functional programming, since its initial formalisation3. Partial application, however, generally aims at using an existing function conditioned on some argument as a basis to build functions with a reduced arity. In this case this would be useful since ultimately we want to create a smooth, continuous function based on the control points $(x_k, y_k)$. The partial application implementation is done in Rust as\npub fn partial(x: Vec\u0026lt;f64\u0026gt;, y: Vec\u0026lt;f64\u0026gt;) -\u0026gt; impl Fn(f64) -\u0026gt; f64 { move |p| { let mut spline = MonotonicCubicSpline::new(\u0026amp;x, \u0026amp;y); spline.interpolate(p) } } An example of how to generate a concrete smoothed continuous function from a set of control points can be:\nlet x = vec![0.0, 2.0, 3.0, 10.0]; let y = vec![1.0, 4.0, 8.0, 10.5]; let g = partial(x, y); // calculate an interpolated point let point = g(0.39); The full code can be found here.\nFritsch, F. N., \u0026amp; Carlson, R. E. (1980). Monotone piecewise cubic interpolation. SIAM Journal on Numerical Analysis, 17(2), 238246.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nhttps://www.rust-lang.org/\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nCurry, H. B., Feys, R., Craig, W., Hindley, J. R., \u0026amp; Seldin, J. P. (1958). Combinatory logic. : North-Holland Amsterdam.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"/monotonic-cubic-spline-interpolation-with-some-rust.html","tags":null,"title":"Monotonic Cubic Spline interpolation (with some Rust)"},{"categories":null,"contents":"To instal navi1 on Linux you can use\n$ bash \u0026lt;(curl -sL https://raw.githubusercontent.com/denisidoro/navi/master/scripts/install) When installing navi on Ubuntu 20 you will get the error\ninvalid preview window layout: up:2:nohidden invalid preview window layout: up:2:nohidden invalid preview window layout: up:2:nohidden This is very likely due to the fact that the fzf2 version is too old (perhaps 0.20).\nTo upgrade to the latest version of fzf use:\n$ git clone --depth 1 https://github.com/junegunn/fzf.git ~/.fzf $ ~/.fzf/install Importing cheatsheetsYou can import cheatsheets from any git repository that includes .cheat files:\n$ navi repo add https://github.com/ruivieira/cheatsheets External variablesIt is possible to populate variables with external data in navi. To do so, specify how fzf will extract the data. For instance, the command\ndocker stop \u0026lt;container_id\u0026gt; will stop the container \u0026lt;container_id\u0026gt;, which we could extract from the output of\n$ docker ps We can then specify docker ps and the input of docker stop and instruct fzf on the data\u0026rsquo;s format. For instance, here (line 2) we say that from the command of docker ps we should extract the first column, delimited by spaces and remove the first line, since it is the header.\ndocker stop \u0026lt;container_id\u0026gt; $ container_id: docker ps --- --column 1 --header-lines 1 --delimiter \u0026#39;\\s\\s+\u0026#39; https://github.com/denisidoro/navi\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nhttps://github.com/junegunn/fzf\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"/navi.html","tags":null,"title":"Navi"},{"categories":null,"contents":"Notes on Neovim.\nInstallationFedoraTo install Neovim on Fedora simply run:\nsudo dnf install -y neovim python3-neovim vim-plugTo install vim-plug1 run:\nsh -c \u0026#39;curl -fLo \u0026#34;${XDG_DATA_HOME:-$HOME/.local/share}\u0026#34;/nvim/site/autoload/plug.vim --create-dirs \\ https://raw.githubusercontent.com/junegunn/vim-plug/master/plug.vim\u0026#39; The config file if not existing can created at ~/.config/nvim/init.vim.\nThe syntax is (for instance to add Crystal support):\ncall plug#begin() Plug \u0026#39;vim-crystal/vim-crystal\u0026#39; call plug#end() https://github.com/junegunn/vim-plug#neovim\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"/neovim.html","tags":null,"title":"Neovim"},{"categories":null,"contents":"My \u0026ldquo;now\u0026rdquo; page.\n","permalink":"/now.html","tags":null,"title":"now"},{"categories":null,"contents":"In the Random Forest algorithm, we build a decision tree (DT) based on a certain training dataset. This tree will be split in order to minimise some criteria function.\nHowever, it is not desirable that individual DTs get too large with too many splits, so a common approach is to train each tree with a subset of the training data (sampled with replacement). This will ensure that individual tree maintain a manageable size, while the variance of the tree ensemble is reduced and the overall bias is not altered.\nThis training subset is usually called the bootstrap samples. In the image below, we can see an illustration of the sampling with replacement.\n","permalink":"/oob-score-in-random-forests.html","tags":null,"title":"OOB score in random forests"},{"categories":null,"contents":"Notes on OpenShift1. To deploy operators on OpenShift see operators.\nOpenShift properApp selectionTo get the name of all resources matching a certain label:\n$ oc get all --selector app=$APPNAME -o name To delete said resources use:\n$ oc delete all --selector app=$APPNAME Image loadinggRPC routesTo enable sending gRPC requests to an OpenShift route, you will need to create a route that exposes the gRPC service and configure the necessary settings:\nDeploy your gRPC application to OpenShift: First, you need to have a gRPC application running on OpenShift. You can use oc new-app or oc create commands to deploy your application. Create a Service for your gRPC application: You need to create a Kubernetes service that exposes your gRPC application within the OpenShift cluster. Create a YAML file (e.g., grpc-service.yaml) with the following content: apiVersion: v1 kind: Service metadata: name: grpc-service labels: app: grpc-service spec: selector: app: grpc-service ports: - protocol: TCP port: 80 targetPort: \u0026lt;gRPC-server-port\u0026gt; Replace \u0026lt;gRPC-server-port\u0026gt; with the port your gRPC server is listening on. Then, apply the service configuration using the oc apply command:\noc apply -f grpc-service.yaml Create a Route for your gRPC service: Create a YAML file (e.g., grpc-route.yaml) with the following content:\napiVersion: route.openshift.io/v1 kind: Route metadata: name: grpc-route spec: to: kind: Service name: grpc-service port: targetPort: \u0026lt;gRPC-server-port\u0026gt; tls: termination: passthrough Replace \u0026lt;gRPC-server-port\u0026gt; with the port your gRPC server is listening on. The termination field is set to passthrough to allow end-to-end TLS encryption for gRPC traffic.\nApply the route configuration using the oc apply command:\noc apply -f grpc-route.yaml Get the Route\u0026rsquo;s hostname: You can get the hostname of your newly created route by running:\noc get route grpc-route -o jsonpath=\u0026#39;{.spec.host}\u0026#39; This hostname is what your gRPC clients will use to connect to the service.\nConfigure your gRPC client: Update your gRPC client to use the route\u0026rsquo;s hostname to send requests to the gRPC service. Remember to use the appropriate TLS settings, as gRPC communication is encrypted by default. With these steps completed, your gRPC service should be accessible through the OpenShift route, and you can send gRPC requests to it.\nCodeReady ContainersSSH into VMTo SSH into a running CRC machine use\nssh -i ~/.crc/machines/crc/id_ecdsa core@192.168.130.11 From inside the VM you can issue normal oc commands, such as listing the nodes\noc get nodes --context admin --cluster crc --kubeconfig /opt/kubeconfig https://www.redhat.com/en/technologies/cloud-computing/openshift\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"/openshift.html","tags":null,"title":"OpenShift"},{"categories":null,"contents":"Typically the hyper-parameters which will have the most significant impact on the behaviour of a random forest are the following:\nhe number of decision trees in a random forest\nThe split criteria\nMaximum depth of individual trees\nMaximum number of leaf nodes\nRandom features per split\number of samples in bootstrap dataset\nWe will look at each of these hyper-parameters individually with examples of how to select them.\nDataTo understand how we can optimise the hyperparameters in a random forest model, we will use scikit-learn\u0026rsquo;s RandomForestClassifier and a subset of Titanic1 dataset.\nFirst, we will import the features and labels using Pandas.\nimport pandas as pd train_features = pd.read_csv(\u0026#34;data/svm-hyperparameters-train-features.csv\u0026#34;) train_label = pd.read_csv(\u0026#34;data/svm-hyperparameters-train-label.csv\u0026#34;) Let\u0026rsquo;s look at a random sample of entries from this dataset, both for features and labels.\ntrain_features.sample(10) Some of the available features are:\nPclass, ticket class\nSex\nAge, age in years\nSibsp, number of siblings/spouses aboard\nParch, number of parents/children aboard\nFare, passenger fare\ntrain_label.sample(10) The outcome label indicates whether a passenger survived the disaster.\nAs part of the typical initial steps for model training, we will prepare the data by splitting it into a training and testing subset.\nfrom sklearn.model_selection import train_test_split X_train, X_test, y_train, y_test = train_test_split(train_features, train_label, test_size=0.33, random_state=23) Naive modelFirst we will train a \u0026ldquo;naive\u0026rdquo; model, that is a model using the defaults provided by RandomForestClassifier2. These defaults are:\nn_estimators = 10\ncriterion=gini\nmax_depth=None\nmin_samples_split=2\nmin_samples_leaf=1\nmin_weight_fraction_leaf=0.0\nmax_features=auto\nmax_leaf_nodes=None\nmin_impurity_decrease=0.0\nmin_impurity_split=None\nbootstrap=True\noob_score=False\nn_jobs=1\nrandom_state=None\nverbose=0\nwarm_start=False\nclass_weight=None\nWe will instantiate a random forest classifier:\nfrom sklearn.ensemble import RandomForestClassifier rf = RandomForestClassifier() And training it using the X_train and y_train subsets using the appropriate fit method3.\ntrue_labels = train_label.values.ravel() rf.fit(X_train, y_train.values.ravel()) We can now evaluate trained naive model\u0026rsquo;s score.\nfrom sklearn.metrics import precision_score predicted_labels = rf.predict(X_test) precision_score(y_test, predicted_labels) Hyperparameter searchA simple example of a generic hyperparameter search using the GridSearchCV method in scikit-learn. The score used to measure the \u0026ldquo;best\u0026rdquo; model is the mean_test_score, but other metrics could be used, such as the Out-of-bag (OOB) error.\nparameters = { \u0026#34;n_estimators\u0026#34;:[5,10,50,100,250], \u0026#34;max_depth\u0026#34;:[2,4,8,16,32,None] } rfc = RandomForestClassifier() from sklearn.model_selection import GridSearchCV cv = GridSearchCV(rfc,parameters,cv=5) cv.fit(X_train, y_train.values.ravel()) def display(results): print(f\u0026#39;Best parameters are: {results.best_params_}\u0026#39;) print(\u0026#34;\\n\u0026#34;) mean_score = results.cv_results_[\u0026#39;mean_test_score\u0026#39;] std_score = results.cv_results_[\u0026#39;std_test_score\u0026#39;] params = results.cv_results_[\u0026#39;params\u0026#39;] for mean,std,params in zip(mean_score,std_score,params): print(f\u0026#39;{round(mean,3)} + or -{round(std,3)} for the {params}\u0026#39;) display(cv) ParametersNumber of decision treesThis is specified using the n_estimators hyper-parameter on the random forest initialisation.\nTypically, a higher number of trees will lead to greater accuracy at the expense of model size and training time.\ncv = GridSearchCV(rfc,{\u0026#34;n_estimators\u0026#34;:[2, 4, 8, 16, 32, 64, 128, 256, 512]},cv=5) cv.fit(X_train, y_train.values.ravel()) results = pd.DataFrame({\u0026#34;n_estimators\u0026#34;: [param[\u0026#34;n_estimators\u0026#34;] for param in cv.cv_results_[\u0026#39;params\u0026#39;]], \u0026#34;mean_score\u0026#34;: list(cv.cv_results_[\u0026#39;mean_test_score\u0026#39;]), \u0026#34;std_score\u0026#34;: cv.cv_results_[\u0026#39;std_test_score\u0026#39;]}) results from plotnine import * ( ggplot(results) + geom_boxplot(aes(x=\u0026#39;factor(n_estimators)\u0026#39;, y=\u0026#39;mean_score\u0026#39;)) + geom_errorbar(aes(x=\u0026#39;factor(n_estimators)\u0026#39;, ymin=\u0026#39;mean_score - std_score\u0026#39;, ymax=\u0026#39;mean_score + std_score\u0026#39;)) + theme_classic() + xlab(\u0026#39;Number of trees\u0026#39;) + ylab(\u0026#39;Mean score\u0026#39;) ) The split criteriaAt each node, a random forest decides, according to a specific algorithm, which feature and value split the tree. Therefore, the choice of splitting algorithm is crucial for the random forest\u0026rsquo;s performance.\nSince, in this example, we are dealing with a classification problem, the choices of split algorithm are, for instance:\nGini\nEntropy\nIf we were dealing with a random forest for regression, other methods (such as MSE) would be a possible choice. We will now compare both split algorithms as specified above, in training a random forest with our data:\nrfc = RandomForestClassifier(n_estimators=256) cv = GridSearchCV(rfc,{\u0026#34;criterion\u0026#34;: [\u0026#34;gini\u0026#34;, \u0026#34;entropy\u0026#34;]},cv=5) cv.fit(X_train, y_train.values.ravel()) results = pd.DataFrame({\u0026#34;criterion\u0026#34;: [param[\u0026#34;criterion\u0026#34;] for param in cv.cv_results_[\u0026#39;params\u0026#39;]], \u0026#34;mean_score\u0026#34;: list(cv.cv_results_[\u0026#39;mean_test_score\u0026#39;]), \u0026#34;std_score\u0026#34;: cv.cv_results_[\u0026#39;std_test_score\u0026#39;]}) results Maximum depth of individual treesIn theory, the \u0026ldquo;longer\u0026rdquo; the tree, the more splits it can have and better accommodate the data. However, at the tree level can this can lead to overfitting. Although this is a problem for decision trees, it is not necessarily a problem for the ensemble, the random forest. Although the key is to strike a balance between trees that aren\u0026rsquo;t too large or too short, there\u0026rsquo;s no universal heuristic to determine the size. Let\u0026rsquo;s try a few option for maximum depth:\nrfc = RandomForestClassifier(n_estimators=256, criterion=\u0026#34;entropy\u0026#34;) cv = GridSearchCV(rfc,{\u0026#39;max_depth\u0026#39;: [2, 4, 8, 16, 32, None]},cv=5) cv.fit(X_train, y_train.values.ravel()) results = pd.DataFrame({\u0026#34;max_depth\u0026#34;: [param[\u0026#34;max_depth\u0026#34;] for param in cv.cv_results_[\u0026#39;params\u0026#39;]], \u0026#34;mean_score\u0026#34;: list(cv.cv_results_[\u0026#39;mean_test_score\u0026#39;]), \u0026#34;std_score\u0026#34;: cv.cv_results_[\u0026#39;std_test_score\u0026#39;]}) results = results.dropna() results from plotnine import * ( ggplot(results) + geom_boxplot(aes(x=\u0026#39;factor(max_depth)\u0026#39;, y=\u0026#39;mean_score\u0026#39;)) + theme_classic() + xlab(\u0026#39;Max tree depth\u0026#39;) + ylab(\u0026#39;Mean score\u0026#39;) ) Maximum number of leaf nodes This hyperparameter can be of importance to other topics, such as explainability.\nIt is specified in scikit-learn using the max_leaf_nodes parameter. Let\u0026rsquo;s try a few different values:\nrfc = RandomForestClassifier(n_estimators=256, criterion=\u0026#34;entropy\u0026#34;, max_depth=8) cv = GridSearchCV(rfc,{\u0026#39;max_leaf_nodes\u0026#39;: [2**i for i in range(1, 8)]},cv=5) cv.fit(X_train, y_train.values.ravel()) results = pd.DataFrame({\u0026#34;max_leaf_nodes\u0026#34;: [param[\u0026#34;max_leaf_nodes\u0026#34;] for param in cv.cv_results_[\u0026#39;params\u0026#39;]], \u0026#34;mean_score\u0026#34;: list(cv.cv_results_[\u0026#39;mean_test_score\u0026#39;]), \u0026#34;std_score\u0026#34;: cv.cv_results_[\u0026#39;std_test_score\u0026#39;]}) results = results.dropna() results from plotnine import * ( ggplot(results) + geom_boxplot(aes(x=\u0026#39;factor(max_leaf_nodes)\u0026#39;, y=\u0026#39;mean_score\u0026#39;)) + geom_errorbar(aes(x=\u0026#39;factor(max_leaf_nodes)\u0026#39;, ymin=\u0026#39;mean_score - std_score\u0026#39;, ymax=\u0026#39;mean_score + std_score\u0026#39;)) + theme_classic() + xlab(\u0026#39;Maximum leaf nodes\u0026#39;) + ylab(\u0026#39;Mean score\u0026#39;) ) Random features per splitThis is an important hyperparameter that will depend on how noisy the original data is. Typically, if the data is not very noisy, the number of used random features can be kept low. Otherwise, it needs to be kept high.\nAn important consideration is also the following trade-off:\nA low number of random features decrease the forest\u0026rsquo;s overall variance\nA low number of random features increases the bias\nA high number of random features increases computational time\nIn scikit-learn this is specified with the max_features parameter. Assuming $N_f$ is the total number of features, some possible values for this parameter are:\nsqrt, this will take the max_features as the rounded $\\sqrt{N_f}$\nlog2, as above, takes the $\\log_2(N_f)$\nThe actual maximum number of features can be directly specified\nLet\u0026rsquo;s try a simple benchmark, even though our data does not have many features to begin with:\nrfc = RandomForestClassifier(n_estimators=256, criterion=\u0026#34;entropy\u0026#34;, max_depth=8) cv = GridSearchCV(rfc,{\u0026#39;max_features\u0026#39;: [\u0026#34;sqrt\u0026#34;, \u0026#34;log2\u0026#34;, 1, 2, 3, 4, 5, 6]},cv=5) cv.fit(X_train, y_train.values.ravel()) results = pd.DataFrame({\u0026#34;max_features\u0026#34;: [param[\u0026#34;max_features\u0026#34;] for param in cv.cv_results_[\u0026#39;params\u0026#39;]], \u0026#34;mean_score\u0026#34;: list(cv.cv_results_[\u0026#39;mean_test_score\u0026#39;]), \u0026#34;std_score\u0026#34;: cv.cv_results_[\u0026#39;std_test_score\u0026#39;]}) results from plotnine import * ( ggplot(results) + geom_boxplot(aes(x=\u0026#39;factor(max_features)\u0026#39;, y=\u0026#39;mean_score\u0026#39;)) + geom_errorbar(aes(x=\u0026#39;factor(max_features)\u0026#39;, ymin=\u0026#39;mean_score - std_score\u0026#39;, ymax=\u0026#39;mean_score + std_score\u0026#39;)) + theme_classic() + xlab(\u0026#39;Maximum number of features\u0026#39;) + ylab(\u0026#39;Mean score\u0026#39;) ) Bootstrap dataset sizeThis hyperparameter relates to the proportion of the training data to be used by decision trees.\nIt is specified in scikit-learn by max_samples and can take the value of either:\nNone, take the entirety of the samples\nAn integer, representing the actual number of samples\nA float, representing a proportion between 0 and 1 or the samples to take.\nLet\u0026rsquo;s try a hyperparameter search with some values:\nrfc = RandomForestClassifier(n_estimators=256, criterion=\u0026#34;entropy\u0026#34;, max_depth=8, max_features=6) cv = GridSearchCV(rfc,{\u0026#39;max_samples\u0026#39;: [i/10.0 for i in range(1, 10)]},cv=5) cv.fit(X_train, y_train.values.ravel()) results = pd.DataFrame({\u0026#34;max_samples\u0026#34;: [param[\u0026#34;max_samples\u0026#34;] for param in cv.cv_results_[\u0026#39;params\u0026#39;]], \u0026#34;mean_score\u0026#34;: list(cv.cv_results_[\u0026#39;mean_test_score\u0026#39;]), \u0026#34;std_score\u0026#34;: cv.cv_results_[\u0026#39;std_test_score\u0026#39;]}) results from plotnine import * ( ggplot(results) + geom_boxplot(aes(x=\u0026#39;factor(max_samples)\u0026#39;, y=\u0026#39;mean_score\u0026#39;)) + geom_errorbar(aes(x=\u0026#39;factor(max_samples)\u0026#39;, ymin=\u0026#39;mean_score - std_score\u0026#39;, ymax=\u0026#39;mean_score + std_score\u0026#39;)) + theme_classic() + xlab(\u0026#39;Proportion bootstrap samples\u0026#39;) + ylab(\u0026#39;Mean score\u0026#39;) ) Titanic Dataset - https://www.kaggle.com/c/titanic-dataset/data\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nhttps://sklearn.org/modules/generated/sklearn.ensemble.RandomForestClassifier.html\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nhttps://sklearn.org/modules/generated/sklearn.ensemble.RandomForestClassifier.html#sklearn.ensemble.RandomForestClassifier.fit\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"/optimising-random-forest-hyperparamaters.html","tags":null,"title":"Optimising random forest hyperparameters"},{"categories":null,"contents":"Pandas1 provides high-performance, easy-to-use data structures and data analysis tools for the Python language.\nPandas basics Extending pandas dataframes https://pandas.pydata.org/\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"/pandas.html","tags":null,"title":"Pandas"},{"categories":null,"contents":" Creating dataframesLet\u0026rsquo;s start with the basics. How to create a dataframe.\nLoading from CSVDataframes can be created from CSV files by using the following method:\nimport pandas as pd pd.read_csv(\u0026#39;../../data/mpg.csv\u0026#39;) mpg cylinders displacement horsepower weight acceleration model_year origin name 0 18.0 8 307.0 130 3504 12.0 70 1 chevrolet chevelle malibu 1 15.0 8 350.0 165 3693 11.5 70 1 buick skylark 320 2 18.0 8 318.0 150 3436 11.0 70 1 plymouth satellite 3 16.0 8 304.0 150 3433 12.0 70 1 amc rebel sst 4 17.0 8 302.0 140 3449 10.5 70 1 ford torino ... ... ... ... ... ... ... ... ... ... 393 27.0 4 140.0 86 2790 15.6 82 1 ford mustang gl 394 44.0 4 97.0 52 2130 24.6 82 2 vw pickup 395 32.0 4 135.0 84 2295 11.6 82 1 dodge rampage 396 28.0 4 120.0 79 2625 18.6 82 1 ford ranger 397 31.0 4 119.0 82 2720 19.4 82 1 chevy s-10 398 rows  9 columns\nIf you want to bypass the column name detection (from tcolumnsheader), you can supply the column names directly:\ncolumns = [f\u0026#34;field_{i}\u0026#34; for i in range(9)] pd.read_csv(\u0026#39;../../data/mpg.csv\u0026#39;, names=columns, skiprows=1, header=None) field_0 field_1 field_2 field_3 field_4 field_5 field_6 field_7 field_8 0 18.0 8 307.0 130 3504 12.0 70 1 chevrolet chevelle malibu 1 15.0 8 350.0 165 3693 11.5 70 1 buick skylark 320 2 18.0 8 318.0 150 3436 11.0 70 1 plymouth satellite 3 16.0 8 304.0 150 3433 12.0 70 1 amc rebel sst 4 17.0 8 302.0 140 3449 10.5 70 1 ford torino ... ... ... ... ... ... ... ... ... ... 393 27.0 4 140.0 86 2790 15.6 82 1 ford mustang gl 394 44.0 4 97.0 52 2130 24.6 82 2 vw pickup 395 32.0 4 135.0 84 2295 11.6 82 1 dodge rampage 396 28.0 4 120.0 79 2625 18.6 82 1 ford ranger 397 31.0 4 119.0 82 2720 19.4 82 1 chevy s-10 398 rows  9 columns\nRandom dataframesimport numpy as np N=100 df = pd.DataFrame({ \u0026#39;a\u0026#39;:np.random.randn(N), \u0026#39;b\u0026#39;:np.random.choice( [5,7,np.nan], N), \u0026#39;c\u0026#39;:np.random.choice( [\u0026#39;foo\u0026#39;,\u0026#39;bar\u0026#39;,\u0026#39;baz\u0026#39;], N), }) df.head() a b c 0 -1.663535 5.0 baz 1 0.408342 NaN foo 2 0.424239 NaN bar 3 1.512528 5.0 foo 4 -2.062024 5.0 foo Concatenate dataframesRow-wiseTo concatenate dataframes row-wise (i.e. to append more rows to dataframes with the same structure) we can use the .concat() method. For instance, if we create a new random dataframe:\ndf_extra = pd.DataFrame({ \u0026#39;a\u0026#39;:np.random.randn(N), \u0026#39;b\u0026#39;:np.random.choice( [11,12,13], N), \u0026#39;c\u0026#39;:np.random.choice( [\u0026#39;zombie\u0026#39;,\u0026#39;woof\u0026#39;,\u0026#39;nite\u0026#39;], N), }) df_extra.head() a b c 0 -0.308607 13 nite 1 1.241705 11 nite 2 1.449900 11 zombie 3 0.741891 12 nite 4 -1.586658 12 zombie We can now concatenate an arbitray number of dataframes by passing them as a list:\ndf_all = pd.concat([df, df_extra]) df_all.sample(9) a b c 98 -0.412014 11.0 zombie 38 -0.656568 5.0 foo 15 0.263778 13.0 zombie 53 -0.133630 NaN baz 21 -0.106806 NaN bar 34 -0.675752 7.0 foo 13 0.181684 12.0 zombie 8 -1.189809 12.0 zombie 0 -0.308607 13.0 nite Column operationsCheck column existenceThe in keyword can be used directly to check column existence.\n\u0026#39;b\u0026#39; in df True Renaming columnsdf.rename(columns={\u0026#34;a\u0026#34;: \u0026#34;new_name\u0026#34;}, inplace=True) df.columns Index(['new_name', 'b', 'c'], dtype='object') Using a mapping function. In this case str.upper():\ndf.rename(columns=str.upper, inplace=True) df.columns Index(['NEW_NAME', 'B', 'C'], dtype='object') We can also use a lambda. For instance, using lambda x: x.capitalize() would result:\ndf.rename(columns=lambda x: x.capitalize(), inplace=True) df.columns Index(['New_name', 'B', 'C'], dtype='object') A list of column names can be passed directly to columns.\ndf.columns = [\u0026#34;first\u0026#34;, \u0026#34;second\u0026#34;, \u0026#34;third\u0026#34;] df.columns Index(['first', 'second', 'third'], dtype='object') Dropping columnsA column can be dropped using the .drop() method along with the column keyword. For instance in the dataframe df: We can drop the second column using:\ndf.drop(columns=\u0026#39;second\u0026#39;) first third 0 -1.663535 baz 1 0.408342 foo 2 0.424239 bar 3 1.512528 foo 4 -2.062024 foo ... ... ... 95 -0.856850 foo 96 0.850722 foo 97 -1.076690 bar 98 2.074288 baz 99 -1.176129 baz 100 rows  2 columns\nThe del keyword is also a possibility. However, del changes the dataframe in-place, therefore we will make a copy of the dataframe first.\ndf_copy = df.copy() df_copy first second third 0 -1.663535 5.0 baz 1 0.408342 NaN foo 2 0.424239 NaN bar 3 1.512528 5.0 foo 4 -2.062024 5.0 foo ... ... ... ... 95 -0.856850 NaN foo 96 0.850722 NaN foo 97 -1.076690 7.0 bar 98 2.074288 7.0 baz 99 -1.176129 7.0 baz 100 rows  3 columns\ndel df_copy[\u0026#39;second\u0026#39;] df_copy first third 0 -1.663535 baz 1 0.408342 foo 2 0.424239 bar 3 1.512528 foo 4 -2.062024 foo ... ... ... 95 -0.856850 foo 96 0.850722 foo 97 -1.076690 bar 98 2.074288 baz 99 -1.176129 baz 100 rows  2 columns\nYet another possibility is to drop the column by index. For instance:\ndf.drop(columns=df.columns[1]) first third 0 -1.663535 baz 1 0.408342 foo 2 0.424239 bar 3 1.512528 foo 4 -2.062024 foo ... ... ... 95 -0.856850 foo 96 0.850722 foo 97 -1.076690 bar 98 2.074288 baz 99 -1.176129 baz 100 rows  2 columns\nOr we could use ranges, for instance:\ndf.drop(columns=df.columns[0:2]) third 0 baz 1 foo 2 bar 3 foo 4 foo ... ... 95 foo 96 foo 97 bar 98 baz 99 baz 100 rows  1 columns\nSubsetting and indexingIndexing performanceLet\u0026rsquo;s assume the case where you have a column BOOL with values Y or N that you want to replace with an integer 1 or 0 value. The inital1 instinct would be to do something like:\ndf[\u0026#34;BOOL\u0026#34;] = df[\u0026#34;BOOL\u0026#34;].eq(\u0026#34;Y\u0026#34;).mul(1) This will result in the warning\nSettingWithCopyWarning: A value is trying to be set on a copy of a slice from a DataFrame. Try using .loc[row_indexer,col_indexer] = value instead Pandas documentation recommends the usage of the following idiom, since it can be considerably faster:\ndf.loc[:, (\u0026#34;BOOL\u0026#34;)] = df.loc[:, (\u0026#34;BOOL\u0026#34;)].eq(\u0026#34;Y\u0026#34;).mul(1) and Pythonic?\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"/pandas-basics.html","tags":null,"title":"Pandas basics"},{"categories":null,"contents":"Notes on Pikchr1.\nInstallationDownload pikchr~ from the downloads page To create the CLI command, compile using\n$ gcc -DPIKCHR_SHELL -o pikchr pikchr.c -lm And add it to your path.\nExamplesA: box \u0026#34;head\u0026#34; fit B: box \u0026#34;tail\u0026#34; fit C: box \u0026#34;something\u0026#34; with .sw at A.nw fit wid dist(A.w, B.e) https://pikchr.org/\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"/pikchr.html","tags":null,"title":"Pikchr"},{"categories":null,"contents":"A page with notes about Podman. There is also a Podman cheatsheet available.\nCommon problemsDid not resolve to an aliasIf referencing an image, say postgres:14, the following error shows:\nError: short-name \u0026#34;postgres:14\u0026#34; did not resolve to an alias and no unqualified-search registries are defined in \u0026#34;/etc/containers/registries.conf\u0026#34; It can be the case that you referencing unqualified images1. The previous behaviour (using unqualified image names) can be restated by adding the line below in /etc/containers/registries.conf.\nunqualified-search-registries = [\u0026#34;docker.io\u0026#34;] See man 5 containers-registries.conf for more information.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"/podman.html","tags":null,"title":"Podman"},{"categories":null,"contents":"Last year (2020) we spent Christmas in \u0026ldquo;lockdown\u0026rdquo; and we tried to make ourselves our full traditional Portuguese Christmas recipes from scratch \u0026ndash; while not being in Portugal. Herein lies the first issue: there are many different \u0026ldquo;traditions\u0026rdquo;, but these are the ones that me and my partner are used to.\nTraditionally, Christmas celebrations in Portugal start on the night of Christmas eve and carry on during Christmas day. The main meals are then dinner on the 24th December and lunch on the 25th December.\nChristmas eve dinnerAs mentioned, we will follow two separate traditions. From my family\u0026rsquo;s side, originally from the north of Portugal (Porto), we typically have boiled salted cod with vegetables (cabbage, onion and carrots), seasoned with olive oil, vinegar and garlic. From my partner\u0026rsquo;s side, the meal typically consists of octopus rice, accompanied with salted cod fishcakes (\u0026quot;bolinhos de bacalhau\u0026quot;) and pan-fried octopus (\u0026quot;filetes de polvo\u0026quot;).\nSalted codSalted cod is a staple from Portuguese cuisine, with possible origins in the cod salt-curing methods of Basque fishermen that ventured into Newfoundland in the 1500s1. Going through the centuries, even as recently as 1884, Portuguese writer Ea de Queiroz wrote in a letter to Oliveira Martins about his love of a \u0026ldquo;bacalhau de cebolada\u0026rdquo;2.\nThe salted cod needs to be soaked in water for at least four days to remove the excess salt (picture above, on the left).\nIt\u0026rsquo;s a really simple dish: just put everything on a pot and let it boil for approximately one hour. Since the salted cod has a really firm fleshy texture, it won\u0026rsquo;t fall apart like fresh fish when boiled for a long time. Usually there\u0026rsquo;s no need to add salt, since the cod will probably still have quite a lot of salt in it, but it never hurts to double check.\nWe will cook around two to three times the amount of cod and vegetables that we need for the actual meal. The reason for this is that the starter for the next day (oupa-velha) is made from the left-overs of the Christmas eve\u0026rsquo;s dinner. So essentially, we have to make sure we have plenty of left-overs!\nAnd here it is: ready to tuck in. As you can see, I like my salted cod with a very generous amount of olive oil and vinegar.\nOctopus riceOctopus is another northern Portuguese tradition, especially in the Minho and Trs-os-Montes, possibly due to the proximity with Galiza (Galicia) where octopus fishing has been historically a very important activity.\nNext it\u0026rsquo;s the octopus rice. Boil the octopus with just some salt for seasoning. Knowing when the octopus is ready is really an art. Make sure its not undercooked, but don\u0026rsquo;t overcook it either since it will be quite chewy. Brown chopped onions in olive oil and add the water from boiling the octopus along with rice, the octopus and chopped parsley. The rice should have a fair amount of water and not end up dry.\nPart of the octopus goes into the rice and the rest is pan-fried (\u0026quot;filetes de polvo\u0026quot;). They are battered, with eggs and flour, and deep-fried.\nWe then proceed to the cod fishcakes (\u0026quot;bolinhos de bacalhau\u0026quot;). These are done by shredding some salted code and mixing it with mashed potato, salt and parsley and then deep-fried.\nChristmas day lunchRoupa-velha The reason why we cook way more quantities than we need for the salted cod, is to make something called \u0026ldquo;roupa-velha\u0026rdquo; (literal translation \u0026ldquo;old clothes\u0026rdquo;) as a starter on the 25th. This a left-over dish and we use all the left-overs from the Christmas Eve dinner.\nStart by shredding the cooked salted cod and removing all the fish bones and skin.\nWe then add a good amount of garlic (two or three cloves at least), and prepare a pan with some olive oil. We put first the garlic and let it brown.\nWhen the garlic is brown we add all the left-overs (potato, sliced egg, carrot, cabbage and shredded code). We stir it for at least 15 minutes and add vinegar. Lots of vinegar.\nAnd here it is. Must be eaten while pipping hot.\nLamb roastUsually on the 25th of December we eat a roast (turkey, lamb, goat or pork). We went for a lamb roast. It was seasoned with lemon, rosemary, garlic, paprika, olive oil and salt for four days.\nIt is accompanied by roast potatoes and carrots and (optionally) some white rice.\nAnd here it is!\nDessertsAletria and arroz doceAletria is a typical Christmas dessert which is quite similar to rice pudding in taste, but instead of rice, it is done with vermicelli pasta.\nThe preparation is quite similar to rice pudding, but adding some lemon peels to the milk mix.\nA cinnamon decoration is a must, here shown with a festive \u0026ldquo;Feliz Natal\u0026rdquo; (Merry Christmas).\n\u0026ldquo;Arroz doce\u0026rdquo; (literal translation Sweet Rice) is very similar to rice pudding, also with the addition of some lemon.\nFilhsFilhs are a type of slightly sweet doughy pancake, usually sprinkled with sugar and cinnamon, traditional during Christmas. These are specific type of filh called \u0026ldquo;Filh tendida no joelho\u0026rdquo;, traditional from the Beiras Portuguese region, where the dough is stretched on top of the knee.\nThe dough has to be proven at a certain temperature. Here is the contraption we\u0026rsquo;ve used: a heating fan, heater and an Hibernate (!) book.\nThe dough must be proved (in our case at least 10 hours) so it can be stretched and fried lightly in olive oil on both sides. After draining any excess oil, they are sprinkled with a sugar and cinnamon mix.\nRabanadas\u0026ldquo;Rabanadas\u0026rdquo; are in essence very similar to \u0026ldquo;French toast\u0026rdquo;. The way to prepare them is to leave dried bread soaking in milk and drained before deep-frying. After draining, until they are mostly dry and without much excess oil, they are sprinkled (generously) with a mixture of sugar and cinnamon.\nBolo-ReiBolo-Rei (literal translation \u0026ldquo;King cake\u0026rdquo;) is a traditional Portuguese Christmas cake. A similar recipe to the modern one can be traced to the 19th century in Loire, southern France, sold for the first time around 1869 at the Confeitaria Nacional, in Lisboa3. Since then it has become very popular and a common sight at Portuguese Christmas tables.\nAlthough it is perfectly possible to do it at home, the one we had was store-bought.\n2021 updateThis year (2021) we did another take on the \u0026ldquo;Portuguese Christmas abroad\u0026rdquo; theme. The recipes are still same as last year, so I\u0026rsquo;ve decided to just update with some of the 2021 photos.\nOctopusLast year I didn\u0026rsquo;t include a photo of the octopus being cooked before preparing the Octopus rice, so here it is:\nAnd here is the fried octopus for this year\u0026rsquo;s Christmas eve dinner:\nFilhsSomething I\u0026rsquo;ve also forgot to add last year was a picture of the Filhs dough after proving. This year it grew a lot more than last year because we added raw bread dough instead of baker\u0026rsquo;s yeast. Using raw bread dough is actually the proper way to do it, but we didn\u0026rsquo;t have it last year.\nLast year I also didn\u0026rsquo;t include photos of the actual frying of the Filhs. They are deep-fried in olive oil, (not other vegetable oils) so for this amount of dough a considerable amount was needed. Something between 500-750ml.\nAs here is the final product for 2021:\nRabanadasThe 2021 rabanadas. The full story on rabanadas is bove:\nCodfish cakesHere are the 2021 bolinhos de bacalhau (salted cod fishcakes):\nSalted codCabbage, carrots, onions and eggs waiting for their turn:\nThe salted cod draining a little bit Preparing the bacalhau:\nCooking everything!\nThe final product:\nAletria and arroz doceAletria and arroz doce:\nRoupa-velha Silva, Antnio Jos Marques da (2015), \u0026ldquo;The fable of the cod and the promised sea: About Portuguese traditions of bacalhau\u0026rdquo;, in Barata, Filipe Themudo; Rocha, Joo Magalhes (eds.), Heritages and Memories from the Sea, vora, Portugal: 1st International Conference of the UNESCO Chair in Intangible Heritage and Traditional Know-How: Linking Heritage\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n\u0026ldquo;A comida como hbito e identidade: o bacalhau e os portugueses\u0026rdquo;, (ISCTE-IUL, Departamento de Antropologia, Escola de Cincias Humanas e Sociais), em 28 de fevereiro de 2013\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nde Lurdes Modesto, Maria, Afonso Praa, and Nuno Calvet. \u0026ldquo;Festas e comeres do povo portugus\u0026rdquo;. 1999.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"/portuguese-christmas-recipes.html","tags":null,"title":"Portuguese Christmas recipes"},{"categories":null,"contents":" [Unit tests](Unit testing) ","permalink":"/programming.html","tags":null,"title":"Programming"},{"categories":null,"contents":"Run specific testsRun all tests in a project$ pytest Run tests in a Single DirectoryTo run all the tests from one directory, use the directory as a parameter to pytest:\n$ pytest tests/my-directory Run tests in a Single Test File/ModuleTo run a file full of tests, list the file with the relative path as a parameter to pytest:\n$ pytest tests/my-directory/test_demo.py Run a Single Test FunctionTo run a single test function, add :: and the test function name:\n$ pytest -v tests/my-directory/test_demo.py::test_specific_function -v is used so you can see which function was run.\nRun a Single Test ClassTo run just a class, do like we did with functions and add ::, then the class name to the file parameter:\n$ pytest -v tests/my-directory/test_demo.py::TestClassName Run a Single Test Method of a Test ClassIf you don\u0026rsquo;t want to run all of a test class, just one method, just add another :: and the method name:\npytest -v tests/my-directory/test_demo.py::TestClassName::test_specific_method Run a Set of Tests Based on Test NameThe -k option enables you to pass in an expression to run tests that have certain names specified by the expression as a substring of the test name. It is possible to use and, or, and not to create complex expressions. For example, to run all of the functions that have _raises in their name:\n$ pytest -v -k _raises Resources A Gentle Introduction to Testing with PyTest ","permalink":"/pytest.html","tags":null,"title":"pytest"},{"categories":null,"contents":"SummaryMain page for all things Python. Other pages cover specific topics, such as:\nPython environments Code style Python collections Python Pweave Pandas Notes on Python grammar of graphics Monkey patching Abstract classes InstallationAnacondaAn option to install Python is to use Anaconda1. Download the appropriate installation file from https://www.anaconda.com/products/individual. And then run the installer with, e.g., sh ./Anaconda3-2021.11-Linux-x86_64.sh. The installation would typically be under $HOME/anaconda3. There is a page dedicated to configuring and using [Anaconda].\nLanguage changesIn 2021, the Python steering council accepted the proposal to add a pattern-matching primitive to the language. The proposal consists of PEP634 along with PEP635 and PEP636.\npages/backlog/Python 3.10 Cool New Features for You to Try Python 3.9 changes These include new dictionary operators, topological ordering, IPv6 scoped addresses, new math and string functions, and HTTP codes. pipNo binary installTo install a package from source with pip specify\npip install --no-binary $PACKAGE Since requirements files are passed as command-line options to pip, you can also specify it as\nsome-package --no-binary another-package Additionaly this will also work on setup.py\u0026rsquo;s install_requires. For instance:\nsetup( install_requires=[ \u0026#34;some-package==0.0.1 --no-binary\u0026#34; ]) ModulesRelative import in Python 3If a relative import is present inside a Python 3 file (e.g. file1) inside a module (e.g. mymod), say\nfrom .foo import bar We will encounter the error\nImportError: attempted relative import with no known parent package A possible solution is to include the following in your module\u0026rsquo;s __init__.py:\nimport os, sys sys.path.append(os.path.dirname(os.path.realpath(__file__))) Ternary operatorTernary operators help reduce the amount of very small if-else blocks. Python does not have a ternary operator like other languages. However, conditionals can be used to the same effect:\ny = 7 x = 0 if (y == 1) else 1 print(x) 1 for \u0026hellip; elsefor-else blocks allow to capture if a condition was met inside a for-loop. For instance, consider the following for-loop:\nlocations = [\u0026#39;a\u0026#39;, \u0026#39;b\u0026#39;, \u0026#39;c\u0026#39;, \u0026#39;d\u0026#39;, \u0026#39;f\u0026#39;] treasure = False for location in locations: if location == \u0026#39;x\u0026#39;: treasure = True break if not treasure: print(\u0026#34;X marks the spot, but not found\u0026#34;) X marks the spot, but not found We can simplify the above logic using a for-else loop:\nfor location in locations: if location == \u0026#39;x\u0026#39;: break else: print(\u0026#34;X marks the spot, but not found\u0026#34;) X marks the spot, but not found Boolean unravellingandunravelling the and boolean operator. The operation can be rewritten as the function u_and:\ndef u_and(a, b): result = a if a: result = b return result For instance:\na = True ; b = None print(a and b, u_and(a, b)) a = True ; b = True print(a and b, u_and(a, b)) a = False ; b = True print(a and b, u_and(a, b)) None None True True False False orOn the other hand, or cand be unravelled as:\ndef u_or(a, b): result = a if not a: result = b return result As an example:\na = True ; b = None print(a or b, u_or(a, b)) a = True ; b = True print(a or b, u_or(a, b)) a = False ; b = True print(a or b, u_or(a, b)) True True True True True True The many faces of printConcatenating argumentsvar1 = \u0026#34;Foo\u0026#34; var2 = \u0026#34;Bar\u0026#34; print(\u0026#34;I am \u0026#34;, var1, \u0026#34; not \u0026#34;, var2) I am Foo not Bar It is also possible to use separators by using the sep argument:\nvar1 = \u0026#34;Foo\u0026#34; var2 = \u0026#34;Bar\u0026#34; print(\u0026#34;I am\u0026#34;, var1, \u0026#34;not\u0026#34;, var2, sep=\u0026#34;!\u0026#34;) I am!Foo!not!Bar String terminationThe end argument allows to specify the suffix of the whole string.\nprint(\u0026#34;This is on radio\u0026#34;, end=\u0026#34; (over)\u0026#34;) This is on radio (over) Filesystem operationsGet home directoryFor Python +3.5:\nfrom pathlib import Path home = str(Path.home()) print(home) /Users/rui List files recursivelyFor Python +3.5, use glob:\nimport glob # root_dir with trailing slash (i.e. /root/dir/) root_dir = \u0026#34;./\u0026#34; for filename in glob.iglob(root_dir + \u0026#39;**/*.md\u0026#39;, recursive=True): print(filename[:-3]) ./Python ./Python code style ./Python Abstract classes ./New Features in Python 3.9 ./Understanding Decorators in Python Date operationsOffset-aware operationsLet\u0026rsquo;s say you have a date without timezone (offset naive), for instance:\nfrom datetime import datetime, timezone ts = datetime.now().replace(tzinfo=None) print(ts) 2022-08-07 14:08:13.498348 And you want to calculate the $\\delta$ with a datetime which has a time (offset aware). We\u0026rsquo;ll get an error.\ntry: delta = datetime.now(timezone.utc) - ts except TypeError as error: print(error) can't subtract offset-naive and offset-aware datetimes The solution is to add a timezone to the offset naive date. For instance:\nts = datetime.now(timezone.utc) delta = datetime.now(timezone.utc) - ts delta datetime.timedelta(microseconds=55) Strings Python strings are immutable, but only sometimes PackagingPyOxidizerAn alternative to package Python applications is PyOxidizer.\nPyOxidizer is often used to generate binaries embedding a Python interpreter and a custom Python application. However, its configuration files support additional functionality, such as the ability to produce Windows\nhttps://www.anaconda.com/\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"/python.html","tags":null,"title":"Python"},{"categories":null,"contents":"Abstract classes were added to Python with PEP-31191. One of the goals was to add interfaces to reinforce type checking. For instance:\nfrom abc import ABC, abstractmethod class AbstractFoo(ABC): @abstractmethod def calculate_foo(self): pass def foo(self): return self.calculate_foo() Sub-class hooksThe Abstract class implementation in Python has have a way of determinining what is a child of it even if it\u0026rsquo;s not a direct subclass of it. This is done by using sub-class hooks. For instance:\nclass AbstractFoo(ABC): @abstractmethod def calculate_foo(self): pass def foo(self): return self.calculate_foo() @classmethod def __subclasshook__(cls, C): return hasattr(C, \u0026#34;calculate_foo\u0026#34;) will return True if a class implements calculate_foo, even if it\u0026rsquo;s not a sub-class of AbstractFoo. For instance:\nclass NotAFoo: def calculate_foo(self): print(\u0026#34;I can calculate Foo!\u0026#34;) isinstance(NotAFoo(), AbstractFoo) True Since __subclasshook__ can contain any code that can be evaluated at runtime, we can get creative. For instance, we can create an abstract class for classes that have names like 1960s Frank Zappa albums.\nclass SixtiesZappa(ABC): @classmethod def __subclasshook__(cls, C): albums = [album.title().replace(\u0026#34; \u0026#34;, \u0026#34;\u0026#34;) for album in [\u0026#34;Freak Out\u0026#34;, \u0026#34;Absolutely Free\u0026#34;, \u0026#34;Lumpy Gravy\u0026#34;, \u0026#34;We\u0026#39;re Only in It for the Money\u0026#34;, \u0026#34;Cruising with Ruben \u0026amp; the Jets\u0026#34;, \u0026#34;Mothermania\u0026#34;, \u0026#34;Uncle Meat\u0026#34;, \u0026#34;Hot Rats\u0026#34;]] name = C.__name__ return name in albums class HotRats: pass isinstance(HotRats(), SixtiesZappa) True class WeaselsRippedMyFlesh: pass isinstance(WeaselsRippedMyFlesh(), SixtiesZappa) False PEP-3119, from 2007: https://peps.python.org/pep-3119/\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"/python-abstract-classes.html","tags":null,"title":"Python Abstract classes"},{"categories":null,"contents":"General notes on Python\u0026rsquo;s code style and practices.\nLinters Pylint1 pycodestyle2 (formerly PEP8) Flake83, meta-package including PyFlakes, pycodestyle, Ned Batchelders McCabe script4 Pylama, including pycodestyle, pydocstyle, PyFlakes and Ned Batchelders McCabe script Radon5 gjslint6 At the moment, my linter of choice is pylint.\nFormattersBlackBlack7 the famous formatter. Black became stable software in 29th January 20228. The stability policy is avaible here.\nhttps://www.pylint.org/\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nhttps://pypi.org/project/pycodestyle/\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nhttps://pypi.org/project/flake8/\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nhttps://github.com/PyCQA/mccabe)\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nhttps://radon.readthedocs.io/en/latest/\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nhttps://atom.io/packages/linter-gjslint\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nhttps://pypi.org/project/black/\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nhttps://pypi.org/project/black/22.1.0/\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"/python-code-style.html","tags":null,"title":"Python code style"},{"categories":null,"contents":"InterpretersTo install different Python interpreters I strongly recommend asdf1.\nLet\u0026rsquo;s look at to install Python in two different OSes, macOS and Fedora.\nmacOSTo install asdf on a macOS, first install the general dependencies with\n$ brew install coreutils curl git then install asdf itself with\n$ brew install asdf Add to the shell, in our case zsh with:\n$ echo -e \u0026#34;\\n. $(brew --prefix asdf)/asdf.sh\u0026#34; \u0026gt;\u0026gt; ~/.zshrc Add a plugin, in our case Python, with\n$ asdf plugin add Python You can list all available versions with\n$ asdf list all Python Install a specific version, say,\n$ asdf install Python 3.9.0 FedoraTo install asdf on a Fedora, first install the general dependencies\n$ sudo dnf install curl git Clone the repository\n$ git clone https://github.com/asdf-vm/asdf.git \\ ~/.asdf --branch v0.8.0 Add to zsh with\n`$ . $HOME/.asdf/asdf.sh` pyenvCompiling on macOSpyenv can be notoriously problematic on macOS. For instance, running pyenv doctor on my laptop2 will result in:\nCloning ~/.pyenv/plugins/pyenv-doctor/bin/..... Installing python-pyenv-doctor... python-build: use readline from homebrew python-build: use zlib from xcode sdk BUILD FAILED (OS X 10.15.7 using python-build 20180424) Inspect or clean up the working tree at /var/folders/c2/9d2fsqt57t10zn1f2ylp1jxw0000gn/T/python-build.20210128094523.17091 Results logged to /var/folders/c2/9d2fsqt57t10zn1f2ylp1jxw0000gn/T/python-build.20210128094523.17091.log Last 10 log lines: checking readline/readline.h, presence... no checking for readline/readline.h,... no checking readline/rlconf.h usability... yes checking readline/rlconf.h presence... yes checking for readline/rlconf.h... yes checking for SSL_library_init in -lssl... no configure: WARNING: OpenSSL \u0026lt;1.1 not installed. Checking v1.1 or beyond... checking for OPENSSL_init_ssl in -lssl... no configure: error: OpenSSL is not installed. make: *** No targets specified and no makefile found. Stop. Problem(s) detected while checking system. See https://github.com/pyenv/pyenv/wiki/Common-build-problems for known solutions. The problem in this case is that pyenv can\u0026rsquo;t find the relevant C headers for compilation of new versions. This can be fixed by using:\n$ CFLAGS=\u0026#34;-I$(brew --prefix openssl)/include \\ -I$(brew --prefix readline)/include \\ -I$(xcrun --show-sdk-path)/usr/include\u0026#34; \\ LDFLAGS=\u0026#34;-L$(brew --prefix openssl)/lib \\ -L$(brew --prefix readline)/lib \\ -L$(xcrun --show-sdk-path)/usr/lib\u0026#34; \\ pyenv doctor and the output will be:\nCloning ~/.pyenv/plugins/pyenv-doctor/bin/..... Installing python-pyenv-doctor... python-build: use readline from homebrew python-build: use zlib from xcode sdk Installed python-pyenv-doctor to /var/folders/c2/9d2fsqt57t10zn1f2ylp1jxw0000gn/T/pyenv-doctor.20210128095003.18889/prefix Congratulations! You are ready to build pythons! PoetryPoetry as Jupyter kernelTo register a poetry environment (named foo) as a Jupyter kernel, run:\npoetry run python -m ipykernel install --user --name foo HatchInstallationTo install Hatch in macOS use\nbrew install hatch Hatch can also be installed with pip\npip install hatch Or pipx for global access\npipx install hatch venvCreate a new venv with the command:\n$ virtualenv venv Alternatively, create the virtualenv name foo in ~/.virtualenvs using\n$ python -m venv ~/.virtualenvs/foo and activate it using (under Bash or zsh) with:\n$ source venv/bin/activate AnacondaFirst download Anaconda (or Miniconda). Once installed you can proceed to create environments3.\nCreating environmentsAn environment foo can be created using\nconda create --name foo One it is created, it can be activated using\nconda activate foo https://asdf-vm.com/\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nI\u0026rsquo;m running Big Sur at the moment of writing\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nThe remainder will assume that you have installed Anaconda, rather than Miniconda.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"/python-environments.html","tags":null,"title":"Python environments"},{"categories":null,"contents":"import pandas as pd mpg = pd.read_csv(\u0026#34;./data/mpg.csv\u0026#34;) mpg.head() mpg cylinders displacement horsepower weight acceleration model_year origin name 0 18.0 8 307.0 130 3504 12.0 70 1 chevrolet chevelle malibu 1 15.0 8 350.0 165 3693 11.5 70 1 buick skylark 320 2 18.0 8 318.0 150 3436 11.0 70 1 plymouth satellite 3 16.0 8 304.0 150 3433 12.0 70 1 amc rebel sst 4 17.0 8 302.0 140 3449 10.5 70 1 ford torino from plotnine import * from plotnine.data import * ggplot(data=mpg) +\\ geom_point(mapping=aes(x=\u0026#34;displ\u0026#34;, y=\u0026#34;hwy\u0026#34;)) + theme_classic() \u0026lt;ggplot: (324084173)\u0026gt; ggplot(data=mpg) +\\ geom_point(mapping=aes(x=\u0026#34;displ\u0026#34;, y=\u0026#34;hwy\u0026#34;, color=\u0026#34;class\u0026#34;)) + theme_classic() \u0026lt;ggplot: (324192241)\u0026gt; ggplot(data=mpg) +\\ geom_point(mapping=aes(x=\u0026#34;displ\u0026#34;, y=\u0026#34;hwy\u0026#34;, size=\u0026#34;class\u0026#34;)) + theme_classic() \u0026lt;ggplot: (324188691)\u0026gt; # Left ggplot(data=mpg) +\\ geom_point(mapping=aes(x=\u0026#34;displ\u0026#34;, y=\u0026#34;hwy\u0026#34;, alpha=\u0026#34;manufacturer\u0026#34;)) + theme_classic() \u0026lt;ggplot: (324284401)\u0026gt; # Right ggplot(data=mpg) +\\ geom_point(mapping=aes(x=\u0026#34;displ\u0026#34;, y=\u0026#34;hwy\u0026#34;, shape=\u0026#34;manufacturer\u0026#34;)) + theme_classic() \u0026lt;ggplot: (324342522)\u0026gt; ggplot(data=mpg) +\\ geom_point(mapping=aes(x=\u0026#34;displ\u0026#34;, y=\u0026#34;hwy\u0026#34;), color=\u0026#34;blue\u0026#34;) + theme_classic() \u0026lt;ggplot: (324407845)\u0026gt; ggplot(data=mpg) +\\ geom_smooth(mapping=aes(x=\u0026#34;displ\u0026#34;, y=\u0026#34;hwy\u0026#34;)) + theme_classic() \u0026lt;ggplot: (324408381)\u0026gt; ggplot(data=mpg) +\\ geom_smooth(mapping=aes(x=\u0026#34;displ\u0026#34;, y=\u0026#34;hwy\u0026#34;, linetype=\u0026#34;drv\u0026#34;)) + theme_classic() \u0026lt;ggplot: (324443965)\u0026gt; ggplot(data=mpg) +\\ geom_point(mapping=aes(x=\u0026#34;displ\u0026#34;, y=\u0026#34;hwy\u0026#34;)) +\\ geom_smooth(mapping=aes(x=\u0026#34;displ\u0026#34;, y=\u0026#34;hwy\u0026#34;)) + theme_classic() \u0026lt;ggplot: (324525996)\u0026gt; ggplot(data=mpg, mapping=aes(x=\u0026#34;displ\u0026#34;, y=\u0026#34;hwy\u0026#34;)) +\\ geom_point(mapping=aes(color=\u0026#34;class\u0026#34;)) +\\ geom_smooth() + theme_classic() \u0026lt;ggplot: (324562484)\u0026gt; ","permalink":"/python-grammar-of-graphics.html","tags":null,"title":"Python grammar of graphics"},{"categories":null,"contents":"When preparing a Jupyter1 notebook for a workshop on recommendation engines which I\u0026rsquo;ve presented with a colleague, I was faced with the following problem:\n\u0026ldquo;How to break a large class definition into several cells so it can be presented step-by-step.\u0026rdquo;\nHaving the ability to declare a rather complex (and large) Python class in separate cells has several advantages, the obvious one being the ability to fully document each method\u0026rsquo;s functionality with Markdown, rather than comments. Python does allow for functionality to be added to classes after their declaration via the assignment of methods through attributes. This is commonly known as \u0026ldquo;monkey patching\u0026rdquo; and hinges on the concepts of bound and unbound methods.\nI will show a quick and general overview of the methods that Python puts at our disposal for dynamic runtime object manipulation, but for a more in-depth please consult the official Python documentation.\nBound and unbound methodsLet\u0026rsquo;s first look at bound methods. If we assume a class called Class and an instance instance, with an instance method bound and class method unbound such that\nclass Class: def bound(self): return \u0026#34;I\u0026#39;m a bound method\u0026#34; @staticmethod def unbound(): return \u0026#34;I\u0026#39;m an unbound method\u0026#34; instance = Class() Then foo is a bound method and bar is an unbound method. This definition, in practice, can be exemplified by the standard way of calling .foo(), which is\ninstance.bound() : I\u0026#39;m a bound method which in turn is equivalent to\nClass.bound(instance) : I\u0026#39;m a bound method The standard way of calling unbound is , similarly\ninstance.unbound() : I\u0026#39;m an unbound method This, however, is equivalent to\nClass.unbound() : I\u0026#39;m an unbound method In the unbound case, we can see there\u0026rsquo;s no need to pass the class instance. unbound is not bound to the class instance.\nAs mentioned before, Python allow us to change the class attributes at runtime. If we consider a method such as\ndef newBound(self): return \u0026#34;I\u0026#39;m a (new!) bound method\u0026#34; we can then add it to the class, even after declaring it. For instance:\nClass.newBound = newBound instance = Class() instance.newBound() # Class.newBound(instance) : I\u0026#39;m a (new!) bound method It is interesting to note that any type of function definition will work, since functions are first class objects in Python. As such, if the method can be written as a single statement, a ~lambda~ could also be used, i.e.\nClass.newBound = lambda self: \u0026#34;I\u0026#39;m a lambda\u0026#34; instance.newBound() : I\u0026#39;m a (new!) bound method A limitation of the \u0026ldquo;monkey patching\u0026rdquo; method, is that attributes can only be changed at the class definition level. As an example, although possible, it is not trivial to add the .newBound() method to instance.\nA solution is to either call the descriptor methods (which allow for instance attribute manipulation), or declare the instance attribute as a MethodType.\nTo illustrate this in our case:\nimport types instance.newBound = types.MethodType(newBound, instance) instance.newBound() # Prints \u0026#34;I\u0026#39;m a lambda\u0026#34; : I\u0026#39;m a (new!) bound method This method is precisely, as mentioned, to change attributes for a specific instance, so in this case, if we try to access the bound method from another instance anotherInstance, it would fail\nanotherInstance = Class() anotherInstance.newBound() # fails with AttributeError : I\u0026#39;m a lambda Abstract classesPython supports abstract classes, i.e. the definition of \u0026ldquo;blueprint\u0026rdquo; classes for which we delegate the concrete implementation of abstract methods to subclasses. In Python 3.x this is done via the @abstractmethod annotation. If we declare a class such as\nfrom abc import ABC, abstractmethod class AbstractClass(ABC): @abstractmethod def abstractMethod(self): pass we can then implement abstractMethod in all of AbstractClass\u0026rsquo;s subclasses:\nclass ConcreteClass(AbstractClass): def abstractMethod(self): print(\u0026#34;Concrete class abstract method\u0026#34;) We could, obviously, do this in Python without abstract classes, but this mechanism allows for a greater safety, since implementation of abstract methods is mandatory in this case. With regular classes, not implementing abstractMethod would simply assume we were using the parent\u0026rsquo;s definition.\nUnfortunately, monkey patching of abstract methods is not supported in Python. We could monkey patch the concrete class:\nConcreteClass.newBound = lambda self: print(\u0026#34;New \u0026#39;child\u0026#39; bound\u0026#34;) c = ConcreteClass() c.newBound() # prints \u0026#34;New \u0026#39;child\u0026#39; bound\u0026#34; : New \u0026#39;child\u0026#39; bound And we could even add a new bound method to the superclass, which will be available to all subclasses:\nAbstractClass.newBound = lambda self: print(\u0026#34;New \u0026#39;parent\u0026#39; bound\u0026#34;) c = ConcreteClass() c.newBound() # prints \u0026#34;New \u0026#39;parent\u0026#39; bound\u0026#34; : New \u0026#39;child\u0026#39; bound However, we can\u0026rsquo;t add abstract methods with monkey patching. This is a documented exception of this functionality with the specific warning that\nDynamically adding abstract methods to a class, or attempting to modify the abstraction status of a method or class once it is created, are not supported. The abstractmethod() only affects subclasses derived using regular inheritance; \u0026ldquo;virtual subclasses\u0026rdquo; registered with the ABC\u0026rsquo;s register() method are not affected.\nPrivate methodsWe can dynamically add and replace inner methods, such as:\nclass Class: def _inner(self): print(\u0026#34;Inner bound\u0026#34;) def __private(self): print(\u0026#34;Private bound\u0026#34;) def callNewPrivate(self): self.__newPrivate() Class._newInner = lambda self: print(\u0026#34;New inner bound\u0026#34;) c = Class() c._inner() # prints \u0026#34;Inner bound\u0026#34; c._newInner() # prints \u0026#34;New inner bound\u0026#34; : Inner bound : New inner bound However, private methods behave differently. Python enforces name mangling for private methods. As specified in the documentation:\nSince there is a valid use-case for class-private members (namely to avoid name clashes of names with names defined by subclasses), there is limited support for such a mechanism, called name mangling. Any identifier of the form __spam (at least two leading underscores, at most one trailing underscore) is textually replaced with _classname__spam, where classname is the current class name with leading underscore(s) stripped. This mangling is done without regard to the syntactic position of the identifier, as long as it occurs within the definition of a class.\nWe can then still access the private methods (although we probably shouldn\u0026rsquo;t), but monkey patching won\u0026rsquo;t work as before due to the above.\nc._Class__private() # Private bound Class.__newPrivate = lambda self: print(\u0026#34;New private bound\u0026#34;) c = Class() c._Class__newPrivate() # fails with AttributeError We have defined a new method called __newPrivate() but interestingly, this method is not private. We can see this by calling it directly (which is allowed) and by calling the new \u0026ldquo;private\u0026rdquo; method from inside the class as self.__newPrivate():\nc.__newPrivate() # prints \u0026#34;New private bound\u0026#34; c.callNewPrivate() # fails with AttributeError (can\u0026#39;t find _Class_NewPrivate) It is possible to perform some OOP abuse and declare the private method by mangling the name ourselves. In this case we could then do:\nClass._Class__newPrivate = lambda self: print(\u0026#34;New private bound\u0026#34;) c = Class() c._Class__newPrivate() # prints \u0026#34;New private bound\u0026#34; c.callNewPrivate() # prints \u0026#34;New private bound\u0026#34; BuiltinsIs it possible to monkey patch builtin classes in Python, e.g. int or float? In short, yes, it is.\nAlthough the usefulness is arguable and I strongly urge not to do this in any production scenario, we\u0026rsquo;ll look at how to achieve this, for the sake of completeness. A very interesting and educational read is available from the Forbidden Fruit Python module.\nPrimitive (or builtin) classes in Python are typically written in C and as such some of these meta-programming facilities require jumping through extra hoops (as well as being a Very Bad Idea). Let\u0026rsquo;s first look at the integer class representation, int.\nA int doesn\u0026rsquo;t allow bound methods to be added dynamically as previously. For instance:\np = 5 type(p) # int We can try to add a method to int to square the value of the instance:\nint.square = lambda self: self ** 2 This fails with the error TypeError: can't set attributes of built-in/extension type 'int'. The solution (as presented in Forbidden Fruit) is to first create classes to hold the ctype information of a builtin (C) class. We subclass ctypes Python representation of a C struct in native byte order and hold the signed int size and pointer to PyObject.\nimport ctypes class PyObject(ctypes.Structure): pass PyObject.fields = [ (\u0026#39;ob_refcnt\u0026#39;, ctypes.c_int), (\u0026#39;ob_type\u0026#39;, ctypes.POINTER(PyObject)), ] Next we create a holder for Python objects slots, containing a reference to the ctype structure:\nclass SlotsProxy(PyObject): _fields_ = [(\u0026#39;dict\u0026#39;, ctypes.POINTER(PyObject))] The final step is extract the PyProxyDict from the object referenced by the pointer. Ideally, we should get the builtin\u0026rsquo;s namespace so we can freely set attributes as we did previously. A helper function to retrieve the builtins (mutable) namespace can then be:\ndef patch(klass): name = klass.__name__ target = klass.__dict__ proxy_dict = SlotsProxy.from_address(id(target)) namespace = {} ctypes.pythonapi.PyDict_SetItem( ctypes.py_object(namespace), ctypes.py_object(name), proxy_dict.dict, ) return namespace[name] We can now easily patch builtin classes. Let\u0026rsquo;s try to add the square method again by first retrieving the namespace (stored below in d) and setting it directly\nd = patch(int) d[\u0026#34;square\u0026#34;] = lambda self: self ** 2 p.square() # 25 All future instance of int will also contain the square method now:\n(2 + p).square() # 49 Conclusion\u0026ldquo;Monkey patching\u0026rdquo; is usually, and rightly so, considered a code smell, due to the increased indirection and potential source of unwanted surprises. However, having the ability to \u0026ldquo;monkey patch\u0026rdquo; classes in Python allows us to write Jupyter notebooks in a more literate, fluid way rather than presenting the user with a \u0026ldquo;wall of code\u0026rdquo;. Thank you for reading. If you have any comments or suggestions please drop me a message on Mastodon.\nhttps://jupyter.org/\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"/python-monkey-patching-for-readability.html","tags":null,"title":"Python monkey patching (for readability)"},{"categories":null,"contents":"InstallingInstalling pweave is a matter of simply running1:\npip3 install pweave At the moment of writing, the editor which, IMO, has the best support for pweave is Atom (using Hydrogen). To install the necessary packages run:\napm install language-weave Hydrogen apm install language-markdown atom-html-preview pdf-view I recommend using a separate pyenv for this.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"/python-pweave.html","tags":null,"title":"Python Pweave"},{"categories":null,"contents":"pytestTests with pytest.\n","permalink":"/python-testing.html","tags":null,"title":"Python testing"},{"categories":null,"contents":"UnionsFor Python 3.10 onwards1, Unions can be specified with the character |. For instance:\nUnion[int, str, float] # before 3.10 int | str | float # after 3.10 https://docs.python.org/3/library/typing.html#typing.Union\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"/python-typing.html","tags":null,"title":"Python typing"},{"categories":null,"contents":"Page for Quarkus.\nCreating projectsAn example on how to create a Quarkus project:\nmvn io.quarkus:quarkus-maven-plugin:2.5.1.Final:create \\ -DprojectGroupId=com.okta.rest \\ -DprojectArtifactId=quarkus \\ -DclassName=\u0026#34;com.okta.rest.quarkus.HelloResource\u0026#34; \\ -Dpath=\u0026#34;/hello\u0026#34; \\ -Dextensions=\u0026#34;smallrye-jwt,resteasy-reactive\u0026#34; ","permalink":"/quarkus.html","tags":null,"title":"Quarkus"},{"categories":null,"contents":"All of the examples presented in this page will be done using Scikit-learn.\n","permalink":"/random-forest.html","tags":null,"title":"Random Forest"},{"categories":null,"contents":"IntroductionA random walk is a stochastic process which describes a path made of consecutive random steps.\nGaussianIn Gaussian random walk the steps follow a continuous Gaussian distribution. We will look at two different types, the univariate and multivariate kind.\nUnivariateA univariate Gaussian Random Walk, is a series of i.i.d. $\\mathcal{N}(0,1)$ random variables such that\n$$ \\begin{align*} X_0\u0026amp;=0 \\ X_t\u0026amp;=X_{t1}+\\epsilon_t \\end{align*} $$\nWhere $t=1,2,\\dots$ and $\\epsilon_t$ is a series of i.i.d. $\\mathcal{N}(0,1)$ random variables.\nLet\u0026rsquo;s illustrate a simple univariate Gaussian random walk in Python, by plotting 1000 realisations.\nimport numpy as np N = 1000 np.random.seed(23) realisations = [] for i in range(N): realisations.append(np.cumsum(np.random.normal(size=100))) import matplotlib.pyplot as plt from plotutils import * for i in range(N): plt.plot(realisations[i], c=\u0026#34;k\u0026#34;, alpha=0.1) plt.title(\u0026#34;Univariate Gaussian random walk\u0026#34;) plt.xlabel(\u0026#34;t\u0026#34;) plt.ylabel(\u0026#34;$x_t$\u0026#34;) plt.show() ","permalink":"/random-walk.html","tags":null,"title":"Random walk"},{"categories":null,"contents":"What I\u0026rsquo;m reading now and what I\u0026rsquo;ve read in the past.\nReading now The Dark Light Years, by Brian Aldiss (1964) Past readingsBooks I have read recently (most recently first).\nTitle Cover Plague From Space by Harry Harrison (1965) Duende Meadow, by Paul Cook (1985) Hospital Station, by James White (1962) Retief: Envoy to New Worlds, by Keith Laumer (1972) The Ballad of Beta-2, by Samuel R. Delany (1965) Damnation Alley, by Roger Zelazny (1977) The Practicing Stoic: A Philosophical User\u0026rsquo;s Manual. (May 2021) ","permalink":"/reading-list.html","tags":null,"title":"Reading list"},{"categories":null,"contents":"SandboxRHODS can be trialled on the RHODS developer sandbox.\n","permalink":"/rhods.html","tags":null,"title":"RHODS"},{"categories":null,"contents":"Receiver operating characteristicROC (Receiver operating characteristic).\nimport pandas as pd data = pd.read_csv(\u0026#34;./data/credit-bias.zip\u0026#34;) We plot on the x the False-Positive rate and plot on the y the True-positive rate.\n","permalink":"/roc.html","tags":null,"title":"ROC"},{"categories":null,"contents":"A typical way of measuring the difference between observations and results from a predictor.\nThe formal definition is:\n$$ \\begin{aligned} RMSE(\\hat{\\theta}) \u0026amp;= \\sqrt{\\operatorname{MSE}(\\hat{\\theta})} \\\\ \u0026amp;= \\sqrt{\\operatorname{E}((\\hat{\\theta}-\\theta)^2)}. \\end{aligned} $$\nFor $N$ observations $Y={y_1, \\dots,y_N}$ we can express it as:\n$$ RMSE=\\sqrt{\\frac{\\sum _ {n=1}^{N}(\\hat{y} _{n}-y _{n})^{2}}{N}}. $$\nExampleimport numpy as np X = 2 * np.random.rand(1000,1) X_b = np.c_[np.ones((1000,1)), X] Y = 1 + 2.5 * X + np.random.randn(1000,1) ","permalink":"/root-mean-squared-error.html","tags":null,"title":"Root Mean Squared Error"},{"categories":null,"contents":"A page about RSS.\nHistoryRSS has been called\nRDF Site Summary Rich Site Summary and (most recently) Really Simple Syndication Initially developed by Netscape in 1999 for the my.netscape.com portal.\nArticles It\u0026rsquo;s Time to Get Back Into RSS | Daniel Miessler Ask HN: How do you RSS? | Hacker News - https://news.ycombinator.com/item?id=23577265 https://danielmiessler.com/blog/its-time-to-get-back-into-rss/ RSS Feed Best Practises ","permalink":"/rss.html","tags":null,"title":"RSS"},{"categories":null,"contents":"Install$ curl --proto \u0026#39;=https\u0026#39; --tlsv1.2 -sSf https://sh.rustup.rs | sh $ source $HOME/.cargo/env Create a new project using$ cargo new hello_world --bin # for a binary $ cargo new hello_world # for a library Exercises Rust exercises, resolution of the rustlings exercises ReferenceUploading to crates.ioUse\ncargo publish Static constants\u0026ldquo;Lazy\u0026rdquo; static constants can be defined using the lazy_static macro from the lazy_static crate.\nThe crate can be added to the dependencies with\n[dependencies] lazy_static = \u0026#34;1.4.0\u0026#34; and by adding to the Rust source code:\n#[macro_use] extern crate lazy_static; Static \u0026ldquo;global\u0026rdquo; constants can then be added via:\nlazy_static! { static ref HASHMAP: HashMap\u0026lt;u32, \u0026amp;\u0026#39;static str\u0026gt; = { let mut m = HashMap::new(); m.insert(0, \u0026#34;foo\u0026#34;); m.insert(1, \u0026#34;bar\u0026#34;); m.insert(2, \u0026#34;baz\u0026#34;); m }; } List folders recursivelyUsing the glob crate:\nuse glob::glob; fn main() { for entry in glob(\u0026#34;./**/*.md\u0026#34;).expect(\u0026#34;Failed to read glob pattern\u0026#34;) { match entry { Ok(path) =\u0026gt; println!(\u0026#34;{:?}\u0026#34;, path.display()), Err(e) =\u0026gt; println!(\u0026#34;{:?}\u0026#34;, e), } } } Duration between two timesUsing Rust\u0026rsquo;s standard SystemTime1, a duration between the present moment and a specific time can be calculated using:\nlet duration = SystemTime::now() .duration_since(another_system_time) .ok() .unwrap(); https://doc.rust-lang.org/std/time/struct.SystemTime.html\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"/rust.html","tags":null,"title":"Rust"},{"categories":null,"contents":"Notes on the Scala language.\n[Scala cookbook ","permalink":"/scala.html","tags":null,"title":"Scala"},{"categories":null,"contents":"val a = (0 until 10) : a: Range = Range(0, 1, 2, 3, 4, 5, 6, 7, 8, 9) println(a.map(i =\u0026gt; i + 10)) : Vector(10, 11, 12, 13, 14, 15, 16, 17, 18, 19) ","permalink":"/scala-cookbook.html","tags":null,"title":"Scala cookbook"},{"categories":null,"contents":"Collection of notes on Python\u0026rsquo;s scikit-learn machine learning library.\nOptimising random forest hyperparamaters 15 Lesser-Known Useful SkLearn Models You Should Use Now ","permalink":"/scikit-learn.html","tags":null,"title":"Scikit-learn"},{"categories":null,"contents":"This file exists solely to respond to /search URL with the related search layout template.\nNo content shown here is rendered, all content is based in the template layouts/page/search.html\nSetting a very low sitemap priority will tell search engines this is not important content.\nThis implementation uses Fusejs, jquery and mark.js\nInitial setupSearch depends on additional output content type of JSON in config.toml\n[outputs] home = [\u0026#34;HTML\u0026#34;, \u0026#34;JSON\u0026#34;] Searching additional filedsTo search additional fields defined in front matter, you must add it in 2 places.\nEdit layouts/_default/index.JSONThis exposes the values in /index.json i.e. add category\n... \u0026#34;contents\u0026#34;:{{ .Content | plainify | jsonify }} {{ if .Params.tags }}, \u0026#34;tags\u0026#34;:{{ .Params.tags | jsonify }}{{end}}, \u0026#34;categories\u0026#34; : {{ .Params.categories | jsonify }}, ... Edit fuse.js options to Searchstatic/js/search.js\nkeys: [ \u0026#34;title\u0026#34;, \u0026#34;contents\u0026#34;, \u0026#34;tags\u0026#34;, \u0026#34;categories\u0026#34; ] ","permalink":"/search.html","tags":null,"title":"Search Results"},{"categories":null,"contents":"Deploying machine learning models in production comes with several requirements. We must manage the model lifecycle. We need reproducibility and typically use containerised workflows.\nSeldon1 is a tool which aims at providing a production workflow for machine learning models, allowing to build model serving containers which expose well-defined APIs.\nIn this post, I\u0026rsquo;ll show how to create a simple model and how to deploy it with Seldon. The model is a customer segmentation one. The goal is to classify a customer according to a segment (0, 1 or 2), according to its age, income, whether they engaged with previous campaigns and the campaign type.\nOnce we train the model, we deploy it with Seldon in a container orchestration platform such as Kubernetes2 and OpenShift3.\nCreate dataWe use the Python\u0026rsquo;s Scikit-learn4 to train our model. However, we must first simulate some data to train it. We start by simulating the users age ($a$) and income ($c$). We assume income is correlated with age.\n$$ \\begin{aligned} c|a \u0026amp;\\sim \\mathcal{N}\\left(a + 20, 100\\right) \\\\ a|k \u0026amp;\\sim \\mathcal{U}\\left(A_k, B_k\\right),\\quad A=\\left\\lbrace16, 25, 50, 61\\right\\rbrace,B=\\left\\lbrace24, 49, 60, 90\\right\\rbrace \\\\ k \u0026amp;\\sim \\mathcal{M}\\left(4, \\left\\lbrace 0.15, 0.4, 0.2, 0.25\\right\\rbrace\\right) \\end{aligned} $$\nLet\u0026rsquo;s assume we have eight distinct events ($e=\\left(0, 1, \\dots, 7\\right)$). We sample them from a multinomial distribution and also assume that two different age bands have different distributions, just to add some variation.\n$$ e = \\begin{cases} \\mathcal{M}\\left(7, \\left\\lbrace 0.026, 0.195, 0.156, 0.208, 0.130, 0.205, 0.078 \\right\\rbrace\\right) \u0026amp; \\text{if}\\ a \u0026lt; 50 \\\\ \\mathcal{M}\\left(7, \\left\\lbrace 0.052, 0.143, 0.169, 0.182, 0.164, 0.182, 0.104 \\right\\rbrace\\right) \u0026amp; \\text{if}\\ a \\geq 50 \\end{cases} $$\nThe responses are calculated as 0 or 1, representing \u0026ldquo;true\u0026rdquo; or \u0026ldquo;false\u0026rdquo;, and sampled from Bernoulli distributions, with different distributions depending on the event, again just to add some variation.\n$$ r = \\begin{cases} \\text{Bernoulli}\\left(0.6\\right) \u0026amp; \\text{if}\\ e \\in \\left(2, 3, 4, 6\\right) \\\\ \\text{Bernoulli}\\left(0.4\\right) \u0026amp; \\text{if}\\ e \\in \\left(1, 5, 7\\right) \\end{cases} $$\nTo predict the response of a customer, we use a logistic model, with coefficients $\\beta_{age}=-0.0004$ and $\\beta_{income}=0.0001$. For the customer level, we use a negative binomial model with coefficients $\\beta_{age}=-0.0233$ and $\\beta_{income}=0.0054$. This results in the following distribution of customer levels:\nFinally, we create the response according to negative binomial model with coefficients $\\beta_{level}=0.1862$ and $\\beta_{response}=0.2076$. We get the following segments, stratified by age and income:\nTrain modelNow that we have our simulated data, we can train a model. Generally, it is straightforward to train model data when in pandas data frame format. Let\u0026rsquo;s proceed with creating a data frame with the data we\u0026rsquo;ve just generated:\nimport pandas as pd data = { \u0026#34;age\u0026#34;: age, \u0026#34;income\u0026#34;: income, \u0026#34;class\u0026#34;: _class, \u0026#34;response\u0026#34;: response, \u0026#34;segment\u0026#34;: segment, \u0026#34;events\u0026#34;: events, } df = pd.DataFrame(data) We now create the training and testing datasets. The first thing is to define the classifier\u0026rsquo;s inputs and outputs and then splitting each of them into training and testing. Here I have used a split of 60%/40% for training and testing respectively.\nfrom sklearn.model_selection import train_test_split cols = [\u0026#34;age\u0026#34;, \u0026#34;income\u0026#34;, \u0026#34;response\u0026#34;, \u0026#34;events\u0026#34;] inputs = df[cols] outputs = df[\u0026#34;segment\u0026#34;] # split dataset X_train, X_test, y_train, y_test = train_test_split( inputs, outputs, test_size=0.4, random_state=23 ) We use a Random Forest classifier as the underlying algorithm for our model. These are available in sciki-learn with the RandomForestClassifier class. However, scikit-learn does not support categorical variables out of the box5. To deal with them, we build a Pipeline, which allows to chain multiple transformations to our data, including a categorical variable processor, such as OrdinalEncoder6. We use DataFrameMapper to apply the encoder to the response and events columns and leave the remaining unchanged.\nfrom sklearn.ensemble import RandomForestClassifier from sklearn import preprocessing from sklearn.pipeline import Pipeline def build_RF_pipeline(inputs, outputs, rf=None): if not rf: rf = RandomForestClassifier() pipeline = Pipeline( [ ( \u0026#34;mapper\u0026#34;, DataFrameMapper( [ ([\u0026#34;response\u0026#34;, \u0026#34;events\u0026#34;], preprocessing.OrdinalEncoder()), ([\u0026#34;age\u0026#34;, \u0026#34;income\u0026#34;], None), ] ), ), (\u0026#34;classifier\u0026#34;, rf), ] ) pipeline.fit(inputs, outputs) return pipeline The actual training involves a simple hyper-parameter estimation using RandomizedSearchCV. This method performs a type of parameter grid search but restricting the search to only the specified values. For the scope of this post, it is not necessary to perform an exhaustive hyperparameter estimation. The RF_estimation function returns the best-fitted model after searching with the test dataset.\ndef RF_estimation( inputs, outputs, estimator_steps=10, depth_steps=10, min_samples_split=None, min_samples_leaf=None, ): # hyper-parameter estimation n_estimators = [ int(x) for x in np.linspace(start=50, stop=100, num=estimator_steps) ] max_depth = [int(x) for x in np.linspace(3, 10, num=depth_steps)] max_depth.append(None) if not min_samples_split: min_samples_split = [1, 2, 4] if not min_samples_leaf: min_samples_leaf = [1, 2, 4] bootstrap = [True, False] random_grid = { \u0026#34;n_estimators\u0026#34;: n_estimators, \u0026#34;max_depth\u0026#34;: max_depth, \u0026#34;min_samples_split\u0026#34;: min_samples_split, \u0026#34;min_samples_leaf\u0026#34;: min_samples_leaf, \u0026#34;bootstrap\u0026#34;: bootstrap, } rf_random = RandomizedSearchCV( estimator=RandomForestClassifier(), param_distributions=random_grid, n_iter=100, scoring=\u0026#34;neg_mean_absolute_error\u0026#34;, cv=3, verbose=1, random_state=42, n_jobs=-1, ) rf_random.fit(inputs, outputs) best_random = rf_random.best_estimator_ return best_random After applying the parameter estimation, we take the best scoring model and calculate the MSE. Unsurprisingly (given the simple model and simulated data), we get a very good fit.\nrf_predictions = random_forest_pipeline.predict(X_test) print(f\u0026#34;MSE: {random_forest_pipeline.score(X_test, y_test)*100}%\u0026#34;) # MSE: 99.95% The final step is serialising the model. Serialisation is necessary since we only serve the pre-trained model. To do so, we use the joblib library and save the model to a model.pkl file.\nimport joblib # save mode in filesystem joblib.dump(random_forest_pipeline, \u0026#34;model.pkl\u0026#34;) Deploy modelIt is important to note that we don\u0026rsquo;t need the model training code included in the Seldon server. The purpose of Seldon is not to train models, but to deploy them and manage their lifecycle. This workflow means that a typical Seldon deployment would only include the prediction endpoint implementation and a serialised model. This provision is made by firstly create a wrapper for our model which implements the Seldon endpoints.\nSimple modelWe create a Python script called Model.py 7. The primary prediction endpoint uses the following signature:\ndef predict(self, X: np.ndarray, names: Iterable[str], meta: Dict = None) The wrapper is straightforward, in this example. We use the joblib library again, to load the serialised model model.pkl, and then pass through any JSON payload as inputs (X) to the model to get a prediction as well as using Python\u0026rsquo;s default logging to provide some feedback.\nimport joblib import logging class Model(object): def __init__(self): logger.info(\u0026#34;Initializing.\u0026#34;) logger.info(\u0026#34;Loading model.\u0026#34;) self.model = joblib.load(\u0026#34;model.pkl\u0026#34;) def predict(self, X, features_names): return self.model.predict_proba(X) We now build the model using the s2i (source-to-image). As the name implies, s2i\u0026rsquo;s allow to create a container image from source code, taking care of any necessary intermediate steps. Seldon support several types of builds (such as Python, R and Java)8.\nTypically s2i\u0026rsquo;s rely on certain conventions (over configuration) on your application structure. A requirement when building a Seldon model using its s2i is to provide some specific environment variables. These are usually stored in a file located in $REPO/.s2i/environment. For instance, for this model we use:\nMODEL_NAME=Model API_TYPE=REST SERVICE_TYPE=MODEL PERSISTENCE=0 The MODEL_NAME corresponds to the script we\u0026rsquo;ve created previously, Model.py and instructs Seldon to use it as the REST endpoint provider. API_TYPE defines the endpoint interface. We use the REST interface, other possibilities include gRPC, for instance.\nTo build the container image using the s2i, assuming you want an image named $NAME and tagged with $TAG, we simply need to run:\n$ s2i build $REPO \\ seldonio/seldon-core-s2i-python36:0.18 \\ $NAME:$TAG You can provide the location of your source code either by specifying a remote Git repository or by passing a local one. Once the container image builds, you can now run it using, for instance:\ndocker run -i --rm -p 5000:5000 $NAME:$TAG Let\u0026rsquo;s get a prediction from the model:\n$ curl --header \u0026#34;Content-Type: application/json\u0026#34; \\ --request POST \\ --data \u0026#39;{\u0026#34;data\u0026#34;:{\u0026#34;ndarray\u0026#34;:[34.0, 100.0, 1, 2]()}}\u0026#39; \\ http://localhost:5000/predict This will return a prediction:\n{ \u0026#34;data\u0026#34;: { \u0026#34;names\u0026#34;: [\u0026#34;t:0\u0026#34;,\u0026#34;t:1\u0026#34;,\u0026#34;t:2\u0026#34;], \u0026#34;ndarray\u0026#34;: [0.0,0.9980208571211083,0.00197914287889168]}, \u0026#34;meta\u0026#34;: {} } This response corresponds to the probability of each segment (0, 1 and 2), respectively. We can see that a customer with this profile is classified as a segment 1 with an associated probability of 99.8%.\nWith metricsSeldon provides basic metrics by default, covering service, predictor and model name, version and image. However, you can directly add custom metrics. Going back to our Model wrapper class, we add a new method called metrics which returns custom metrics. The metrics are compatible with Prometheus and, therefore, the metric type should be familiar if you have dealt with Prometheus before. These include, for instance:\nCounters Gauges Timers Let\u0026rsquo;s add to the wrapper:\nimport joblib import logging class Model(object): def __init__(self): logger.info(\u0026#34;Initializing.\u0026#34;) logger.info(\u0026#34;Loading model.\u0026#34;) self.model = joblib.load(\u0026#34;model.pkl\u0026#34;) def predict(self, X, features_names): return self.model.predict_proba(X) # new custom metrics endpoint def metrics(self): return [ # a counter which will increase by the given value {\u0026#34;type\u0026#34;: \u0026#34;COUNTER\u0026#34;, \u0026#34;key\u0026#34;: \u0026#34;mycounter\u0026#34;, \u0026#34;value\u0026#34;: 1}, # a gauge which will be set to given value {\u0026#34;type\u0026#34;: \u0026#34;GAUGE\u0026#34;, \u0026#34;key\u0026#34;: \u0026#34;mygauge\u0026#34;, \u0026#34;value\u0026#34;: 10}, # a timer which will add sum and count metrics - assumed millisecs {\u0026#34;type\u0026#34;: \u0026#34;TIMER\u0026#34;, \u0026#34;key\u0026#34;: \u0026#34;mytimer\u0026#34;, \u0026#34;value\u0026#34;: 1.1}, ] If we now request a new prediction, as previously, we can see the custom metrics included in the model\u0026rsquo;s response.\n{ \u0026#34;data\u0026#34;: { \u0026#34;names\u0026#34;: [\u0026#34;t:0\u0026#34;,\u0026#34;t:1\u0026#34;,\u0026#34;t:2\u0026#34;], \u0026#34;ndarray\u0026#34;:[0.0,0.9980208571211083,0.00197914287889168]}, \u0026#34;meta\u0026#34;: { \u0026#34;metrics\u0026#34;: [ {\u0026#34;key\u0026#34;:\u0026#34;mycounter\u0026#34;,\u0026#34;type\u0026#34;:\u0026#34;COUNTER\u0026#34;,\u0026#34;value\u0026#34;:1}, {\u0026#34;key\u0026#34;:\u0026#34;mygauge\u0026#34;,\u0026#34;type\u0026#34;:\u0026#34;GAUGE\u0026#34;,\u0026#34;value\u0026#34;:10}, {\u0026#34;key\u0026#34;:\u0026#34;mytimer\u0026#34;,\u0026#34;type\u0026#34;:\u0026#34;TIMER\u0026#34;,\u0026#34;value\u0026#34;:1.1}] } } These values are available via the Prometheus endpoint.\nThe model can also be easily deployed in a container platform, for instance, OpenShift. Assuming you are logged to a cluster and your image is a registry accessible by OpenShift, you can simply deploy it using:\n$ oc new-app $NAME:$TAG I hope this was useful to you. Happy coding!\nhttps://github.com/SeldonIO/seldon-core\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nhttps://kubernetes.io/\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nhttps://www.openshift.com/\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nhttps://scikit-learn.org/stable/\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nAs of the time of writing.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nOther encoders are available in scikit-learn. I recommend you experiment with some of them.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nYou can use any file name, as long as it\u0026rsquo;s consistent with .s2i/environment, which we\u0026rsquo;ll look at soon.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nMore information can be found here.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"/serving-models-with-seldon.html","tags":null,"title":"Serving models with Seldon"},{"categories":null,"contents":"According to Bash\u0026rsquo;s man:\n/bin/bash The bash executable /etc/profile The system-wide initialization file, executed for login shells ~/.bash_profile The personal initialization file, executed for login shells ~/.bashrc The individual per-interactive-shell startup file ~/.bash_logout The individual login shell cleanup file, executed when a login shell exits ~/.inputrc Individual readline initialization file With zsh, .zshrc is always read for an interactive shell, whether it\u0026rsquo;s a login one or not.\n","permalink":"/shell-configurations.html","tags":null,"title":"Shell configurations"},{"categories":null,"contents":"TTYReset cursorSometimes, especially when using ANSI escape code heavy applications, your terminal state might get mangled. If that\u0026rsquo;s the case, it\u0026rsquo;s just a matter of performing a VT320 escape sequence to reset the state. For instance, in zsh, using the unhide command:\necho -en \u0026#34;\\e[?25h\u0026#34; ResolutionGet screen resolutionLinuxOn linux you can get the x and y resolutions, respectively:\n$ xrandr --current | grep \u0026#39;*\u0026#39; | uniq | awk \u0026#39;{print $1}\u0026#39; | cut -d \u0026#39;x\u0026#39; -f1 $ xrandr --current | grep \u0026#39;*\u0026#39; | uniq | awk \u0026#39;{print $1}\u0026#39; | cut -d \u0026#39;x\u0026#39; -f2 ","permalink":"/shell-tricks.html","tags":null,"title":"Shell tricks"},{"categories":null,"contents":"AssetsThis site\u0026rsquo;s CSS size is 84.1Kb. This, however, includes the following dependencies:\nLaTeX processor (MathJax1) custom monospaced font (Jetbrains Mono) custom serif font (Vollkorn) The base CSS is heavily inspired by 58 bytes of css to look great nearly everywhere.\nThe CSS evolution (size, number of rules, etc) can be tracked over at Project Wallace.\nThe site is generated from a set of org-mode files using Emacs, which mainly performs the following tasks:\nGather backlinks to each page Build the client-side search index convert the org-mode files to HTML2 NavigationSearchThe site is searchable from here. The search page also allows for query string searches using the q keyword, for instance:\n/search.html?q=statistics This allows you to add a custom search engine to most modern browsers, such as Firefox or Chrome.\nSearch is done 100% client-side, so there\u0026rsquo;s absolutely no information collected regarding your search queries.\nHighlighting of terms is also possible just adding the query parameter ?h=... to any page. For instance, to highlight the term privacy on this page, simply go to\n/site-details.html?h=privacy or click here.\nDeep linkingThis site implements \u0026ldquo;deep links\u0026rdquo;. This means that any section of text selected will generate a new URL which links directly to it. This link can be bookmarked or shared. Please keep in mind that this intented for transitory linking, since long-term structure of the page is not guaranteed.\nKeyword focusSome {{{focus(keywords,page)}}} are clickable and will highlight all occurences of a certain term throughtout the page. The main use case is, for instance, code-heavy pages where a certain variable or term might benefit from standing out to understand the concepts more easily.\nPrivacyNo cookies are used on this site.\nAll site traffic statistics are captured using GoatCounter. Goatcounter is an open-source, privacy-friendly analytics site, which doesn\u0026rsquo;t use cookies and collects minimal information, just enough to produce a few useful summaries. In line with the desire for total transparency for all visitors, I\u0026rsquo;ve made the realtime stats dashboard for this site public and available at https://ruivieira-dev.goatcounter.com/. You can see for yourself which data is collected.\nPlease let me know if you have any concerns about this site\u0026rsquo;s privacy policy by dropping me a message at @ruivieira@mastodon.technology.\nAs mentioned previously, search is done 100% client-side, so no information is collected about your search queries.\nThe privacy record of this site can be verified independently by using Blacklight.\nJavascriptIf you want to disable Javascript for this site, please do! It won\u0026rsquo;t affect any of the main content. All pages will work the same with or without Javascript, except the following:\nThe link graph page The earch page Deep linking Keyword focus All of these are simply navigation helpers and the content does not depend on this functionality.\nSome sections are progressively enhanced by Javascript, but will work without it. As an example, many dates are represented by a time tag such as\n\u0026lt;time datetime=\u0026#34;2021-11-28 15:21:54 +0000 GMT\u0026#34; itemprop=\u0026#34;datePublished\u0026#34;\u0026gt;2021-11-28\u0026lt;/time\u0026gt; These are converted to a relative date by Javascript resulting in\n2021-11-28 For instance, depending on wether you have Javascript enabled or not, the above will either display \u0026ldquo;x days ago\u0026rdquo; or \u0026ldquo;2021-11-28\u0026rdquo;.\nText modeApart from the obvious lack of images, the vast majority of this site will also work with a text-based browser (such as lynx).\nEven the code examples are quite readable (for instance, pandas dataframes are properly rendered as text tables).\nGive it a go by installing lynx and running lynx https://ruivieira.dev.\nHere\u0026rsquo;s how this page looks like using lynx:\nTo see other \u0026ldquo;supported\u0026rdquo; browsers (such as Internet Explorer 6, NCA Mosaic 2 and Kristall) see the Brutalist Web Design page.\nKeeping up to dateCommits logThe commits log can be view in here or by clicking the hash at the top of most pages.\nMastodon botIf you want to keep up to date and be notified when new content is added to this site, at the moment the best way is to follow the Mesozoic Mastodon bot.\nThis bot is part of a Git pre-push hook and will \u0026ldquo;toot\u0026rdquo; whenever changes to the site\u0026rsquo;s source are made.\nhttps://www.mathjax.org/\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nPreviously this site was written as plain HTML+CSS, this is touched upon at \u0026ldquo;(Semi) handcrafted RSS\u0026rdquo;.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"/site-details.html","tags":null,"title":"Site details"},{"categories":null,"contents":"IntroductionSMILE is a Machine Learning library for Scala and Java.\nIt implements:\nClassification ","permalink":"/smile-library.html","tags":null,"title":"SMILE library"},{"categories":null,"contents":"Spearman rank correlationThe Spearman correlation coefficient (or Spearman\u0026rsquo;s $\\rho$) measures rank correlation between two variables. It is used to detect the existence of monotonic relationship between variables.\nAssuming monotonicity, the Spearman\u0026rsquo;s $\\rho$ will take values between $-1$ and $1$, representing completely opposite or identical ranks, respectively1, or, in other words, a negative monotonic relationship or a positive one.\nDue to the dependance on ranks, the Spearman\u0026rsquo;s $\\rho$ is used for ordinal value, although discrete and continous values are possible.\nIf we consider a dataset of size $n$, and $X_i, Y_i$ as the scores, we can then calculate the ranks as $\\operatorname{R}({X_i}), \\operatorname{R}({Y_i})$, and $\\rho$ as\n$$ r_s = \\rho_{\\operatorname{R}(X),\\operatorname{R}(Y)} = \\frac{\\operatorname{cov}(\\operatorname{R}(X), \\operatorname{R}(Y))} {\\sigma_{\\operatorname{R}(X)} \\sigma_{\\operatorname{R}(Y)}}, $$\nHere $\\rho$ is the Pearson correlation coefficient, but applied to the rank variables, $\\operatorname{cov}(\\operatorname{R}(X), \\operatorname{R}(Y))cov(R(X),R(Y))$ is the covariance of the rank variables, $\\sigma_{\\operatorname{R}(X)}$ and $\\sigma_{\\operatorname{R}(Y)}$ are the standard deviations of the rank variables.\nIf all the ranks are distinct integers, the simplified form can be applied\n$$ r_s = 1 - \\frac{6 \\sum d_i^2}{n(n^2 - 1)}, $$\nwhere $d_i = \\operatorname{R}(X_i) - \\operatorname{R}(Y_i)$ is the difference between the two ranks of each observation, $n$ is the number of observations.\nThe Spearman rank correlation test can be used when\nThe variables are quantitative or ordinal Variables do not meet normality assumption Variables have a monotonic relationship ExampleFor the example we will use the scipy implementation and the iris dataset.\nfrom sklearn import datasets import pandas as pd import scipy # create ordinal categories def categorize_petal_len(x): if x \u0026lt;= 1.6: return \u0026#39;LOW\u0026#39; elif x \u0026lt;= 5.1: return \u0026#39;AVERAGE\u0026#39; else: return \u0026#39;HIGH\u0026#39; iris = datasets.load_iris() df = iris |\u0026gt; .data |\u0026gt; pd.DataFrame df[\u0026#34;class\u0026#34;] = iris.target df.columns = [\u0026#39;sepal_len\u0026#39;, \u0026#39;sepal_wid\u0026#39;, \u0026#39;petal_len\u0026#39;, \u0026#39;petal_wid\u0026#39;, \u0026#39;class\u0026#39;] df.dropna(how=\u0026#34;all\u0026#34;, inplace=True) df[\u0026#39;petal_len_cats\u0026#39;] = df[\u0026#39;petal_len\u0026#39;].apply(categorize_petal_len) df sepal_len sepal_wid petal_len petal_wid class petal_len_cats 0 5.1 3.5 1.4 0.2 0 LOW 1 4.9 3.0 1.4 0.2 0 LOW 2 4.7 3.2 1.3 0.2 0 LOW 3 4.6 3.1 1.5 0.2 0 LOW 4 5.0 3.6 1.4 0.2 0 LOW ... ... ... ... ... ... ... 145 6.7 3.0 5.2 2.3 2 HIGH 146 6.3 2.5 5.0 1.9 2 AVERAGE 147 6.5 3.0 5.2 2.0 2 HIGH 148 6.2 3.4 5.4 2.3 2 HIGH 149 5.9 3.0 5.1 1.8 2 AVERAGE 150 rows  6 columns\nfrom scipy.stats import spearmanr sp = (X, Y) -\u0026gt; spearmanr(df[X], df[Y]) correlation, p_value = sp(\u0026#39;sepal_len\u0026#39;, \u0026#39;sepal_wid\u0026#39;) f\u0026#34;correlation = {correlation}, p-value = {p_value}\u0026#34; |\u0026gt; print correlation = -0.166777658283235, p-value = 0.04136799424884587 Assuming no repeated ranks.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"/spearman-correlation.html","tags":null,"title":"Spearman correlation"},{"categories":null,"contents":"ConfigurationConfig fileTo add a known server to the config file, use the following syntax:\nHost mymachine HostName 127.0.0.1 User root Port 7654 There is no method to specify or provide on the command line the password in a non-interactive manner for ssh authentication using a OpenSSH built-in mechanism.\nServersAlternativeAn alternative SSH server implementation is tinyssh1\nTroubleshootingFailed signingIf you get a error similar to:\nsign_and_send_pubkey: signing failed for RSA \u0026#34;/home/foo/.ssh/id_rsa\u0026#34; from agent: agent refused operation foo@example.com: Permission denied (publickey). This might be related to permission errors. The fix is:\nchmod 700 ~/.ssh chmod 600 ~/.ssh/* SSH agentIf adding SSH identities with ssh-add and it fails with\nCould not open a connection to your authentication agent. Try evaluation the ssh-agent in the shell with:\n$ eval \u0026#34;$(ssh-agent)\u0026#34; https://tinyssh.org/\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"/ssh.html","tags":null,"title":"SSH"},{"categories":null,"contents":"Tests for statistical dependence.\nSpearman correlation ","permalink":"/statistical-dependence.html","tags":null,"title":"Statistical dependence"},{"categories":null,"contents":"Useful algorithms:\nConformalised density and distance-based anomaly detection in time-series data1 elford algorithm Anomaly detection in streams with extreme value theory Robust random cut forest based anomaly detection on streams2 Time-series anomaly detection service at Microsoft3 Half-Space Trees Experimental dataLet\u0026rsquo;s assume the following sequence of observations that we will use with a variety of algorithms. The data is labelled with label with 0 for normal observations and 1 for anomalies.\nimport pandas as pd df = pd.read_csv(\u0026#34;../../data/streamad/uniDS.csv\u0026#34;) The Welford algorithmThe Welford\u0026rsquo;s method is an online algorithm (idescribe single-pass) to calculate running variance and standard deviation. It is formulated from the difference between the squared difference sums of $N$ and $N-1$ observations.\nA basic implementation of the Welford\u0026rsquo;s method can be:\nimport math class Welford(object): def __init__(self): self.k = 0 self.M = 0 self.S = 0 def update(self,x): if x is None: return self.k += 1 newM = self.M + (x - self.M)*1./self.k newS = self.S + (x - self.M)*(x - newM) self.M, self.S = newM, newS @property def mean(self): return self.M @property def meanfull(self): return self.mean, self.std/math.sqrt(self.k) @property def std(self): if self.k==1: return 0 return math.sqrt(self.S/(self.k-1)) def __repr__(self): return \u0026#34;\u0026lt;Welford: {} +- {}\u0026gt;\u0026#34;.format(self.mean, self.std) Applying Welford\u0026rsquo;s algorothm to our data we have:\nwelford = Welford() means = [] for value in df.value.to_list(): welford.update(value) means.append(welford.mean) Anomaly detection in streams with extreme value theoryA more in-detail page is available at Streaming anomaly detection with Extreme Value Theory.\nSPOTAn example with streamad\u0026rsquo;s SPOT4 detector. This is available in the streamad.model.SpotDetector package.\nfrom streamad.util import StreamGenerator, UnivariateDS, plot from streamad.util.dataset import CustomDS from streamad.model import SpotDetector ds = CustomDS(\u0026#34;../../data/streamad/uniDS.csv\u0026#34;) stream = StreamGenerator(ds.data) model = SpotDetector() scores = [] for x in stream.iter_item(): score = model.fit_score(x) if score: scores.append(score) else: scores.append(0) data, label, date, features = ds.data, ds.label, ds.date, ds.features Half-Space TreesSee Half-Space Trees.\nhttps://arxiv.org/abs/1608.04585\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nhttp://proceedings.mlr.press/v48/guha16.pdf\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nhttps://arxiv.org/abs/1906.03821\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nhttps://dl.acm.org/doi/10.1145/3097983.3098144\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"/streaming-anomaly-detection.html","tags":null,"title":"Streaming anomaly detection"},{"categories":null,"contents":"Situations where streaming statistics are useful:\nUnknown number of observations Online streaming data Dataset too big for local processing For the remainder, let\u0026rsquo;s consider a set of observations $y_i$, weights $w_i$, such that\n$$ y_1,\\dots,y_i \\in \\mathbb{R} \\ w_1,\\dots,w_i\\quad w_i \\geq 0 $$\nMean and variances A naive approach to calculating a weighted streaming mean, $\\widehat{\\mu}$ and unbiased streaming variance, , would be to calculate:\n$$ \\begin{align*} \\widehat{\\mu}\u0026amp;=\\frac{T^{(n)}}{S^{(n)}} \\ \\widehat{\\mathbb{V}}\u0026amp;=\\frac{n}{(n-1)S^{(n)}}\\left(U^{(n)}-S^{(n)}\\widehat{\\mu}^2\\right) \\end{align*} $$\nwhere\n$$ \\begin{align*} S^{(i+1)}\u0026amp;=S^{(i)}+w_i\\ T^{(i+1)}\u0026amp;=T^{(i)}+w_i y_i\\ U^{(i+1)}\u0026amp;=U^{(i)}+w_i y^2_i \\end{align*} $$\nThis calculation however does not hold for large $n$ values.\nAn alternative calculation was suggested by West1, where we calculate:\n$$ \\begin{align*} \\widehat{\\mu}\u0026amp;=\\frac{\\sum_i w_i y_i}{\\sum_i w_i} \\ \\widehat{\\mathbb{V}}\u0026amp;=\\frac{\\sum_i w_i(X_i-\\mu)^2}{\\frac{n-1}{n}\\sum_i w_i} \\end{align*} $$\nLet\u0026rsquo;s look at an example in Python.\nfrom plotutils import * import numpy as np mu = 10.0 sigma = 20.0 N = 100_000 Y = np.random.normal(loc=mu, scale=sigma, size=N) import matplotlib.pyplot as plt import matplotlib matplotlib.rc(\u0026#39;figure\u0026#39;, figsize=(12, 6)) plt.hist(Y, bins=50,color=\u0026#34;lightpink\u0026#34;) plt.show() As expected, the mean and variance when calculated in \u0026ldquo;batch\u0026rdquo; mode should be equivalent to the original values $\\mu$ and $\\sigma$.\nprint(f\u0026#34;mean: {np.mean(Y)}\u0026#34;) print(f\u0026#34;std: {np.std(Y)}\u0026#34;) mean: 10.040100563607174 std: 19.946298541957912 class StreamingStatistcs: def __init__(self): self.sum = 0.0 self.mean = 0.0 self.t = 0 self.n = 0 self.var = None def calculate(self, y, w): q = y - self.mean tmp_sum = self.sum + w r = q*w / tmp_sum self.mean += r self.t += q*r*self.sum self.sum = tmp_sum self.n += 1 if (self.sum == 0.0 or self.n \u0026lt; 2): self.var = 0 else: self.var = (self.t*self.n)/(self.sum*(self.n-1)) return (self.mean, self.var) means = [] vars = [] st = StreamingStatistcs() for y in Y: m, v = st.calculate(y, 1.0) means.append(m) vars.append(np.sqrt(v)) fig, (ax1, ax2) = plt.subplots(1, 2) fig.suptitle(\u0026#39;Streaming statistics for $N=10^5$ observations\u0026#39;) ax1.plot(means) ax1.hlines(y=10, xmin=0, xmax=N, colors=\u0026#34;red\u0026#34;) ax1.set_title(\u0026#34;Streaming mean\u0026#34;) ax2.plot(vars) ax2.hlines(y=20, xmin=0, xmax=N, colors=\u0026#34;red\u0026#34;) ax2.set_title(\u0026#34;Streaming variance\u0026#34;) plt.show() cite:\u0026amp;west1979updating West, D. (1979). Updating mean and variance estimates: an improved method. Communications of the ACM, 22(9), 532\u0026ndash;535.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"/streaming-statistics.html","tags":null,"title":"Streaming statistics"},{"categories":null,"contents":"Software fettuccine, a Python library to manage code workflows (PyPi) Python Kubernetes utilities library, k8sutils (PyPi) TrustyAI Kubernetes Operator ","permalink":"/stuff-i-did-in-2023.html","tags":null,"title":"Stuff I did in 2023"},{"categories":null,"contents":"ServicesystemdTo enable the syncthing service at the user level on a systemd based OS (e.g. Fedora) use\nsystemctl --user enable --now syncthing.service The service might be lost after package or OS updates. To re-enable it, use:\nsystemctl --user daemon-reload systemctl --user restart syncthing.service ","permalink":"/syncthing.html","tags":null,"title":"Syncthing"},{"categories":null,"contents":"Generating synthetic dataSynthetic data will be used mainly for these scenarios:\nRegression Classification Here we will mainly look at the methods provided by scikit-learn to generate synthetic datasets. For more advanced methods, such as using the SDV library please check the SDV page. It support methods such as Gaussian copulas, CTGAN and CopulaGAN.\nRegression dataWhat does a regression consist of?\nFor this section we will mainly use scikit-learn\u0026rsquo;s make_regression method.\nFor reproducibility, we will set a random_state.\nWe will create a dataset using make_regression\u0026rsquo;s random linear regression model with input features $x=(f_1,f_2,f_3,f_4)$ and an output $y$.\nimport numpy as np import pandas as pd from sklearn.datasets import make_regression from scipy.stats import linregress N_FEATURES = 4 N_TARGETS = 1 N_SAMPLES = 100 dataset = make_regression( n_samples=N_SAMPLES, n_features=N_FEATURES, n_informative=2, n_targets=N_TARGETS, bias=0.0, effective_rank=None, tail_strength=0.5, noise=0.0, shuffle=True, coef=False, random_state=random_state, ) print(dataset[0][:10]) print(dataset[1][:10]) [[ 0.87305874 -1.63096187 0.52538404 -0.19035824] [ 1.00698671 0.79834941 -0.04057655 -0.31358605] [-0.61464273 1.65110321 0.75791487 -0.0039844 ] [-1.08536678 1.82337823 0.4612592 -1.72325306] [-1.67774847 -0.54401341 0.86347869 -0.30250463] [-0.02427254 0.75537599 -0.04644972 -0.85153564] [-0.48085576 0.82100952 -0.9390196 -0.25870492] [-0.66772841 -2.46244005 -0.19855095 -1.85756579] [-0.29810663 -0.02239635 0.25363492 -1.22688366] [ 1.48146924 0.38269965 -1.18208819 -1.31062148]] [ 20.00449025 -30.41054677 52.65371365 -119.26376184 33.78805456 -78.12189078 -88.41673748 -177.21674804 -90.13920313 -197.90799195] Let\u0026rsquo;s turn this dataset into a Pandas DataFrame:\ndf = pd.DataFrame(data=dataset[0], columns=[f\u0026#34;f{i+1}\u0026#34; for i in range(N_FEATURES)]) df[\u0026#34;y\u0026#34;] = dataset[1] df.head() f1 f2 f3 f4 y 0 0.873 -1.631 0.525 -0.190 20.004 1 1.007 0.798 -0.041 -0.314 -30.411 2 -0.615 1.651 0.758 -0.004 52.654 3 -1.085 1.823 0.461 -1.723 -119.264 4 -1.678 -0.544 0.863 -0.303 33.788 Let\u0026rsquo;s plot the data:\nChanging the Gaussian noise levelThe noise parameter in make_regression allows to adjust the scale of the data\u0026rsquo;s gaussian centered noise.\ndataset = make_regression( n_samples=N_SAMPLES, n_features=N_FEATURES, n_informative=2, n_targets=N_TARGETS, bias=0.0, effective_rank=None, tail_strength=0.5, noise=2.0, shuffle=True, coef=False, random_state=random_state, ) df = pd.DataFrame(data=dataset[0], columns=[f\u0026#34;f{i+1}\u0026#34; for i in range(N_FEATURES)]) df[\u0026#34;y\u0026#34;] = dataset[1] Visualising increasing noiseLet\u0026rsquo;s increase the noise by $10^i$, for $i=1, 2, 3$ and see what the data looks like.\ndf = pd.DataFrame(data=np.zeros((N_SAMPLES, 1))) def create_noisy_data(noise): return make_regression( n_samples=N_SAMPLES, n_features=1, n_informative=1, n_targets=1, bias=0.0, effective_rank=None, tail_strength=0.5, noise=noise, shuffle=True, coef=False, random_state=random_state, ) for i in range(3): data = create_noisy_data(10 ** i) df[f\u0026#34;f{i+1}\u0026#34;] = data[0] df[f\u0026#34;y{i+1}\u0026#34;] = data[1] Classification dataTo generate data for classification we will use the make_classification method.\nfrom sklearn.datasets import make_classification N = 4 data = make_classification( n_samples=N_SAMPLES, n_features=N, n_informative=4, n_redundant=0, n_repeated=0, n_classes=2, n_clusters_per_class=1, weights=None, flip_y=0.01, class_sep=1.0, hypercube=True, shift=0.0, scale=1.0, shuffle=True, random_state=random_state, ) df = pd.DataFrame(data[0], columns=[f\u0026#34;f{i+1}\u0026#34; for i in range(N)]) df[\u0026#34;y\u0026#34;] = data[1] df.head() f1 f2 f3 f4 y 0 -3.216 -0.416 -1.295 -1.882 0 1 -1.426 -1.257 -1.734 -1.804 0 2 2.798 -3.010 -1.085 -3.134 1 3 0.633 2.502 -1.553 1.625 1 4 1.494 0.912 -1.887 -1.457 1 Cluster separationAccording to the docs1, class_sep is the factor multiplying the hypercube size.\nLarger values spread out the clusters/classes and make the classification task easier.\nN_FEATURES = 4 data = make_classification( n_samples=N_SAMPLES, n_features=N_FEATURES, n_informative=4, n_redundant=0, n_repeated=0, n_classes=2, n_clusters_per_class=1, weights=None, flip_y=0.01, class_sep=3.0, hypercube=True, shift=0.0, scale=1.0, shuffle=True, random_state=None, ) df = pd.DataFrame(data[0], columns=[f\u0026#34;f{i+1}\u0026#34; for i in range(N_FEATURES)]) df[\u0026#34;y\u0026#34;] = data[1] We can make the cluster separability more difficult, by decreasing the value of class_sep.\nN_FEATURES = 4 data = make_classification( n_samples=N_SAMPLES, n_features=N_FEATURES, n_informative=4, n_redundant=0, n_repeated=0, n_classes=2, n_clusters_per_class=1, weights=None, flip_y=0.01, class_sep=0.5, hypercube=True, shift=0.0, scale=1.0, shuffle=True, random_state=None, ) df = pd.DataFrame(data[0], columns=[f\u0026#34;f{i+1}\u0026#34; for i in range(N_FEATURES)]) df[\u0026#34;y\u0026#34;] = data[1] Noise levelAccording to the documentation2, flip_y is the fraction of samples whose class is assigned randomly.\nLarger values introduce noise in the labels and make the classification task harder.\nN_FEATURES = 4 for i in range(6): data = make_classification( n_samples=N_SAMPLES, n_features=N_FEATURES, n_informative=4, n_redundant=0, n_repeated=0, n_classes=2, n_clusters_per_class=1, weights=None, flip_y=0.1 * i, class_sep=1.0, hypercube=True, shift=0.0, scale=1.0, shuffle=False, random_state=random_state, ) df = pd.DataFrame(data[0], columns=[f\u0026#34;f{i+1}\u0026#34; for i in range(N_FEATURES)]) df[\u0026#34;y\u0026#34;] = data[1] plt.subplot(2, 3, i + 1) plt.title(f\u0026#34;$flip_y={round(0.1*i,2)}$\u0026#34;) plt.scatter( df[\u0026#34;f1\u0026#34;], df[\u0026#34;f2\u0026#34;], s=50, c=df[\u0026#34;y\u0026#34;], cmap=\u0026#39;gray\u0026#39;, edgecolor=\u0026#39;gray\u0026#39; ) plt.xlabel(f\u0026#34;${var1[0]}_{var1[1]}$\u0026#34;) plt.ylabel(f\u0026#34;${var2[0]}_{var2[1]}$\u0026#34;) ax = plt.gca() ax.set_facecolor((247.0/255.0, 239.0/255.0, 217.0/255.0)) plt.tight_layout() plt.tight_layout(pad=3.0) df = pd.DataFrame(data=np.zeros((N_SAMPLES, 1))) for i in range(3): data = make_classification( n_samples=N_SAMPLES, n_features=2, n_informative=2, n_redundant=0, n_repeated=0, n_classes=2, n_clusters_per_class=1, weights=None, flip_y=0, class_sep=i + 0.5, hypercube=True, shift=0.0, scale=1.0, shuffle=False, random_state=random_state, ) df[f\u0026#34;f{i+1}1\u0026#34;] = data[0][:, 0] df[f\u0026#34;f{i+1}2\u0026#34;] = data[0][:, 1] df[f\u0026#34;t{i+1}\u0026#34;] = data[1] It is noteworthy that many paremeters in scikit-learn for synthetic data generation allow inputs per feature or cluster. To do so, we simple pass the parameter value as an array. For instance, to\nN = 4 data = make_classification( n_samples=N_SAMPLES, n_features=N, n_informative=4, n_redundant=0, n_repeated=0, n_classes=2, n_clusters_per_class=1, weights=None, flip_y=0.01, class_sep=1.0, hypercube=True, shift=0.0, scale=1.0, shuffle=True, random_state=random_state, ) df = pd.DataFrame(data[0], columns=[f\u0026#34;f{i+1}\u0026#34; for i in range(N)]) df[\u0026#34;y\u0026#34;] = data[1] Separabilityfrom sklearn.datasets import make_blobs N_FEATURE = 4 data = make_blobs( n_samples=60, n_features=N_FEATURE, centers=3, cluster_std=1.0, center_box=(-5.0, 5.0), shuffle=True, random_state=None, ) df = pd.DataFrame(data[0], columns=[f\u0026#34;f{i+1}\u0026#34; for i in range(N_FEATURE)]) df[\u0026#34;y\u0026#34;] = data[1] To make a cluster more separable we can change cluster_std.\ndata = make_blobs( n_samples=60, n_features=N_FEATURES, centers=3, cluster_std=0.3, center_box=(-5.0, 5.0), shuffle=True, random_state=None, ) df = pd.DataFrame(data[0], columns=[f\u0026#34;f{i+1}\u0026#34; for i in range(N_FEATURES)]) df[\u0026#34;y\u0026#34;] = data[1] By decreasing cluster_std we make them less separable.\ndata = make_blobs( n_samples=60, n_features=N_FEATURES, centers=3, cluster_std=2.5, center_box=(-5.0, 5.0), shuffle=True, random_state=None, ) df = pd.DataFrame(data[0], columns=[f\u0026#34;f{i+1}\u0026#34; for i in range(N_FEATURES)]) df[\u0026#34;y\u0026#34;] = data[1] Anisotropic datadata = make_blobs(n_samples=50, n_features=2, centers=3, cluster_std=1.5) transformation = [0.5, -0.5], [-0.4, 0.8]() data_0 = np.dot(data[0], transformation) df = pd.DataFrame(data_0, columns=[f\u0026#34;f{i}\u0026#34; for i in range(1, 3)]) df[\u0026#34;y\u0026#34;] = data[1] Concentric clustersSometimes we might be interested in creating a non-separable cluster. The simples way is to create concentric clusters with the make_circles method.\nfrom sklearn.datasets import make_circles data = make_circles( n_samples=N_SAMPLES, shuffle=True, noise=None, random_state=random_state, factor=0.6 ) df = pd.DataFrame(data[0], columns=[f\u0026#34;f{i+1}\u0026#34; for i in range(2)]) df[\u0026#34;y\u0026#34;] = data[1] Adding noiseThe noise parameter allows to create a concentric noisy dataset.\ndata = make_circles( n_samples=N_SAMPLES, shuffle=True, noise=0.15, random_state=random_state, factor=0.6 ) df = pd.DataFrame(data[0], columns=[f\u0026#34;f{i+1}\u0026#34; for i in range(2)]) df[\u0026#34;y\u0026#34;] = data[1] Moon clustersA shape that can be useful to other methods (such as Counterfactuals, for instance) is the one generated by the make_moons method.\nfrom sklearn.datasets import make_moons data = make_moons( n_samples=N_SAMPLES, shuffle=True, noise=None, random_state=random_state ) df = pd.DataFrame(data[0], columns=[f\u0026#34;f{i+1}\u0026#34; for i in range(2)]) df[\u0026#34;y\u0026#34;] = data[1] Adding noiseAs usual, the noise parameter allows to control the noise.\ndata = make_moons( n_samples=N_SAMPLES, shuffle=True, noise=0.1, random_state=random_state ) df = pd.DataFrame(data[0], columns=[f\u0026#34;f{i+1}\u0026#34; for i in range(2)]) df[\u0026#34;y\u0026#34;] = data[1] Time-series dataRandom walkSee [Random walk].\nSimple periodicNo trendGenerate a simple HMM with a sine state and gaussian observations:\nimport numpy as np def generate_sine(period, n): cycles = n / period length = np.pi * 2 * cycles return np.sin(np.arange(0, length, length / n)) We will now get a set of $n=1000$ observations with a $p=10$ period\nN=1000 data = generate_sine(10, N) * np.random.uniform(10, size=N) TrendN=1000 data = (generate_sine(10, N) * np.random.uniform(10, size=N)) + np.arange(N)/200.0 Univariate dataUsing the streamad library:\nfrom streamad.util.dataset import CustomDS from streamad.util import StreamGenerator, plot ds = CustomDS(\u0026#34;../../data/streamad/uniDS.csv\u0026#34;) stream = StreamGenerator(ds.data) https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_classification.html\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nhttps://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_classification.html\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"/synthetic-data-generation.html","tags":null,"title":"Synthetic Data Generation"},{"categories":null,"contents":" Synthetic data with SDV and Gaussian copulas Synthetic data with SDV and CTGAN Synthetic data with SDV and CopulaGAN ","permalink":"/synthetic-data-generation-with-sdv.html","tags":null,"title":"Synthetic Data Generation with SDV"},{"categories":null,"contents":" import pandas as pd import warnings warnings.filterwarnings(\u0026#39;ignore\u0026#39;) data = pd.read_csv(\u0026#34;data/svm-hyperparameters-train-features.csv\u0026#34;) data.head() Pclass Sex Age SibSp Parch Fare 0 3 1 22.0 1 0 7.2500 1 1 0 38.0 1 0 71.2833 2 3 0 26.0 0 0 7.9250 3 1 0 35.0 1 0 53.1000 4 3 1 35.0 0 0 8.0500 data.describe(include=\u0026#39;all\u0026#39;) Pclass Sex Age SibSp Parch Fare count 891.000000 891.000000 891.000000 891.000000 891.000000 891.000000 mean 2.308642 0.647587 29.758889 0.523008 0.381594 32.204208 std 0.836071 0.477990 13.002570 1.102743 0.806057 49.693429 min 1.000000 0.000000 0.420000 0.000000 0.000000 0.000000 25% 2.000000 0.000000 22.000000 0.000000 0.000000 7.910400 50% 3.000000 1.000000 30.000000 0.000000 0.000000 14.454200 75% 3.000000 1.000000 35.000000 1.000000 0.000000 31.000000 max 3.000000 1.000000 80.000000 8.000000 6.000000 512.329200 from sdv.tabular import CopulaGAN model = CopulaGAN() model.fit(data) new_data = model.sample(200) new_data.head() Pclass Sex Age SibSp Parch Fare 0 3 0 32.02 0 0 18.2784 1 3 1 20.90 5 0 109.6910 2 3 0 40.80 0 1 179.5139 3 3 0 33.13 1 0 17.3447 4 1 1 20.62 0 0 9.6040 new_data.describe(include=\u0026#39;all\u0026#39;) Pclass Sex Age SibSp Parch Fare count 200.000000 200.000000 200.000000 200.000000 200.000000 200.000000 mean 2.490000 0.450000 31.404700 0.405000 0.115000 46.756109 std 0.820528 0.498742 12.297721 0.919348 0.461345 52.828023 min 1.000000 0.000000 0.450000 0.000000 0.000000 3.034900 25% 2.000000 0.000000 24.712500 0.000000 0.000000 11.768475 50% 3.000000 0.000000 30.295000 0.000000 0.000000 27.859150 75% 3.000000 1.000000 34.307500 1.000000 0.000000 58.361350 max 3.000000 1.000000 73.340000 7.000000 3.000000 381.859600 from sdv.evaluation import evaluate evaluate(new_data, data) 0.539229052965023 model = CopulaGAN( field_transformers={ \u0026#39;Pclass\u0026#39;: \u0026#39;categorical\u0026#39;, \u0026#39;Sex\u0026#39;: \u0026#39;categorical\u0026#39;, \u0026#39;Age\u0026#39;: \u0026#39;float\u0026#39;, \u0026#39;SibSp\u0026#39;: \u0026#39;boolean\u0026#39;, \u0026#39;Parch\u0026#39;: \u0026#39;integer\u0026#39;, \u0026#39;Fare\u0026#39;: \u0026#39;float\u0026#39; }, field_distributions={ \u0026#39;Fare\u0026#39;: \u0026#39;truncated_gaussian\u0026#39; } ) model.fit(data) new_data = model.sample(200) new_data.head() Pclass Sex Age SibSp Parch Fare 0 1 0 1.07 1 0 13.8318 1 3 0 15.58 0 0 46.6937 2 1 1 26.53 0 0 53.8841 3 1 0 29.58 0 0 5.2646 4 1 0 30.13 0 0 5.1415 new_data.describe(include=\u0026#39;all\u0026#39;) Pclass Sex Age SibSp Parch Fare count 200.000000 200.000000 200.000000 200.000000 200.000000 200.000000 mean 2.210000 0.410000 22.935600 0.550000 0.145000 49.155439 std 0.932873 0.493068 13.955182 0.498742 0.441531 42.733997 min 1.000000 0.000000 0.430000 0.000000 0.000000 4.749400 25% 1.000000 0.000000 13.427500 0.000000 0.000000 13.340950 50% 3.000000 0.000000 25.875000 1.000000 0.000000 34.373700 75% 3.000000 1.000000 30.225000 1.000000 0.000000 75.206275 max 3.000000 1.000000 71.340000 1.000000 3.000000 220.761200 evaluate(new_data, data) 0.4983713994365315 ","permalink":"/synthetic-data-with-svd-and-copulagan.html","tags":null,"title":"Synthetic data with SDV and CopulaGAN"},{"categories":null,"contents":" Synthetic data with SDV and CTGANimport pandas as pd import warnings warnings.filterwarnings(\u0026#39;ignore\u0026#39;) data = pd.read_csv(\u0026#34;data/svm-hyperparameters-train-features.csv\u0026#34;) data.head() Pclass Sex Age SibSp Parch Fare 0 3 1 22.0 1 0 7.2500 1 1 0 38.0 1 0 71.2833 2 3 0 26.0 0 0 7.9250 3 1 0 35.0 1 0 53.1000 4 3 1 35.0 0 0 8.0500 data.describe(include=\u0026#39;all\u0026#39;) Pclass Sex Age SibSp Parch Fare count 891.000000 891.000000 891.000000 891.000000 891.000000 891.000000 mean 2.308642 0.647587 29.758889 0.523008 0.381594 32.204208 std 0.836071 0.477990 13.002570 1.102743 0.806057 49.693429 min 1.000000 0.000000 0.420000 0.000000 0.000000 0.000000 25% 2.000000 0.000000 22.000000 0.000000 0.000000 7.910400 50% 3.000000 1.000000 30.000000 0.000000 0.000000 14.454200 75% 3.000000 1.000000 35.000000 1.000000 0.000000 31.000000 max 3.000000 1.000000 80.000000 8.000000 6.000000 512.329200 from sdv.tabular import CTGAN model = CTGAN() model.fit(data) new_data = model.sample(200) new_data.head() Pclass Sex Age SibSp Parch Fare 0 3 1 31.40 1 1 24.7142 1 3 1 63.44 1 1 0.6418 2 3 0 32.73 0 0 2.8117 3 1 0 27.53 0 0 40.4747 4 2 1 46.31 2 1 104.2955 new_data.describe(include=\u0026#39;all\u0026#39;) Pclass Sex Age SibSp Parch Fare count 200.000000 200.000000 200.000000 200.000000 200.000000 200.000000 mean 2.605000 0.535000 37.052600 0.740000 0.450000 32.947807 std 0.715215 0.500025 15.763543 1.212332 0.692748 56.156945 min 1.000000 0.000000 2.870000 0.000000 0.000000 0.000000 25% 2.000000 0.000000 27.217500 0.000000 0.000000 12.590775 50% 3.000000 1.000000 33.100000 0.000000 0.000000 18.435050 75% 3.000000 1.000000 46.692500 1.000000 1.000000 24.083125 max 3.000000 1.000000 80.000000 6.000000 2.000000 380.969200 from sdv.evaluation import evaluate evaluate(new_data, data) 0.5513349938501996 model = CTGAN( epochs=500, batch_size=100, generator_dim=(256, 256, 256), discriminator_dim=(256, 256, 256) ) model.fit(data) new_data = model.sample(200) new_data.head() Pclass Sex Age SibSp Parch Fare 0 3 1 30.69 0 0 13.5556 1 2 0 0.42 1 0 33.1753 2 3 1 30.86 0 0 34.4404 3 1 1 11.07 0 0 29.0068 4 2 0 54.40 0 0 36.6160 new_data.describe(include=\u0026#39;all\u0026#39;) Pclass Sex Age SibSp Parch Fare count 200.000000 200.000000 200.000000 200.000000 200.000000 200.000000 mean 2.350000 0.540000 36.034900 0.365000 0.395000 34.128625 std 0.699964 0.499648 16.254839 0.809352 0.762813 42.191675 min 1.000000 0.000000 0.420000 0.000000 0.000000 2.339800 25% 2.000000 0.000000 28.737500 0.000000 0.000000 10.917300 50% 2.000000 1.000000 31.645000 0.000000 0.000000 19.874400 75% 3.000000 1.000000 49.312500 1.000000 0.000000 35.050750 max 3.000000 1.000000 80.000000 5.000000 2.000000 269.388000 evaluate(new_data, data) 0.6065965175235917 ","permalink":"/synthetic-data-with-sdv-and-ctgan.html","tags":null,"title":"Synthetic data with SDV and CTGAN"},{"categories":null,"contents":" import pandas as pd data = pd.read_csv(\u0026#34;data/svm-hyperparameters-train-features.csv\u0026#34;) data.head() Pclass Sex Age SibSp Parch Fare 0 3 1 22.000 1 0 7.250 1 1 0 38.000 1 0 71.283 2 3 0 26.000 0 0 7.925 3 1 0 35.000 1 0 53.100 4 3 1 35.000 0 0 8.050 data.describe(include=\u0026#34;all\u0026#34;) Pclass Sex Age SibSp Parch Fare count 891.000 891.000 891.000 891.000 891.000 891.000 mean 2.309 0.648 29.759 0.523 0.382 32.204 std 0.836 0.478 13.003 1.103 0.806 49.693 min 1.000 0.000 0.420 0.000 0.000 0.000 25% 2.000 0.000 22.000 0.000 0.000 7.910 50% 3.000 1.000 30.000 0.000 0.000 14.454 75% 3.000 1.000 35.000 1.000 0.000 31.000 max 3.000 1.000 80.000 8.000 6.000 512.329 from sdv.tabular import GaussianCopula model = GaussianCopula() model.fit(data) N_SAMPLES = 1000 new_df = model.sample(N_SAMPLES) new_df.head() Pclass Sex Age SibSp Parch Fare 0 3 1 12.780 1 1 2.070 1 2 1 44.930 0 0 95.806 2 2 1 21.980 1 0 72.020 3 2 0 30.120 2 1 84.968 4 2 1 23.480 1 1 37.170 new_df.describe() Pclass Sex Age SibSp Parch Fare count 1000.000 1000.000 1000.000 1000.000 1000.000 1000.000 mean 2.127 0.595 30.168 0.972 0.671 46.234 std 0.697 0.491 13.224 0.786 0.666 35.294 min 1.000 0.000 0.620 0.000 0.000 0.180 25% 2.000 0.000 20.693 0.000 0.000 16.959 50% 2.000 1.000 29.795 1.000 1.000 38.102 75% 3.000 1.000 39.163 1.000 1.000 67.841 max 3.000 1.000 77.000 4.000 3.000 166.853 \u0026lt;ggplot: (338236528)\u0026gt; \u0026lt;ggplot: (338395612)\u0026gt; \u0026lt;ggplot: (338577035)\u0026gt; \u0026lt;ggplot: (338383996)\u0026gt; model = GaussianCopula( field_transformers={ \u0026#39;Pclass\u0026#39;: \u0026#39;categorical\u0026#39;, \u0026#39;Sex\u0026#39;: \u0026#39;categorical\u0026#39;, \u0026#39;Age\u0026#39;: \u0026#39;float\u0026#39;, \u0026#39;SibSp\u0026#39;: \u0026#39;boolean\u0026#39;, \u0026#39;Parch\u0026#39;: \u0026#39;integer\u0026#39;, \u0026#39;Fare\u0026#39;: \u0026#39;float\u0026#39; } ) model.fit(data) new_df = model.sample(N_SAMPLES) new_df.head() Pclass Sex Age SibSp Parch Fare 0 3 0 24.330 0 1 24.759 1 1 1 58.070 0 0 22.957 2 2 0 23.700 0 0 15.298 3 1 1 30.200 0 0 19.092 4 1 1 51.370 0 0 99.106 \u0026lt;ggplot: (338395206)\u0026gt; \u0026lt;ggplot: (338261950)\u0026gt; \u0026lt;ggplot: (338558748)\u0026gt; \u0026lt;ggplot: (338314674)\u0026gt; data.Fare.describe() count 891.000 mean 32.204 std 49.693 min 0.000 25% 7.910 50% 14.454 75% 31.000 max 512.329 Name: Fare, dtype: float64 distributions = model.get_distributions() distributions {'Pclass.value': 'copulas.univariate.truncated_gaussian.TruncatedGaussian', 'Sex.value': 'copulas.univariate.truncated_gaussian.TruncatedGaussian', 'Age.value': 'copulas.univariate.truncated_gaussian.TruncatedGaussian', 'SibSp.value': 'copulas.univariate.truncated_gaussian.TruncatedGaussian', 'Parch.value': 'copulas.univariate.truncated_gaussian.TruncatedGaussian', 'Fare.value': 'copulas.univariate.truncated_gaussian.TruncatedGaussian'} model = GaussianCopula( field_transformers={ \u0026#39;Pclass\u0026#39;: \u0026#39;categorical\u0026#39;, \u0026#39;Sex\u0026#39;: \u0026#39;categorical\u0026#39;, \u0026#39;Age\u0026#39;: \u0026#39;float\u0026#39;, \u0026#39;SibSp\u0026#39;: \u0026#39;boolean\u0026#39;, \u0026#39;Parch\u0026#39;: \u0026#39;integer\u0026#39;, \u0026#39;Fare\u0026#39;: \u0026#39;float\u0026#39; }, field_distributions={ \u0026#39;Fare\u0026#39;: \u0026#39;truncated_gaussian\u0026#39; } ) model.fit(data) new_df = model.sample(N_SAMPLES) new_df.Fare.describe() count 1000.000 mean 47.928 std 36.876 min 0.044 25% 18.738 50% 40.028 75% 69.421 max 195.331 Name: Fare, dtype: float64 \u0026lt;ggplot: (338496543)\u0026gt; ","permalink":"/synthetic-data-with-svd-and-gaussian-copulas.html","tags":null,"title":"Synthetic data with SVD and Gaussian copulas"},{"categories":null,"contents":"(Based on Rasmus Bth\u0026rsquo;s post)\nA scaled $t$ distribution, with $\\mu$ mean, $s$ scale and $\\nu$ degrees of freedom, can be simulated\nfrom a mixture of Normals with $\\mu$ mean and precisions following a Gamma distribution:\n$$ \\begin{aligned} y \u0026amp;\\sim \\mathcal{N}\\left(\\mu,\\sigma\\right) \\\\ \\sigma^2 \u0026amp;\\sim \\mathcal{IG}\\left(\\frac{\\nu}{2},s^2\\frac{\\nu}{2}\\right) \\end{aligned} $$\nSince I\u0026rsquo;ve recently pickep up again the crystal-gsl in my spare time, I\u0026rsquo;ve decided to replicate the previously mentioned post using a Crystal one-liner.\nTo simulate 10,000 samples from $t_2\\left(0,3\\right)$ using the mixture, we can then write:\nsamples = (0..10000).map { |x| Normal.sample 0.0, 1.0/Math.sqrt(Gamma.sample 1.0, 9.0) } We can see the mixture distribution (histogram) converging nicely to the $(t_2(0,3)$ (red):\n","permalink":"/t-as-mixture-of-normals.html","tags":null,"title":"t as mixture of Normals"},{"categories":null,"contents":"Gradient-less counterfactuals.\n","permalink":"/thompson-sampling.html","tags":null,"title":"Thompson sampling"},{"categories":null,"contents":"NomenclatureConsider:\na set of contexts $\\mathcal{X}$ a set of actions $\\mathcal{A}$ and rewards in $\\mathbb{R}$ RationaleDefinitionFor each iteration $t$:\nA \u0026ldquo;player\u0026rdquo; obtains a context$x\\in \\mathcal{X}$ Plays an action$a\\in \\mathcal{A}$ Receives a reward$r\\in \\mathcal{R}$ This rewards is distributed according to the context and the resulting action The player\u0026rsquo;s goal is to execute actions that maximize the cumulative rewards. ImplementationThe implementation will focus on these concepts:\na likelihood function $P(r|\\theta ,a,x)$ a set $\\Theta$ of parameters $\\theta$ of the distribution of $r$ a prior distribution $P(\\theta )$ on these parameters past observations triplets $\\mathcal{D}={(x;a;r)}$ a posterior distribution $P(\\theta |{\\mathcal {D}})\\propto P({\\mathcal {D}}|\\theta )P(\\theta )$, where $P({\\mathcal {D}}|\\theta )$ is the likelihood function. Thompson sampling consists in playing the action $a^{\\ast }\\in {\\mathcal {A}}$ according to the probability that it maximizes the expected reward, i.e.action $a^{\\ast }$ is chosen with probability\n$$ \\int \\mathbb {I} \\left[\\mathbb {E} (r|a^{\\ast },x,\\theta )=\\max _{a\u0026rsquo;}\\mathbb {E} (r|a\u0026rsquo;,x,\\theta )\\right]P(\\theta |{\\mathcal {D}})d\\theta , $$\nwhere $\\mathbb {I}$ is the indicator function.\nIn practice, the rule is implemented by sampling. In each round, parameters $\\theta^\\ast$ are sampled from the posterior $P(\\theta |{\\mathcal {D}})$, and an action $a^{\\ast }$ chosen that maximizes ${\\mathbb {E}}[r|\\theta ^{\\ast },a^{\\ast },x]$, i.e. the expected reward given the sampled parameters, the action, and the current context. Conceptually, this means that the player instantiates their beliefs randomly in each round according to the posterior distribution, and then acts optimally according to them. In most practical applications, it is computationally onerous to maintain and sample from a posterior distribution over models. As such, Thompson sampling is often used in conjunction with approximate sampling techniques.\nExampleN_TRIALS = 2000 N_ARMS = 16 N_FEATURES = 5 BEST_ARMS = [3, 7, 9, 15] We now define a function to generate context vectors for all arms for each of the trial. We need:\nn_trials, number of trials ($N_T$) n_arms, number of arms per trial ($N_A$) n_features, number of feature per context vector ($N_f$) This function will return a matrix of size $N_{T} \\times N_{A} \\times N_{f}$\ndef make_design_matrix(n_trials: int, n_arms: int, n_features: int) -\u0026gt; np.ndarray: available_arms = np.arange(n_arms) X = np.array([[np.random.uniform(0, 1, size = n_features) for _ in np.arange(n_arms)] for _ in np.arange(n_trials)]) return X X = make_design_matrix(n_trials=N_TRIALS, n_arms=N_ARMS, n_features=N_FEATURES) This will have the shape\n![[Thompson sampling trials.excalidraw.svg]]\nThe following function will generate the true $\\Theta = {\\theta_1,\\dots,\\theta_n}$ for testing purposes. We provide:\n$N_A$, number of arms (n_arms) $N_f$, number of features for the context vector (n_features) best_arms, arms in which we should give some bias values (for good) bias, value to be added to the best arms A matrix of size $N_{A} \\times N_{f}$, each value is a random value with $\\mu = 0$ and standard deviation of $\\frac{1}{4}$. However, for the best arms, we will add the bias.\n![[Thompson sampling thetas.excalidraw.svg]]\ndef make_theta(n_arms: int, n_features: int, best_arms, bias = 1): true_theta = np.array( [np.random.normal(size=n_features, scale=1.0/4.0) for _ in range(n_arms)]) true_theta[best_arms] += bias return true_theta true_theta = make_theta( n_arms=N_ARMS, n_features=N_FEATURES, best_arms=BEST_ARMS) A function is also available to generate rewards. It creates rewards for each arm, given a context.\nWe provide:\n$a$, this is the arm index ($0\\leq a \\leq N_{A}-1$) x, is the context that we are observing for the arm index (arm) $\\theta$, is the theta (true or predicted) that are are using to estimate the reward for each arm (theta) scale_noise, we may need to add some random noise ($\\mu=0$ and standard deviation as scale_noise) This will return the estimated score for the arm (with the arm index and the context observed corresponding to the given theta).\ndef generate_reward(arm, x, theta, scale_noise = 1.0/10.0): signal = theta[arm].dot(x) noise = np.random.normal(scale=scale_noise) return signal + noise random_payoffs = np.array( [generate_reward( arm=np.random.choice(N_ARMS), x=X[t, np.random.choice(N_ARMS)], theta=true_theta) for t in range(N_TRIALS)]) # Defining oracle (best payoffs based on the true_theta) oracles = np.array( [np.max( [generate_reward( arm=arm, x=X[t, arm], theta=true_theta) for arm in range(N_ARMS)]) for t in range(N_TRIALS)]) We also create a function to generate the cumulative regret over time.\nWe provide:\npayoffs, an array of $T$ payoffs (for $T$ number of trials) oracles, an array of best values for $T$ trials (oracles) And we get an array of the cumulative sum over time (of size $T$).\ndef make_regret(payoffs: np.ndarray, oracles: np.ndarray) -\u0026gt; np.ndarray: return np.cumsum(oracles - payoffs) payoffs = [ [generate_reward( arm=arm, x=X[t, arm], theta=true_theta) for arm in np.arange(N_ARMS)] for t in np.arange(N_TRIALS)] ave_rewards = np.mean(payoffs, axis=0) The actual samplingThe method to perform the actual sampling is next. We provide:\n$\\delta$ (delta), with $0 \u0026lt; \\delta \u0026lt; 1$. With probability $1 - \\delta$, linear thompson sampling satisfies the theoretical regret bound. $R$, with $R \\geq 0$. Assume that the residual $ri(t) - bi(t)^T \\hat{\\mu}$ is R-sub-gaussian. In this case, $R^2$ represents the variance for residuals of the linear model $bi(t)^T$. $\\epsilon$ (epsilon), with $0 \u0026lt; \\epsilon \u0026lt; 1$ A parameter used by the Thompson Sampling algorithm. If the total trials $T$ is known, we can choose $\\epsilon = \\frac{1}{\\ln{T}}$. delta=0.5 R = 0.01 epsilon=0.5 We use r_payoffs to store the payoff for each trial (the payoff for the selected arm based on the true_theta). As such, we initialise a zero array of size n_trials.\nr_payoffs = np.zeros(N_TRIALS) v = R * np.sqrt(24 / epsilon * N_FEATURES * np.log(1 / delta)) Model initialisation:\nB = np.identity(N_FEATURES) mu_hat = np.zeros(shape=(N_FEATURES, 1)) f = np.zeros(shape=(N_FEATURES,1)) for t in range(N_TRIALS): context = X[t] mu_tilde = np.random.multivariate_normal(mu_hat.flat, v**2 * np.linalg.inv(B))[..., np.newaxis] score_array = context.dot(mu_tilde) chosen_arm = np.argmax(score_array) context_t = context[chosen_arm] reward = generate_reward(arm=chosen_arm, x=context_t, theta=true_theta) r_payoffs[t] = reward context_t = np.reshape(context_t, (-1, 1)) B += context_t.dot(context_t.T) f += reward*context_t mu_hat = np.linalg.inv(B).dot(f) ","permalink":"/thompson-sampling.html","tags":null,"title":"Thompson sampling"},{"categories":null,"contents":"IntroductionA time-series is commonly described as a data set that captures observations over time.\nConceptsPeaks and troughsLet\u0026rsquo;s start by creating a random walk.\nimport numpy as np import pandas as pd N = 10000 step_set = [-1, 0, 1] origin = np.zeros((1, 1)) step_shape = (N, 1) steps = np.random.choice(a=step_set, size=step_shape) path = np.concatenate([origin, steps]).cumsum(0) df = pd.DataFrame(path, columns =[\u0026#39;y\u0026#39;]) from scipy.signal import find_peaks subset = df.head(100) peaks = find_peaks(subset[\u0026#34;y\u0026#34;]) troughs = find_peaks(-subset[\u0026#34;y\u0026#34;]) peaks (array([ 9, 20, 30, 37, 48, 52, 64, 77, 79, 84, 92]), {}) AutocorrelationPandas provides an autocorrelation1 plot function.\npd.plotting.autocorrelation_plot(df[\u0026#34;y\u0026#34;]) plt.show() DifferencingCalculating the difference between $x_t$ and $x_{t-1}$.\nstationary = df[\u0026#39;y\u0026#39;].diff() ToolsIn here we\u0026rsquo;ll look at some tools (mostly for Python) which allow for time-series analysis.\nDataimport pandas as pd df = pd.read_csv(\u0026#34;../../data/streamad/uniDS.csv\u0026#34;) TsfreshTsfresh2 (Time Series Feature Extraction Based on Scalable Hypothesis Tests) is a Python package that automatically calculates and extracts several time series features for classification and regression. Typically used for feature engineering.\nfrom tsfresh import extract_features, extract_relevant_features, select_features from tsfresh.utilities.dataframe_functions import impute, make_forecasting_frame from tsfresh.feature_extraction import ComprehensiveFCParameters, settings data = df[\u0026#39;timestamp\u0026#39;,\u0026#39;value\u0026#39;]() df_pass, y_air = make_forecasting_frame(data.value, kind=\u0026#34;value\u0026#34;, max_timeshift=100, rolling_direction=1) Rolling: 100%|| 30/30 [00:05\u0026lt;00:00, 5.17it/s] https://pandas.pydata.org/docs/reference/api/pandas.plotting.autocorrelation_plot.html\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nhttps://tsfresh.readthedocs.io/en/latest/\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"/time-series-analysis.html","tags":null,"title":"Time-series analysis"},{"categories":null,"contents":"","permalink":"/transformation-functions.html","tags":null,"title":"Transformation functions"},{"categories":null,"contents":"SynopsisInference data is represented in TrustyAI1 using objects implementing the Prediction interface. The objects always include two general entities representing collections of inputs (PredictionInput) and outputs (PredictionOutput). These in turn are comprised of Features and Outputs, respectively.\nCommunication with KServe/ModelMesh deployed models is done, however, using gRPC, with the KServe v2 protocol.\nThe basis of prediction requests is the KServe ModelInferenceRequest message, where model responses are based on the ModelInferenceResponse message. Payloads (and data in general) are encoded as InferInputTensor and InferResponseTensor. Here we look at several scenarios to convert data to/from KServe tensors and TrustyAI Predictions.\nSingle inputnp codecCurrently, the conversion between PredictionInput and PredictionOutput to KServe Tensor format works in the following way: We assume a consistent datatype between the features and serialise it as a $(1, N_f)$ shaped tensor.\nNote that although a JSON representation of the data is used, it is only for illustration of the structure. The TrustyAI structures are converted to Protobuf format.\nThis is using the default KServe conversion codec np.\npd codecKServe also allows to use the pd conversion codec, where each feature is a $(1, 1)$ shaped Tensor. This allows for greater flexibility, especially in the common case where each feature has a different datatype. In TrustyAI, this conversion works the following way:\nOutputsFor the single input case, the output conversion is straightforward. We get a $(1, N_f)$ shaped tensor for the outputs, which allows for direct conversion to a single PredictionOutput, such that\nMultiple inputsThere are basically two ways of dealing with the multiple inputs, such as in the single inputs case. In this case we will consider $N_{obs}$ inputs, each with $N_f$ features.\nnp codecWith the np codec, we simply produce a $(N_{obs}, N_f)$ shaped tensor, sharing the same datatype.\npd codec OutputsWith multiple outputs come multiple outputs and here the situation is not as straightforward. Let\u0026rsquo;s assume that for the previous inputs we get two features as the inference result. Usually, the outputs will come as a $(1, N_{obs}\\times N_f)$ shaped tensor, so we will need to reshape it. So we\u0026rsquo;ll have something like\n{ \u0026#34;model_name\u0026#34;: \u0026#34;dummy3\u0026#34;, \u0026#34;model_version\u0026#34;: \u0026#34;v0.0.1\u0026#34;, \u0026#34;id\u0026#34;: \u0026#34;3aacc8a0-a3cc-47ce-bd03-614df833d18e\u0026#34;, \u0026#34;parameters\u0026#34;: {}, \u0026#34;outputs\u0026#34;: [ { \u0026#34;name\u0026#34;: \u0026#34;predict\u0026#34;, \u0026#34;shape\u0026#34;: [ 3, 2 ], \u0026#34;datatype\u0026#34;: \u0026#34;INT64\u0026#34;, \u0026#34;data\u0026#34;: [ 2, 2, 1, 4, 2, 2 ] } ] } Since we know the final shape of the multiple outputs (with the shape value), we can reshape it using: Dataframe utilitiesA collection of Prediction can be also represented as a Dataframe. Dataframe will represent the data in tabular format along with the necessary metadata related to the feature types, feature domains and if the feature belongs in the inputs or outputs.\nA new data structure, called a TensorDataframe is an extension of the Dataframe, sharing a common API, but adding a few convenience methods to convert either individual rows (or the entire data) to tensors. The first methods are\n.rowAsSingleArrayInputTensor(), which converts a single row\u0026rsquo;s inputs to the np tensor format .rowAsSingleDataframeInputTensor(), which converts a single row\u0026rsquo;s inputs to the pd tensor format If an entire dataset needs to be converted (say for batch requests), we can use: .rowBatchArrayInputTensor(), converts an entire dataframe to the np tensor format rowBatchDataframeInputTensor(), converts an entire dataframe to the pd tensor format SchemasA table with the values of the schemas for each scenario.\nScenario Format # Input features # Output features Batch size Top inputs count Input(s) shape Top output counts Output(s) shape 1 input, 1 output, no batch NP 1 1 1 1 [0,1] 1 [0,1] 1 input, 2 outputs, no batch NP 1 2 1 1 [1,1] 1 [1,2] 2 inputs, 1 output, no batch NP 2 1 1 1 [1,2] 1 [1,1] 2 inputs, 2 outputs, no batch NP 2 2 1 1 [1,2] 1 [1,2] 1 input, 1 output, no batch PD 1 1 1 1 [1] 1 [1] 1 input, 2 outputs, no batch PD 1 2 1 1 [1] 2 [1],[1] 2 inputs, 1 output, no batch PD 2 1 1 2 [1],[1] 1 [1] 2 inputs, 2 outputs, no batch PD 2 2 1 2 [1],[1] 2 [1],[1] 1 input, 1 output, batch NP 1 1 10 1 [10, 1] 1 [10, 1] 1 input, 2 outputs, batch NP 1 2 10 1 [10,1] 1 [10,2] 2 inputs, 1 output, batch NP 2 1 10 1 [10,2] 1 [10,1] 2 inputs, 2 outputs, batch NP 2 2 10 1 [10,2] 1 [10,2] 1 input, 1 output, batch PD 1 1 10 1 [1, 10] 1 [1, 10] 1 input, 2 outputs, batch PD 1 2 10 1 [1, 10] 2 [1, 10],[1, 10] 2 inputs, 1 output, batch PD 2 1 10 2 [1, 10],[1, 10] 1 [1, 10] 2 inputs, 2 outputs, batch PD 2 2 10 2 [1, 10],[1, 10] 2 [1, 10],[1, 10] Which schemas have $\u0026gt;1$ I/O counts?\nScenario Format # Input features # Output features Batch size Top inputs count Input(s) shape Top output counts Output(s) shape 1 input, 2 outputs, no batch PD 1 2 1 1 [1] 2 [1],[1] 2 inputs, 1 output, no batch PD 2 1 1 2 [1],[1] 1 [1] 2 inputs, 2 outputs, no batch PD 2 2 1 2 [1],[1] 2 [1],[1] 1 input, 2 outputs, batch PD 1 2 10 1 [1, 10] 2 [1, 10],[1, 10] 2 inputs, 1 output, batch PD 2 1 10 2 [1, 10],[1, 10] 1 [1, 10] 2 inputs, 2 outputs, batch PD 2 2 10 2 [1, 10],[1, 10] 2 [1, 10],[1, 10] If the input/output counts are $\u0026gt;1$, then we definitely have a PD payload. This is regardless of batch size.\nWhich schemas have $=1$ I/O counts?\nScenario Format # Input features # Output features Batch size Top inputs count Input(s) shape Top output counts Output(s) shape 1 input, 1 output, no batch NP 1 1 1 1 [1] 1 [1] 1 input, 2 outputs, no batch NP 1 2 1 1 [1,1] 1 [1,2] 2 inputs, 1 output, no batch NP 2 1 1 1 [1,2] 1 [1,1] 2 inputs, 2 outputs, no batch NP 2 2 1 1 [1,2] 1 [1,2] 1 input, 1 output, no batch PD 1 1 1 1 [1] 1 [1] 1 input, 1 output, batch NP 1 1 10 1 [10, 1] 1 [10, 1] 1 input, 2 outputs, batch NP 1 2 10 1 [10,1] 1 [10,2] 2 inputs, 1 output, batch NP 2 1 10 1 [10,2] 1 [10,1] 2 inputs, 2 outputs, batch NP 2 2 10 1 [10,2] 1 [10,2] 1 input, 1 output, batch PD 1 1 10 1 [1, 10] 1 [1, 10] If the top input/ouput counts are $=1$, then we definitely have an NP (the PD in there just means a 1 input, 1 output PD, which is indistinguishable).\nLet\u0026rsquo;s look at the inputs only. The first case is when the count is $=1$.\nScenario Format # Input features # Output features Batch size Top inputs count Input(s) shape Top output counts Output(s) shape 1 input, 1 output, no batch NP 1 1 1 1 [0,1] 1 [1] 1 input, 2 outputs, no batch NP 1 2 1 1 [1,1] 1 [1,2] 2 inputs, 1 output, no batch NP 2 1 1 1 [1,2] 1 [1,1] 2 inputs, 2 outputs, no batch NP 2 2 1 1 [1,2] 1 [1,2] 1 input, 1 output, no batch PD 1 1 1 1 [1] 1 [1] 1 input, 2 outputs, no batch PD 1 2 1 1 [1] 2 [1],[1] 1 input, 1 output, batch NP 1 1 10 1 [10, 1] 1 [10, 1] 1 input, 2 outputs, batch NP 1 2 10 1 [10,1] 1 [10,2] 2 inputs, 1 output, batch NP 2 1 10 1 [10,2] 1 [10,1] 2 inputs, 2 outputs, batch NP 2 2 10 1 [10,2] 1 [10,2] 1 input, 1 output, batch PD 1 1 10 1 [1, 10] 1 [1, 10] 1 input, 2 outputs, batch PD 1 2 10 1 [1, 10] 2 [1, 10],[1, 10] Now let\u0026rsquo;s look at the case were $\u0026gt;1$\nScenario Format # Input features # Output features Batch size Top inputs count Input(s) shape Top output counts Output(s) shape 2 inputs, 1 output, no batch PD 2 1 1 2 [1],[1] 1 [1] 2 inputs, 2 outputs, no batch PD 2 2 1 2 [1],[1] 2 [1],[1] 2 inputs, 1 output, batch PD 2 1 10 2 [1, 10],[1, 10] 1 [1, 10] 2 inputs, 2 outputs, batch PD 2 2 10 2 [1, 10],[1, 10] 2 [1, 10],[1, 10] Logic:\nflowchart TD A{Input count = 1?} --\u0026gt;|Yes| B{Shape 0 \u0026gt; 1} B --\u0026gt;|Yes| D[NP batch] B --\u0026gt;|No| C{Just one\\nelement?} C --\u0026gt;|No| F[Either NP, no batch, multi-featute\\nOR\\nPD, batch, 1 feature] C --\u0026gt;|Yes| G[1 feature, no batch] A --\u0026gt;|No| H{Single element\\nshapes?} H --\u0026gt;|Yes| I[Multi-feature\\nPD\\nNo batch] H --\u0026gt;|No| J[Multi-feature\\nPD\\nBatch] Regarding the outputs, let\u0026rsquo;s first look at the case where the output counts are $=1$\nScenario Format # Input features # Output features Batch size Top inputs count Input(s) shape Top output counts Output(s) shape 1 input, 1 output, no batch NP 1 1 1 1 [1] 1 [1] 1 input, 2 outputs, no batch NP 1 2 1 1 [1,1] 1 [1,2] 2 inputs, 1 output, no batch NP 2 1 1 1 [1,2] 1 [1,1] 2 inputs, 2 outputs, no batch NP 2 2 1 1 [1,2] 1 [1,2] 1 input, 1 output, no batch PD 1 1 1 1 [1] 1 [1] 2 inputs, 1 output, no batch PD 2 1 1 2 [1],[1] 1 [1] 1 input, 1 output, batch NP 1 1 10 1 [10, 1] 1 [10, 1] 1 input, 2 outputs, batch NP 1 2 10 1 [10,1] 1 [10,2] 2 inputs, 1 output, batch NP 2 1 10 1 [10,2] 1 [10,1] 2 inputs, 2 outputs, batch NP 2 2 10 1 [10,2] 1 [10,2] 1 input, 1 output, batch PD 1 1 10 1 [1, 10] 1 [1, 10] We can see the same logic as in the inputs. When the output counts $\u0026gt;1$:\nScenario Format # Input features # Output features Batch size Top inputs count Input(s) shape Top output counts Output(s) shape 1 input, 2 outputs, no batch PD 1 2 1 1 [1] 2 [1],[1] 2 inputs, 2 outputs, no batch PD 2 2 1 2 [1],[1] 2 [1],[1] 1 input, 2 outputs, batch PD 1 2 10 1 [1, 10] 2 [1, 10],[1, 10] 2 inputs, 2 outputs, batch PD 2 2 10 2 [1, 10],[1, 10] 2 [1, 10],[1, 10] flowchart TD A{Output count = 1?} --\u0026gt;|Yes| B{Shape 0 \u0026gt; 1} B --\u0026gt;|Yes| D[NP batch] B --\u0026gt;|No| C{Just one\\nelement?} C --\u0026gt;|No| F[Either NP, no batch, multi-featute\\nOR\\nPD, batch, 1 feature] C --\u0026gt;|Yes| G[1 feature, no batch] A --\u0026gt;|No| H{Single element\\nshapes?} H --\u0026gt;|Yes| I[Multi-feature\\nPD\\nNo batch] H --\u0026gt;|No| J[Multi-feature\\nPD\\nBatch] https://github.com/trustyai-explainability/trustyai-explainability\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"/trustyai-kserve-conversions.html","tags":null,"title":"TrustyAI-KServe conversions"},{"categories":null,"contents":"ReadonlyTypescript supports the readonly attribute. Assume the following function to sort arrays:\nfunction sortNumbers(array: Array\u0026lt;number\u0026gt;) { return array.sort((a, b) =\u0026gt; a - b) } const numbers = [7, 3, 5] const sortedNumbers = sortNumbers(numbers) console.log(sortedNumbers) console.log(numbers) We can see that the function sortNumbers changed both arrays. We can add the Readonly type, but this alone won\u0026rsquo;t work. The array still needs to be changed in place and this will generate an error.\nfunction sortNumbers(array: Readonly\u0026lt;Array\u0026lt;number\u0026gt;\u0026gt;) { return array.sort((a, b) =\u0026gt; a - b) } By doing a defensive copy of the array, this will work:\nfunction sortNumbers(array: Readonly\u0026lt;Array\u0026lt;number\u0026gt;\u0026gt;) { return [...array].sort((a, b) =\u0026gt; a - b) } ","permalink":"/typescript.html","tags":null,"title":"Typescript"},{"categories":null,"contents":"SummaryA collection of notes on typography.\nMonospaced fontsCurrently I am favouring the Iosevka font for monospaced.\nThe Monoid font has a post to explain some design decisions1. Serif FontsVollkorn is the font used currently in this site.\nhttps://medium.com/larsenwork-andreas-larsen/class-based-contextual-positioning-in-monospaced-fonts-cb6b8b9ffe6f\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"/typography.html","tags":null,"title":"Typography"},{"categories":null,"contents":"","permalink":"/unfairness-detection.html","tags":null,"title":"Unfairness detection"},{"categories":null,"contents":" Unit test should start with should. Unit Testing is Overrated Start test names with \u0026lsquo;should\u0026rsquo; Still No Consensus On Testing Private Methods FrameworksPythonUnit testing with Python is typically done with pytest.\n","permalink":"/unit-testing.html","tags":null,"title":"Unit testing"},{"categories":null,"contents":" IntroductionDouglas McIlroy1 summarized the Unix philosophy as follows:\nWrite programs that do one thing and do it well. Write programs to work together. Write programs to handle text streams, because that is a universal interface. Philosophy?The UNIX philosophy of \u0026ldquo;small is beautiful\u0026rdquo; and \u0026ldquo;do one thing and do it well\u0026rdquo; is often compared to the minimalist philosophy of the Zen Buddhism. The UNIX philosophy can also be compared to the philosophy of Socrates, which emphasizes the importance of understanding the essence of things and not getting bogged down by unnecessary complexity.\nConcepts Modularity: The idea that a program or system should be broken down into smaller, self-contained components that can be easily reused and modified. Simplicity: The idea that a program or system should be designed to be as simple and straightforward as possible, with minimal unnecessary complexity. Text-based interfaces: The idea that programs and systems should use simple text-based interfaces that can be easily used and understood by humans, rather than more complex graphical interfaces. Small, focused programs: The idea that programs should be small and focused, with a specific purpose and function, rather than large and complex. Use of pipes and filters: The idea that programs should be designed to work together and accept input from, and produce output for, other programs, using pipes and filters. Open-source: The idea that programs should be open-source and freely available for anyone to use and modify, rather than closed and proprietary. Unix shell: A command-line interface and scripting language, which provide a powerful tool for automating tasks and connecting different programs together. Articles Unix Philosophy with an example https://en.wikipedia.org/wiki/Douglas_McIlroy\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"/unix-philosophy.html","tags":null,"title":"UNIX philosophy"},{"categories":null,"contents":"Copy paste Press v to select characters, or uppercase V to select whole lines y to copy Press P to paste before the cursor, or p to paste after ","permalink":"/vim-keys.html","tags":null,"title":"Vim keys"},{"categories":null,"contents":"Download a filesaveFile: function() { const data = JSON.stringify(this.myData) const blob = new Blob([data], {type: \u0026#39;text/plain\u0026#39;}) const e = document.createEvent(\u0026#39;MouseEvents\u0026#39;), a = document.createElement(\u0026#39;a\u0026#39;); a.download = \u0026#34;myData.json\u0026#34;; a.href = window.URL.createObjectURL(blob); a.dataset.downloadurl = [\u0026#39;text/json\u0026#39;, a.download, a.href].join(\u0026#39;:\u0026#39;); e.initEvent(\u0026#39;click\u0026#39;, true, false, window, 0, 0, 0, 0, 0, false, false, false, false, 0, null); a.dispatchEvent(e); } ","permalink":"/vue.html","tags":null,"title":"Vue"},{"categories":null,"contents":"BackupExternal mediaSince I switch OSes frequently [1]1. I macOS and GNU/Linux (specifically Fedora and Ubuntu) daily, Windows seldomly. having a minimum-hassle filesystem for my external drives would be very convenient. exFAT [2]2. ExFAT. seems to me like the best solution. In fact, exFAT is the default FS for SD cards and USB flash drives with more than 32Gb. So perhaps, if have a large-ish USB pen or an external drive, chances are you don\u0026rsquo;t even need to reformat it.\nSoftwareMy software of choice for backups at the moment is Kopia [3]3. Kopia website. WorkflowOptimise PNGsTo optimise an entired folder of PNGs, you could use optipng. Install it on macOS using\n$ brew install optipng and then apply it recusively with\n$ find . -name \u0026#34;*.png\u0026#34; -exec optipng -o7 {} \\; ","permalink":"/workflow.html","tags":null,"title":"Workflow"},{"categories":null,"contents":"IntroductionXGBoost1 is a popular regularizing gradient boosting framework.\nInstallationIn most systems, installing XGBoost can be done simply by using pip\n$ pip install xgboost ExampleTraining XGBoost with the credit-bias dataset.\nimport pandas as pd data = pd.read_csv(\u0026#34;../data/credit-bias-train.zip\u0026#34;) data.head() NewCreditCustomer Amount Interest LoanDuration Education NrOfDependants EmploymentDurationCurrentEmployer IncomeFromPrincipalEmployer IncomeFromPension IncomeFromFamilyAllowance ... Mortgage Other Owner Owner_with_encumbrance Tenant Entrepreneur Fully Partially Retiree Self_employed 0 False 2125.0 20.97 60 4.0 0.0 6.0 0.0 301.0 0.0 ... 0 0 1 0 0 0 0 0 1 0 1 False 3000.0 17.12 60 5.0 0.0 6.0 900.0 0.0 0.0 ... 0 0 1 0 0 1 0 0 0 0 2 True 9100.0 13.67 60 4.0 1.0 3.0 600.0 0.0 0.0 ... 1 0 0 0 0 1 0 0 0 0 3 True 635.0 42.66 60 2.0 0.0 1.0 745.0 0.0 0.0 ... 0 0 0 0 1 0 1 0 0 0 4 False 5000.0 24.52 60 4.0 1.0 5.0 1000.0 0.0 0.0 ... 0 0 0 0 0 0 1 0 0 0 5 rows  40 columns\nX_df = data.drop(\u0026#39;PaidLoan\u0026#39;, axis=1) y_df = data[\u0026#39;PaidLoan\u0026#39;] y_df.describe() count 58003 unique 2 top True freq 29219 Name: PaidLoan, dtype: object from sklearn.model_selection import train_test_split train_x, test_x, train_y, test_y = train_test_split(X_df, y_df, test_size=0.25, random_state=42) Hyperparameter estimationRuns a grid search to find the tuning parameters that maxisimise the area under the curve (AUC). train_x is the training data frame with loan details and train_y is the default target column for training. The method returns the best parameters and corresponding AUC score.\nThe objective parameter2 specifies the learning task and the corresponding learning objective. Possible values include:\nObjective function reg:squarederror, regression with squared loss. reg:squaredlogerror, regression with squared log loss reg:logistic, logistic regression reg:pseudohubererror, regression with Pseudo Huber loss, a twice differentiable alternative to absolute loss. binary:logistic, logistic regression for binary classification, output probability binary:logitraw, logistic regression for binary classification, output score before logistic transformation binary:hinge, hinge loss for binary classification. This makes predictions of 0 or 1, rather than producing probabilities. count:poisson, poisson regression for count data, output mean of Poisson distribution survival:cox, Cox regression for right censored survival time data (negative values are considered right censored). survival:aft, Accelerated failure time model for censored survival time data. See Survival Analysis with Accelerated Failure Time for details. aft_loss_distribution, Probability Density Function used by survival:aft objective and aft-nloglik metric. multi:softmax, set XGBoost to do multiclass classification using the softmax objective, you also need to set num_class (number of classes) multi:softprob, same as softmax, but output a vector of ndata * nclass, which can be further reshaped to ndata * nclass matrix. The result contains predicted probability of each data point belonging to each class. rank:pairwise, Use LambdaMART to perform pairwise ranking where the pairwise loss is minimized rank:ndcg, Use LambdaMART to perform list-wise ranking where Normalized Discounted Cumulative Gain (NDCG) is maximized rank:map, Use LambdaMART to perform list-wise ranking where Mean Average Precision (MAP) is maximized reg:gamma, gamma regression with log-link. Output is a mean of gamma distribution. It might be useful, e.g., for modeling insurance claims severity, or for any outcome that might be gamma-distributed. reg:tweedie, Tweedie regression with log-link. It might be useful, e.g., for modeling total loss in insurance, or for any outcome that might be Tweedie-distributed. Weight balancescale_pos_weight (default 1) controls the balance of positive and negative weights, useful for unbalanced classes. A typical value to consider is sum(negative instances) / sum(positive instances).\nfrom sklearn.model_selection import GridSearchCV from xgboost.sklearn import XGBClassifier from typing import Tuple def find_best_xgboost_model(train_x: pd.DataFrame, train_y: pd.Series) -\u0026gt; Tuple[dict, float]: scale_pos_weight = (len(train_y) - train_y.sum()) / train_y.sum() param_test = { \u0026#39;max_depth\u0026#39;: [1, 2, 4, 8], \u0026#39;learning_rate\u0026#39;: [0.05, 0.06, 0.07], \u0026#39;n_estimators\u0026#39;: [10, 50, 100] } gsearch = GridSearchCV(estimator=XGBClassifier( use_label_encoder=False, objective=\u0026#39;binary:logistic\u0026#39;, scale_pos_weight=scale_pos_weight, tree_method = \u0026#34;hist\u0026#34;, seed=27), param_grid=param_test, scoring=\u0026#39;roc_auc\u0026#39;, n_jobs=-1, cv=8) gsearch.fit(train_x, train_y) return gsearch.best_params_, gsearch.best_score_ best_params, best_score = find_best_xgboost_model(train_x, train_y) Using the xgboost model parameters, it predicts the probabilities of defaulting.\nbest_params_, best tuning parameters train_x, training dataframe with loan details train_y, default target column for training test_x, testing dataframe with loan details test_y, default target column for testing The result is a series of probabilities whether loan entry will default or not and corresponding model\u0026rsquo;s AUC score\nfrom sklearn.metrics import roc_auc_score def xgboost_predict(best_params_: dict, train_x: pd.DataFrame, train_y: pd.Series, test_x: pd.DataFrame, test_y: pd.Series) -\u0026gt; Tuple[list, float]: scale_pos_weight = (len(train_y) - train_y.sum()) / train_y.sum() xgb_model = XGBClassifier(objective=\u0026#39;binary:logistic\u0026#39;, scale_pos_weight=scale_pos_weight, seed=27, max_depth=best_params_[\u0026#39;max_depth\u0026#39;], learning_rate=best_params_[\u0026#39;learning_rate\u0026#39;], n_estimators=best_params_[\u0026#39;n_estimators\u0026#39;] ) xgb_model.fit(train_x, train_y) predicted_probabilities_ = xgb_model.predict_proba(test_x)[:, 1] auc_ = roc_auc_score(test_y, predicted_probabilities_) return predicted_probabilities_, auc_ predicted_probabilities, auc = xgboost_predict(best_params, train_x, train_y, test_x, test_y) print(\u0026#34;AUC: {}\u0026#34;.format(auc)) AUC: 0.7356799122465589 Filters the original loan dataframe to just include the loans from the test dataframe and then it adds the predicted probabilities.\nloans_df_, original loan dataframe test_index, indices from the test dataframes predicted_probabilities_, the probabilities forecasted by the XGBoost model Returns the loans dataframe with predictions\nimport numpy as np def prepare_test_with_predictions(loans_df_: pd.DataFrame, test_index: pd.Index, predicted_probabilities_: np.array)\\ -\u0026gt;pd.DataFrame: loan_test_df = loans_df_.loc[test_index] loan_test_df[\u0026#39;predicted_probabilities\u0026#39;] = predicted_probabilities_ return loan_test_df loans_with_predictions_df = prepare_test_with_predictions(data, test_x.index, predicted_probabilities) loans_with_predictions_df.head() NewCreditCustomer Amount Interest LoanDuration Education NrOfDependants EmploymentDurationCurrentEmployer IncomeFromPrincipalEmployer IncomeFromPension IncomeFromFamilyAllowance ... Other Owner Owner_with_encumbrance Tenant Entrepreneur Fully Partially Retiree Self_employed predicted_probabilities 30299 False 530.0 10.68 36 4.0 NaN 5.0 0.0 0.0 0.0 ... 0 0 0 1 0 0 0 0 0 0.641520 34126 False 530.0 21.57 24 4.0 NaN 1.0 0.0 0.0 0.0 ... 0 0 0 0 0 0 0 0 0 0.770486 11200 False 2300.0 15.62 36 4.0 0.0 6.0 1159.0 0.0 0.0 ... 0 1 0 0 0 1 0 0 0 0.748680 25133 True 530.0 27.36 36 4.0 NaN 6.0 0.0 0.0 0.0 ... 0 0 1 0 0 0 0 0 0 0.469619 42758 True 4250.0 18.94 60 4.0 NaN 1.0 0.0 0.0 0.0 ... 0 0 0 0 0 0 0 0 0 0.527547 5 rows  41 columns\nVisualisationimport seaborn as sns sns.histplot(loans_with_predictions_df[\u0026#39;predicted_probabilities\u0026#39;], stat=\u0026#39;density\u0026#39;) \u0026lt;AxesSubplot:xlabel='predicted_probabilities', ylabel='Density'\u0026gt; ROC and AUCBased on actuals and predicted values3, it calculates their false positive rate (fpr), the true positive rate (tpr). It also returns the corresponding thresholds used as well as the value for the area under the curve.\nactuals, series of actual values indicating whether the loan defaulted or not predicted_probabilities, series of predicted probabilities of the loan defaulting Return a unique series of false and true positive rates with corresponding series of thresholds and value for total area under the curve.\nfrom sklearn.metrics import roc_curve, auc def get_roc_auc_data(actuals: pd.Series, predicted_probabilities: pd.Series) -\u0026gt; Tuple[np.array, np.array, np.array, float]: fpr, tpr, thresholds = roc_curve(actuals, predicted_probabilities, pos_label=1) auc_score = auc(fpr, tpr) return fpr, tpr, thresholds, auc_score fpr, tpr, thresholds, auc_score = get_roc_auc_data(loans_with_predictions_df[\u0026#39;PaidLoan\u0026#39;], loans_with_predictions_df[\u0026#39;predicted_probabilities\u0026#39;]) sns.histplot(fpr) \u0026lt;AxesSubplot:ylabel='Count'\u0026gt; https://github.com/dmlc/xgboost\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nhttps://xgboost.readthedocs.io/en/stable/parameter.html#learning-task-parameters\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nSee ROC.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"/xgboost.html","tags":null,"title":"XGBoost"},{"categories":null,"contents":"FunctionsDefault argumentsExample of a function with default arguments in zsh\nfunction e() { if [ \u0026#34;$1\u0026#34; != \u0026#34;\u0026#34; ] then subl $1 else subl . fi } File name without the extensionIf you want the full path without the extension:\n$ myfile=/path/to/story.txt $ echo ${myfile:r} /path/to/story $ myfile=story.txt $ echo ${myfile:r} story If you want just the file name minus the path:\n$ myfile=/path/to/story.txt $ echo ${myfile:t} story.txt Check this out you can combine those two symbols!\n$ myfile=/path/to/story.txt $ echo ${myfile:t:r} story PerformanceProfiling Why does zsh start so slowly ","permalink":"/zsh.html","tags":null,"title":"zsh"}]