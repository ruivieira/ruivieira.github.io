<!doctype html><html lang=en-uk><head><script data-goatcounter=https://ruivieira-dev.goatcounter.com/count async src=//gc.zgo.at/count.js></script>
<script src=https://unpkg.com/@alpinejs/intersect@3.x.x/dist/cdn.min.js></script>
<script src=https://unpkg.com/alpinejs@3.x.x/dist/cdn.min.js></script>
<script type=module src=/js/deeplinks/deeplinks.js></script>
<link rel=preload href=/lib/fonts/fa-brands-400.woff2 as=font type=font/woff2 crossorigin=anonymous><link rel=preload href=/lib/fonts/fa-regular-400.woff2 as=font type=font/woff2 crossorigin=anonymous><link rel=preload href=/lib/fonts/fa-solid-900.woff2 as=font type=font/woff2 crossorigin=anonymous><link rel=preload href=/fonts/firacode/FiraCode-Regular.woff2 as=font type=font/woff2 crossorigin=anonymous><link rel=preload href=/fonts/prociono/Prociono-Regular.woff2 as=font type=font/woff2 crossorigin=anonymous><link rel=stylesheet href=/css/kbd.css type=text/css><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><title>Introduction to Balanced Box-Decomposition Trees · Rui Vieira</title><link rel=canonical href=/introduction-to-balanced-box-decomposition-trees.html><meta name=viewport content="width=device-width,initial-scale=1"><meta name=robots content="all,follow"><meta name=googlebot content="index,follow,snippet,archive"><meta property="og:title" content="Introduction to Balanced Box-Decomposition Trees"><meta property="og:description" content="Stardate 96893.29.
You are the USS Euler&rsquo;s Science Officer at a moment when the computer graphical displays and voice systems went down. You only have enough deuterium for a short travel and need to find the nearest star system. This is not a simple matter of looking at a chart. You have multiple dimensions in which you can travel. In a bid for galactic peace, the Federation mandated that both Emacs and Vim should be installed in all computers."><meta property="og:type" content="article"><meta property="og:url" content="/introduction-to-balanced-box-decomposition-trees.html"><meta property="article:section" content="posts"><meta property="article:modified_time" content="2023-01-15T15:53:40+00:00"><meta name=twitter:card content="summary"><meta name=twitter:title content="Introduction to Balanced Box-Decomposition Trees"><meta name=twitter:description content="Stardate 96893.29.
You are the USS Euler&rsquo;s Science Officer at a moment when the computer graphical displays and voice systems went down. You only have enough deuterium for a short travel and need to find the nearest star system. This is not a simple matter of looking at a chart. You have multiple dimensions in which you can travel. In a bid for galactic peace, the Federation mandated that both Emacs and Vim should be installed in all computers."><link rel=stylesheet href=/css/styles.css><!--[if lt IE 9]><script src=https://oss.maxcdn.com/html5shiv/3.7.2/html5shiv.min.js></script>
<script src=https://oss.maxcdn.com/respond/1.4.2/respond.min.js></script><![endif]--><link rel=icon type=image/png href=images/favicon.ico></head><body class="max-width mx-auto px3 ltr" x-data="{currentHeading: undefined}"><div class="content index py4"><div id=header-post><a id=menu-icon href=#><i class="fas fa-eye fa-lg"></i></a>
<a id=menu-icon-tablet href=#><i class="fas fa-eye fa-lg"></i></a>
<a id=top-icon-tablet href=# onclick='$("html, body").animate({scrollTop:0},"fast")' style=display:none aria-label="Top of Page"><i class="fas fa-chevron-up fa-lg"></i></a>
<span id=menu><span id=nav><ul><li><a href=/>Home</a></li><li><a href=/map/>All pages</a></li><li><a href=/search.html>Search</a></li></ul></span><br><div id=share style=display:none></div><div id=toc><h4>Contents</h4><nav id=TableOfContents><ul><li><a href=#space-decomposition :class="{'toc-h2':true, 'toc-highlight': currentHeading == '#space-decomposition' }">Space decomposition</a></li><li><a href=#tree-querying :class="{'toc-h2':true, 'toc-highlight': currentHeading == '#tree-querying' }">Tree querying</a></li><li><a href=#filtering-and-k-nn :class="{'toc-h2':true, 'toc-highlight': currentHeading == '#filtering-and-k-nn' }">Filtering and k-NN</a></li></ul></nav></ul></nav></div></span></div><article class=post itemscope itemtype=http://schema.org/BlogPosting><header><h1 class=posttitle itemprop="name headline">Introduction to Balanced Box-Decomposition Trees</h1><div class=meta><div class=postdate>Updated <time datetime="2023-01-15 15:53:40 +0000 GMT" itemprop=datePublished>2023-01-15</time>
<span class=commit-hash>(<a href=/log/index.html#fe73f60>fe73f60</a>)</span></div></div></header><div class=content itemprop=articleBody><p>Stardate 96893.29.</p><p>You are the USS Euler&rsquo;s Science Officer at a moment when the computer graphical displays and voice systems went down.
You only have enough deuterium for a short travel and need to find the nearest star system. This is not a simple matter of looking at a chart.
You have multiple dimensions in which you can travel. In a bid for galactic peace, the Federation mandated that <em>both</em> <a href=/emacs.html>Emacs</a> and Vim should be installed in all computers.
You open your favourite editor and, fortunately, know exactly how to formulate the solution to your problem: <strong>a $d$-dimensional nearest neighbour algorithm</strong>.</p><p>Given a dataset $\mathcal{D}$ of $n$ points in a space $X$ we want to be able to tell which are the <em>closest</em> point to a query point $q \in X$, preferably in a way which is computationally cheaper than <em>brute force</em> methods (<em>e.g.</em> iterating through all of the points) which typically solve this problem in $\mathcal{O}(dn)$ <sup id=fnref:1><a href=#fn:1 class=footnote-ref role=doc-noteref>1</a></sup>.</p><p>$X$ could have $d$ dimensions (that is $\mathcal{D} \subset X : \mathbb{R}^d$) and we define <em>closest</em> using<sup id=fnref:2><a href=#fn:2 class=footnote-ref role=doc-noteref>2</a></sup> Minkowski distance metrics, that is:</p><p>$$
L_m = \left(\sum_{i=1}^d |p_i - q_i|^m\right)^{\frac{1}{m}},\qquad p,q \in X : \mathbb{R}^d.
$$</p><p>A potential solution for this problem would be to use <em>kd</em>-trees, which for low dimension scenarios provide $\mathcal{O}(\log n)$ query times <sup id=fnref:3><a href=#fn:3 class=footnote-ref role=doc-noteref>3</a></sup>. However, as the number of dimensions increase (as quickly as $d>2$) the query times also increase as $2^d$.</p><p>The case can be made then for <em>approximate</em> nearest neighbour (NN) algorithms and that&rsquo;s precisely what we will discuss here, namely the <em>Balanced Box-Decomposition Tree</em> (BBD, <sup id=fnref1:1><a href=#fn:1 class=footnote-ref role=doc-noteref>1</a></sup>). The definition of <em>approximate</em> NN for a query point $q$ can be given as</p><p>$$
\text{dist}(p, q) \leq (1+\epsilon)\text{dist}(p^{\star},q),\qquad \epsilon > 0,
$$</p><p>where $p$ is the <em>approximate</em> NN and $p^{\star}$ is the <em>true</em> NN. Let&rsquo;s consider, for the sake of visualisation, a small two dimensional dataset $\mathcal{D} \to \mathbb{R}^2$ as shown in Figure 1.</p><p><figure><img src=/site/images/bbdtrees/small_data.png alt=small_data.png></figure><em>Figure 1.</em> A small test dataset in $\mathbb{R}^2,n=7$.</p><h2 id=space-decomposition x-intersect="currentHeading = '#space-decomposition'">Space decomposition</h2><p>BBD trees belong to the category of hierarchical space decomposition trees. In BBD trees, specifically, space is divided in $d$-dimensional rectangles and <em>cells</em>. Cells can either represent another $d$-dimensional rectangle or the intersection of two rectangles (one, the <em>outer box</em> fully enclosing the other, the <em>inner box</em>). Another important distinction of BBD trees is that rectangle&rsquo;s <em>size</em> (in this context, the largest length in all of the $d$ dimensions) is bounded by a constant value.
The space decomposition must follow an additional rule which is boxes must be <em>sticky</em>. If we consider a inner box $[x_{inner}, y_{inner}]$ contained in a outer box $[x_{outer}, y_{outer}]$, such that</p><p>$$
[x_{inner}, y_{inner}] \subseteq [x_{outer}, y_{outer}],
$$</p><p>then, considering $w = y_{inner} - x_{inner}$, the box is considered <em>sticky</em> if either</p><p>$$
\begin{aligned}
x_{inner}-x_{outer} = 0 &\lor x_{inner}-x_{outer} \nleq w \\
y_{outer}-y_{inner} = 0 &\lor y_{outer}-y_{inner} \nleq w.
\end{aligned}
$$</p><p>An illustration of the stickiness concept can viewed in the diagram below.</p><p><figure><img src=/site/images/bbdtrees/sticky.png alt=sticky.png></figure><strong>Figure 2.</strong> Visualisation of the &ldquo;stickiness&rdquo; criteria for $\mathbb{R}^2$ rectangles.</p><p>Stickiness provides some important geometric properties to the space decomposition which will be discussed further on. The actual process of space decomposition will produce a tree of nodes, each with an associated $d$-dimensional rectangle enclosing a set of points. Each node will be further decomposed into children nodes, containing a region of space with a subset of the parent&rsquo;s data points. If a node has no children it will be called a <em>leaf</em> node. The division process can occur either by means of:</p><ul><li>a <em>fair split</em>, this is done by partitioning the space with an hyperplane, resulting in a <em>low</em> and <em>high</em> children nodes</li><li>a <em>shrink</em>, splitting the box into a inner box (the <em>inner</em> child) and a outer box (the <em>outer</em> child).</li></ul><p><figure><img src=/site/images/bbdtrees/split.png alt=split.png></figure><strong>Figure 3.</strong> &ldquo;Fair split&rdquo; and &ldquo;shrinking&rdquo; division strategies example in $\mathbb{R}^2$ with respective high/low and outer/inner children.</p><p>The initial node of the tree, the <em>root node</em>, will include all the dataset points, $\mathcal{D}$. In the Figure 4 we can see a representation of the root node for the dataset presented above. We can see the node boundaries in dashed red lines as well as the node&rsquo;s center, marked as $\mu_{root}$.</p><p><figure><img src=/site/images/bbdtrees/root_node.png alt=root_node.png></figure><strong>Figure 4.</strong> Associated cell for the BBD-tree root node for the example dataset. Node boundaries in red and node centre labelled as $\mu_{root}$.</p><p>The actual method to calculate the division can either be based on the <em>midpoint algorithm</em> or the <em>middle interval algorithm</em>. The method used for these examples is the latter, for which more details can be found in <sup id=fnref:4><a href=#fn:4 class=footnote-ref role=doc-noteref>4</a></sup>. The next step is to divide the space according to the previously mentioned rules. As an example, we can see the root node&rsquo;s respective children in Figure 5.</p><p><figure><img src=/site/images/bbdtrees/root_children.png alt=root_children.png></figure><strong>Figure 5.</strong> BBD-tree root node&rsquo;s lower (<em>left</em>) and upper (<em>right</em>) children. Node boundaries in red and centres labelled with a red cross.</p><p>This process is repeated until the child nodes are leaves and cannot be divided anymore.
To better visualise the construction process it would be helpful to have a larger tree, so we will now consider still the 2-dimensional case, but now with a larger dataset (Figure 6), consisting of 2000 samples in total, each half from a bivariate Gaussian distribution:</p><p>$$
\begin{aligned}
\text{X}_1 &\sim \mathcal{N}([0,0], \mathbf{I}) \\
\text{X}_2 &\sim \mathcal{N}([3, 3], \mathbf{I}).
\end{aligned}
$$</p><p><figure><img src=/site/images/bbdtrees/gaussian_data.png alt=gaussian_data.png></figure><strong>Figure 6.</strong> Larger example dataset in $\mathbb{R}^2$ consisting of a realisation of $n=2000$ from two bivariate Gaussian distributions centred in $\mu_1=(0,0)$ and $\mu_2=(3,3)$ and with $\Sigma=\mathbf{I}$.</p><p>With this larger dataset, we have enough points to illustrate the tree node building. This time, we will start from the root node and always follow either the &ldquo;lower&rdquo; nodes or the &ldquo;upper&rdquo; nodes (as show in Figure 7). We can clearly see the cells getting smaller, until finally we have a single point included (<em>i.e.</em> a <em>leaf</em> node).</p><p><figure><img src=/site/images/bbdtrees/gaussian_boxes.gif alt=gaussian_boxes.gif></figure><strong>Figure 7.</strong> BBD-tree node building process for the bivariate dataset. On the left we traverse the upper tree nodes and on the right the lower tree nodes.</p><p>This division process illustrates an important property of BBD-trees. Although other space decomposition algorithms (such as <em>kd</em>-trees) display a geometric reduction of number of points enclosed in each <em>cell</em>, methods such as the BBD-tree, which impose constraints on the cell&rsquo;s size aspect ratio as stated before, display not only a geometric reduction in the number of points, but also in the cell&rsquo;s size as well. The construction cost of a BBD-tree is $\mathcal{O}(dn \log n)$ and the tree itself will have $\mathcal{O}(n)$ nodes and $\mathcal{O}(\log n)$ height.</p><h2 id=tree-querying x-intersect="currentHeading = '#tree-querying'">Tree querying</h2><p>Now that we have successfully constructed a BBD-tree, we want to actually find the (approximate) nearest neighbour of an arbitrary query point $q$ (Figure 8).</p><p><figure><img src=/site/images/bbdtrees/gaussian_query_point.png alt=gaussian_query_point.png></figure><strong>Figure 8.</strong> Query point $q$ (red) for the bivariate dataset.</p><p>The first step consists in descending the tree in order to locate the smallest cell containing the query point $q$. This process is illustrated for the bivariate data in Figure 9.</p><p><figure><img src=/site/images/bbdtrees/gaussian_query.gif alt=gaussian_query.gif></figure><strong>Figure 9.</strong> BBD-tree descent to locate the smallest cell containing $q$ (red).</p><p>Once the cell has been located, we proceed to enumerate all the <em>leaf</em> nodes contained by it and calculate our distance metric $L_2$ in this case) between the query point $q$ and the leaf nodes, eventually declaring the point with the smallest $L_2$ as the aproximate NN. BBD-trees provide strong guarantees that the ANN will be located within this cell and not in a neighbouring cell. In Figure 10 we zoomed in the smallest cell containing $q$ and show the associated calculated $L_2$ distance for each node.</p><p><figure><img src=/site/images/bbdtrees/gaussian_dist.gif alt=gaussian_dist.gif></figure><strong>Figure 10.</strong> $L_2$ distance between leaf nodes and the query point $q$ inside the smallest cell containing $q$.</p><p>An important property of BBD-trees is that the tree structure does not need to be recalculated if we change either $\epsilon$ or if we decide to use another $L_m$ distance metric <sup id=fnref2:1><a href=#fn:1 class=footnote-ref role=doc-noteref>1</a></sup>. The query time for a point $q$ in a BBD-tree is $\mathcal{O}(\log n)$. For comparison, if you recall, the query time for a <em>brute force</em> method is typically $\mathcal{O}(dn)$.</p><h2 id=filtering-and-_k_-nn x-intersect="currentHeading = '#filtering-and-_k_-nn'">Filtering and <em>k</em>-NN</h2><p>Great. Now that you solved the USS Euler&rsquo;s problem, you want to make a suggestion to the federation. Where to place several star-bases and how to divide the system&rsquo;s coverage between them. An immediate generalisation of this method is easily applicable to the problem of clustering. Note that, at the moment, we are not concerned with determining the &ldquo;best&rdquo; clusters for our data<sup id=fnref:5><a href=#fn:5 class=footnote-ref role=doc-noteref>5</a></sup>. Given a set of points $Z = {z_1, z_2, \dots, z_n}$, we are concerned now in partitioning the data in clusters centred in each of the $Z$ points. A way of looking at this, is that we are building, for each point $z_n$ a Voronoi cell $V(z_n)$. This is achieved by a method called <em>filtering</em>. Filtering, in general terms, works by walking the tree with the list of <em>candidate centres</em> ($Z$) and pruning points from the candidate list as we move down. We will denote an arbitrary node as $n$, $z^{\star}_w$ and $n_w$ respectively as the candidate and the node weight, $z^{\star}_n$ and $n_n$ as the candidate and node count. The algorithm steps, as detailed in <sup id=fnref:6><a href=#fn:6 class=footnote-ref role=doc-noteref>6</a></sup>, are detailed below:</p><ul><li>Filter($n$, $Z$) {<ul><li>$C \leftarrow n.cell$</li><li><strong>if</strong> ($n$ is a leaf) {<ul><li>$z^{\star} \leftarrow$ the closest point in $Z$ to $n.point$</li><li>$z^{\star}_w \leftarrow z^{\star}_w + n.point$</li><li>$z^{\star}_n \leftarrow z^{\star}_n + 1\qquad$</li></ul></li><li>} *<em>else</em> {<ul><li>$z^{\star} \leftarrow$ the closest point in $Z$ to $C$&rsquo;s midpoint</li><li><strong>for each</strong> ($z \in Z \setminus {z^{\star}}$) {<ul><li><strong>if</strong> ($z.isFarther(z^{\star},C)$) {</li><li>$Z \leftarrow Z \setminus {z}$</li></ul></li></ul></li><li>}</li><li><strong>if</strong> ($|Z|=1$) {<ul><li>$z^{\star}_w \leftarrow z^{\star}_w + n_w$</li><li>$z^{\star}_n \leftarrow z^{\star}_n + n_n$</li><li>} <strong>else</strong> {<ul><li>Filter($n_{left}, Z$)</li><li>Filter($n_{right}, Z$)</li></ul></li><li>}
}</li></ul></li></ul></li></ul><p>To illustrate the assignment of data points to the centres, we will consider the previous bivariate Gaussian data along with two centres, $z_1 = {0,0}$ and $z_2 = {3, 3}$. Figure 11 shows the process of splitting the dataset $\mathcal{D}$ into two clusters, namely the subsets of data points closer to $z_1$ or $z_2$.</p><p><figure><img src=/site/images/bbdtrees/gaussian_filtering.gif alt=gaussian_filtering.gif></figure><strong>Figure 11.</strong> Assignment of points in $\mathcal{D}$ to $Z$. Data points coloured according to the assigned center. Lines represent the distance from the cells midpoint to $Z$.</p><p>We can see in Figure 12 the final cluster assignment of the data points. With a $\mathbb{R}^2$ dataset and only two centres the organisation of points follows a simple perpendicular bisection of the segment connecting the centres, as expected.</p><p><figure><img src=/site/images/bbdtrees/gaussian_filtering_clusters.png alt=gaussian_filtering_clusters.png></figure><strong>Figure 12.</strong> Final $\mathcal{D}$ point assignment to clusters centred in $z_1$ and $z_2$.</p><p>In Figure 13 we can see more clearly the dataset clusters changing when center $z_1$ is moving around the plane. BBD-trees can play an important role in improving $k$-means performance, as described in <sup id=fnref1:6><a href=#fn:6 class=footnote-ref role=doc-noteref>6</a></sup>.</p><p><figure><img src=/site/images/bbdtrees/gaussian_clustering_dynamic.gif alt=gaussian_clustering_dynamic.gif></figure><strong>Figure 13.</strong> Dynamic assignment of points to a cluster using a BBD-tree.</p><p>This concludes a (short) introduction to BBD-trees, I hope you enjoyed it. If you have any comments or suggestions, please let me know at <a href=https://mastodon.technology/@ruivieira>Mastodon</a>.</p><div class=footnotes role=doc-endnotes><hr><ol><li id=fn:1><p>Arya, S., Mount, D. M., Netanyahu, N. S., Silverman, R., & Wu, A. Y. (1998). An optimal algorithm for approximate nearest neighbor searching fixed dimensions. <em>Journal of the ACM</em>. <a href=https://doi.org/10.1145/293347.293348>https://doi.org/10.1145/293347.293348</a>&#160;<a href=#fnref:1 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a>&#160;<a href=#fnref1:1 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a>&#160;<a href=#fnref2:1 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:2><p>The $L_m$ distance may be pre-computed in this method to avoid recalculation for each query.&#160;<a href=#fnref:2 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:3><p>Friedman, J. H., & Bentley, J. L. (1977). RA Finkel. An algorithm for finding best matches in logarithmic expected time. <em>ACM Transactions on Mathematical Software</em>, 3(3), 209-226.&#160;<a href=#fnref:3 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:4><p>Callahan, P. B., & Kosaraju, S. R. (1995). A decomposition of multidimensional point sets with applications to k-nearest-neighbors and n-body potential fields. <em>Journal of the ACM</em>, <em>42</em>(1), 67-90.&#160;<a href=#fnref:4 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:5><p>This would be a <em>k</em>-means problem. I intend to write a blog post on <em>k</em>-means clustering (and the role BBD-trees can play) in the future.&#160;<a href=#fnref:5 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:6><p>Kanungo, T., Mount, D. M., Netanyahu, N. S., Piatko, C. D., Silverman, R., & Wu, A. Y. (2002). An efficient k-means clustering algorithms: Analysis and implementation. <em>IEEE Transactions on Pattern Analysis and Machine Intelligence</em>, <em>24</em>(7), 881–892. <a href=https://doi.org/10.1109/TPAMI.2002.1017616>https://doi.org/10.1109/TPAMI.2002.1017616</a>&#160;<a href=#fnref:6 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a>&#160;<a href=#fnref1:6 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li></ol></div></div></article><div id=footer-post-container><div id=footer-post><div id=nav-footer style=display:none><ul><li><a href=/>Home</a></li><li><a href=/map/>All pages</a></li><li><a href=/search.html>Search</a></li></ul></div><div id=toc-footer style=display:none><nav id=TableOfContents><ul><li><a href=#space-decomposition>Space decomposition</a></li><li><a href=#tree-querying>Tree querying</a></li><li><a href=#filtering-and-_k_-nn>Filtering and <em>k</em>-NN</a></li></ul></nav></div><div id=share-footer style=display:none></div><div id=actions-footer><a id=menu-toggle class=icon href=# onclick='return $("#nav-footer").toggle(),!1' aria-label=Menu><i class="fas fa-bars fa-lg" aria-hidden=true></i> Menu</a>
<a id=toc-toggle class=icon href=# onclick='return $("#toc-footer").toggle(),!1' aria-label=TOC><i class="fas fa-list fa-lg" aria-hidden=true></i> TOC</a>
<a id=share-toggle class=icon href=# onclick='return $("#share-footer").toggle(),!1' aria-label=Share><i class="fas fa-share-alt fa-lg" aria-hidden=true></i> share</a>
<a id=top style=display:none class=icon href=# onclick='$("html, body").animate({scrollTop:0},"fast")' aria-label="Top of Page"><i class="fas fa-chevron-up fa-lg" aria-hidden=true></i> Top</a></div></div></div><footer id=footer><div class=footer-left>Copyright &copy; 2023 Rui Vieira</div><div class=footer-right><nav><ul><li><a href=/>Home</a></li><li><a href=/map/>All pages</a></li><li><a href=/search.html>Search</a></li></ul></nav></div></footer></div></body><link rel=stylesheet href=/css/fa.min.css><script src=/js/jquery-3.6.0.min.js></script>
<script src=/js/mark.min.js></script>
<script src=/js/main.js></script>
<script>MathJax={tex:{inlineMath:[["$","$"],["\\(","\\)"]]},svg:{fontCache:"global"}}</script><script type=text/javascript id=MathJax-script async src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js></script></html>