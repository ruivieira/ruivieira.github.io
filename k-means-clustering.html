<!doctype html><html lang=en-uk><head><script data-goatcounter=https://ruivieira-dev.goatcounter.com/count async src=//gc.zgo.at/count.js></script>
<script type=module src=/js/deeplinks/deeplinks.js></script>
<link rel=preload href=/lib/fonts/fa-brands-400.woff2 as=font type=font/woff2 crossorigin=anonymous><link rel=preload href=/lib/fonts/fa-regular-400.woff2 as=font type=font/woff2 crossorigin=anonymous><link rel=preload href=/lib/fonts/fa-solid-900.woff2 as=font type=font/woff2 crossorigin=anonymous><link rel=preload href=/fonts/firacode/FiraCode-Regular.woff2 as=font type=font/woff2 crossorigin=anonymous><link rel=preload href=/fonts/prociono/Prociono-Regular.woff2 as=font type=font/woff2 crossorigin=anonymous><link rel=stylesheet href=/css/kbd.css type=text/css><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><title>K-means clustering Â· Rui Vieira</title><link rel=canonical href=/k-means-clustering.html><meta name=viewport content="width=device-width,initial-scale=1"><meta name=robots content="all,follow"><meta name=googlebot content="index,follow,snippet,archive"><meta property="og:title" content="K-means clustering"><meta property="og:description" content="Introduction K-means is still one of the fundamental clustering algorithms. It is used in such diverse fields as Natural Language Processing (NLP), social sciences and medical sciences.
The core idea behind K-means is that we want to group data in clusters. Data points will be assigned to a specific cluster depending on it&rsquo;s distance to a cluster&rsquo;s center, usually called the centroid.
It is important to note that typically, the mean distance to a centroid is used to partition the clusters, however, difference distances can be used and different pivot points."><meta property="og:type" content="article"><meta property="og:url" content="/k-means-clustering.html"><meta property="article:section" content="posts"><meta property="article:modified_time" content="2023-01-15T15:53:40+00:00"><meta name=twitter:card content="summary"><meta name=twitter:title content="K-means clustering"><meta name=twitter:description content="Introduction K-means is still one of the fundamental clustering algorithms. It is used in such diverse fields as Natural Language Processing (NLP), social sciences and medical sciences.
The core idea behind K-means is that we want to group data in clusters. Data points will be assigned to a specific cluster depending on it&rsquo;s distance to a cluster&rsquo;s center, usually called the centroid.
It is important to note that typically, the mean distance to a centroid is used to partition the clusters, however, difference distances can be used and different pivot points."><link rel=stylesheet href=/css/styles.css><!--[if lt IE 9]><script src=https://oss.maxcdn.com/html5shiv/3.7.2/html5shiv.min.js></script>
<script src=https://oss.maxcdn.com/respond/1.4.2/respond.min.js></script><![endif]--><link rel=icon type=image/png href=images/favicon.ico></head><body class="max-width mx-auto px3 ltr"><div class="content index py4"><div id=header-post><a id=menu-icon href=#><i class="fas fa-eye fa-lg"></i></a>
<a id=menu-icon-tablet href=#><i class="fas fa-eye fa-lg"></i></a>
<a id=top-icon-tablet href=# onclick='$("html, body").animate({scrollTop:0},"fast")' style=display:none aria-label="Top of Page"><i class="fas fa-chevron-up fa-lg"></i></a>
<span id=menu><span id=nav><ul><li><a href=/>Home</a></li><li><a href=/map/>All pages</a></li><li><a href=/search.html>Search</a></li></ul></span><br><div id=share style=display:none></div><div id=toc><h4>Contents</h4><nav id=TableOfContents><ul><li><a href=#introduction>Introduction</a><ul><li><a href=#data-assignement>Data assignement</a></li><li><a href=#centroid-update-step>Centroid update step</a></li></ul></li><li><a href=#partitioning>Partitioning</a><ul><li><a href=#pam>PAM</a></li></ul></li><li><a href=#elbow-method>Elbow method</a></li></ul></nav></ul></nav></div></span></div><article class=post itemscope itemtype=http://schema.org/BlogPosting><header><h1 class=posttitle itemprop="name headline">K-means clustering</h1><div class=meta><div class=postdate>Updated <time datetime="2023-01-15 15:53:40 +0000 GMT" itemprop=datePublished>2023-01-15</time>
<span class=commit-hash>(<a href=/log/index.html#fe73f60>fe73f60</a>)</span></div></div></header><div class=content itemprop=articleBody><h2 id=introduction>Introduction</h2><p><em>K</em>-means is still one of the fundamental clustering algorithms. It is used in such diverse fields as Natural Language Processing (NLP), social sciences and medical sciences.</p><p>The core idea behind <em>K</em>-means is that we want to group data in <em>clusters</em>. Data points will be assigned to a specific cluster depending on it&rsquo;s distance to a cluster&rsquo;s center, usually called the <em>centroid</em>.</p><p>It is important to note that typically, the mean distance to a centroid is used to partition the clusters, however, difference distances can be used and different pivot points. An example is the <em>K</em>-medoids clustering algorithm.</p><p>We will define the two main steps of a generic <em>K</em>-means clustering algorithm, namely the data assignement and the centroid update step.</p><h3 id=data-assignement>Data assignement</h3><p>The criteria to determine whether a point is closer to one centroid is typically an <a href=/distance-metrics.html#euclidean-distance-l2>Euclidean distance</a> ($L^2$) .
If we consider a set of $n$ centroids $C$, such that</p><p>$$
C = \lbrace c_1, c_2, \dots, c_n \rbrace
$$</p><p>We assign each data point in $\mathcal{D}=\lbrace x_1, x_2, \dots, x_n \rbrace$ to the nearest centroid according to its distance, such that</p><p>$$
\underset{c_i \in C}{\arg\min} ; dist(c_i,x)^2
$$</p><p>As mentioned previously $dist(.)$ is typically the standard ($L^2$) <a href=/distance-metrics.html#euclidean-distance-l2>Euclidean distance</a>.
We define the subset of points assigned to a centroid $i$ as $S_i$.</p><h3 id=centroid-update-step>Centroid update step</h3><p>This step corresponds to updating the centroids using the mean of add points assign to a cluster, $S_i$. That is</p><p>$$
c_i=\frac{1}{|S_i|}\sum_{x_i \in S_i} x_i
$$</p><h2 id=partitioning>Partitioning</h2><p>Different algorithms can be used for cluster partitioning, for instance:</p><ul><li>PAM</li><li>CLARA</li><li>CLARANS</li></ul><h3 id=pam>PAM</h3><p>To illustrate the PAM partitioning method, we will use a synthetic dataset created along the guidelines in <a href=/synthetic-data-generation.html#separability>synthetic data generation</a>.</p><h2 id=elbow-method>Elbow method</h2><p>In order to use the &ldquo;Elbow method&rdquo; we calculate the <a href=/distance-metrics.html#within-cluster-sum-of-squares-wcss>Within-Cluster Sum of Squares (WCSS)</a> for a varying number of clusters, $K$.</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=font-weight:700>import</span> <span style=color:#555>numpy</span> <span style=font-weight:700>as</span> <span style=color:#555>np</span>
</span></span><span style=display:flex><span><span style=font-weight:700>import</span> <span style=color:#555>matplotlib.pyplot</span> <span style=font-weight:700>as</span> <span style=color:#555>plt</span>
</span></span><span style=display:flex><span><span style=font-weight:700>import</span> <span style=color:#555>pandas</span> <span style=font-weight:700>as</span> <span style=color:#555>pd</span> 
</span></span><span style=display:flex><span><span style=font-weight:700>import</span> <span style=color:#555>sklearn</span>
</span></span></code></pre></div><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>dataset <span style=font-weight:700>=</span> pd<span style=font-weight:700>.</span>read_csv(<span style=color:#b84>&#39;../../data/mall-customers.zip&#39;</span>)
</span></span><span style=display:flex><span>X <span style=font-weight:700>=</span> dataset<span style=font-weight:700>.</span>iloc[:, [<span style=color:#099>3</span>, <span style=color:#099>4</span>]]<span style=font-weight:700>.</span>values
</span></span></code></pre></div><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=font-weight:700>from</span> <span style=color:#555>sklearn.cluster</span> <span style=font-weight:700>import</span> KMeans
</span></span><span style=display:flex><span>wcss <span style=font-weight:700>=</span> [] 
</span></span><span style=display:flex><span><span style=font-weight:700>for</span> i <span style=font-weight:700>in</span> <span style=color:#999>range</span>(<span style=color:#099>1</span>, <span style=color:#099>11</span>): 
</span></span><span style=display:flex><span>    kmeans <span style=font-weight:700>=</span> KMeans(n_clusters <span style=font-weight:700>=</span> i, init <span style=font-weight:700>=</span> <span style=color:#b84>&#39;k-means++&#39;</span>, random_state <span style=font-weight:700>=</span> <span style=color:#099>42</span>)
</span></span><span style=display:flex><span>    kmeans<span style=font-weight:700>.</span>fit(X) 
</span></span><span style=display:flex><span>    wcss<span style=font-weight:700>.</span>append(kmeans<span style=font-weight:700>.</span>inertia_)
</span></span></code></pre></div><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=font-weight:700>from</span> <span style=color:#555>plotutils</span> <span style=font-weight:700>import</span> <span style=font-weight:700>*</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>plt<span style=font-weight:700>.</span>plot(<span style=color:#999>range</span>(<span style=color:#099>1</span>, <span style=color:#099>11</span>), wcss)
</span></span><span style=display:flex><span>plt<span style=font-weight:700>.</span>xlabel(<span style=color:#b84>&#39;Number of clusters&#39;</span>)
</span></span><span style=display:flex><span>plt<span style=font-weight:700>.</span>ylabel(<span style=color:#b84>&#39;WCSS&#39;</span>) 
</span></span><span style=display:flex><span>plt<span style=font-weight:700>.</span>show()
</span></span></code></pre></div><figure><img src=/site/Machine%20learning/K-means%20clustering_files/figure-gfm/cell-5-output-1.png alt="K-means clustering_files/figure-gfm/cell-5-output-1.png"></figure><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>kmeans <span style=font-weight:700>=</span> KMeans(n_clusters <span style=font-weight:700>=</span> <span style=color:#099>5</span>, init <span style=font-weight:700>=</span> <span style=color:#b84>&#34;k-means++&#34;</span>, random_state <span style=font-weight:700>=</span> <span style=color:#099>42</span>)
</span></span><span style=display:flex><span>y_kmeans <span style=font-weight:700>=</span> kmeans<span style=font-weight:700>.</span>fit_predict(X)
</span></span><span style=display:flex><span>y_kmeans
</span></span></code></pre></div><pre><code>array([2, 3, 2, 3, 2, 3, 2, 3, 2, 3, 2, 3, 2, 3, 2, 3, 2, 3, 2, 3, 2, 3,
       2, 3, 2, 3, 2, 3, 2, 3, 2, 3, 2, 3, 2, 3, 2, 3, 2, 3, 2, 3, 2, 0,
       2, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 1, 4, 0, 4, 1, 4, 1, 4,
       0, 4, 1, 4, 1, 4, 1, 4, 1, 4, 0, 4, 1, 4, 1, 4, 1, 4, 1, 4, 1, 4,
       1, 4, 1, 4, 1, 4, 1, 4, 1, 4, 1, 4, 1, 4, 1, 4, 1, 4, 1, 4, 1, 4,
       1, 4, 1, 4, 1, 4, 1, 4, 1, 4, 1, 4, 1, 4, 1, 4, 1, 4, 1, 4, 1, 4,
       1, 4], dtype=int32)
</code></pre><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>ps <span style=font-weight:700>=</span> <span style=color:#099>30</span>
</span></span><span style=display:flex><span>plt<span style=font-weight:700>.</span>scatter(X[y_kmeans <span style=font-weight:700>==</span> <span style=color:#099>0</span>, <span style=color:#099>0</span>], X[y_kmeans <span style=font-weight:700>==</span> <span style=color:#099>0</span>, <span style=color:#099>1</span>], s <span style=font-weight:700>=</span> ps, c <span style=font-weight:700>=</span> colours[<span style=color:#099>0</span>], label <span style=font-weight:700>=</span> <span style=color:#b84>&#39;Cluster1&#39;</span>)
</span></span><span style=display:flex><span>plt<span style=font-weight:700>.</span>scatter(X[y_kmeans <span style=font-weight:700>==</span> <span style=color:#099>1</span>, <span style=color:#099>0</span>], X[y_kmeans <span style=font-weight:700>==</span> <span style=color:#099>1</span>, <span style=color:#099>1</span>], s <span style=font-weight:700>=</span> ps, c <span style=font-weight:700>=</span> colours[<span style=color:#099>1</span>], label <span style=font-weight:700>=</span> <span style=color:#b84>&#39;Cluster2&#39;</span>)
</span></span><span style=display:flex><span>plt<span style=font-weight:700>.</span>scatter(X[y_kmeans <span style=font-weight:700>==</span> <span style=color:#099>2</span>, <span style=color:#099>0</span>], X[y_kmeans <span style=font-weight:700>==</span> <span style=color:#099>2</span>, <span style=color:#099>1</span>], s <span style=font-weight:700>=</span> ps, c <span style=font-weight:700>=</span> colours[<span style=color:#099>2</span>], label <span style=font-weight:700>=</span> <span style=color:#b84>&#39;Cluster3&#39;</span>)
</span></span><span style=display:flex><span>plt<span style=font-weight:700>.</span>scatter(X[y_kmeans <span style=font-weight:700>==</span> <span style=color:#099>3</span>, <span style=color:#099>0</span>], X[y_kmeans <span style=font-weight:700>==</span> <span style=color:#099>3</span>, <span style=color:#099>1</span>], s <span style=font-weight:700>=</span> ps, c <span style=font-weight:700>=</span> colours[<span style=color:#099>3</span>], label <span style=font-weight:700>=</span> <span style=color:#b84>&#39;Cluster4&#39;</span>)
</span></span><span style=display:flex><span>plt<span style=font-weight:700>.</span>scatter(X[y_kmeans <span style=font-weight:700>==</span> <span style=color:#099>4</span>, <span style=color:#099>0</span>], X[y_kmeans <span style=font-weight:700>==</span> <span style=color:#099>4</span>, <span style=color:#099>1</span>], s <span style=font-weight:700>=</span> ps, c <span style=font-weight:700>=</span> colours[<span style=color:#099>4</span>], label <span style=font-weight:700>=</span> <span style=color:#b84>&#39;Cluster5&#39;</span>) 
</span></span><span style=display:flex><span>plt<span style=font-weight:700>.</span>scatter(kmeans<span style=font-weight:700>.</span>cluster_centers_[:, <span style=color:#099>0</span>], kmeans<span style=font-weight:700>.</span>cluster_centers_[:, <span style=color:#099>1</span>], s <span style=font-weight:700>=</span> <span style=color:#099>100</span>, c <span style=font-weight:700>=</span> <span style=color:#b84>&#39;black&#39;</span>, label <span style=font-weight:700>=</span> <span style=color:#b84>&#39;Centroids&#39;</span>)
</span></span><span style=display:flex><span>plt<span style=font-weight:700>.</span>xlabel(<span style=color:#b84>&#39;Annual Income (k$)&#39;</span>) 
</span></span><span style=display:flex><span>plt<span style=font-weight:700>.</span>ylabel(<span style=color:#b84>&#39;Spending Score (1-100)&#39;</span>) 
</span></span><span style=display:flex><span>plt<span style=font-weight:700>.</span>legend() 
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>plt<span style=font-weight:700>.</span>show()
</span></span></code></pre></div><figure><img src=/site/Machine%20learning/K-means%20clustering_files/figure-gfm/cell-7-output-1.png alt="K-means clustering_files/figure-gfm/cell-7-output-1.png"></figure></div></article><div id=footer-post-container><div id=footer-post><div id=nav-footer style=display:none><ul><li><a href=/>Home</a></li><li><a href=/map/>All pages</a></li><li><a href=/search.html>Search</a></li></ul></div><div id=toc-footer style=display:none><nav id=TableOfContents><ul><li><a href=#introduction>Introduction</a><ul><li><a href=#data-assignement>Data assignement</a></li><li><a href=#centroid-update-step>Centroid update step</a></li></ul></li><li><a href=#partitioning>Partitioning</a><ul><li><a href=#pam>PAM</a></li></ul></li><li><a href=#elbow-method>Elbow method</a></li></ul></nav></div><div id=share-footer style=display:none></div><div id=actions-footer><a id=menu-toggle class=icon href=# onclick='return $("#nav-footer").toggle(),!1' aria-label=Menu><i class="fas fa-bars fa-lg" aria-hidden=true></i> Menu</a>
<a id=toc-toggle class=icon href=# onclick='return $("#toc-footer").toggle(),!1' aria-label=TOC><i class="fas fa-list fa-lg" aria-hidden=true></i> TOC</a>
<a id=share-toggle class=icon href=# onclick='return $("#share-footer").toggle(),!1' aria-label=Share><i class="fas fa-share-alt fa-lg" aria-hidden=true></i> share</a>
<a id=top style=display:none class=icon href=# onclick='$("html, body").animate({scrollTop:0},"fast")' aria-label="Top of Page"><i class="fas fa-chevron-up fa-lg" aria-hidden=true></i> Top</a></div></div></div><footer id=footer><div class=footer-left>Copyright &copy; 2023 Rui Vieira</div><div class=footer-right><nav><ul><li><a href=/>Home</a></li><li><a href=/map/>All pages</a></li><li><a href=/search.html>Search</a></li></ul></nav></div></footer></div></body><link rel=stylesheet href=/css/fa.min.css><script src=/js/jquery-3.6.0.min.js></script>
<script src=/js/mark.min.js></script>
<script src=/js/main.js></script>
<script>MathJax={tex:{inlineMath:[["$","$"],["\\(","\\)"]]},svg:{fontCache:"global"}}</script><script type=text/javascript id=MathJax-script async src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js></script></html>