<!doctype html><html lang=en-uk><head><script data-goatcounter=https://ruivieira-dev.goatcounter.com/count async src=//gc.zgo.at/count.js></script>
<script src=https://unpkg.com/@alpinejs/intersect@3.x.x/dist/cdn.min.js></script>
<script src=https://unpkg.com/alpinejs@3.x.x/dist/cdn.min.js></script>
<script type=module src=/js/deeplinks/deeplinks.js></script>
<link rel=preload href=/lib/fonts/fa-brands-400.woff2 as=font type=font/woff2 crossorigin=anonymous><link rel=preload href=/lib/fonts/fa-regular-400.woff2 as=font type=font/woff2 crossorigin=anonymous><link rel=preload href=/lib/fonts/fa-solid-900.woff2 as=font type=font/woff2 crossorigin=anonymous><link rel=preload href=/fonts/firacode/FiraCode-Regular.woff2 as=font type=font/woff2 crossorigin=anonymous><link rel=preload href=/fonts/vollkorn/Vollkorn-Regular.woff2 as=font type=font/woff2 crossorigin=anonymous><link rel=stylesheet href=/css/kbd.css type=text/css><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><title>Model performance metrics Â· Rui Vieira</title><link rel=canonical href=/model-performance-metrics.html><meta name=viewport content="width=device-width,initial-scale=1"><meta name=robots content="all,follow"><meta name=googlebot content="index,follow,snippet,archive"><meta property="og:title" content="Model performance metrics"><meta property="og:description" content="Difference in Positive Proportions in Predicted Labels (DPPL)DPPL (Difference in Positive Proportions in Predicted Labels) is a metric used to evaluate the performance of machine learning models in imbalanced datasets. It measures the difference in the proportion of positive predictions made by the model between the minority class and the majority class.
A low value of DPPL indicates that the model is able to make similar positive predictions for instances of the minority class and instances of the majority class, which is desirable in imbalanced datasets where it is important to ensure that the minority class is not overlooked."><meta property="og:type" content="article"><meta property="og:url" content="/model-performance-metrics.html"><meta property="article:section" content="posts"><meta property="article:modified_time" content="2023-05-28T11:45:56+01:00"><meta name=twitter:card content="summary"><meta name=twitter:title content="Model performance metrics"><meta name=twitter:description content="Difference in Positive Proportions in Predicted Labels (DPPL)DPPL (Difference in Positive Proportions in Predicted Labels) is a metric used to evaluate the performance of machine learning models in imbalanced datasets. It measures the difference in the proportion of positive predictions made by the model between the minority class and the majority class.
A low value of DPPL indicates that the model is able to make similar positive predictions for instances of the minority class and instances of the majority class, which is desirable in imbalanced datasets where it is important to ensure that the minority class is not overlooked."><link rel=stylesheet href=/css/styles.css><!--[if lt IE 9]><script src=https://oss.maxcdn.com/html5shiv/3.7.2/html5shiv.min.js></script>
<script src=https://oss.maxcdn.com/respond/1.4.2/respond.min.js></script><![endif]--><link rel=icon type=image/png href=images/favicon.ico></head><body class="max-width mx-auto px3 ltr" x-data="{currentHeading: undefined}"><div class="content index py4"><div id=header-post><a id=menu-icon href=#><i class="fas fa-eye fa-lg"></i></a>
<a id=menu-icon-tablet href=#><i class="fas fa-eye fa-lg"></i></a>
<a id=top-icon-tablet href=# onclick='$("html, body").animate({scrollTop:0},"fast")' style=display:none aria-label="Top of Page"><i class="fas fa-chevron-up fa-lg"></i></a>
<span id=menu><span id=nav><ul><li><a href=/>Home</a></li><li><a href=/blog/>Blog</a></li><li><a href=/draw/>Drawings</a></li><li><a href=/map/>All pages</a></li><li><a href=/search.html>Search</a></li></ul></span><br><div id=share style=display:none></div><div id=toc><h4>Contents</h4><nav id=TableOfContents><ul><li><a href=#difference-in-positive-proportions-in-predicted-labels-dppl :class="{'toc-h2':true, 'toc-highlight': currentHeading == '#difference-in-positive-proportions-in-predicted-labels-dppl' }">Difference in Positive Proportions in Predicted Labels (DPPL)</a></li><li><a href=#model-quality :class="{'toc-h2':true, 'toc-highlight': currentHeading == '#model-quality' }">Model quality</a></li></ul></nav><h4>Related</h4><nav><ul><li class="header-post toc"><span class=backlink-count>1</span>
<a href=/machine-learning.html>Machine Learning</a></li></ul></nav></div></span></div><article class=post itemscope itemtype=http://schema.org/BlogPosting><header><h1 class=posttitle itemprop="name headline">Model performance metrics</h1><div class=meta><div class=postdate>Updated <time datetime="2023-05-28 11:45:56 +0100 BST" itemprop=datePublished>2023-05-28</time>
<span class=commit-hash>(<a href=/log/index.html#1454e0a>1454e0a</a>)</span></div></div></header><div class=content itemprop=articleBody><h2 id=difference-in-positive-proportions-in-predicted-labels-dppl x-intersect="currentHeading = '#difference-in-positive-proportions-in-predicted-labels-dppl'">Difference in Positive Proportions in Predicted Labels (DPPL)</h2><p>DPPL (Difference in Positive Proportions in Predicted Labels) is a metric used to evaluate the performance of <a href=/machine-learning.html>machine learning</a> models in imbalanced datasets. It measures the difference in the proportion of positive predictions made by the model between the minority class and the majority class.</p><p>A low value of DPPL indicates that the model is able to make similar positive predictions for instances of the minority class and instances of the majority class, which is desirable in imbalanced datasets where it is important to ensure that the minority class is not overlooked.</p><p>It is a classification-specific metric, as it is only applicable to models that perform binary or multi-class classification. It assesses the model&rsquo;s ability to make positive predictions for instances of the minority class similar to the instances of the majority class, which is important in imbalanced datasets where the minority class is often under-represented.</p><p>DPPL is also a threshold-independent metric, which means that it does not depend on the specific threshold used to make binary predictions. This makes it useful in situations where the threshold used to make predictions may need to be adjusted depending on the specific use case.</p><p>The formula for the DPPL metric is as follows:</p><p>$$
DPPL = \vert p_{m} - p_M \vert
$$</p><p>where:</p><ul><li><p>$p_m$ is the proportion of positive predictions made by the model for instances of the minority class</p></li><li><p>$p_M$ is the proportion of positive predictions made by the model for instances of the majority class</p><p>It calculates the absolute difference between the proportion of positive predictions made by the model for the minority class and the majority class.</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=font-weight:700>import</span> <span style=color:#555>pandas</span> <span style=font-weight:700>as</span> <span style=color:#555>pd</span>
</span></span><span style=display:flex><span><span style=font-weight:700>import</span> <span style=color:#555>numpy</span> <span style=font-weight:700>as</span> <span style=color:#555>np</span>
</span></span><span style=display:flex><span><span style=color:#998;font-style:italic># assume data is a pandas dataframe with the columns &#39;label&#39; and &#39;prediction&#39;</span>
</span></span><span style=display:flex><span><span style=color:#998;font-style:italic># calculate the proportion of positive predictions for the minority class</span>
</span></span><span style=display:flex><span>p_minority <span style=font-weight:700>=</span> data[data[<span style=color:#b84>&#39;label&#39;</span>] <span style=font-weight:700>==</span> minority_class][<span style=color:#b84>&#39;prediction&#39;</span>]<span style=font-weight:700>.</span>mean()
</span></span><span style=display:flex><span><span style=color:#998;font-style:italic># calculate the proportion of positive predictions for the majority class</span>
</span></span><span style=display:flex><span>p_majority <span style=font-weight:700>=</span> data[data[<span style=color:#b84>&#39;label&#39;</span>] <span style=font-weight:700>==</span> majority_class][<span style=color:#b84>&#39;prediction&#39;</span>]<span style=font-weight:700>.</span>mean()
</span></span><span style=display:flex><span><span style=color:#998;font-style:italic># calculate DPPL</span>
</span></span><span style=display:flex><span>DPPL <span style=font-weight:700>=</span> <span style=color:#999>abs</span>(p_minority <span style=font-weight:700>-</span> p_majority)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#999>print</span>(<span style=color:#b84>&#34;DPPL: &#34;</span>, DPPL)
</span></span></code></pre></div></li></ul><h2 id=model-quality x-intersect="currentHeading = '#model-quality'">Model quality</h2><ul><li>Model quality in machine learning can be measured using various metrics depending on the type of problem you are trying to solve. Here are some common metrics for evaluating model quality:<ul><li><strong>Accuracy</strong>: This metric measures the percentage of correct predictions made by the model on the test data.</li><li><strong>Precision</strong>: This metric measures the proportion of true positive predictions out of all the positive predictions made by the model.</li><li><strong>Recall</strong>: This metric measures the proportion of true positive predictions out of all the actual positive instances in the test data.</li><li><strong>F1 score</strong>: This metric is the harmonic mean of precision and recall and is used when both precision and recall are important.</li><li><strong>Mean Squared Error (MSE)</strong>: This metric is used to evaluate the performance of regression models. It measures the average of the squared differences between the predicted values and the actual values.</li><li><strong>Mean Absolute Error (MAE)</strong>: This metric is used to evaluate the performance of regression models. It measures the average of the absolute differences between the predicted values and the actual values.</li><li><strong>Area Under the Receiver Operating Characteristic Curve (AUC-ROC)</strong>: This metric is used to evaluate the performance of binary classification models. It measures the trade-off between the true positive rate and the false positive rate.</li><li><strong>Mean Average Precision (mAP)</strong>: This metric is used to evaluate the performance of object detection and image segmentation models. It measures the average precision across all classes.</li></ul></li><li>When selecting a metric to measure model quality, consider the problem type, the desired outcome, and the specific needs of your project. It&rsquo;s also important to keep in mind that a single metric may not be sufficient to evaluate the performance of a model, so it&rsquo;s a good idea to consider multiple metrics when evaluating model quality.</li></ul></div></article><div id=footer-post-container><div id=footer-post><div id=nav-footer style=display:none><ul><li><a href=/>Home</a></li><li><a href=/blog/>Blog</a></li><li><a href=/draw/>Drawings</a></li><li><a href=/map/>All pages</a></li><li><a href=/search.html>Search</a></li></ul></div><div id=toc-footer style=display:none><nav id=TableOfContents><ul><li><a href=#difference-in-positive-proportions-in-predicted-labels-dppl>Difference in Positive Proportions in Predicted Labels (DPPL)</a></li><li><a href=#model-quality>Model quality</a></li></ul></nav></div><div id=share-footer style=display:none></div><div id=actions-footer><a id=menu-toggle class=icon href=# onclick='return $("#nav-footer").toggle(),!1' aria-label=Menu><i class="fas fa-bars fa-lg" aria-hidden=true></i> Menu</a>
<a id=toc-toggle class=icon href=# onclick='return $("#toc-footer").toggle(),!1' aria-label=TOC><i class="fas fa-list fa-lg" aria-hidden=true></i> TOC</a>
<a id=share-toggle class=icon href=# onclick='return $("#share-footer").toggle(),!1' aria-label=Share><i class="fas fa-share-alt fa-lg" aria-hidden=true></i> share</a>
<a id=top style=display:none class=icon href=# onclick='$("html, body").animate({scrollTop:0},"fast")' aria-label="Top of Page"><i class="fas fa-chevron-up fa-lg" aria-hidden=true></i> Top</a></div></div></div><footer id=footer><div class=footer-left>Copyright &copy; 2023 Rui Vieira</div><div class=footer-right><nav><ul><li><a href=/>Home</a></li><li><a href=/blog/>Blog</a></li><li><a href=/draw/>Drawings</a></li><li><a href=/map/>All pages</a></li><li><a href=/search.html>Search</a></li></ul></nav></div></footer></div></body><link rel=stylesheet href=/css/fa.min.css><script src=/js/jquery-3.6.0.min.js></script>
<script src=/js/mark.min.js></script>
<script src=/js/main.js></script>
<script>MathJax={tex:{inlineMath:[["$","$"],["\\(","\\)"]]},svg:{fontCache:"global"}}</script><script type=text/javascript id=MathJax-script async src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js></script></html>