<!doctype html><html lang=en-uk><head><script data-goatcounter=https://ruivieira-dev.goatcounter.com/count async src=//gc.zgo.at/count.js></script>
<script src=https://unpkg.com/@alpinejs/intersect@3.x.x/dist/cdn.min.js></script>
<script src=https://unpkg.com/alpinejs@3.x.x/dist/cdn.min.js></script>
<script type=module src=https://ruivieira.dev/js/deeplinks/deeplinks.js></script>
<link rel=preload href=https://ruivieira.dev/lib/fonts/fa-brands-400.woff2 as=font type=font/woff2 crossorigin=anonymous><link rel=preload href=https://ruivieira.dev/lib/fonts/fa-regular-400.woff2 as=font type=font/woff2 crossorigin=anonymous><link rel=preload href=https://ruivieira.dev/lib/fonts/fa-solid-900.woff2 as=font type=font/woff2 crossorigin=anonymous><link rel=preload href=https://ruivieira.dev/fonts/firacode/FiraCode-Regular.woff2 as=font type=font/woff2 crossorigin=anonymous><link rel=preload href=https://ruivieira.dev/fonts/vollkorn/Vollkorn-Regular.woff2 as=font type=font/woff2 crossorigin=anonymous><link rel=stylesheet href=https://ruivieira.dev/css/kbd.css type=text/css><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><title>ModelMesh Â· Rui Vieira</title><link rel=canonical href=https://ruivieira.dev/modelmesh.html><meta name=viewport content="width=device-width,initial-scale=1"><meta name=robots content="all,follow"><meta name=googlebot content="index,follow,snippet,archive"><meta property="og:title" content="ModelMesh"><meta property="og:description" content="This is originally at https://github.com/kserve/modelmesh-serving/blob/main/docs/quickstart.md
Getting startedTo quickly get started using ModelMesh Serving, here is a brief guide.
Prerequisites A Kubernetes cluster v 1.16+1 with cluster administrative privileges kubectl2 and kustomize3 (v3.2.0+) At least 4 vCPU and 8 GB memory4. For more details, please see the deployed components section. 1. Install ModelMesh ServingGet the latest releaseRELEASE=release-0.10 git clone -b $RELEASE --depth 1 --single-branch https://github.com/kserve/modelmesh-serving.git cd modelmesh-serving Run install scriptkubectl create namespace modelmesh-serving ."><meta property="og:type" content="article"><meta property="og:url" content="https://ruivieira.dev/modelmesh.html"><meta property="article:section" content="posts"><meta property="article:modified_time" content="2023-09-02T17:28:34+01:00"><meta name=twitter:card content="summary"><meta name=twitter:title content="ModelMesh"><meta name=twitter:description content="This is originally at https://github.com/kserve/modelmesh-serving/blob/main/docs/quickstart.md
Getting startedTo quickly get started using ModelMesh Serving, here is a brief guide.
Prerequisites A Kubernetes cluster v 1.16+1 with cluster administrative privileges kubectl2 and kustomize3 (v3.2.0+) At least 4 vCPU and 8 GB memory4. For more details, please see the deployed components section. 1. Install ModelMesh ServingGet the latest releaseRELEASE=release-0.10 git clone -b $RELEASE --depth 1 --single-branch https://github.com/kserve/modelmesh-serving.git cd modelmesh-serving Run install scriptkubectl create namespace modelmesh-serving ."><link rel=stylesheet href=https://ruivieira.dev/css/styles.css><!--[if lt IE 9]><script src=https://oss.maxcdn.com/html5shiv/3.7.2/html5shiv.min.js></script>
<script src=https://oss.maxcdn.com/respond/1.4.2/respond.min.js></script><![endif]--><link rel=icon type=image/png href=https://ruivieira.dev/images/favicon.ico></head><body class="max-width mx-auto px3 ltr" x-data="{currentHeading: undefined}"><div class="content index py4"><div id=header-post><a id=menu-icon href=#><i class="fas fa-eye fa-lg"></i></a>
<a id=menu-icon-tablet href=#><i class="fas fa-eye fa-lg"></i></a>
<a id=top-icon-tablet href=# onclick='$("html, body").animate({scrollTop:0},"fast")' style=display:none aria-label="Top of Page"><i class="fas fa-chevron-up fa-lg"></i></a>
<span id=menu><span id=nav><ul><li><a href=https://ruivieira.dev/>Home</a></li><li><a href=https://ruivieira.dev/blog/>Blog</a></li><li><a href=https://ruivieira.dev/draw/>Drawings</a></li><li><a href=https://ruivieira.dev/map/>All pages</a></li><li><a href=https://ruivieira.dev/search.html>Search</a></li></ul></span><br><div id=share style=display:none></div><div id=toc><h4>Contents</h4><nav id=TableOfContents><ul><li><a href=#getting-started :class="{'toc-h2':true, 'toc-highlight': currentHeading == '#getting-started' }">Getting started</a></li><li><a href=#prerequisites :class="{'toc-h3':true, 'toc-highlight': currentHeading == '#prerequisites' }">Prerequisites</a></li><li><a href=#1-install-modelmesh-serving :class="{'toc-h3':true, 'toc-highlight': currentHeading == '#1-install-modelmesh-serving' }">1. Install ModelMesh Serving</a></li><li><a href=#get-the-latest-release :class="{'toc-h4':true, 'toc-highlight': currentHeading == '#get-the-latest-release' }">Get the latest release</a></li><li><a href=#run-install-script :class="{'toc-h4':true, 'toc-highlight': currentHeading == '#run-install-script' }">Run install script</a></li><li><a href=#verify-installation :class="{'toc-h4':true, 'toc-highlight': currentHeading == '#verify-installation' }">Verify installation</a></li><li><a href=#2-deploy-a-model :class="{'toc-h3':true, 'toc-highlight': currentHeading == '#2-deploy-a-model' }">2. Deploy a model</a></li><li><a href=#3-perform-an-inference-request :class="{'toc-h3':true, 'toc-highlight': currentHeading == '#3-perform-an-inference-request' }">3. Perform an inference request</a></li><li><a href=#grpc-request :class="{'toc-h4':true, 'toc-highlight': currentHeading == '#grpc-request' }">gRPC request</a></li><li><a href=#rest-request :class="{'toc-h4':true, 'toc-highlight': currentHeading == '#rest-request' }">REST request</a></li><li><a href=#4-optional-deleting-your-modelmesh-serving-installation :class="{'toc-h3':true, 'toc-highlight': currentHeading == '#4-optional-deleting-your-modelmesh-serving-installation' }">4. (Optional) Deleting your ModelMesh Serving installation</a></li><li><a href=#implementing-a-custom-serving-runtime :class="{'toc-h2':true, 'toc-highlight': currentHeading == '#implementing-a-custom-serving-runtime' }">Implementing a Custom Serving Runtime</a></li><li><a href=#model-server-management-spi :class="{'toc-h3':true, 'toc-highlight': currentHeading == '#model-server-management-spi' }">Model server Management SPI</a></li><li><a href=#model-sizing :class="{'toc-h4':true, 'toc-highlight': currentHeading == '#model-sizing' }">Model sizing</a></li><li><a href=#runtimestatus :class="{'toc-h4':true, 'toc-highlight': currentHeading == '#runtimestatus' }">runtimeStatus</a></li><li><a href=#loadmodel :class="{'toc-h4':true, 'toc-highlight': currentHeading == '#loadmodel' }">loadModel</a></li><li><a href=#unloadmodel :class="{'toc-h4':true, 'toc-highlight': currentHeading == '#unloadmodel' }">unloadModel</a></li><li><a href=#inferencing :class="{'toc-h4':true, 'toc-highlight': currentHeading == '#inferencing' }">Inferencing</a></li><li><a href=#deploying-a-runtime :class="{'toc-h3':true, 'toc-highlight': currentHeading == '#deploying-a-runtime' }">Deploying a Runtime</a></li><li><a href=#runtime-container-resource-allocations :class="{'toc-h4':true, 'toc-highlight': currentHeading == '#runtime-container-resource-allocations' }">Runtime container resource allocations</a></li><li><a href=#integrating-with-existing-model-servers :class="{'toc-h2':true, 'toc-highlight': currentHeading == '#integrating-with-existing-model-servers' }">Integrating with existing model servers</a></li><li><a href=#reference :class="{'toc-h3':true, 'toc-highlight': currentHeading == '#reference' }">Reference</a></li><li><a href=#spec-attributes :class="{'toc-h4':true, 'toc-highlight': currentHeading == '#spec-attributes' }">Spec Attributes</a></li><li><a href=#endpoint-formats :class="{'toc-h4':true, 'toc-highlight': currentHeading == '#endpoint-formats' }">Endpoint formats</a></li><li><a href=#full-example :class="{'toc-h4':true, 'toc-highlight': currentHeading == '#full-example' }">Full Example</a></li><li><a href=#storage-helper :class="{'toc-h4':true, 'toc-highlight': currentHeading == '#storage-helper' }">Storage Helper</a></li><li><a href=#installation :class="{'toc-h2':true, 'toc-highlight': currentHeading == '#installation' }">Installation</a></li><li><a href=#prerequisites :class="{'toc-h3':true, 'toc-highlight': currentHeading == '#prerequisites' }">Prerequisites</a></li><li><a href=#setup-the-etcd-connection-information :class="{'toc-h3':true, 'toc-highlight': currentHeading == '#setup-the-etcd-connection-information' }">Setup the etcd connection information</a></li><li><a href=#installation :class="{'toc-h3':true, 'toc-highlight': currentHeading == '#installation' }">Installation</a></li><li><a href=#setup-additional-namespaces :class="{'toc-h3':true, 'toc-highlight': currentHeading == '#setup-additional-namespaces' }">Setup additional namespaces</a></li><li><a href=#delete-the-installation :class="{'toc-h3':true, 'toc-highlight': currentHeading == '#delete-the-installation' }">Delete the installation</a></li></ul></nav><h4>Related</h4><nav><ul><li class="header-post toc"><span class=backlink-count>2</span>
<a href=https://ruivieira.dev/model-serving.html>Model serving</a></li></ul></nav></div></span></div><article class=post itemscope itemtype=http://schema.org/BlogPosting><header><h1 class=posttitle itemprop="name headline">ModelMesh</h1><div class=meta><div class=postdate>Updated <time datetime="2023-09-02 17:28:34 +0100 BST" itemprop=datePublished>2023-09-02</time>
<span class=commit-hash>(<a href=https://ruivieira.dev/log/index.html#d64c4a5>d64c4a5</a>)</span></div></div></header><div class=content itemprop=articleBody><p><strong>This is originally at <a href=https://github.com/kserve/modelmesh-serving/blob/main/docs/quickstart.md>https://github.com/kserve/modelmesh-serving/blob/main/docs/quickstart.md</a></strong></p><h2 id=getting-started x-intersect="currentHeading = '#getting-started'">Getting started</h2><p>To quickly get started using ModelMesh Serving, here is a brief guide.</p><h3 id=prerequisites x-intersect="currentHeading = '#prerequisites'">Prerequisites</h3><ul><li>A <a href=https://ruivieira.dev/kubernetes.html>Kubernetes</a> cluster v 1.16+<sup id=fnref:1><a href=#fn:1 class=footnote-ref role=doc-noteref>1</a></sup> with cluster administrative privileges</li><li><code>kubectl</code><sup id=fnref:2><a href=#fn:2 class=footnote-ref role=doc-noteref>2</a></sup> and <code>kustomize</code><sup id=fnref:3><a href=#fn:3 class=footnote-ref role=doc-noteref>3</a></sup> (v3.2.0+)</li><li>At least 4 vCPU and 8 GB memory<sup id=fnref:4><a href=#fn:4 class=footnote-ref role=doc-noteref>4</a></sup>. For more details, please see <a href=https://github.com/kserve/modelmesh-serving/blob/main/docs/install/README.md#deployed-components>the deployed components section</a>.</li></ul><h3 id=1-install-modelmesh-serving x-intersect="currentHeading = '#1-install-modelmesh-serving'">1. Install ModelMesh Serving</h3><h4 id=get-the-latest-release x-intersect="currentHeading = '#get-the-latest-release'">Get the latest release</h4><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span><span style=color:teal>RELEASE</span><span style=font-weight:700>=</span>release-0.10
</span></span><span style=display:flex><span>git clone -b <span style=color:teal>$RELEASE</span> --depth <span style=color:#099>1</span> --single-branch https://github.com/kserve/modelmesh-serving.git
</span></span><span style=display:flex><span><span style=color:#999>cd</span> modelmesh-serving
</span></span></code></pre></div><h4 id=run-install-script x-intersect="currentHeading = '#run-install-script'">Run install script</h4><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>kubectl create namespace modelmesh-serving
</span></span><span style=display:flex><span>./scripts/install.sh --namespace-scope-mode --namespace modelmesh-serving --quickstart
</span></span></code></pre></div><p>This will install ModelMesh Serving in the <code>modelmesh-serving</code> namespace, along with an <code>etcd</code> and <code>MinIO</code> instances.
Eventually after running this script, you should see a <code>Successfully installed ModelMesh Serving!</code> message.</p><blockquote><p>[!Note]
These etcd and MinIO deployments are intended for development/experimentation and not for production.</p></blockquote><h4 id=verify-installation x-intersect="currentHeading = '#verify-installation'">Verify installation</h4><p>Check that the pods are running:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>kubectl get pods
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>NAME                                        READY   STATUS    RESTARTS   AGE
</span></span><span style=display:flex><span>pod/etcd                                    1/1     Running   <span style=color:#099>0</span>          5m
</span></span><span style=display:flex><span>pod/minio                                   1/1     Running   <span style=color:#099>0</span>          5m
</span></span><span style=display:flex><span>pod/modelmesh-controller-547bfb64dc-mrgrq   1/1     Running   <span style=color:#099>0</span>          5m
</span></span></code></pre></div><p>Check that the <code>ServingRuntime</code>s are available:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>kubectl get servingruntimes
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>NAME           DISABLED   MODELTYPE    CONTAINERS   AGE
</span></span><span style=display:flex><span>mlserver-0.x              sklearn      mlserver     5m
</span></span><span style=display:flex><span>ovms-1.x                  openvino_ir  ovms         5m
</span></span><span style=display:flex><span>torchserve-0.x            pytorch-mar  torchserve   5m
</span></span><span style=display:flex><span>triton-2.x                tensorflow   triton       5m
</span></span></code></pre></div><p><code>ServingRuntime</code>s are automatically provisioned based on the framework of the model deployed.
Three <code>ServingRuntime</code>s are included with ModelMesh Serving by default. The current mappings for these
are:</p><table><thead><tr><th>ServingRuntime</th><th>Supported Frameworks</th></tr></thead><tbody><tr><td>mlserver-0.x</td><td>sklearn, xgboost, lightgbm</td></tr><tr><td>ovms-1.x</td><td>openvino_ir, onnx</td></tr><tr><td>torchserve-0.x</td><td>pytorch-mar</td></tr><tr><td>triton-2.x</td><td>tensorflow, pytorch, onnx, tensorrt</td></tr></tbody></table><h3 id=2-deploy-a-model x-intersect="currentHeading = '#2-deploy-a-model'">2. Deploy a model</h3><p>With ModelMesh Serving now installed, try deploying a model using the KServe <code>InferenceService</code> CRD.</p><blockquote><p>[!Note]
While both the KServe controller and ModelMesh controller will reconcile <code>InferenceService</code> resources, the ModelMesh controller will
only handle those <code>InferenceService</code>s with the <code>serving.kserve.io/deploymentMode: ModelMesh</code> annotation. Otherwise, the KServe controller will
handle reconciliation. Likewise, the KServe controller will not reconcile an <code>InferenceService</code> with the <code>serving.kserve.io/deploymentMode: ModelMesh</code>
annotation, and will defer under the assumption that the ModelMesh controller will handle it.</p></blockquote><p>Here, we deploy an <a href>pages/site/Scikit-learn</a> MNIST model which is served from the local MinIO container:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>kubectl apply -f - <span style=color:#b84>&lt;&lt;EOF
</span></span></span><span style=display:flex><span><span style=color:#b84>apiVersion: serving.kserve.io/v1beta1
</span></span></span><span style=display:flex><span><span style=color:#b84>kind: InferenceService
</span></span></span><span style=display:flex><span><span style=color:#b84>metadata:
</span></span></span><span style=display:flex><span><span style=color:#b84>  name: example-sklearn-isvc
</span></span></span><span style=display:flex><span><span style=color:#b84>  annotations:
</span></span></span><span style=display:flex><span><span style=color:#b84>    serving.kserve.io/deploymentMode: ModelMesh
</span></span></span><span style=display:flex><span><span style=color:#b84>spec:
</span></span></span><span style=display:flex><span><span style=color:#b84>  predictor:
</span></span></span><span style=display:flex><span><span style=color:#b84>    model:
</span></span></span><span style=display:flex><span><span style=color:#b84>      modelFormat:
</span></span></span><span style=display:flex><span><span style=color:#b84>        name: sklearn
</span></span></span><span style=display:flex><span><span style=color:#b84>      storage:
</span></span></span><span style=display:flex><span><span style=color:#b84>        key: localMinIO
</span></span></span><span style=display:flex><span><span style=color:#b84>        path: sklearn/mnist-svm.joblib
</span></span></span><span style=display:flex><span><span style=color:#b84>EOF</span>
</span></span></code></pre></div><p>Note: the above YAML uses the <code>InferenceService</code> predictor <a href=https://github.com/kserve/kserve/tree/master/docs/samples/storage/storageSpec>storage spec</a>. You can also continue
using the <code>storageUri</code> field in lieu of the storage spec:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>kubectl apply -f - <span style=color:#b84>&lt;&lt;EOF
</span></span></span><span style=display:flex><span><span style=color:#b84>apiVersion: serving.kserve.io/v1beta1
</span></span></span><span style=display:flex><span><span style=color:#b84>kind: InferenceService
</span></span></span><span style=display:flex><span><span style=color:#b84>metadata:
</span></span></span><span style=display:flex><span><span style=color:#b84>  name: example-sklearn-isvc
</span></span></span><span style=display:flex><span><span style=color:#b84>  annotations:
</span></span></span><span style=display:flex><span><span style=color:#b84>    serving.kserve.io/deploymentMode: ModelMesh
</span></span></span><span style=display:flex><span><span style=color:#b84>    serving.kserve.io/secretKey: localMinIO
</span></span></span><span style=display:flex><span><span style=color:#b84>spec:
</span></span></span><span style=display:flex><span><span style=color:#b84>  predictor:
</span></span></span><span style=display:flex><span><span style=color:#b84>    model:
</span></span></span><span style=display:flex><span><span style=color:#b84>      modelFormat:
</span></span></span><span style=display:flex><span><span style=color:#b84>        name: sklearn
</span></span></span><span style=display:flex><span><span style=color:#b84>      storageUri: s3://modelmesh-example-models/sklearn/mnist-svm.joblib
</span></span></span><span style=display:flex><span><span style=color:#b84>EOF</span>
</span></span></code></pre></div><p>After applying this <code>InferenceService</code>, you should see that it is likely not yet ready.</p><pre tabindex=0><code>kubectl get isvc

NAME                    URL   READY   PREV   LATEST   PREVROLLEDOUTREVISION   LATESTREADYREVISION   AGE
example-sklearn-isvc          False                                                                 3s
</code></pre><p>Eventually, you should see the <code>ServingRuntime</code> pods that will hold the SKLearn model become <code>Running</code>.</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>kubectl get pods
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>...
</span></span><span style=display:flex><span>modelmesh-serving-mlserver-0.x-7db675f677-twrwd   3/3     Running   <span style=color:#099>0</span>          2m
</span></span><span style=display:flex><span>modelmesh-serving-mlserver-0.x-7db675f677-xvd8q   3/3     Running   <span style=color:#099>0</span>          2m
</span></span></code></pre></div><p>Then, checking on the <code>InferenceService</code> again, you should see that the one we deployed is now ready with a provided URL:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>kubectl get isvc
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>NAME                    URL                                               READY   PREV   LATEST   PREVROLLEDOUTREVISION   LATESTREADYREVISION   AGE
</span></span><span style=display:flex><span>example-sklearn-isvc    grpc://modelmesh-serving.modelmesh-serving:8033   True                                                                  97s
</span></span></code></pre></div><p>You can describe the <code>InferenceService</code> to get more status information:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>kubectl describe isvc example-sklearn-isvc
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>Name:         example-sklearn-isvc
</span></span><span style=display:flex><span>...
</span></span><span style=display:flex><span>Status:
</span></span><span style=display:flex><span>  Components:
</span></span><span style=display:flex><span>    Predictor:
</span></span><span style=display:flex><span>      Grpc URL:  grpc://modelmesh-serving.modelmesh-serving:8033
</span></span><span style=display:flex><span>      Rest URL:  http://modelmesh-serving.modelmesh-serving:8008
</span></span><span style=display:flex><span>      URL:       grpc://modelmesh-serving.modelmesh-serving:8033
</span></span><span style=display:flex><span>  Conditions:
</span></span><span style=display:flex><span>    Last Transition Time:  2022-07-18T18:01:54Z
</span></span><span style=display:flex><span>    Status:                True
</span></span><span style=display:flex><span>    Type:                  PredictorReady
</span></span><span style=display:flex><span>    Last Transition Time:  2022-07-18T18:01:54Z
</span></span><span style=display:flex><span>    Status:                True
</span></span><span style=display:flex><span>    Type:                  Ready
</span></span><span style=display:flex><span>  Model Status:
</span></span><span style=display:flex><span>    Copies:
</span></span><span style=display:flex><span>      Failed Copies:  <span style=color:#099>0</span>
</span></span><span style=display:flex><span>      Total Copies:   <span style=color:#099>2</span>
</span></span><span style=display:flex><span>    States:
</span></span><span style=display:flex><span>      Active Model State:  Loaded
</span></span><span style=display:flex><span>      Target Model State:
</span></span><span style=display:flex><span>    Transition Status:     UpToDate
</span></span><span style=display:flex><span>  URL:                     grpc://modelmesh-serving.modelmesh-serving:8033
</span></span><span style=display:flex><span>...
</span></span></code></pre></div><h3 id=3-perform-an-inference-request x-intersect="currentHeading = '#3-perform-an-inference-request'">3. Perform an inference request</h3><p>Now that a model is loaded and available, you can then perform inference.
Currently, only gRPC inference requests are supported by ModelMesh, but REST support is enabled via a <a href=https://github.com/kserve/rest-proxy>REST proxy</a> container. By default, ModelMesh Serving uses a
<a href=https://kubernetes.io/docs/concepts/services-networking/service/#headless-services>headless Service</a>
since a normal Service has issues load balancing gRPC requests. See more info
<a href=https://kubernetes.io/blog/2018/11/07/grpc-load-balancing-on-kubernetes-without-tears/>here</a>.</p><h4 id=grpc-request x-intersect="currentHeading = '#grpc-request'">gRPC request</h4><p>To test out <strong>gRPC</strong> inference requests, you can port-forward the headless service <em>in a separate terminal window</em>:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>kubectl port-forward --address 0.0.0.0 service/modelmesh-serving <span style=color:#099>8033</span> -n modelmesh-serving
</span></span></code></pre></div><p>Then a gRPC client generated from the <a href>KServe notes</a> <a href=https://github.com/kserve/kserve/blob/master/docs/predict-api/v2/grpc_predict_v2.proto>grpc_predict_v2.proto</a>
file can be used with <code>localhost:8033</code>. A ready-to-use Python example of this can be found <a href=https://github.com/pvaneck/model-serving-sandbox/tree/main/grpc-predict>here</a>.</p><p>Alternatively, you can test inference with <a href=https://github.com/fullstorydev/grpcurl>grpcurl</a>. This can easily be installed with <code>brew install grpcurl</code> if on macOS.</p><p>With <code>grpcurl</code>, a request can be sent to the SKLearn MNIST model like the following. Make sure that the <code>MODEL_NAME</code>
variable below is set to the name of your <code>InferenceService</code>.</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span><span style=color:teal>MODEL_NAME</span><span style=font-weight:700>=</span>example-sklearn-isvc
</span></span><span style=display:flex><span>grpcurl <span style=color:#b84>\
</span></span></span><span style=display:flex><span><span style=color:#b84></span>  -plaintext <span style=color:#b84>\
</span></span></span><span style=display:flex><span><span style=color:#b84></span>  -proto fvt/proto/kfs_inference_v2.proto <span style=color:#b84>\
</span></span></span><span style=display:flex><span><span style=color:#b84></span>  -d <span style=color:#b84>&#39;{ &#34;model_name&#34;: &#34;&#39;</span><span style=color:#b84>&#34;</span><span style=color:#b84>${</span><span style=color:teal>MODEL_NAME</span><span style=color:#b84>}</span><span style=color:#b84>&#34;</span><span style=color:#b84>&#39;&#34;, &#34;inputs&#34;: [{ &#34;name&#34;: &#34;predict&#34;, &#34;shape&#34;: [1, 64], &#34;datatype&#34;: &#34;FP32&#34;, &#34;contents&#34;: { &#34;fp32_contents&#34;: [0.0, 0.0, 1.0, 11.0, 14.0, 15.0, 3.0, 0.0, 0.0, 1.0, 13.0, 16.0, 12.0, 16.0, 8.0, 0.0, 0.0, 8.0, 16.0, 4.0, 6.0, 16.0, 5.0, 0.0, 0.0, 5.0, 15.0, 11.0, 13.0, 14.0, 0.0, 0.0, 0.0, 0.0, 2.0, 12.0, 16.0, 13.0, 0.0, 0.0, 0.0, 0.0, 0.0, 13.0, 16.0, 16.0, 6.0, 0.0, 0.0, 0.0, 0.0, 16.0, 16.0, 16.0, 7.0, 0.0, 0.0, 0.0, 0.0, 11.0, 13.0, 12.0, 1.0, 0.0] }}]}&#39;</span> <span style=color:#b84>\
</span></span></span><span style=display:flex><span><span style=color:#b84></span>  localhost:8033 <span style=color:#b84>\
</span></span></span><span style=display:flex><span><span style=color:#b84></span>  inference.GRPCInferenceService.ModelInfer
</span></span></code></pre></div><p>This should give you output like the following:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-json data-lang=json><span style=display:flex><span>{
</span></span><span style=display:flex><span>  <span style=color:navy>&#34;modelName&#34;</span>: <span style=color:#b84>&#34;example-sklearn-isvc__isvc-3642375d03&#34;</span>,
</span></span><span style=display:flex><span>  <span style=color:navy>&#34;outputs&#34;</span>: [
</span></span><span style=display:flex><span>    {
</span></span><span style=display:flex><span>      <span style=color:navy>&#34;name&#34;</span>: <span style=color:#b84>&#34;predict&#34;</span>,
</span></span><span style=display:flex><span>      <span style=color:navy>&#34;datatype&#34;</span>: <span style=color:#b84>&#34;INT64&#34;</span>,
</span></span><span style=display:flex><span>      <span style=color:navy>&#34;shape&#34;</span>: [<span style=color:#b84>&#34;1&#34;</span>],
</span></span><span style=display:flex><span>      <span style=color:navy>&#34;contents&#34;</span>: {
</span></span><span style=display:flex><span>        <span style=color:navy>&#34;int64Contents&#34;</span>: [<span style=color:#b84>&#34;8&#34;</span>]
</span></span><span style=display:flex><span>      }
</span></span><span style=display:flex><span>    }
</span></span><span style=display:flex><span>  ]
</span></span><span style=display:flex><span>}
</span></span></code></pre></div><h4 id=rest-request x-intersect="currentHeading = '#rest-request'">REST request</h4><blockquote><p>[!Note]
The REST proxy is currently in an alpha state and may still have issues with certain usage scenarios.</p></blockquote><p>You will need to port-forward a different port for REST.</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>kubectl port-forward --address 0.0.0.0 service/modelmesh-serving <span style=color:#099>8008</span> -n modelmesh-serving
</span></span></code></pre></div><p>With <code>curl</code>, a request can be sent to the SKLearn MNIST model like the following. Make sure that the <code>MODEL_NAME</code>
variable below is set to the name of your <code>InferenceService</code>.</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span><span style=color:teal>MODEL_NAME</span><span style=font-weight:700>=</span>example-sklearn-isvc
</span></span><span style=display:flex><span>curl -X POST -k http://localhost:8008/v2/models/<span style=color:#b84>${</span><span style=color:teal>MODEL_NAME</span><span style=color:#b84>}</span>/infer -d <span style=color:#b84>&#39;{&#34;inputs&#34;: [{ &#34;name&#34;: &#34;predict&#34;, &#34;shape&#34;: [1, 64], &#34;datatype&#34;: &#34;FP32&#34;, &#34;data&#34;: [0.0, 0.0, 1.0, 11.0, 14.0, 15.0, 3.0, 0.0, 0.0, 1.0, 13.0, 16.0, 12.0, 16.0, 8.0, 0.0, 0.0, 8.0, 16.0, 4.0, 6.0, 16.0, 5.0, 0.0, 0.0, 5.0, 15.0, 11.0, 13.0, 14.0, 0.0, 0.0, 0.0, 0.0, 2.0, 12.0, 16.0, 13.0, 0.0, 0.0, 0.0, 0.0, 0.0, 13.0, 16.0, 16.0, 6.0, 0.0, 0.0, 0.0, 0.0, 16.0, 16.0, 16.0, 7.0, 0.0, 0.0, 0.0, 0.0, 11.0, 13.0, 12.0, 1.0, 0.0]}]}&#39;</span>
</span></span></code></pre></div><p>This should give you a response like the following:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-json data-lang=json><span style=display:flex><span>{
</span></span><span style=display:flex><span>  <span style=color:navy>&#34;model_name&#34;</span>: <span style=color:#b84>&#34;example-sklearn-isvc__ksp-7702c1b55a&#34;</span>,
</span></span><span style=display:flex><span>  <span style=color:navy>&#34;outputs&#34;</span>: [
</span></span><span style=display:flex><span>    {
</span></span><span style=display:flex><span>      <span style=color:navy>&#34;name&#34;</span>: <span style=color:#b84>&#34;predict&#34;</span>,
</span></span><span style=display:flex><span>      <span style=color:navy>&#34;datatype&#34;</span>: <span style=color:#b84>&#34;FP32&#34;</span>,
</span></span><span style=display:flex><span>      <span style=color:navy>&#34;shape&#34;</span>: [<span style=color:#099>1</span>],
</span></span><span style=display:flex><span>      <span style=color:navy>&#34;data&#34;</span>: [<span style=color:#099>8</span>]
</span></span><span style=display:flex><span>    }
</span></span><span style=display:flex><span>  ]
</span></span><span style=display:flex><span>}
</span></span></code></pre></div><h3 id=4-optional-deleting-your-modelmesh-serving-installation x-intersect="currentHeading = '#4-optional-deleting-your-modelmesh-serving-installation'">4. (Optional) Deleting your ModelMesh Serving installation</h3><p>To delete all ModelMesh Serving resources that were installed, run the following from the root of the project:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>./scripts/delete.sh --namespace modelmesh-serving
</span></span></code></pre></div><h2 id=implementing-a-custom-serving-runtime x-intersect="currentHeading = '#implementing-a-custom-serving-runtime'">Implementing a Custom Serving Runtime</h2><p>ModelMesh Serving serves different kinds of models via different <em>Serving Runtime</em> implementations. A Serving Runtime is one or more containers which:</p><ul><li>Can dynamically load and unload models from disk into memory on demand</li><li>Exposes a gRPC service endpoint to serve inferencing requests for loaded models</li></ul><p>More specifically, the container(s) must:</p><ol><li>Implement the simple <a href=#model-server-management-spi>model management gRPC SPI</a> which comprises RPC methods to load/unload models, report their size, and report the runtime&rsquo;s total capacity</li><li>Implement one or more other <em>arbitrary</em> gRPC services to serve inferencing requests for already-loaded models</li></ol><p>These gRPC services for (2) must all be served from the same server endpoint. The management service SPI may be served by that same endpoint or a different one. Each of these endpoints may listen on a <code>localhost</code> port, or a unix domain socket. For best performance, a domain socket is preferred for the inferencing endpoint, and the corresponding file should be created in an empty dir within one of the containers. This dir will become a mount in <em>all</em> of the runtime containers when they are run.</p><h3 id=model-server-management-spi x-intersect="currentHeading = '#model-server-management-spi'">Model server Management SPI</h3><p>Below is a description of how to implement the <code>mmesh.ModelRuntime</code> gRPC service, specified in <a href=https://github.com/kserve/modelmesh-serving/blob/main/docs/model-runtime.proto><code>model-runtime.proto</code></a>. Note that this is currently subject to change, but we will try to ensure that any changes are backwards-compatible or at least will require minimal change on the runtime side.</p><h4 id=model-sizing x-intersect="currentHeading = '#model-sizing'">Model sizing</h4><p>So that ModelMesh Serving can decide when/where models should be loaded and unloaded, a given serving runtime implementation must communicate details of how much capacity it has to hold loaded models in memory, as well as how much each loaded model consumes.</p><p>Model sizes are communicated in a few different ways:</p><ul><li><p>A rough &ldquo;global&rdquo; default/average size for all models must be provided in the <code>defaultModelSizeInBytes</code> field in the response to the <a href=#runtimestatus><code>runtimeStatus</code></a> rpc method. This should be a very conservative estimate.</p></li><li><p>A <em>predicted</em> size can optionally be provided by implementing the <code>predictModelSize</code> rpc method. This will be called prior to <code>loadModel</code> and if implemented should return immediately (for example it should not make remote calls which could be delayed).</p></li><li><p>The more precise size of an already-loaded model can be provided by either:</p><ol><li>Including it in the <code>sizeInBytes</code> field of the response to the <code>loadModel</code> rpc method</li><li>Not setting in the <code>loadModel</code> response, and instead implementing the separate <code>modelSize</code> method to return the size. This will be called immediately after <code>loadModel</code> returns, and isn&rsquo;t required to be implemented if the first option is used.</li></ol><p>The second of these last two options is preferred when a separate step is required to determine the size after the model has already been loaded. This is so that the model can start to be used for inferencing immediately, while the sizing operation is still in progress.</p></li></ul><p>Capacity is indicated once via the <code>capacityInBytes</code> field in the response to the <a href=#runtimestatus><code>runtimeStatus</code></a> rpc method and assumed to be constant.</p><p>Ideally, the value of <code>capacityInBytes</code> should be calculated dynamically as a function of your model server container&rsquo;s allocated memory. One way to arrange this is via Kubernetes&rsquo; <a href=https://kubernetes.io/docs/tasks/inject-data-application/downward-api-volume-expose-pod-information/#store-container-fields>Downward API</a> - mapping the container&rsquo;s <code>requests.memory</code> property to an environment variable. Of course some amount of fixed overhead should likely be subtracted from this value:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:navy>env</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span>- <span style=color:navy>name</span>:<span style=color:#bbb> </span>MODEL_SERVER_MEM_REQ_BYTES<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:navy>valueFrom</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:navy>resourceFieldRef</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span><span style=color:navy>containerName</span>:<span style=color:#bbb> </span>my-model-server<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span><span style=color:navy>resource</span>:<span style=color:#bbb> </span>requests.memory<span style=color:#bbb>
</span></span></span></code></pre></div><h4 id=runtimestatus x-intersect="currentHeading = '#runtimestatus'"><code>runtimeStatus</code></h4><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-protobuf data-lang=protobuf><span style=display:flex><span><span style=font-weight:700>message</span> <span style=color:#458;font-weight:700>RuntimeStatusRequest</span> {}<span style=color:#a61717;background-color:#e3d2d2>
</span></span></span></code></pre></div><p>This is polled at the point that the main model-mesh container starts to check that the runtime is ready. You should return a response with <code>status</code> set to <code>STARTING</code> until the runtime is ready to accept other requests and load/serve models at which point <code>status</code> should be set to <code>READY</code>.</p><p>The other fields in the response only need to be set in the <code>READY</code> response (and will be ignored prior to that). Once <code>READY</code> is returned, no further calls will be made unless the model-mesh container unexpectedly restarts.</p><p>Currently, to ensure overall consistency of the system, it is required that runtimes purge any loaded/loading models when receiving a <code>runtimeStatus</code> call, and do not return <code>READY</code> until this is complete. Typically, it&rsquo;s only called during initialization prior to any load/unloadModel calls and hence this &ldquo;purge&rdquo; will be a no-op. But runtimes should also handle the case where there <em>are</em> models loaded. It is likely that this requirement will be removed in a future update, but ModelMesh Serving will remain compatible with runtimes that still include the logic.</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-protobuf data-lang=protobuf><span style=display:flex><span><span style=font-weight:700>message</span> <span style=color:#458;font-weight:700>RuntimeStatusResponse</span> {<span style=color:#a61717;background-color:#e3d2d2>
</span></span></span><span style=display:flex><span><span style=color:#a61717;background-color:#e3d2d2></span>    <span style=font-weight:700>enum</span> Status {<span style=color:#a61717;background-color:#e3d2d2>
</span></span></span><span style=display:flex><span><span style=color:#a61717;background-color:#e3d2d2></span>        STARTING <span style=font-weight:700>=</span> <span style=color:#099>0</span>;<span style=color:#a61717;background-color:#e3d2d2>
</span></span></span><span style=display:flex><span><span style=color:#a61717;background-color:#e3d2d2></span>        READY <span style=font-weight:700>=</span> <span style=color:#099>1</span>;<span style=color:#a61717;background-color:#e3d2d2>
</span></span></span><span style=display:flex><span><span style=color:#a61717;background-color:#e3d2d2></span>        FAILING <span style=font-weight:700>=</span> <span style=color:#099>2</span>; <span style=color:#998;font-style:italic>//not used yet
</span></span></span><span style=display:flex><span><span style=color:#998;font-style:italic></span>    }<span style=color:#a61717;background-color:#e3d2d2>
</span></span></span><span style=display:flex><span><span style=color:#a61717;background-color:#e3d2d2></span>    Status status <span style=font-weight:700>=</span> <span style=color:#099>1</span>;<span style=color:#a61717;background-color:#e3d2d2>
</span></span></span><span style=display:flex><span><span style=color:#a61717;background-color:#e3d2d2></span>    <span style=color:#998;font-style:italic>// memory capacity for static loaded models, in bytes
</span></span></span><span style=display:flex><span><span style=color:#998;font-style:italic></span>    <span style=color:#458;font-weight:700>uint64</span> capacityInBytes <span style=font-weight:700>=</span> <span style=color:#099>2</span>;<span style=color:#a61717;background-color:#e3d2d2>
</span></span></span><span style=display:flex><span><span style=color:#a61717;background-color:#e3d2d2></span>    <span style=color:#998;font-style:italic>// maximum number of model loads that can be in-flight at the same time
</span></span></span><span style=display:flex><span><span style=color:#998;font-style:italic></span>    <span style=color:#458;font-weight:700>uint32</span> maxLoadingConcurrency <span style=font-weight:700>=</span> <span style=color:#099>3</span>;<span style=color:#a61717;background-color:#e3d2d2>
</span></span></span><span style=display:flex><span><span style=color:#a61717;background-color:#e3d2d2></span>    <span style=color:#998;font-style:italic>// timeout for model loads in milliseconds
</span></span></span><span style=display:flex><span><span style=color:#998;font-style:italic></span>    <span style=color:#458;font-weight:700>uint32</span> modelLoadingTimeoutMs <span style=font-weight:700>=</span> <span style=color:#099>4</span>;<span style=color:#a61717;background-color:#e3d2d2>
</span></span></span><span style=display:flex><span><span style=color:#a61717;background-color:#e3d2d2></span>    <span style=color:#998;font-style:italic>// conservative &#34;default&#34; model size,
</span></span></span><span style=display:flex><span><span style=color:#998;font-style:italic></span>    <span style=color:#998;font-style:italic>// such that &#34;most&#34; models are smaller than this
</span></span></span><span style=display:flex><span><span style=color:#998;font-style:italic></span>    <span style=color:#458;font-weight:700>uint64</span> defaultModelSizeInBytes <span style=font-weight:700>=</span> <span style=color:#099>5</span>;<span style=color:#a61717;background-color:#e3d2d2>
</span></span></span><span style=display:flex><span><span style=color:#a61717;background-color:#e3d2d2></span>    <span style=color:#998;font-style:italic>// version string for this model server code
</span></span></span><span style=display:flex><span><span style=color:#998;font-style:italic></span>    <span style=color:#458;font-weight:700>string</span> runtimeVersion <span style=font-weight:700>=</span> <span style=color:#099>6</span>;<span style=color:#a61717;background-color:#e3d2d2>
</span></span></span><span style=display:flex><span><span style=color:#a61717;background-color:#e3d2d2>
</span></span></span><span style=display:flex><span><span style=color:#a61717;background-color:#e3d2d2></span>    <span style=font-weight:700>message</span> <span style=color:#458;font-weight:700>MethodInfo</span> {<span style=color:#a61717;background-color:#e3d2d2>
</span></span></span><span style=display:flex><span><span style=color:#a61717;background-color:#e3d2d2></span>        <span style=color:#998;font-style:italic>// &#34;path&#34; of protobuf field numbers leading to the string
</span></span></span><span style=display:flex><span><span style=color:#998;font-style:italic></span>        <span style=color:#998;font-style:italic>// field within the request method corresponding to the
</span></span></span><span style=display:flex><span><span style=color:#998;font-style:italic></span>        <span style=color:#998;font-style:italic>// model name or id
</span></span></span><span style=display:flex><span><span style=color:#998;font-style:italic></span>        <span style=font-weight:700>repeated</span> <span style=color:#458;font-weight:700>uint32</span> idInjectionPath <span style=font-weight:700>=</span> <span style=color:#099>1</span>;<span style=color:#a61717;background-color:#e3d2d2>
</span></span></span><span style=display:flex><span><span style=color:#a61717;background-color:#e3d2d2></span>    }<span style=color:#a61717;background-color:#e3d2d2>
</span></span></span><span style=display:flex><span><span style=color:#a61717;background-color:#e3d2d2>
</span></span></span><span style=display:flex><span><span style=color:#a61717;background-color:#e3d2d2></span>    <span style=color:#998;font-style:italic>// optional map of per-gRPC rpc method configuration
</span></span></span><span style=display:flex><span><span style=color:#998;font-style:italic></span>    <span style=color:#998;font-style:italic>// keys should be fully-qualified gRPC method name
</span></span></span><span style=display:flex><span><span style=color:#998;font-style:italic></span>    <span style=color:#998;font-style:italic>// (including package/service prefix)
</span></span></span><span style=display:flex><span><span style=color:#998;font-style:italic></span>    map&lt;<span style=color:#458;font-weight:700>string</span>,MethodInfo&gt; methodInfos <span style=font-weight:700>=</span> <span style=color:#099>8</span>;<span style=color:#a61717;background-color:#e3d2d2>
</span></span></span><span style=display:flex><span><span style=color:#a61717;background-color:#e3d2d2>
</span></span></span><span style=display:flex><span><span style=color:#a61717;background-color:#e3d2d2></span>    <span style=color:#998;font-style:italic>// EXPERIMENTAL - Set to true to enable the mode where
</span></span></span><span style=display:flex><span><span style=color:#998;font-style:italic></span>    <span style=color:#998;font-style:italic>// each loaded model reports a maximum inferencing
</span></span></span><span style=display:flex><span><span style=color:#998;font-style:italic></span>    <span style=color:#998;font-style:italic>// concurrency via the maxConcurrency field of
</span></span></span><span style=display:flex><span><span style=color:#998;font-style:italic></span>    <span style=color:#998;font-style:italic>// the LoadModelResponse message. Additional requests
</span></span></span><span style=display:flex><span><span style=color:#998;font-style:italic></span>    <span style=color:#998;font-style:italic>// are queued in the modelmesh framework. Turning this
</span></span></span><span style=display:flex><span><span style=color:#998;font-style:italic></span>    <span style=color:#998;font-style:italic>// on will also enable latency-based autoscaling for
</span></span></span><span style=display:flex><span><span style=color:#998;font-style:italic></span>    <span style=color:#998;font-style:italic>// the models, which attempts to minimize request
</span></span></span><span style=display:flex><span><span style=color:#998;font-style:italic></span>    <span style=color:#998;font-style:italic>// queueing time and requires no other configuration.
</span></span></span><span style=display:flex><span><span style=color:#998;font-style:italic></span>    <span style=color:#458;font-weight:700>bool</span> limitModelConcurrency <span style=font-weight:700>=</span> <span style=color:#099>9</span>;<span style=color:#a61717;background-color:#e3d2d2>
</span></span></span><span style=display:flex><span><span style=color:#a61717;background-color:#e3d2d2></span>}<span style=color:#a61717;background-color:#e3d2d2>
</span></span></span></code></pre></div><h4 id=loadmodel x-intersect="currentHeading = '#loadmodel'"><code>loadModel</code></h4><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-protobuf data-lang=protobuf><span style=display:flex><span><span style=font-weight:700>message</span> <span style=color:#458;font-weight:700>LoadModelRequest</span> {<span style=color:#a61717;background-color:#e3d2d2>
</span></span></span><span style=display:flex><span><span style=color:#a61717;background-color:#e3d2d2></span>    <span style=color:#458;font-weight:700>string</span> modelId <span style=font-weight:700>=</span> <span style=color:#099>1</span>;<span style=color:#a61717;background-color:#e3d2d2>
</span></span></span><span style=display:flex><span><span style=color:#a61717;background-color:#e3d2d2>
</span></span></span><span style=display:flex><span><span style=color:#a61717;background-color:#e3d2d2></span>    <span style=color:#458;font-weight:700>string</span> modelType <span style=font-weight:700>=</span> <span style=color:#099>2</span>;<span style=color:#a61717;background-color:#e3d2d2>
</span></span></span><span style=display:flex><span><span style=color:#a61717;background-color:#e3d2d2></span>    <span style=color:#458;font-weight:700>string</span> modelPath <span style=font-weight:700>=</span> <span style=color:#099>3</span>;<span style=color:#a61717;background-color:#e3d2d2>
</span></span></span><span style=display:flex><span><span style=color:#a61717;background-color:#e3d2d2></span>    <span style=color:#458;font-weight:700>string</span> modelKey <span style=font-weight:700>=</span> <span style=color:#099>4</span>;<span style=color:#a61717;background-color:#e3d2d2>
</span></span></span><span style=display:flex><span><span style=color:#a61717;background-color:#e3d2d2></span>}<span style=color:#a61717;background-color:#e3d2d2>
</span></span></span></code></pre></div><p>The runtime should load a model with name/id specified by the <code>modelId</code> field into memory ready for serving, from the path specified by the <code>modelPath</code> field. At this time, <strong>the <code>modelType</code> field value should be ignored</strong>.</p><p>The <code>modelKey</code> field will contain a JSON string with the following contents:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-json data-lang=json><span style=display:flex><span>{
</span></span><span style=display:flex><span>  <span style=color:navy>&#34;model_type&#34;</span>: {
</span></span><span style=display:flex><span>    <span style=color:navy>&#34;name&#34;</span>: <span style=color:#b84>&#34;mytype&#34;</span>,
</span></span><span style=display:flex><span>    <span style=color:navy>&#34;version&#34;</span>: <span style=color:#b84>&#34;2&#34;</span>
</span></span><span style=display:flex><span>  }
</span></span><span style=display:flex><span>}
</span></span></code></pre></div><p>Where <code>model_type</code> corresponds to the <code>modelFormat</code> section from the originating <a href=../predictors><code>InferenceSerivce</code> predictor</a>. Note that <code>version</code> is optional and may not be present. In future, additional attributes might be present in the outer json object so your implementation should ignore them gracefully.</p><p>The response shouldn&rsquo;t be returned until the model has loaded successfully and is ready to use.</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-protobuf data-lang=protobuf><span style=display:flex><span><span style=font-weight:700>message</span> <span style=color:#458;font-weight:700>LoadModelResponse</span> {<span style=color:#a61717;background-color:#e3d2d2>
</span></span></span><span style=display:flex><span><span style=color:#a61717;background-color:#e3d2d2></span>    <span style=color:#998;font-style:italic>// OPTIONAL - If nontrivial cost is involved in
</span></span></span><span style=display:flex><span><span style=color:#998;font-style:italic></span>    <span style=color:#998;font-style:italic>// determining the size, return 0 here and
</span></span></span><span style=display:flex><span><span style=color:#998;font-style:italic></span>    <span style=color:#998;font-style:italic>// do the sizing in the modelSize function
</span></span></span><span style=display:flex><span><span style=color:#998;font-style:italic></span>    <span style=color:#458;font-weight:700>uint64</span> sizeInBytes <span style=font-weight:700>=</span> <span style=color:#099>1</span>;<span style=color:#a61717;background-color:#e3d2d2>
</span></span></span><span style=display:flex><span><span style=color:#a61717;background-color:#e3d2d2>
</span></span></span><span style=display:flex><span><span style=color:#a61717;background-color:#e3d2d2></span>    <span style=color:#998;font-style:italic>// EXPERIMENTAL - Applies only if limitModelConcurrency = true
</span></span></span><span style=display:flex><span><span style=color:#998;font-style:italic></span>    <span style=color:#998;font-style:italic>// was returned from runtimeStatus rpc.
</span></span></span><span style=display:flex><span><span style=color:#998;font-style:italic></span>    <span style=color:#998;font-style:italic>// See RuntimeStatusResponse.limitModelConcurrency for more detail
</span></span></span><span style=display:flex><span><span style=color:#998;font-style:italic></span>    <span style=color:#458;font-weight:700>uint32</span> maxConcurrency <span style=font-weight:700>=</span> <span style=color:#099>2</span>;<span style=color:#a61717;background-color:#e3d2d2>
</span></span></span><span style=display:flex><span><span style=color:#a61717;background-color:#e3d2d2></span>}<span style=color:#a61717;background-color:#e3d2d2>
</span></span></span></code></pre></div><h4 id=unloadmodel x-intersect="currentHeading = '#unloadmodel'"><code>unloadModel</code></h4><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-protobuf data-lang=protobuf><span style=display:flex><span><span style=font-weight:700>message</span> <span style=color:#458;font-weight:700>UnloadModelRequest</span> {<span style=color:#a61717;background-color:#e3d2d2>
</span></span></span><span style=display:flex><span><span style=color:#a61717;background-color:#e3d2d2></span>    <span style=color:#458;font-weight:700>string</span> modelId <span style=font-weight:700>=</span> <span style=color:#099>1</span>;<span style=color:#a61717;background-color:#e3d2d2>
</span></span></span><span style=display:flex><span><span style=color:#a61717;background-color:#e3d2d2></span>}<span style=color:#a61717;background-color:#e3d2d2>
</span></span></span></code></pre></div><p>The runtime should unload the previously loaded (or failed) model specified by <code>modelId</code>, and not return a response until the unload is complete and corresponding resources have been freed. If the specified model is not found/loaded, the runtime should return immediately (without error).</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-protobuf data-lang=protobuf><span style=display:flex><span><span style=font-weight:700>message</span> <span style=color:#458;font-weight:700>UnloadModelResponse</span> {}<span style=color:#a61717;background-color:#e3d2d2>
</span></span></span></code></pre></div><h4 id=inferencing x-intersect="currentHeading = '#inferencing'">Inferencing</h4><p>The model runtime server can expose any number of protobuf-based gRPC services on the <code>grpcDataEndpoint</code> to use for inferencing requests. ModelMesh Serving is agnostic to specific service definitions (request/response message content), but for tensor-in/tensor-out based services it is recommended to conform to the <a href=https://github.com/kserve/kserve/blob/master/docs/predict-api/v2/required_api.md#grpc>KServe V2 dataplane API spec</a>.</p><p>A given model runtime server will be guaranteed to only receive model inferencing requests for models that had previously completed loading successfully (via a <a href=#loadmodel><code>loadModel</code></a> request), and to have not been unloaded since.</p><p>Though generally agnostic to the specific API methods, ModelMesh Serving does need to be able to set/override the model name/id used in a given request. There are two options for obtaining the model name/id within the (which will correspond to the same id previously passed to <code>loadModel</code>):</p><ol><li>Obtain from one of the <code>mm-model-id</code> or <code>mm-model-id-bin</code> gRPC metadata headers (latter required for non-ASCII UTF-8 ids). Precisely how this is done depends on the implementation language - see gRPC documentation for more information (<em>TODO</em> specific refs/examples here).</li><li>Provide the location of a specific string field within your request protobuf message (per RPC method) which will be replaced with the target model id. This is done via the <code>methodInfos</code> map in the runtime&rsquo;s response to the <a href=#runtimestatus><code>runtimeStatus</code></a> RPC method. Each applicable inferencing method should have an entry whose <code>idInjectionPath</code> field is set to a list of field numbers corresponding to the heirarchy of nested messages within the request message, the last of which being the number of the string field to replace. For example, if the id is a string field in the top-level request message with number 1 (as is the case in the KServe V2 <a href=https://github.com/kserve/kserve/blob/master/docs/predict-api/v2/required_api.md#inference-1><code>ModelInferRequest</code></a>), this list would be set to just <code>[1]</code>.</li></ol><p>Option 2 is particularly applicable when <a href=#integrating-with-existing-model-servers>integrating with an existing gRPC-based model server</a>.</p><h3 id=deploying-a-runtime x-intersect="currentHeading = '#deploying-a-runtime'">Deploying a Runtime</h3><p>Each Serving Runtime implementation is defined using the custom resource type <code>ServingRuntime</code> which defines information about the runtime such as which container images need to be loaded, and the local gRPC endpoints on which they will listen. When the resource is applied to the Kubernetes cluster, the model server will deploy the runtime specific containers which will then enable support for the corresponding model types.</p><p>The following is an example of a <code>ServingRuntime</code> custom resource</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:navy>apiVersion</span>:<span style=color:#bbb> </span>serving.kserve.io/v1alpha1<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:navy>kind</span>:<span style=color:#bbb> </span>ServingRuntime<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:navy>metadata</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:navy>name</span>:<span style=color:#bbb> </span>example-runtime<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:navy>spec</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:navy>supportedModelFormats</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span>- <span style=color:navy>name</span>:<span style=color:#bbb> </span>new-modelformat<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:navy>version</span>:<span style=color:#bbb> </span><span style=color:#b84>&#34;1&#34;</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:navy>autoSelect</span>:<span style=color:#bbb> </span><span style=font-weight:700>true</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:navy>containers</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span>- <span style=color:navy>name</span>:<span style=color:#bbb> </span>model-server<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:navy>image</span>:<span style=color:#bbb> </span>samplemodelserver:latest<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:navy>multiModel</span>:<span style=color:#bbb> </span><span style=font-weight:700>true</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:navy>grpcEndpoint</span>:<span style=color:#bbb> </span><span style=color:#b84>&#34;port:8085&#34;</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:navy>grpcDataEndpoint</span>:<span style=color:#bbb> </span><span style=color:#b84>&#34;port:8090&#34;</span><span style=color:#bbb>
</span></span></span></code></pre></div><p>In each entry of the <code>supportedModelFormats</code> list, <code>autoSelect: true</code> can optionally be specified to indicate that that the given <code>ServingRuntime</code> can be considered for automatic placement of <code>InferenceService</code>s with the corresponding model type/format if no runtime is explicitly specified.
For example, if a user applies an <code>InferenceService</code> with <code>predictor.model.modelFormat.name: new-modelformat</code> and no <code>runtime</code> value, the above <code>ServingRuntime</code> will be used since it contains an &ldquo;auto-selectable&rdquo; supported model format that matches <code>new-modelformat</code>. If <code>autoSelect</code> were <code>false</code> or unspecified, the <code>InferenceService</code> would fail to load with the message &ldquo;No <code>ServingRuntime</code> supports specified model type and/or protocol&rdquo; unless the runtime <code>example-runtime</code> was specified directly in the YAML.</p><h4 id=runtime-container-resource-allocations x-intersect="currentHeading = '#runtime-container-resource-allocations'">Runtime container resource allocations</h4><p><em>TODO</em> more detail coming here</p><h2 id=integrating-with-existing-model-servers x-intersect="currentHeading = '#integrating-with-existing-model-servers'">Integrating with existing model servers</h2><p>The ability to specify multiple containers provides a nice way to integrate with existing model servers via an adapter pattern, as long as they provide the required capability of dynamically loading and unloading models.</p><p><em>Note: In the above diagram, only the adapter and model server containers are explicitly specified in the <code>ServingRuntime</code> CR, the others are included automatically.</em></p><p>The <a href=https://github.com/kserve/modelmesh-serving/tree/main/config/runtimes>built-in runtimes</a> based on <a href=https://github.com/kserve/modelmesh-serving/blob/main/config/runtimes/triton-2.x.yaml>Nvidia&rsquo;s Triton Inferencing Server</a> and the <a href=https://github.com/SeldonIO/MLServer>Seldon MLServer</a>, and their corresponding adapters serve as good examples of this and can be used as a reference.</p><h3 id=reference x-intersect="currentHeading = '#reference'">Reference</h3><h4 id=spec-attributes x-intersect="currentHeading = '#spec-attributes'">Spec Attributes</h4><p>Available attributes in the <code>ServingRuntime</code> spec:</p><table><thead><tr><th>Attribute</th><th>Description</th></tr></thead><tbody><tr><td><code>multiModel</code></td><td>Whether this <code>ServingRuntime</code> is ModelMesh-compatible and intended for multi-model usage (as opposed to KServe single-model serving).</td></tr><tr><td><code>disabled</code></td><td>Disables this runtime</td></tr><tr><td><code>containers</code></td><td>List of containers associated with the runtime</td></tr><tr><td><code>containers[ ].image</code></td><td>The container image for the current container</td></tr><tr><td><code>containers[ ].command</code></td><td>Executable command found in the provided image</td></tr><tr><td><code>containers[ ].args</code></td><td>List of command line arguments as strings</td></tr><tr><td><code>containers[ ].resources</code></td><td>Kubernetes <a href=https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/#requests-and-limits>limits or requests</a></td></tr><tr><td><code>containers[ ].imagePullPolicy</code></td><td>The container image pull policy</td></tr><tr><td><code>containers[ ].workingDir</code></td><td>The working directory for current container</td></tr><tr><td><code>grpcEndpoint</code></td><td>The <a href=#endpoint-formats>port</a> for model management requests</td></tr><tr><td><code>grpcDataEndpoint</code></td><td>The <a href=#endpoint-formats>port or unix socket</a> for inferencing requests arriving to the model server over the gRPC protocol. May be set to the same value as <code>grpcEndpoint</code></td></tr><tr><td><code>supportedModelFormats</code></td><td>List of model types supported by the current runtime</td></tr><tr><td><code>supportedModelFormats[ ].name</code></td><td>Name of the model type</td></tr><tr><td><code>supportedModelFormats[ ].version</code></td><td>Version of the model type. It is recommended to include only the major version here, for example &ldquo;1&rdquo; rather than &ldquo;1.15.4&rdquo;</td></tr><tr><td><code>storageHelper.disabled</code></td><td>Disables the storage helper</td></tr><tr><td><code>nodeSelector</code></td><td>Influence Kubernetes scheduling to <a href=https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/>assign pods to nodes</a></td></tr><tr><td><code>affinity</code></td><td>Influence Kubernetes scheduling to <a href=https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#affinity-and-anti-affinity>assign pods to nodes</a></td></tr><tr><td><code>tolerations</code></td><td>Allow pods to be scheduled onto nodes <a href=https://kubernetes.io/docs/concepts/scheduling-eviction/taint-and-toleration>with matching taints</a></td></tr><tr><td><code>replicas</code></td><td>The number of replicas of the runtime to create. This overrides the <code>podsPerRuntime</code> <a href=https://ruivieira.dev/docs/configuration/README.md>configuration</a></td></tr></tbody></table><h4 id=endpoint-formats x-intersect="currentHeading = '#endpoint-formats'">Endpoint formats</h4><p>Several of the attributes (<code>grpcEndpoint</code>, <code>grpcDataEndpoint</code>) support either Unix Domain Sockets or TCP. The endpoint should be formatted as either <code>port:&lt;number></code> or <code>unix:&lt;path></code>. The provided container must be either listening on the specific TCP socket or at the provided path.</p><hr><p><strong>Warning</strong></p><p>If a unix domain socket is specified for both <code>grpcEndpoint</code> and <code>grpcDataEndpoint</code> then it must either be the same socket (identical path) or reside in the same directory.</p><hr><h4 id=full-example x-intersect="currentHeading = '#full-example'">Full Example</h4><p>The following example demonstrates all of the possible attributes that can be set in the model serving runtime spec:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:navy>apiVersion</span>:<span style=color:#bbb> </span>serving.kserve.io/v1alpha1<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:navy>kind</span>:<span style=color:#bbb> </span>ServingRuntime<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:navy>metadata</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:navy>name</span>:<span style=color:#bbb> </span>example-runtime<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:navy>spec</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:navy>supportedModelFormats</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span>- <span style=color:navy>name</span>:<span style=color:#bbb> </span>my_model_format<span style=color:#bbb> </span><span style=color:#998;font-style:italic># name of the model</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:navy>version</span>:<span style=color:#bbb> </span><span style=color:#b84>&#34;1&#34;</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:navy>autoSelect</span>:<span style=color:#bbb> </span><span style=font-weight:700>true</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:navy>containers</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span>- <span style=color:navy>args</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span>- arg1<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span>- arg2<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:navy>command</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span>- command<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span>- command2<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:navy>env</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span>- <span style=color:navy>name</span>:<span style=color:#bbb> </span>name<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>          </span><span style=color:navy>value</span>:<span style=color:#bbb> </span>value<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span>- <span style=color:navy>name</span>:<span style=color:#bbb> </span>fromSecret<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>          </span><span style=color:navy>valueFrom</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>            </span><span style=color:navy>secretKeyRef</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>              </span><span style=color:navy>key</span>:<span style=color:#bbb> </span>mykey<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:navy>image</span>:<span style=color:#bbb> </span>image<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:navy>name</span>:<span style=color:#bbb> </span>name<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:navy>resources</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span><span style=color:navy>limits</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>          </span><span style=color:navy>memory</span>:<span style=color:#bbb> </span>200Mi<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:navy>imagePullPolicy</span>:<span style=color:#bbb> </span>IfNotPresent<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:navy>workingDir</span>:<span style=color:#bbb> </span><span style=color:#b84>&#34;/container/working/dir&#34;</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:navy>multiModel</span>:<span style=color:#bbb> </span><span style=font-weight:700>true</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:navy>disabled</span>:<span style=color:#bbb> </span><span style=font-weight:700>false</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:navy>storageHelper</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:navy>disabled</span>:<span style=color:#bbb> </span><span style=font-weight:700>true</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:navy>grpcEndpoint</span>:<span style=color:#bbb> </span>port:1234<span style=color:#bbb> </span><span style=color:#998;font-style:italic># or unix:/path</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:navy>grpcDataEndpoint</span>:<span style=color:#bbb> </span>port:1234<span style=color:#bbb> </span><span style=color:#998;font-style:italic># or unix:/path</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:#998;font-style:italic># To influence pod scheduling, one or more of the following can be used</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:navy>nodeSelector</span>:<span style=color:#bbb> </span><span style=color:#998;font-style:italic># https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#nodeselector</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:navy>kubernetes.io/arch</span>:<span style=color:#bbb> </span><span style=color:#b84>&#34;amd64&#34;</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:navy>affinity</span>:<span style=color:#bbb> </span><span style=color:#998;font-style:italic># https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#affinity-and-anti-affinity</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:navy>nodeAffinity</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:navy>requiredDuringSchedulingIgnoredDuringExecution</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span><span style=color:navy>nodeSelectorTerms</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>          </span>- <span style=color:navy>matchExpressions</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>              </span>- <span style=color:navy>key</span>:<span style=color:#bbb> </span><span style=color:#b84>&#34;kubernetes.io/arch&#34;</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>                </span><span style=color:navy>operator</span>:<span style=color:#bbb> </span>In<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>                </span><span style=color:navy>values</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>                  </span>- <span style=color:#b84>&#34;amd64&#34;</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:navy>tolerations</span>:<span style=color:#bbb> </span><span style=color:#998;font-style:italic># https://kubernetes.io/docs/concepts/scheduling-eviction/taint-and-toleration/</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span>- <span style=color:navy>key</span>:<span style=color:#bbb> </span><span style=color:#b84>&#34;example-key&#34;</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:navy>operator</span>:<span style=color:#bbb> </span><span style=color:#b84>&#34;Exists&#34;</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:navy>effect</span>:<span style=color:#bbb> </span><span style=color:#b84>&#34;NoSchedule&#34;</span><span style=color:#bbb>
</span></span></span></code></pre></div><h4 id=storage-helper x-intersect="currentHeading = '#storage-helper'">Storage Helper</h4><p>Storage helper will download the model from the S3 bucket using the secret <code>storage-config</code> and place it in the local path. By default, storage helper is enabled in the serving runtime. Storage helper can be disabled by adding the configuration <code>storageHelper.disabled</code> set to <code>true</code> in serving runtime. If the storage helper is disabled, the custom runtime needs to handle access to and pulling model data from storage itself. Configuration can be passed to the runtime&rsquo;s pods through environment variables.</p><h5 id=example x-intersect="currentHeading = '#example'">Example</h5><p>Consider the custom runtime defined <a href=#full-example>above</a> with the following <code>InferenceService</code>:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:navy>apiVersion</span>:<span style=color:#bbb> </span>serving.kserve.io/v1beta1<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:navy>kind</span>:<span style=color:#bbb> </span>InferenceService<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:navy>metadata</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:navy>name</span>:<span style=color:#bbb> </span>my-mnist-isvc<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:navy>annotations</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:navy>serving.kserve.io/deploymentMode</span>:<span style=color:#bbb> </span>ModelMesh<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb></span><span style=color:navy>spec</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>  </span><span style=color:navy>predictor</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>    </span><span style=color:navy>model</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:navy>modelFormat</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span><span style=color:navy>name</span>:<span style=color:#bbb> </span>my_model_format<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span><span style=color:navy>version</span>:<span style=color:#bbb> </span><span style=color:#b84>&#34;1&#34;</span><span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>      </span><span style=color:navy>storage</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span><span style=color:navy>key</span>:<span style=color:#bbb> </span>my_storage<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span><span style=color:navy>path</span>:<span style=color:#bbb> </span>my_models/mnist-model<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>        </span><span style=color:navy>parameters</span>:<span style=color:#bbb>
</span></span></span><span style=display:flex><span><span style=color:#bbb>          </span><span style=color:navy>bucket</span>:<span style=color:#bbb> </span>my_bucket<span style=color:#bbb>
</span></span></span></code></pre></div><p>If the storage helper is enabled, the model serving container will receive the below model metadata in the <code>loadModel</code> call where <code>modelPath</code> will contain the path of the model in the local file system.</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-json data-lang=json><span style=display:flex><span>{
</span></span><span style=display:flex><span>  <span style=color:navy>&#34;modelId&#34;</span>: <span style=color:#b84>&#34;my-mnist-isvc-&lt;suffix&gt;&#34;</span>,
</span></span><span style=display:flex><span>  <span style=color:navy>&#34;modelType&#34;</span>: <span style=color:#b84>&#34;my_model_format&#34;</span>,
</span></span><span style=display:flex><span>  <span style=color:navy>&#34;modelPath&#34;</span>: <span style=color:#b84>&#34;/models/my-mnist-isvc-&lt;suffix&gt;/&#34;</span>,
</span></span><span style=display:flex><span>  <span style=color:navy>&#34;modelKey&#34;</span>: <span style=color:#b84>&#34;&lt;serialized metadata as JSON, see below&gt;&#34;</span>
</span></span><span style=display:flex><span>}
</span></span></code></pre></div><p>The following metadata for the <code>InferenceService</code> predictor is serialized to a string and embedded as the <code>modelKey</code> field:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-json data-lang=json><span style=display:flex><span>{
</span></span><span style=display:flex><span>  <span style=color:navy>&#34;bucket&#34;</span>: <span style=color:#b84>&#34;my_bucket&#34;</span>,
</span></span><span style=display:flex><span>  <span style=color:navy>&#34;disk_size_bytes&#34;</span>: <span style=color:#099>2415</span>,
</span></span><span style=display:flex><span>  <span style=color:navy>&#34;model_type&#34;</span>: {
</span></span><span style=display:flex><span>    <span style=color:navy>&#34;name&#34;</span>: <span style=color:#b84>&#34;my_model_format&#34;</span>,
</span></span><span style=display:flex><span>    <span style=color:navy>&#34;version&#34;</span>: <span style=color:#b84>&#34;1&#34;</span>
</span></span><span style=display:flex><span>  },
</span></span><span style=display:flex><span>  <span style=color:navy>&#34;storage_key&#34;</span>: <span style=color:#b84>&#34;my_storage&#34;</span>
</span></span><span style=display:flex><span>}
</span></span></code></pre></div><p>If the storage helper is disabled, the model serving container will receive the below model metadata in the <code>loadModel</code> call where <code>modelPath</code> is same as the <code>path</code> provided in the predictor storage spec.</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-json data-lang=json><span style=display:flex><span>{
</span></span><span style=display:flex><span>  <span style=color:navy>&#34;modelId&#34;</span>: <span style=color:#b84>&#34;my-mnist-isvc-&lt;suffix&gt;&#34;</span>,
</span></span><span style=display:flex><span>  <span style=color:navy>&#34;modelType&#34;</span>: <span style=color:#b84>&#34;my_model_format&#34;</span>,
</span></span><span style=display:flex><span>  <span style=color:navy>&#34;modelPath&#34;</span>: <span style=color:#b84>&#34;my_models/mnist-model&#34;</span>,
</span></span><span style=display:flex><span>  <span style=color:navy>&#34;modelKey&#34;</span>: <span style=color:#b84>&#34;&lt;serialized metadata as JSON, see below&gt;&#34;</span>
</span></span><span style=display:flex><span>}
</span></span></code></pre></div><p>The following metadata for the <code>InferenceService</code> predictor is serialized to a string and embedded as the <code>modelKey</code> field:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-json data-lang=json><span style=display:flex><span>{
</span></span><span style=display:flex><span>  <span style=color:navy>&#34;bucket&#34;</span>: <span style=color:#b84>&#34;my_bucket&#34;</span>,
</span></span><span style=display:flex><span>  <span style=color:navy>&#34;model_type&#34;</span>: {
</span></span><span style=display:flex><span>    <span style=color:navy>&#34;name&#34;</span>: <span style=color:#b84>&#34;my_model_format&#34;</span>,
</span></span><span style=display:flex><span>    <span style=color:navy>&#34;version&#34;</span>: <span style=color:#b84>&#34;1&#34;</span>
</span></span><span style=display:flex><span>  },
</span></span><span style=display:flex><span>  <span style=color:navy>&#34;storage_key&#34;</span>: <span style=color:#b84>&#34;my_storage&#34;</span>
</span></span><span style=display:flex><span>}
</span></span></code></pre></div><h2 id=installation x-intersect="currentHeading = '#installation'">Installation</h2><h3 id=prerequisites-1 x-intersect="currentHeading = '#prerequisites-1'">Prerequisites</h3><ul><li><p><strong>Kubernetes cluster</strong> - A Kubernetes cluster is required. You will need <code>cluster-admin</code> authority in order to complete all of the prescribed steps.</p></li><li><p><strong>Kubectl and Kustomize</strong> - The installation will occur via the terminal using <a href=https://kubernetes.io/docs/tasks/tools/#kubectl>kubectl</a> and <a href=https://kubectl.docs.kubernetes.io/installation/kustomize/>kustomize</a>.</p></li><li><p><strong>etcd</strong> - ModelMesh Serving requires an <a href=https://etcd.io/>etcd</a> server in order to coordinate internal state which can be either dedicated or shared. More on this later.</p></li><li><p><strong>S3-compatible object storage</strong> - Before models can be deployed, a remote S3-compatible datastore is needed from which to pull the model data. This could be for example an <a href=https://www.ibm.com/cloud/object-storage>IBM Cloud Object Storage</a> instance, or a locally running <a href=https://github.com/minio/minio>MinIO</a> deployment. Note that this is not required to be in place prior to the initial installation.</p></li></ul><p>We provide an install script to quickly run ModelMesh Serving with a provisioned etcd server. This may be useful for experimentation or development but should not be used in production.</p><p>The install script has a <code>--quickstart</code> option for setting up a self-contained ModelMesh Serving instance. This will deploy and configure local etcd and MinIO servers in the same Kubernetes namespace. Note that this is only for experimentation and/or development use - in particular the connections to these datastores are not secure and the etcd cluster is a single member which is not highly available. Use of <code>--quickstart</code> also configures the <code>storage-config</code> secret to be able to pull from the <a href=../example-models.md>ModelMesh Serving example models bucket</a> which contains the model data for the sample Predictors. For complete details on the manfiests applied with <code>--quickstart</code> see <a href=https://github.com/kserve/modelmesh-serving/blob/main/config/dependencies/quickstart.yaml>config/dependencies/quickstart.yaml</a>.</p><h3 id=setup-the-etcd-connection-information x-intersect="currentHeading = '#setup-the-etcd-connection-information'">Setup the etcd connection information</h3><p>If the <code>--quickstart</code> install option is <strong>not</strong> being used, details of an existing etcd cluster must be specified prior to installation. Otherwise, please skip this step and proceed to <a href=#installation>Installation</a>.</p><p>Create a file named etcd-config.json, populating the values based upon your etcd server. The same etcd server can be shared between environments and/or namespaces, but in this case <em>the <code>root_prefix</code> must be set differently in each namespace&rsquo;s respective secret</em>. The complete json schema for this configuration is documented <a href=https://github.com/IBM/etcd-java/blob/master/etcd-json-schema.md>here</a>.</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-json data-lang=json><span style=display:flex><span>{
</span></span><span style=display:flex><span>  <span style=color:navy>&#34;endpoints&#34;</span>: <span style=color:#b84>&#34;https://etcd-service-hostame:2379&#34;</span>,
</span></span><span style=display:flex><span>  <span style=color:navy>&#34;userid&#34;</span>: <span style=color:#b84>&#34;userid&#34;</span>,
</span></span><span style=display:flex><span>  <span style=color:navy>&#34;password&#34;</span>: <span style=color:#b84>&#34;password&#34;</span>,
</span></span><span style=display:flex><span>  <span style=color:navy>&#34;root_prefix&#34;</span>: <span style=color:#b84>&#34;unique-chroot-prefix&#34;</span>
</span></span><span style=display:flex><span>}
</span></span></code></pre></div><p>Then create the secret using the file (note that the key name within the secret must be <code>etcd_connection</code>):</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>kubectl create secret generic model-serving-etcd --from-file<span style=font-weight:700>=</span><span style=color:teal>etcd_connection</span><span style=font-weight:700>=</span>etcd-config.json
</span></span></code></pre></div><p>A secret named <code>model-serving-etcd</code> will be created and passed to the controller.</p><h3 id=installation-1 x-intersect="currentHeading = '#installation-1'">Installation</h3><p>Install the latest release of <a href=https://github.com/kserve/modelmesh-serving>modelmesh-serving</a> by first cloning the corresponding release branch:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span><span style=color:teal>RELEASE</span><span style=font-weight:700>=</span>release-0.8
</span></span><span style=display:flex><span>git clone -b <span style=color:teal>$RELEASE</span> --depth <span style=color:#099>1</span> --single-branch https://github.com/kserve/modelmesh-serving.git
</span></span><span style=display:flex><span><span style=color:#999>cd</span> modelmesh-serving
</span></span></code></pre></div><p>Run the script to install ModelMesh Serving CRDs, controller, and built-in runtimes into the specified Kubernetes namespaces, after reviewing the command line flags below.</p><p>A Kubernetes <code>--namespace</code> is required, which must already exist. You must also have cluster-admin authority and cluster access must be configured prior to running the install script.</p><p>A list of Kubernetes namespaces <code>--user-namespaces</code> is optional to enable user namespaces for ModelMesh Serving. The script will skip the namespaces which don&rsquo;t already exist.</p><p>The <code>--quickstart</code> option can be specified to install and configure supporting datastores in the same namespace (etcd and MinIO) for experimental/development use. If this is not chosen, the namespace provided must have an Etcd secret named <code>model-serving-etcd</code> created which provides access to the Etcd cluster. See the <a href=#setup-the-etcd-connection-information>instructions above</a> on this step.</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>kubectl create namespace modelmesh-serving
</span></span><span style=display:flex><span>./scripts/install.sh --namespace modelmesh-serving --quickstart
</span></span></code></pre></div><p>See the installation help below for detail:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>./scripts/install.sh --help
</span></span><span style=display:flex><span>usage: ./scripts/install.sh <span style=font-weight:700>[</span>flags<span style=font-weight:700>]</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>Flags:
</span></span><span style=display:flex><span>  -n, --namespace                <span style=font-weight:700>(</span>required<span style=font-weight:700>)</span> Kubernetes namespace to deploy ModelMesh Serving to.
</span></span><span style=display:flex><span>  -p, --install-config-path      Path to <span style=color:#999>local</span> model serve installation configs. Can be ModelMesh Serving tarfile or directory.
</span></span><span style=display:flex><span>  -d, --delete                   Delete any existing instances of ModelMesh Serving in Kube namespace before running install, including CRDs, RBACs, controller, older CRD with serving.kserve.io api group name, etc.
</span></span><span style=display:flex><span>  -u, --user-namespaces          Kubernetes namespaces to <span style=color:#999>enable</span> <span style=font-weight:700>for</span> ModelMesh Serving
</span></span><span style=display:flex><span>  --quickstart                   Install and configure required supporting datastores in the same namespace <span style=font-weight:700>(</span>etcd and MinIO<span style=font-weight:700>)</span> - <span style=font-weight:700>for</span> experimentation/development
</span></span><span style=display:flex><span>  --fvt                          Install and configure required supporting datastores in the same namespace <span style=font-weight:700>(</span>etcd and MinIO<span style=font-weight:700>)</span> - <span style=font-weight:700>for</span> development with fvt enabled
</span></span><span style=display:flex><span>  -dev, --dev-mode-logging       Enable dev mode logging <span style=font-weight:700>(</span>stacktraces on warning and no sampling<span style=font-weight:700>)</span>
</span></span><span style=display:flex><span>  --namespace-scope-mode         Run ModelMesh Serving in namespace scope mode
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>Installs ModelMesh Serving CRDs, controller, and built-in runtimes into specified
</span></span><span style=display:flex><span>Kubernetes namespaces.
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>Expects cluster-admin authority and Kube cluster access to be configured prior to running.
</span></span><span style=display:flex><span>Also requires Etcd secret <span style=color:#b84>&#39;model-serving-etcd&#39;</span> to be created in namespace already.
</span></span></code></pre></div><p>You can optionally provide a local <code>--install-config-path</code> that points to a local ModelMesh Serving tar file or directory containing ModelMesh Serving configs to deploy. If not specified, the <code>config</code> directory from the root of the project will be used.</p><p>You can also optionally use <code>--delete</code> to delete any existing instances of ModelMesh Serving in the designated Kube namespace before running the install.</p><p>The installation will create a secret named <code>storage-config</code> if it does not already exist. If the <code>--quickstart</code> option was chosen, this will be populated with the connection details for the example models bucket in IBM Cloud Object Storage and the local MinIO; otherwise, it will be empty and ready for you to add your own entries.</p><h3 id=setup-additional-namespaces x-intersect="currentHeading = '#setup-additional-namespaces'">Setup additional namespaces</h3><p>To enable additional namespaces for ModelMesh after the initial installation, you need to add a label named <code>modelmesh-enabled</code>, and optionally setup the storage secret <code>storage-config</code> and built-in runtimes, in the user namespaces.</p><p>The following command will add the label to &ldquo;your_namespace&rdquo;.</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>kubectl label namespace your_namespace modelmesh-enabled<span style=font-weight:700>=</span><span style=color:#b84>&#34;true&#34;</span> --overwrite
</span></span></code></pre></div><p>You can also run a script to setup multiple user namespaces. See the setup help below for detail:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>./scripts/setup_user_namespaces.sh --help
</span></span><span style=display:flex><span>Run this script to <span style=color:#999>enable</span> user namespaces <span style=font-weight:700>for</span> ModelMesh Serving, and optionally add the storage secret
</span></span><span style=display:flex><span><span style=font-weight:700>for</span> example models and built-in serving runtimes to the target namespaces.
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>usage: ./scripts/setup_user_namespaces.sh <span style=font-weight:700>[</span>flags<span style=font-weight:700>]</span>
</span></span><span style=display:flex><span>  Flags:
</span></span><span style=display:flex><span>    -u, --user-namespaces         <span style=font-weight:700>(</span>required<span style=font-weight:700>)</span> Kubernetes user namespaces to <span style=color:#999>enable</span> <span style=font-weight:700>for</span> ModelMesh
</span></span><span style=display:flex><span>    -c, --controller-namespace    Kubernetes ModelMesh controller namespace, default is modelmesh-serving
</span></span><span style=display:flex><span>    --create-storage-secret       Create storage secret <span style=font-weight:700>for</span> example models
</span></span><span style=display:flex><span>    --deploy-serving-runtimes     Deploy built-in serving runtimes
</span></span><span style=display:flex><span>    --dev-mode                    Run in development mode meaning the configs are local, not release based
</span></span><span style=display:flex><span>    -h, --help                    Display this <span style=color:#999>help</span>
</span></span></code></pre></div><p>The following command will setup two namespaces with the required label, optional storage secret, and built-in runtimes, so you can deploy sample predictors into any of them immediately.</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>./scripts/setup_user_namespaces.sh -u <span style=color:#b84>&#34;ns1 ns2&#34;</span> --create-storage-secret --deploy-serving-runtimes
</span></span></code></pre></div><h3 id=delete-the-installation x-intersect="currentHeading = '#delete-the-installation'">Delete the installation</h3><p>To wipe out the ModelMesh Serving CRDs, controller, and built-in runtimes from the specified Kubernetes namespaces:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>./scripts/delete.sh --namespace modelmesh-serving
</span></span></code></pre></div><p>(Optional) Delete the specified namespace <code>modelmesh-serving</code></p><pre tabindex=0><code>kubectl delete ns modelmesh-serving
</code></pre><div class=footnotes role=doc-endnotes><hr><ol><li id=fn:1><p>To start a <a href=https://ruivieira.dev/minikube.html>Minikube</a> cluster with a specific <a href=https://ruivieira.dev/kubernetes.html>Kubernetes</a> version see <a href=https://ruivieira.dev/minikube.html#kubernetes-version>this section</a>.&#160;<a href=#fnref:1 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:2><p><a href=https://kubernetes.io/docs/tasks/tools/#kubectl>https://kubernetes.io/docs/tasks/tools/#kubectl</a>&#160;<a href=#fnref:2 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:3><p><a href=https://kubectl.docs.kubernetes.io/installation/kustomize/>https://kubectl.docs.kubernetes.io/installation/kustomize/</a>&#160;<a href=#fnref:3 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:4><p>To start a <a href=https://ruivieira.dev/minikube.html>Minikube</a> cluster with specific memory and vCPU specs, see <a href=https://ruivieira.dev/minikube.html#memory>this section</a>.&#160;<a href=#fnref:4 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li></ol></div></div></article><div id=footer-post-container><div id=footer-post><div id=nav-footer style=display:none><ul><li><a href=https://ruivieira.dev/>Home</a></li><li><a href=https://ruivieira.dev/blog/>Blog</a></li><li><a href=https://ruivieira.dev/draw/>Drawings</a></li><li><a href=https://ruivieira.dev/map/>All pages</a></li><li><a href=https://ruivieira.dev/search.html>Search</a></li></ul></div><div id=toc-footer style=display:none><nav id=TableOfContents><ul><li><a href=#getting-started>Getting started</a><ul><li><a href=#prerequisites>Prerequisites</a></li><li><a href=#1-install-modelmesh-serving>1. Install ModelMesh Serving</a></li><li><a href=#2-deploy-a-model>2. Deploy a model</a></li><li><a href=#3-perform-an-inference-request>3. Perform an inference request</a></li><li><a href=#4-optional-deleting-your-modelmesh-serving-installation>4. (Optional) Deleting your ModelMesh Serving installation</a></li></ul></li><li><a href=#implementing-a-custom-serving-runtime>Implementing a Custom Serving Runtime</a><ul><li><a href=#model-server-management-spi>Model server Management SPI</a></li><li><a href=#deploying-a-runtime>Deploying a Runtime</a></li></ul></li><li><a href=#integrating-with-existing-model-servers>Integrating with existing model servers</a><ul><li><a href=#reference>Reference</a></li></ul></li><li><a href=#installation>Installation</a><ul><li><a href=#prerequisites-1>Prerequisites</a></li><li><a href=#setup-the-etcd-connection-information>Setup the etcd connection information</a></li><li><a href=#installation-1>Installation</a></li><li><a href=#setup-additional-namespaces>Setup additional namespaces</a></li><li><a href=#delete-the-installation>Delete the installation</a></li></ul></li></ul></nav></div><div id=share-footer style=display:none></div><div id=actions-footer><a id=menu-toggle class=icon href=# onclick='return $("#nav-footer").toggle(),!1' aria-label=Menu><i class="fas fa-bars fa-lg" aria-hidden=true></i> Menu</a>
<a id=toc-toggle class=icon href=# onclick='return $("#toc-footer").toggle(),!1' aria-label=TOC><i class="fas fa-list fa-lg" aria-hidden=true></i> TOC</a>
<a id=share-toggle class=icon href=# onclick='return $("#share-footer").toggle(),!1' aria-label=Share><i class="fas fa-share-alt fa-lg" aria-hidden=true></i> share</a>
<a id=top style=display:none class=icon href=# onclick='$("html, body").animate({scrollTop:0},"fast")' aria-label="Top of Page"><i class="fas fa-chevron-up fa-lg" aria-hidden=true></i> Top</a></div></div></div><footer id=footer><div class=footer-left>Copyright &copy; 2023 Rui Vieira</div><div class=footer-right><nav><ul><li><a href=https://ruivieira.dev/>Home</a></li><li><a href=https://ruivieira.dev/blog/>Blog</a></li><li><a href=https://ruivieira.dev/draw/>Drawings</a></li><li><a href=https://ruivieira.dev/map/>All pages</a></li><li><a href=https://ruivieira.dev/search.html>Search</a></li></ul></nav></div></footer></div></body><link rel=stylesheet href=https://ruivieira.dev/css/fa.min.css><script src=https://ruivieira.dev/js/jquery-3.6.0.min.js></script>
<script src=https://ruivieira.dev/js/mark.min.js></script>
<script src=https://ruivieira.dev/js/main.js></script></html>