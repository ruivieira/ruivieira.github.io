<!doctype html><html lang=en-uk><head><script data-goatcounter=https://ruivieira-dev.goatcounter.com/count async src=//gc.zgo.at/count.js></script>
<script type=module src=/js/deeplinks/deeplinks.js></script>
<link rel=preload href=/lib/fonts/fa-brands-400.woff2 as=font type=font/woff2 crossorigin=anonymous><link rel=preload href=/lib/fonts/fa-regular-400.woff2 as=font type=font/woff2 crossorigin=anonymous><link rel=preload href=/lib/fonts/fa-solid-900.woff2 as=font type=font/woff2 crossorigin=anonymous><link rel=preload href=/fonts/firacode/FiraCode-Regular.woff2 as=font type=font/woff2 crossorigin=anonymous><link rel=preload href=/fonts/prociono/Prociono-Regular.woff2 as=font type=font/woff2 crossorigin=anonymous><link rel=stylesheet href=/css/kbd.css type=text/css><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><title>Optimising random forest hyperparameters · Rui Vieira</title><link rel=canonical href=/optimising-random-forest-hyperparamaters.html><meta name=viewport content="width=device-width,initial-scale=1"><meta name=robots content="all,follow"><meta name=googlebot content="index,follow,snippet,archive"><meta property="og:title" content="Optimising random forest hyperparameters"><meta property="og:description" content="Typically the hyper-parameters which will have the most significant impact on the behaviour of a random forest are the following:
he number of decision trees in a random forest The split criteria Maximum depth of individual trees Maximum number of leaf nodes Random features per split umber of samples in bootstrap dataset We will look at each of these hyper-parameters individually with examples of how to select them.
Data To understand how we can optimise the hyperparameters in a random forest model, we will use scikit-learn&rsquo;s RandomForestClassifier and a subset of Titanic1 dataset."><meta property="og:type" content="article"><meta property="og:url" content="/optimising-random-forest-hyperparamaters.html"><meta property="article:section" content="posts"><meta property="article:modified_time" content="2022-09-24T21:50:52+01:00"><meta name=twitter:card content="summary"><meta name=twitter:title content="Optimising random forest hyperparameters"><meta name=twitter:description content="Typically the hyper-parameters which will have the most significant impact on the behaviour of a random forest are the following:
he number of decision trees in a random forest The split criteria Maximum depth of individual trees Maximum number of leaf nodes Random features per split umber of samples in bootstrap dataset We will look at each of these hyper-parameters individually with examples of how to select them.
Data To understand how we can optimise the hyperparameters in a random forest model, we will use scikit-learn&rsquo;s RandomForestClassifier and a subset of Titanic1 dataset."><link rel=stylesheet href=/css/styles.css><!--[if lt IE 9]><script src=https://oss.maxcdn.com/html5shiv/3.7.2/html5shiv.min.js></script>
<script src=https://oss.maxcdn.com/respond/1.4.2/respond.min.js></script><![endif]--><link rel=icon type=image/png href=images/favicon.ico></head><body class="max-width mx-auto px3 ltr"><div class="content index py4"><div id=header-post><a id=menu-icon href=#><i class="fas fa-eye fa-lg"></i></a>
<a id=menu-icon-tablet href=#><i class="fas fa-eye fa-lg"></i></a>
<a id=top-icon-tablet href=# onclick='$("html, body").animate({scrollTop:0},"fast")' style=display:none aria-label="Top of Page"><i class="fas fa-chevron-up fa-lg"></i></a>
<span id=menu><span id=nav><ul><li><a href=/>Home</a></li><li><a href=/map/>All pages</a></li><li><a href=/search.html>Search</a></li></ul></span><br><div id=share style=display:none></div><div id=toc><h4>Contents</h4><nav id=TableOfContents><ul><li><a href=#data>Data</a></li><li><a href=#naive-model>Naive model</a></li><li><a href=#hyperparameter-search>Hyperparameter search</a></li><li><a href=#parameters>Parameters</a><ul><li><a href=#number-of-decision-trees>Number of decision trees</a></li><li><a href=#the-split-criteria>The split criteria</a></li><li><a href=#maximum-depth-of-individual-trees>Maximum depth of individual trees</a></li><li><a href=#maximum-number-of-leaf-nodes>Maximum number of leaf nodes</a></li><li><a href=#random-features-per-split>Random features per split</a></li></ul></li><li><a href=#bootstrap-dataset-size>Bootstrap dataset size</a></li></ul></nav><h4>Related</h4><nav><ul><li class="header-post toc"><span class=backlink-count>1</span>
<a href=/scikit-learn.html>Scikit-learn</a></li></ul></nav></div></span></div><article class=post itemscope itemtype=http://schema.org/BlogPosting><header><h1 class=posttitle itemprop="name headline">Optimising random forest hyperparameters</h1><div class=meta><div class=postdate>Updated <time datetime="2022-09-24 21:50:52 +0100 BST" itemprop=datePublished>2022-09-24</time>
<span class=commit-hash>(<a href=/log/index.html#021d146>021d146</a>)</span></div></div></header><div class=content itemprop=articleBody><p>Typically the hyper-parameters which will have the most significant impact on the behaviour of a random forest are the following:</p><ul><li><a href=#number-of-decision-trees>he number of decision trees</a> in a random forest</li><li><a href=#the-split-criteria>The split criteria</a></li><li><a href=#maximum-depth-of-individual-trees>Maximum depth of individual trees</a></li><li><a href=#maximum-number-of-leaf-nodes>Maximum number of leaf nodes</a></li><li><a href=#random-features-per-split>Random features per split</a></li><li><a href=#bootstrap-dataset-size>umber of samples in bootstrap dataset</a></li></ul><p>We will look at each of these hyper-parameters individually with examples of how to select them.</p><h2 id=data>Data</h2><p>To understand how we can optimise the hyperparameters in a random forest model, we will use <a href=/scikit-learn.html>scikit-learn&rsquo;s</a> <code>RandomForestClassifier</code> and a subset of <em>Titanic</em><sup id=fnref:1><a href=#fn:1 class=footnote-ref role=doc-noteref>1</a></sup> dataset.</p><p>First, we will import the features and labels using <a href=/pandas.html>Pandas</a>.</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=font-weight:700>import</span> <span style=color:#555>pandas</span> <span style=font-weight:700>as</span> <span style=color:#555>pd</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>train_features <span style=font-weight:700>=</span> pd<span style=font-weight:700>.</span>read_csv(<span style=color:#b84>&#34;data/svm-hyperparameters-train-features.csv&#34;</span>)
</span></span><span style=display:flex><span>train_label <span style=font-weight:700>=</span> pd<span style=font-weight:700>.</span>read_csv(<span style=color:#b84>&#34;data/svm-hyperparameters-train-label.csv&#34;</span>)
</span></span></code></pre></div><p>Let&rsquo;s look at a random sample of entries from this dataset, both for features and labels.</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>train_features<span style=font-weight:700>.</span>sample(<span style=color:#099>10</span>)
</span></span></code></pre></div><p>Some of the available features are:</p><ul><li><code>Pclass</code>, ticket class</li><li><code>Sex</code></li><li><code>Age</code>, age in years</li><li><code>Sibsp</code>, number of siblings/spouses aboard</li><li><code>Parch</code>, number of parents/children aboard</li><li><code>Fare</code>, passenger fare</li></ul><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>train_label<span style=font-weight:700>.</span>sample(<span style=color:#099>10</span>)
</span></span></code></pre></div><p>The outcome label indicates whether a passenger survived the disaster.</p><p>As part of the typical initial steps for model training, we will prepare the data by splitting it into a training and testing subset.</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=font-weight:700>from</span> <span style=color:#555>sklearn.model_selection</span> <span style=font-weight:700>import</span> train_test_split
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>X_train, X_test, y_train, y_test <span style=font-weight:700>=</span> train_test_split(train_features, train_label, test_size<span style=font-weight:700>=</span><span style=color:#099>0.33</span>, random_state<span style=font-weight:700>=</span><span style=color:#099>23</span>)
</span></span></code></pre></div><h2 id=naive-model>Naive model</h2><p>First we will train a &ldquo;naive&rdquo; model, that is a model using the defaults provided by <code>RandomForestClassifier</code><sup id=fnref:2><a href=#fn:2 class=footnote-ref role=doc-noteref>2</a></sup>. These defaults are:</p><ul><li><code>n_estimators = 10</code></li><li><code>criterion=’gini’</code></li><li><code>max_depth=None</code></li><li><code>min_samples_split=2</code></li><li><code>min_samples_leaf=1</code></li><li><code>min_weight_fraction_leaf=0.0</code></li><li><code>max_features=’auto’</code></li><li><code>max_leaf_nodes=None</code></li><li><code>min_impurity_decrease=0.0</code></li><li><code>min_impurity_split=None</code></li><li><code>bootstrap=True</code></li><li><code>oob_score=False</code></li><li><code>n_jobs=1</code></li><li><code>random_state=None</code></li><li><code>verbose=0</code></li><li><code>warm_start=False</code></li><li><code>class_weight=None</code></li></ul><p>We will instantiate a random forest classifier:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=font-weight:700>from</span> <span style=color:#555>sklearn.ensemble</span> <span style=font-weight:700>import</span> RandomForestClassifier
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>rf <span style=font-weight:700>=</span> RandomForestClassifier()
</span></span></code></pre></div><p>And training it using the <code>X_train</code> and <code>y_train</code> subsets using the appropriate <code>fit</code> method<sup id=fnref:3><a href=#fn:3 class=footnote-ref role=doc-noteref>3</a></sup>.</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>true_labels <span style=font-weight:700>=</span> train_label<span style=font-weight:700>.</span>values<span style=font-weight:700>.</span>ravel()
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>rf<span style=font-weight:700>.</span>fit(X_train, y_train<span style=font-weight:700>.</span>values<span style=font-weight:700>.</span>ravel())
</span></span></code></pre></div><p>We can now evaluate trained naive model&rsquo;s score.</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=font-weight:700>from</span> <span style=color:#555>sklearn.metrics</span> <span style=font-weight:700>import</span> precision_score
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>predicted_labels <span style=font-weight:700>=</span> rf<span style=font-weight:700>.</span>predict(X_test)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>precision_score(y_test, predicted_labels)
</span></span></code></pre></div><h2 id=hyperparameter-search>Hyperparameter search</h2><p>A simple example of a generic hyperparameter search using the <a href=https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html><code>GridSearchCV</code></a> method in <code>scikit-learn</code>. The score used to measure the &ldquo;best&rdquo; model is the <code>mean_test_score</code>, but other metrics could be used, such as the <a href=/oob-score-in-random-forests.html>Out-of-bag (OOB)</a> error.</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>parameters <span style=font-weight:700>=</span> {
</span></span><span style=display:flex><span>    <span style=color:#b84>&#34;n_estimators&#34;</span>:[<span style=color:#099>5</span>,<span style=color:#099>10</span>,<span style=color:#099>50</span>,<span style=color:#099>100</span>,<span style=color:#099>250</span>],
</span></span><span style=display:flex><span>    <span style=color:#b84>&#34;max_depth&#34;</span>:[<span style=color:#099>2</span>,<span style=color:#099>4</span>,<span style=color:#099>8</span>,<span style=color:#099>16</span>,<span style=color:#099>32</span>,<span style=font-weight:700>None</span>]
</span></span><span style=display:flex><span>    
</span></span><span style=display:flex><span>}
</span></span></code></pre></div><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>rfc <span style=font-weight:700>=</span> RandomForestClassifier()
</span></span></code></pre></div><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=font-weight:700>from</span> <span style=color:#555>sklearn.model_selection</span> <span style=font-weight:700>import</span> GridSearchCV
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>cv <span style=font-weight:700>=</span> GridSearchCV(rfc,parameters,cv<span style=font-weight:700>=</span><span style=color:#099>5</span>)
</span></span><span style=display:flex><span>cv<span style=font-weight:700>.</span>fit(X_train, y_train<span style=font-weight:700>.</span>values<span style=font-weight:700>.</span>ravel())
</span></span></code></pre></div><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=font-weight:700>def</span> <span style=color:#900;font-weight:700>display</span>(results):
</span></span><span style=display:flex><span>    <span style=color:#999>print</span>(<span style=color:#b84>f</span><span style=color:#b84>&#39;Best parameters are: </span><span style=color:#b84>{</span>results<span style=font-weight:700>.</span>best_params_<span style=color:#b84>}</span><span style=color:#b84>&#39;</span>)
</span></span><span style=display:flex><span>    <span style=color:#999>print</span>(<span style=color:#b84>&#34;</span><span style=color:#b84>\n</span><span style=color:#b84>&#34;</span>)
</span></span><span style=display:flex><span>    mean_score <span style=font-weight:700>=</span> results<span style=font-weight:700>.</span>cv_results_[<span style=color:#b84>&#39;mean_test_score&#39;</span>]
</span></span><span style=display:flex><span>    std_score <span style=font-weight:700>=</span> results<span style=font-weight:700>.</span>cv_results_[<span style=color:#b84>&#39;std_test_score&#39;</span>]
</span></span><span style=display:flex><span>    params <span style=font-weight:700>=</span> results<span style=font-weight:700>.</span>cv_results_[<span style=color:#b84>&#39;params&#39;</span>]
</span></span><span style=display:flex><span>    <span style=font-weight:700>for</span> mean,std,params <span style=font-weight:700>in</span> <span style=color:#999>zip</span>(mean_score,std_score,params):
</span></span><span style=display:flex><span>        <span style=color:#999>print</span>(<span style=color:#b84>f</span><span style=color:#b84>&#39;</span><span style=color:#b84>{</span><span style=color:#999>round</span>(mean,<span style=color:#099>3</span>)<span style=color:#b84>}</span><span style=color:#b84> + or -</span><span style=color:#b84>{</span><span style=color:#999>round</span>(std,<span style=color:#099>3</span>)<span style=color:#b84>}</span><span style=color:#b84> for the </span><span style=color:#b84>{</span>params<span style=color:#b84>}</span><span style=color:#b84>&#39;</span>)
</span></span></code></pre></div><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>display(cv)
</span></span></code></pre></div><h2 id=parameters>Parameters</h2><h3 id=number-of-decision-trees>Number of decision trees</h3><p>This is specified using the <code>n_estimators</code> hyper-parameter on the random forest initialisation.</p><p>Typically, a higher number of trees will lead to greater accuracy at the expense of model size and training time.</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>cv <span style=font-weight:700>=</span> GridSearchCV(rfc,{<span style=color:#b84>&#34;n_estimators&#34;</span>:[<span style=color:#099>2</span>, <span style=color:#099>4</span>, <span style=color:#099>8</span>, <span style=color:#099>16</span>, <span style=color:#099>32</span>, <span style=color:#099>64</span>, <span style=color:#099>128</span>, <span style=color:#099>256</span>, <span style=color:#099>512</span>]},cv<span style=font-weight:700>=</span><span style=color:#099>5</span>)
</span></span><span style=display:flex><span>cv<span style=font-weight:700>.</span>fit(X_train, y_train<span style=font-weight:700>.</span>values<span style=font-weight:700>.</span>ravel())
</span></span></code></pre></div><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>results <span style=font-weight:700>=</span> pd<span style=font-weight:700>.</span>DataFrame({<span style=color:#b84>&#34;n_estimators&#34;</span>: [param[<span style=color:#b84>&#34;n_estimators&#34;</span>] <span style=font-weight:700>for</span> param <span style=font-weight:700>in</span> cv<span style=font-weight:700>.</span>cv_results_[<span style=color:#b84>&#39;params&#39;</span>]],
</span></span><span style=display:flex><span>             <span style=color:#b84>&#34;mean_score&#34;</span>: <span style=color:#999>list</span>(cv<span style=font-weight:700>.</span>cv_results_[<span style=color:#b84>&#39;mean_test_score&#39;</span>]),
</span></span><span style=display:flex><span>             <span style=color:#b84>&#34;std_score&#34;</span>: cv<span style=font-weight:700>.</span>cv_results_[<span style=color:#b84>&#39;std_test_score&#39;</span>]})
</span></span><span style=display:flex><span>results
</span></span></code></pre></div><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=font-weight:700>from</span> <span style=color:#555>plotnine</span> <span style=font-weight:700>import</span> <span style=font-weight:700>*</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>(
</span></span><span style=display:flex><span>     ggplot(results) <span style=font-weight:700>+</span> geom_boxplot(aes(x<span style=font-weight:700>=</span><span style=color:#b84>&#39;factor(n_estimators)&#39;</span>, y<span style=font-weight:700>=</span><span style=color:#b84>&#39;mean_score&#39;</span>)) <span style=font-weight:700>+</span>
</span></span><span style=display:flex><span>    geom_errorbar(aes(x<span style=font-weight:700>=</span><span style=color:#b84>&#39;factor(n_estimators)&#39;</span>, ymin<span style=font-weight:700>=</span><span style=color:#b84>&#39;mean_score - std_score&#39;</span>, ymax<span style=font-weight:700>=</span><span style=color:#b84>&#39;mean_score + std_score&#39;</span>)) <span style=font-weight:700>+</span>
</span></span><span style=display:flex><span>    theme_classic() <span style=font-weight:700>+</span> xlab(<span style=color:#b84>&#39;Number of trees&#39;</span>) <span style=font-weight:700>+</span> ylab(<span style=color:#b84>&#39;Mean score&#39;</span>)
</span></span><span style=display:flex><span>)
</span></span></code></pre></div><h3 id=the-split-criteria>The split criteria</h3><p>At each node, a random forest decides, according to a specific algorithm, which feature and value split the tree.
Therefore, the choice of splitting algorithm is crucial for the random forest&rsquo;s performance.</p><p>Since, in this example, we are dealing with a classification problem, the choices of split algorithm are, for instance:</p><ul><li>Gini</li><li>Entropy</li></ul><p>If we were dealing with a random forest for regression, other methods (such as <a href=/error-metrics.html>MSE</a>) would be a possible choice.
We will now compare both split algorithms as specified above, in training a random forest with our data:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>rfc <span style=font-weight:700>=</span> RandomForestClassifier(n_estimators<span style=font-weight:700>=</span><span style=color:#099>256</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>cv <span style=font-weight:700>=</span> GridSearchCV(rfc,{<span style=color:#b84>&#34;criterion&#34;</span>: [<span style=color:#b84>&#34;gini&#34;</span>, <span style=color:#b84>&#34;entropy&#34;</span>]},cv<span style=font-weight:700>=</span><span style=color:#099>5</span>)
</span></span><span style=display:flex><span>cv<span style=font-weight:700>.</span>fit(X_train, y_train<span style=font-weight:700>.</span>values<span style=font-weight:700>.</span>ravel())
</span></span></code></pre></div><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>results <span style=font-weight:700>=</span> pd<span style=font-weight:700>.</span>DataFrame({<span style=color:#b84>&#34;criterion&#34;</span>: [param[<span style=color:#b84>&#34;criterion&#34;</span>] <span style=font-weight:700>for</span> param <span style=font-weight:700>in</span> cv<span style=font-weight:700>.</span>cv_results_[<span style=color:#b84>&#39;params&#39;</span>]],
</span></span><span style=display:flex><span>             <span style=color:#b84>&#34;mean_score&#34;</span>: <span style=color:#999>list</span>(cv<span style=font-weight:700>.</span>cv_results_[<span style=color:#b84>&#39;mean_test_score&#39;</span>]),
</span></span><span style=display:flex><span>             <span style=color:#b84>&#34;std_score&#34;</span>: cv<span style=font-weight:700>.</span>cv_results_[<span style=color:#b84>&#39;std_test_score&#39;</span>]})
</span></span><span style=display:flex><span>results
</span></span></code></pre></div><h3 id=maximum-depth-of-individual-trees>Maximum depth of individual trees</h3><p>In theory, the &ldquo;longer&rdquo; the tree, the more splits it can have and better accommodate the data. However, at the tree level can this can lead to overfitting.
Although this is a problem for decision trees, it is not necessarily a problem for the ensemble, the random forest.
Although the key is to strike a balance between trees that aren&rsquo;t too large or too short, there&rsquo;s no universal heuristic to determine the size.
Let&rsquo;s try a few option for maximum depth:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>rfc <span style=font-weight:700>=</span> RandomForestClassifier(n_estimators<span style=font-weight:700>=</span><span style=color:#099>256</span>,
</span></span><span style=display:flex><span>                           criterion<span style=font-weight:700>=</span><span style=color:#b84>&#34;entropy&#34;</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>cv <span style=font-weight:700>=</span> GridSearchCV(rfc,{<span style=color:#b84>&#39;max_depth&#39;</span>: [<span style=color:#099>2</span>, <span style=color:#099>4</span>, <span style=color:#099>8</span>, <span style=color:#099>16</span>, <span style=color:#099>32</span>, <span style=font-weight:700>None</span>]},cv<span style=font-weight:700>=</span><span style=color:#099>5</span>)
</span></span><span style=display:flex><span>cv<span style=font-weight:700>.</span>fit(X_train, y_train<span style=font-weight:700>.</span>values<span style=font-weight:700>.</span>ravel())
</span></span></code></pre></div><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>results <span style=font-weight:700>=</span> pd<span style=font-weight:700>.</span>DataFrame({<span style=color:#b84>&#34;max_depth&#34;</span>: [param[<span style=color:#b84>&#34;max_depth&#34;</span>] <span style=font-weight:700>for</span> param <span style=font-weight:700>in</span> cv<span style=font-weight:700>.</span>cv_results_[<span style=color:#b84>&#39;params&#39;</span>]],
</span></span><span style=display:flex><span>             <span style=color:#b84>&#34;mean_score&#34;</span>: <span style=color:#999>list</span>(cv<span style=font-weight:700>.</span>cv_results_[<span style=color:#b84>&#39;mean_test_score&#39;</span>]),
</span></span><span style=display:flex><span>             <span style=color:#b84>&#34;std_score&#34;</span>: cv<span style=font-weight:700>.</span>cv_results_[<span style=color:#b84>&#39;std_test_score&#39;</span>]})
</span></span><span style=display:flex><span>results <span style=font-weight:700>=</span> results<span style=font-weight:700>.</span>dropna()
</span></span><span style=display:flex><span>results
</span></span></code></pre></div><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=font-weight:700>from</span> <span style=color:#555>plotnine</span> <span style=font-weight:700>import</span> <span style=font-weight:700>*</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>(
</span></span><span style=display:flex><span>     ggplot(results) <span style=font-weight:700>+</span> geom_boxplot(aes(x<span style=font-weight:700>=</span><span style=color:#b84>&#39;factor(max_depth)&#39;</span>, y<span style=font-weight:700>=</span><span style=color:#b84>&#39;mean_score&#39;</span>)) <span style=font-weight:700>+</span>
</span></span><span style=display:flex><span>    theme_classic() <span style=font-weight:700>+</span> xlab(<span style=color:#b84>&#39;Max tree depth&#39;</span>) <span style=font-weight:700>+</span> ylab(<span style=color:#b84>&#39;Mean score&#39;</span>)
</span></span><span style=display:flex><span>)
</span></span></code></pre></div><h3 id=maximum-number-of-leaf-nodes>Maximum number of leaf nodes</h3><p>This hyperparameter can be of importance to other topics, such as <a href=/explainability.html>explainability</a>.</p><p>It is specified in <code>scikit-learn</code> using the <code>max_leaf_nodes</code> parameter. Let&rsquo;s try a few different values:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>rfc <span style=font-weight:700>=</span> RandomForestClassifier(n_estimators<span style=font-weight:700>=</span><span style=color:#099>256</span>,
</span></span><span style=display:flex><span>                           criterion<span style=font-weight:700>=</span><span style=color:#b84>&#34;entropy&#34;</span>,
</span></span><span style=display:flex><span>                            max_depth<span style=font-weight:700>=</span><span style=color:#099>8</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>cv <span style=font-weight:700>=</span> GridSearchCV(rfc,{<span style=color:#b84>&#39;max_leaf_nodes&#39;</span>: [<span style=color:#099>2</span><span style=font-weight:700>**</span>i <span style=font-weight:700>for</span> i <span style=font-weight:700>in</span> <span style=color:#999>range</span>(<span style=color:#099>1</span>, <span style=color:#099>8</span>)]},cv<span style=font-weight:700>=</span><span style=color:#099>5</span>)
</span></span><span style=display:flex><span>cv<span style=font-weight:700>.</span>fit(X_train, y_train<span style=font-weight:700>.</span>values<span style=font-weight:700>.</span>ravel())
</span></span></code></pre></div><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>results <span style=font-weight:700>=</span> pd<span style=font-weight:700>.</span>DataFrame({<span style=color:#b84>&#34;max_leaf_nodes&#34;</span>: [param[<span style=color:#b84>&#34;max_leaf_nodes&#34;</span>] <span style=font-weight:700>for</span> param <span style=font-weight:700>in</span> cv<span style=font-weight:700>.</span>cv_results_[<span style=color:#b84>&#39;params&#39;</span>]],
</span></span><span style=display:flex><span>             <span style=color:#b84>&#34;mean_score&#34;</span>: <span style=color:#999>list</span>(cv<span style=font-weight:700>.</span>cv_results_[<span style=color:#b84>&#39;mean_test_score&#39;</span>]),
</span></span><span style=display:flex><span>             <span style=color:#b84>&#34;std_score&#34;</span>: cv<span style=font-weight:700>.</span>cv_results_[<span style=color:#b84>&#39;std_test_score&#39;</span>]})
</span></span><span style=display:flex><span>results <span style=font-weight:700>=</span> results<span style=font-weight:700>.</span>dropna()
</span></span><span style=display:flex><span>results
</span></span></code></pre></div><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=font-weight:700>from</span> <span style=color:#555>plotnine</span> <span style=font-weight:700>import</span> <span style=font-weight:700>*</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>(
</span></span><span style=display:flex><span>     ggplot(results) <span style=font-weight:700>+</span> geom_boxplot(aes(x<span style=font-weight:700>=</span><span style=color:#b84>&#39;factor(max_leaf_nodes)&#39;</span>, y<span style=font-weight:700>=</span><span style=color:#b84>&#39;mean_score&#39;</span>)) <span style=font-weight:700>+</span>
</span></span><span style=display:flex><span>    geom_errorbar(aes(x<span style=font-weight:700>=</span><span style=color:#b84>&#39;factor(max_leaf_nodes)&#39;</span>, ymin<span style=font-weight:700>=</span><span style=color:#b84>&#39;mean_score - std_score&#39;</span>, ymax<span style=font-weight:700>=</span><span style=color:#b84>&#39;mean_score + std_score&#39;</span>)) <span style=font-weight:700>+</span>
</span></span><span style=display:flex><span>    theme_classic() <span style=font-weight:700>+</span> xlab(<span style=color:#b84>&#39;Maximum leaf nodes&#39;</span>) <span style=font-weight:700>+</span> ylab(<span style=color:#b84>&#39;Mean score&#39;</span>)
</span></span><span style=display:flex><span>)
</span></span></code></pre></div><h3 id=random-features-per-split>Random features per split</h3><p>This is an important hyperparameter that will depend on how noisy the original data is.
Typically, if the data is not very noisy, the number of used random features can be kept low. Otherwise, it needs to be kept high.</p><p>An important consideration is also the following trade-off:</p><ul><li>A low number of random features decrease the forest&rsquo;s overall variance</li><li>A low number of random features increases the bias</li><li>A high number of random features increases computational time</li></ul><p>In <code>scikit-learn</code> this is specified with the <code>max_features</code> parameter. Assuming $N_f$ is the total number of features,
some possible values for this parameter are:</p><ul><li><code>sqrt</code>, this will take the <code>max_features</code> as the rounded $\sqrt{N_f}$</li><li><code>log2</code>, as above, takes the $\log_2(N_f)$</li><li>The actual maximum number of features can be directly specified</li></ul><p>Let&rsquo;s try a simple benchmark, even though our data does not have many features to begin with:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>rfc <span style=font-weight:700>=</span> RandomForestClassifier(n_estimators<span style=font-weight:700>=</span><span style=color:#099>256</span>,
</span></span><span style=display:flex><span>                             criterion<span style=font-weight:700>=</span><span style=color:#b84>&#34;entropy&#34;</span>,
</span></span><span style=display:flex><span>                             max_depth<span style=font-weight:700>=</span><span style=color:#099>8</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>cv <span style=font-weight:700>=</span> GridSearchCV(rfc,{<span style=color:#b84>&#39;max_features&#39;</span>: [<span style=color:#b84>&#34;sqrt&#34;</span>, <span style=color:#b84>&#34;log2&#34;</span>, <span style=color:#099>1</span>, <span style=color:#099>2</span>, <span style=color:#099>3</span>, <span style=color:#099>4</span>, <span style=color:#099>5</span>, <span style=color:#099>6</span>]},cv<span style=font-weight:700>=</span><span style=color:#099>5</span>)
</span></span><span style=display:flex><span>cv<span style=font-weight:700>.</span>fit(X_train, y_train<span style=font-weight:700>.</span>values<span style=font-weight:700>.</span>ravel())
</span></span></code></pre></div><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>results <span style=font-weight:700>=</span> pd<span style=font-weight:700>.</span>DataFrame({<span style=color:#b84>&#34;max_features&#34;</span>: [param[<span style=color:#b84>&#34;max_features&#34;</span>] <span style=font-weight:700>for</span> param <span style=font-weight:700>in</span> cv<span style=font-weight:700>.</span>cv_results_[<span style=color:#b84>&#39;params&#39;</span>]],
</span></span><span style=display:flex><span>             <span style=color:#b84>&#34;mean_score&#34;</span>: <span style=color:#999>list</span>(cv<span style=font-weight:700>.</span>cv_results_[<span style=color:#b84>&#39;mean_test_score&#39;</span>]),
</span></span><span style=display:flex><span>             <span style=color:#b84>&#34;std_score&#34;</span>: cv<span style=font-weight:700>.</span>cv_results_[<span style=color:#b84>&#39;std_test_score&#39;</span>]})
</span></span><span style=display:flex><span>results
</span></span></code></pre></div><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=font-weight:700>from</span> <span style=color:#555>plotnine</span> <span style=font-weight:700>import</span> <span style=font-weight:700>*</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>(
</span></span><span style=display:flex><span>     ggplot(results) <span style=font-weight:700>+</span> geom_boxplot(aes(x<span style=font-weight:700>=</span><span style=color:#b84>&#39;factor(max_features)&#39;</span>, y<span style=font-weight:700>=</span><span style=color:#b84>&#39;mean_score&#39;</span>)) <span style=font-weight:700>+</span>
</span></span><span style=display:flex><span>    geom_errorbar(aes(x<span style=font-weight:700>=</span><span style=color:#b84>&#39;factor(max_features)&#39;</span>, ymin<span style=font-weight:700>=</span><span style=color:#b84>&#39;mean_score - std_score&#39;</span>, ymax<span style=font-weight:700>=</span><span style=color:#b84>&#39;mean_score + std_score&#39;</span>)) <span style=font-weight:700>+</span>
</span></span><span style=display:flex><span>    theme_classic() <span style=font-weight:700>+</span> xlab(<span style=color:#b84>&#39;Maximum number of features&#39;</span>) <span style=font-weight:700>+</span> ylab(<span style=color:#b84>&#39;Mean score&#39;</span>)
</span></span><span style=display:flex><span>)
</span></span></code></pre></div><h2 id=bootstrap-dataset-size>Bootstrap dataset size</h2><p>This hyperparameter relates to the proportion of the training data to be used by decision trees.</p><p>It is specified in <code>scikit-learn</code> by <code>max_samples</code> and can take the value of either:</p><ul><li><code>None</code>, take the entirety of the samples</li><li>An integer, representing the actual number of samples</li><li>A float, representing a proportion between <code>0</code> and <code>1</code> or the samples to take.</li></ul><p>Let&rsquo;s try a hyperparameter search with some values:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>rfc <span style=font-weight:700>=</span> RandomForestClassifier(n_estimators<span style=font-weight:700>=</span><span style=color:#099>256</span>,
</span></span><span style=display:flex><span>                             criterion<span style=font-weight:700>=</span><span style=color:#b84>&#34;entropy&#34;</span>,
</span></span><span style=display:flex><span>                             max_depth<span style=font-weight:700>=</span><span style=color:#099>8</span>,
</span></span><span style=display:flex><span>                             max_features<span style=font-weight:700>=</span><span style=color:#099>6</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>cv <span style=font-weight:700>=</span> GridSearchCV(rfc,{<span style=color:#b84>&#39;max_samples&#39;</span>: [i<span style=font-weight:700>/</span><span style=color:#099>10.0</span> <span style=font-weight:700>for</span> i <span style=font-weight:700>in</span> <span style=color:#999>range</span>(<span style=color:#099>1</span>, <span style=color:#099>10</span>)]},cv<span style=font-weight:700>=</span><span style=color:#099>5</span>)
</span></span><span style=display:flex><span>cv<span style=font-weight:700>.</span>fit(X_train, y_train<span style=font-weight:700>.</span>values<span style=font-weight:700>.</span>ravel())
</span></span></code></pre></div><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>results <span style=font-weight:700>=</span> pd<span style=font-weight:700>.</span>DataFrame({<span style=color:#b84>&#34;max_samples&#34;</span>: [param[<span style=color:#b84>&#34;max_samples&#34;</span>] <span style=font-weight:700>for</span> param <span style=font-weight:700>in</span> cv<span style=font-weight:700>.</span>cv_results_[<span style=color:#b84>&#39;params&#39;</span>]],
</span></span><span style=display:flex><span>             <span style=color:#b84>&#34;mean_score&#34;</span>: <span style=color:#999>list</span>(cv<span style=font-weight:700>.</span>cv_results_[<span style=color:#b84>&#39;mean_test_score&#39;</span>]),
</span></span><span style=display:flex><span>             <span style=color:#b84>&#34;std_score&#34;</span>: cv<span style=font-weight:700>.</span>cv_results_[<span style=color:#b84>&#39;std_test_score&#39;</span>]})
</span></span><span style=display:flex><span>results
</span></span></code></pre></div><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=font-weight:700>from</span> <span style=color:#555>plotnine</span> <span style=font-weight:700>import</span> <span style=font-weight:700>*</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>(
</span></span><span style=display:flex><span>     ggplot(results) <span style=font-weight:700>+</span> geom_boxplot(aes(x<span style=font-weight:700>=</span><span style=color:#b84>&#39;factor(max_samples)&#39;</span>, y<span style=font-weight:700>=</span><span style=color:#b84>&#39;mean_score&#39;</span>)) <span style=font-weight:700>+</span>
</span></span><span style=display:flex><span>    geom_errorbar(aes(x<span style=font-weight:700>=</span><span style=color:#b84>&#39;factor(max_samples)&#39;</span>, ymin<span style=font-weight:700>=</span><span style=color:#b84>&#39;mean_score - std_score&#39;</span>, ymax<span style=font-weight:700>=</span><span style=color:#b84>&#39;mean_score + std_score&#39;</span>)) <span style=font-weight:700>+</span>
</span></span><span style=display:flex><span>    theme_classic() <span style=font-weight:700>+</span> xlab(<span style=color:#b84>&#39;Proportion bootstrap samples&#39;</span>) <span style=font-weight:700>+</span> ylab(<span style=color:#b84>&#39;Mean score&#39;</span>)
</span></span><span style=display:flex><span>)
</span></span></code></pre></div><div class=footnotes role=doc-endnotes><hr><ol><li id=fn:1><p>Titanic Dataset - <a href=https://www.kaggle.com/c/titanic-dataset/data>https://www.kaggle.com/c/titanic-dataset/data</a>&#160;<a href=#fnref:1 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:2><p><a href=https://sklearn.org/modules/generated/sklearn.ensemble.RandomForestClassifier.html>https://sklearn.org/modules/generated/sklearn.ensemble.RandomForestClassifier.html</a>&#160;<a href=#fnref:2 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:3><p><a href=https://sklearn.org/modules/generated/sklearn.ensemble.RandomForestClassifier.html#sklearn.ensemble.RandomForestClassifier.fit>https://sklearn.org/modules/generated/sklearn.ensemble.RandomForestClassifier.html#sklearn.ensemble.RandomForestClassifier.fit</a>&#160;<a href=#fnref:3 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li></ol></div></div></article><div id=footer-post-container><div id=footer-post><div id=nav-footer style=display:none><ul><li><a href=/>Home</a></li><li><a href=/map/>All pages</a></li><li><a href=/search.html>Search</a></li></ul></div><div id=toc-footer style=display:none><nav id=TableOfContents><ul><li><a href=#data>Data</a></li><li><a href=#naive-model>Naive model</a></li><li><a href=#hyperparameter-search>Hyperparameter search</a></li><li><a href=#parameters>Parameters</a><ul><li><a href=#number-of-decision-trees>Number of decision trees</a></li><li><a href=#the-split-criteria>The split criteria</a></li><li><a href=#maximum-depth-of-individual-trees>Maximum depth of individual trees</a></li><li><a href=#maximum-number-of-leaf-nodes>Maximum number of leaf nodes</a></li><li><a href=#random-features-per-split>Random features per split</a></li></ul></li><li><a href=#bootstrap-dataset-size>Bootstrap dataset size</a></li></ul></nav></div><div id=share-footer style=display:none></div><div id=actions-footer><a id=menu-toggle class=icon href=# onclick='return $("#nav-footer").toggle(),!1' aria-label=Menu><i class="fas fa-bars fa-lg" aria-hidden=true></i> Menu</a>
<a id=toc-toggle class=icon href=# onclick='return $("#toc-footer").toggle(),!1' aria-label=TOC><i class="fas fa-list fa-lg" aria-hidden=true></i> TOC</a>
<a id=share-toggle class=icon href=# onclick='return $("#share-footer").toggle(),!1' aria-label=Share><i class="fas fa-share-alt fa-lg" aria-hidden=true></i> share</a>
<a id=top style=display:none class=icon href=# onclick='$("html, body").animate({scrollTop:0},"fast")' aria-label="Top of Page"><i class="fas fa-chevron-up fa-lg" aria-hidden=true></i> Top</a></div></div></div><footer id=footer><div class=footer-left>Copyright &copy; 2022 Rui Vieira</div><div class=footer-right><nav><ul><li><a href=/>Home</a></li><li><a href=/map/>All pages</a></li><li><a href=/search.html>Search</a></li></ul></nav></div></footer></div></body><link rel=stylesheet href=/css/fa.min.css><script src=/js/jquery-3.6.0.min.js></script>
<script src=/js/mark.min.js></script>
<script src=/js/main.js></script>
<script>MathJax={tex:{inlineMath:[["$","$"],["\\(","\\)"]]},svg:{fontCache:"global"}}</script><script type=text/javascript id=MathJax-script async src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js></script></html>