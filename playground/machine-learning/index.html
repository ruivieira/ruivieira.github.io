<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8" />
    <title>Starboard Notebook</title>
    <meta name="viewport" content="width=device-width,initial-scale=1" />
    <link rel="icon" href="favicon.ico" />
    <style>
      @font-face {
        font-family: JuliaMono;
        src: url("/fonts/JuliaMono-Regular.woff2") format("woff2");
      }
    </style>
    <link href="/css/style.css" rel="stylesheet" />
    <link href="_static/starboard-notebook-0.5.6.css" rel="stylesheet" />
    <script src="_static/vendors~codemirror.chunk.js"></script>
    <script src="_static/codemirror.chunk.js"></script>
    <style>
      .cells-container {
        margin-left: 25% !important;
        max-width: 50%;
      }
      .Í¼1 .cm-line {
        display: block;
        padding: 0px 2px 0px 4px;
        font-family: "JuliaMono";
        font-size: 0.8rem;
      }
      .cm-line > * {
        font-family: "JuliaMono";
        font-size: 0.8rem;
      }
      .katex {
        font-size: 0.9rem;
      }
    </style>
  </head>
  <body>
    <div id="sidebar">
      <h2>Other pages</h2>
      <ul>
        <li><a href="/">Posts</a></li>
        <li><a href="/micro/">Âµ-posts</a></li>
        <li><a href="/pages/about.html">About</a></li>
      </ul>
      <h2>Codex</h2>
      <ul>
        <li><a href="/codex/machine-learning">Machine learning</a></li>
        <li><a href="/codex/python-experiments">Python experiments</a></li>
      </ul>
      <h2>Playground</h2>
      <ul>
        <li><a href="/playground/machine-learning">Machine learning</a></li>
      </ul>
    </div>

    <script>
      // The content of the notebook as a string, remember to escape the string properly.
      window.initialNotebookContent = `
%% md
# Counterfactuals

## Introduction

To create the dataset, we will use the same technique as used in Classification data.

We will create two well separated clusters in $\\mathbb{R}^2$, corresponding to two features $f_1$ and $f_2$,
with an associated outcome $y=f\\left(f_1,f_2\\right)$ with possible values true ($y=1$) and false ($y=1$).

%% python
import pandas as pd
from sklearn.datasets import make_classification

RANDOM_STATE = 23
N_FEATURES = 2
N = 100

data = make_classification(
    n_samples=N,
    n_features=N_FEATURES,
    n_redundant=0,
    n_repeated=0,
    n_classes=2,
    n_clusters_per_class=1,
    weights=None,
    flip_y=0.01,
    class_sep=2.0,
    shift=0.0,
    scale=1.0,
    shuffle=True,
    random_state=RANDOM_STATE,
)
df = pd.DataFrame(data[0], columns=["f1", "f2"])

df["y"] = data[1]

x1 = df[df["y"] == 0]["f1"]
y1 = df[df["y"] == 0]["f2"]
x2 = df[df["y"] == 1]["f1"]
y2 = df[df["y"] == 1]["f2"]
%% python
import matplotlib.pyplot as plt

colours = {0: "pink", 1: "lightgreen", 2: "#fffb96"}
edges = {0: "red", 1: "green", 2: "#cac200"}

x_min = -4
x_max = 4
y_min = -5
y_max = 1

plt.scatter(x1, y1, c=colours[0], edgecolor=edges[0], label="false")
plt.scatter(x2, y2, c=colours[1], edgecolor=edges[1], label="true")
plt.xlim([x_min, x_max])
plt.ylim([y_min, y_max])
plt.xlabel("$f_1$")
plt.ylabel("$f_2$")
plt.title("Outcome for $f(f_1, f_2)$")
plt.legend(loc="upper right")
plt.show()
%% md
We will now train an SVC model with the generated data.
%% python
from sklearn.svm import SVC

svm = SVC(gamma="scale", probability=True)
svm.fit(data[0], data[1])
%% md
And calculate the modelâ€™s accuracy.
%% python
print("SVM classification accuracy: ", svm.score(data[0], data[1]))
%% md
We are now interested in predicting the counterfactual of a certain point ğ‘=(ğ‘“1,ğ‘“2).

To do it, we first chose a point and determined its predicted outcome, ğ‘¦=ğ‘“(ğ‘). We then proceed to find the closest set of features that would produce the opposite outcome, that is

ğ‘â€²=(ğ‘“â€²1,ğ‘“â€²2):ğ‘¦â€²=ğ‘“(ğ‘â€²)=âˆ¼ğ‘¦
Letâ€™s define our input as point ğ‘=(âˆ’0.5,âˆ’1) and visualise it along with the predicted outcome.

%% python

import numpy as np

p = [-0.5, -1]

p_input = np.array(p).reshape(1, -1)
p_prediction = svm.predict(p_input)
print(f"y=f({p}) predicted as " + "true" if p_prediction[0] == 1 else "false")

%% md

We therefore are interested in the set of features ğ‘â€² which will have a prediction of false.

%% python
import numpy as np
from matplotlib.colors import ListedColormap
import colorsys
import matplotlib.colors as mc

def adjust_lightness(color, amount=0.5):
    try:
        c = mc.cnames[color]
    except:
        c = color
    c = colorsys.rgb_to_hls(*mc.to_rgb(c))
    return colorsys.hls_to_rgb(c[0], max(0, min(1, amount * c[1])), c[2])


def adjust_opacity(color, amount=0.5):
    try:
        c = mc.cnames[color]
    except:
        c = color

    rgba = mc.to_rgba(c)
    return (rgba[0], rgba[1], rgba[2], amount)

cmap = ListedColormap(
    [adjust_opacity(colours[0], 0.1), adjust_opacity(colours[1], 0.2)]
)

h = 0.01
xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))
Z = svm.predict(np.c_[xx.ravel(), yy.ravel()])
Z = Z.reshape(xx.shape)
plt.pcolormesh(xx, yy, Z, cmap=cmap, shading="auto")
plt.scatter(x1, y1, c=colours[0], edgecolor=edges[0], label="false")
plt.scatter(x2, y2, c=colours[1], edgecolor=edges[1], label="true")
plt.scatter(p[0], p[1], s=50, c="black", marker="x")
plt.annotate(
    "input",
    xy=(p[0], p[1]),
    xytext=(-20, -15),
    textcoords="offset points",
)

plt.xlim([x_min, x_max])
plt.ylim([y_min, y_max])
plt.xlabel("$f_1$")
plt.ylabel("$f_2$")
plt.title("Outcome for $f(f_1, f_2)$")
plt.legend(loc="upper right")
plt.show()

%% md
We define now a function, nearest, which will return the existing data point that is closest to the input and which provides a counterfactual (that is, false outcome).

%% python

def nearest(p, data):
    """Return the data point closest to p, which has a 'false' outcome"""
    data_n = data[data["y"] == 0]
    distances = dist.pairwise(data_n[["f1", "f2"]], p)
    data_n.insert(3, "distances", distances, True)
    index_min = data_n.distances.idxmin()
    nearest_point = np.array(
        [data_n.loc[index_min]["f1"], data_n.loc[index_min]["f2"]]
    ).reshape(1, -1)
    return nearest_point, index_min

%% md
## Nelder-Mead method

We also define the following functions to determine the distance used in the loss function.

The cityblock metric corresponds to the Manhattan distance. If we assume two points ğ‘ and ğ‘, their Manhattan distance is defined by:

ğ‘‘1(ğ©,ğª)=â€–ğ©âˆ’ğªâ€–1=âˆ‘ğ‘–=1ğ‘›|ğ‘ğ‘–âˆ’ğ‘ğ‘–|

%% python

def loss_function_l1norm(x_dash):
    L = 2 * (logreg.predict(x_dash.reshape(1, -1)) - 1) ** 2 + cdist(
        example, x_dash.reshape(1, -1), metric="cityblock"
    )
    return L


def dist_mad(cf, eg):
    manhat = [
        cdist(eg.T, cf.reshape(1, -1).T, metric="cityblock")[i][i]
        for i in range(len(eg.T))
    ]
    mad = stats.median_abs_deviation(X, scale="normal")
    return sum(manhat / mad)


def loss_function_mad(x_dash):
    target = 0
    L = lamda * (
        svm.predict_proba(x_dash.reshape(1, -1))[0][target] - 1
    ) ** 2 + dist_mad(x_dash.reshape(1, -1), p_input)
    return L

%% md
For completeness we will also show the data point which is closest to our original input ğ‘, but with a different outcome.

%% python
from scipy.optimize import minimize
from sklearn.neighbors import DistanceMetric

# nearest neighbour
dist = DistanceMetric.get_metric("euclidean")
cf_nn, i = nearest(p_input, df)

while int(svm.predict(cf_nn)) != 0:
    data_input = df.drop([i])
    cf_nn, i = nearest(p_input, data_input)

prediction = "true" if svm.predict(cf_nn)[0] == 1 else "false"
print(f"p=({cf_nn}) predicted as {prediction}")

%% md
We use the Nelder-Mead method to minimise the loss function loss_function_mad.

%% python
from scipy import stats
from scipy.optimize import minimize
from scipy.spatial.distance import cdist, pdist

pred_threshold = 0.90

# initial conditions
lamda = 0.1
x0 = np.array([0.5, 0.0]).reshape(1, -1)  # initial guess for cf

res = minimize(
    loss_function_mad,
    x0,
    method="nelder-mead",
    options={"maxiter": 1000, "xatol": 1e-8},
)
cf = res.x.reshape(1, -1)

TARGET = 0  # false
prob_target = svm.predict_proba(cf)[0][TARGET]

i = 0
while prob_target < pred_threshold:
    lamda += 0.1
    x0 = cf  # starting point is current cf
    res = minimize(
        loss_function_mad,
        x0,
        method="nelder-mead",
        options={"maxiter": 1000, "xatol": 1e-8},
    )
    cf = res.x.reshape(1, -1)
    prob_target = svm.predict_proba(cf)[0][TARGET]
    i += 1
    if i == 3000:
        print("Error condition not met after", i, "iterations")
        break

print(
    f"Found counterfactual: {cf[0]} after {i} steps. This point has "
    + f"an outcome of {svm.predict(cf)[0]} with a probability {svm.predict_proba(cf)[0].max()}"
)

%% python
plt.pcolormesh(xx, yy, Z, cmap=cmap, shading="auto")
plt.scatter(x1, y1, c=colours[0], edgecolor=edges[0], label="false")
plt.scatter(x2, y2, c=colours[1], edgecolor=edges[1], label="true")

plt.scatter(p_input[0][0], p_input[0][1], s=100, c="black", marker="x")
plt.annotate(
    "input",
    xy=(p_input[0][0], p_input[0][1]),
    xytext=(-20, -15),
    textcoords="offset points",
)

plt.scatter(cf[0][0], cf[0][1], s=100, c="black", marker="x")
plt.annotate(
    "counterfactual",
    xy=(cf[0][0], cf[0][1]),
    xytext=(-30, 10),
    textcoords="offset points",
)

plt.scatter(cf_nn[0][0], cf_nn[0][1], s=100, c="black", marker="x")
plt.annotate(
    "nearest neighbour",
    xy=(cf_nn[0][0], cf_nn[0][1]),
    xytext=(-10, -15),
    textcoords="offset points",
)

plt.xlim([x_min, x_max - 1])
plt.ylim([y_min, y_max])
plt.xlabel("$f_1$")
plt.ylabel("$f_2$")
plt.title("Counterfactual for $f(f_1, f_2)$")
plt.legend(loc="upper right")
plt.show()
`;
    </script>
    <script src="_static/starboard-notebook-0.5.6.min.js"></script>
  </body>
</html>
