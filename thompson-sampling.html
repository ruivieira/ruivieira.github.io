<!doctype html><html lang=en-uk><head><script data-goatcounter=https://ruivieira-dev.goatcounter.com/count async src=//gc.zgo.at/count.js></script><script src=https://unpkg.com/@alpinejs/intersect@3.x.x/dist/cdn.min.js></script><script src=https://unpkg.com/alpinejs@3.x.x/dist/cdn.min.js></script><script type=module src=https://ruivieira.dev/js/deeplinks/deeplinks.js></script><link rel=preload href=https://ruivieira.dev/lib/fonts/fa-brands-400.woff2 as=font type=font/woff2 crossorigin=anonymous><link rel=preload href=https://ruivieira.dev/lib/fonts/fa-regular-400.woff2 as=font type=font/woff2 crossorigin=anonymous><link rel=preload href=https://ruivieira.dev/lib/fonts/fa-solid-900.woff2 as=font type=font/woff2 crossorigin=anonymous><link rel=preload href=https://ruivieira.dev/fonts/firacode/FiraCode-Regular.woff2 as=font type=font/woff2 crossorigin=anonymous><link rel=preload href=https://ruivieira.dev/fonts/vollkorn/Vollkorn-Regular.woff2 as=font type=font/woff2 crossorigin=anonymous><link rel=stylesheet href=https://ruivieira.dev/css/kbd.css type=text/css><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><title>Thompson sampling · Rui Vieira</title>
<link rel=canonical href=https://ruivieira.dev/thompson-sampling.html><meta name=viewport content="width=device-width,initial-scale=1"><meta name=robots content="all,follow"><meta name=googlebot content="index,follow,snippet,archive"><meta property="og:title" content="Thompson sampling"><meta property="og:description" content="NomenclatureConsider:
a set of contexts $\mathcal{X}$ a set of actions $\mathcal{A}$ and rewards in $\mathbb{R}$ RationaleDefinitionFor each iteration $t$:
A &ldquo;player&rdquo; obtains a context $x\in \mathcal{X}$ Plays an action $a\in \mathcal{A}$ Receives a reward $r\in \mathcal{R}$ This rewards is distributed according to the context and the resulting action The player&rsquo;s goal is to execute actions that maximize the cumulative rewards. ImplementationThe implementation will focus on these concepts:
a likelihood function $P(r|\theta ,a,x)$ a set $\Theta$ of parameters $\theta$ of the distribution of $r$ a prior distribution $P(\theta )$ on these parameters past observations triplets $\mathcal{D}={(x;a;r)}$ a posterior distribution $P(\theta |{\mathcal {D}})\propto P({\mathcal {D}}|\theta )P(\theta )$, where $P({\mathcal {D}}|\theta )$ is the likelihood function."><meta property="og:type" content="article"><meta property="og:url" content="https://ruivieira.dev/thompson-sampling.html"><meta property="article:section" content="posts"><meta property="article:modified_time" content="2023-09-02T17:28:34+01:00"><meta name=twitter:card content="summary"><meta name=twitter:title content="Thompson sampling"><meta name=twitter:description content="NomenclatureConsider:
a set of contexts $\mathcal{X}$ a set of actions $\mathcal{A}$ and rewards in $\mathbb{R}$ RationaleDefinitionFor each iteration $t$:
A &ldquo;player&rdquo; obtains a context $x\in \mathcal{X}$ Plays an action $a\in \mathcal{A}$ Receives a reward $r\in \mathcal{R}$ This rewards is distributed according to the context and the resulting action The player&rsquo;s goal is to execute actions that maximize the cumulative rewards. ImplementationThe implementation will focus on these concepts:
a likelihood function $P(r|\theta ,a,x)$ a set $\Theta$ of parameters $\theta$ of the distribution of $r$ a prior distribution $P(\theta )$ on these parameters past observations triplets $\mathcal{D}={(x;a;r)}$ a posterior distribution $P(\theta |{\mathcal {D}})\propto P({\mathcal {D}}|\theta )P(\theta )$, where $P({\mathcal {D}}|\theta )$ is the likelihood function."><link rel=stylesheet href=https://ruivieira.dev/css/styles.css><!--[if lt IE 9]><script src=https://oss.maxcdn.com/html5shiv/3.7.2/html5shiv.min.js></script><script src=https://oss.maxcdn.com/respond/1.4.2/respond.min.js></script><![endif]--><link rel=icon type=image/png href=https://ruivieira.dev/images/favicon.ico></head><body class="max-width mx-auto px3 ltr" x-data="{currentHeading: undefined}"><div class="content index py4"><div id=header-post><a id=menu-icon href=#><i class="fas fa-eye fa-lg"></i></a>
<a id=menu-icon-tablet href=#><i class="fas fa-eye fa-lg"></i></a>
<a id=top-icon-tablet href=# onclick='$("html, body").animate({scrollTop:0},"fast")' style=display:none aria-label="Top of Page"><i class="fas fa-chevron-up fa-lg"></i></a>
<span id=menu><span id=nav><ul><li><a href=https://ruivieira.dev/>Home</a></li><li><a href=https://ruivieira.dev/blog/>Blog</a></li><li><a href=https://ruivieira.dev/draw/>Drawings</a></li><li><a href=https://ruivieira.dev/map/>All pages</a></li><li><a href=https://ruivieira.dev/search.html>Search</a></li></ul></span><br><div id=share style=display:none></div><div id=toc><h4>Contents</h4><nav id=TableOfContents><ul><li><a href=#nomenclature :class="{'toc-h2':true, 'toc-highlight': currentHeading == '#nomenclature' }">Nomenclature</a></li><li><a href=#rationale :class="{'toc-h2':true, 'toc-highlight': currentHeading == '#rationale' }">Rationale</a></li><li><a href=#definition :class="{'toc-h3':true, 'toc-highlight': currentHeading == '#definition' }">Definition</a></li><li><a href=#implementation :class="{'toc-h3':true, 'toc-highlight': currentHeading == '#implementation' }">Implementation</a></li><li><a href=#example :class="{'toc-h2':true, 'toc-highlight': currentHeading == '#example' }">Example</a></li><li><a href=#the-actual-sampling :class="{'toc-h2':true, 'toc-highlight': currentHeading == '#the-actual-sampling' }">The actual sampling</a></li></ul></nav><h4>Related</h4><nav><ul><li class="header-post toc"><span class=backlink-count>1</span>
<a href=https://ruivieira.dev/machine-learning.html>Machine Learning</a></li></ul></nav></div></span></div><article class=post itemscope itemtype=http://schema.org/BlogPosting><header><h1 class=posttitle itemprop="name headline">Thompson sampling</h1><div class=meta><div class=postdate>Updated <time datetime="2023-09-02 17:28:34 +0100 BST" itemprop=datePublished>2023-09-02</time>
<span class=commit-hash>(<a href=https://ruivieira.dev/log/index.html#d64c4a5>d64c4a5</a>)</span></div></div></header><div class=content itemprop=articleBody><h2 id=nomenclature x-intersect="currentHeading = '#nomenclature'">Nomenclature</h2><p>Consider:</p><ul><li>a set of contexts $\mathcal{X}$</li><li>a set of actions $\mathcal{A}$</li><li>and rewards in $\mathbb{R}$</li></ul><h2 id=rationale x-intersect="currentHeading = '#rationale'">Rationale</h2><h3 id=definition x-intersect="currentHeading = '#definition'">Definition</h3><p>For each iteration $t$:</p><ul><li>A &ldquo;player&rdquo; obtains a context $x\in \mathcal{X}$</li><li>Plays an action $a\in \mathcal{A}$</li><li>Receives a reward $r\in \mathcal{R}$</li><li>This rewards is distributed according to the context and the resulting action</li><li>The player&rsquo;s goal is to execute actions that maximize the <em>cumulative</em> rewards.</li></ul><h3 id=implementation x-intersect="currentHeading = '#implementation'">Implementation</h3><p>The implementation will focus on these concepts:</p><ul><li>a likelihood function $P(r|\theta ,a,x)$</li><li>a set $\Theta$ of parameters $\theta$ of the distribution of $r$</li><li>a prior distribution $P(\theta )$ on these parameters</li><li>past observations triplets $\mathcal{D}={(x;a;r)}$</li><li>a posterior distribution $P(\theta |{\mathcal {D}})\propto P({\mathcal {D}}|\theta )P(\theta )$, where $P({\mathcal {D}}|\theta )$ is the likelihood function.</li></ul><p>Thompson sampling consists in playing the action $a^{\ast }\in {\mathcal {A}}$ according to the probability that it maximizes the expected reward, i.e. action $a^{\ast }$ is chosen with probability</p><p>$$
\int \mathbb {I} \left[\mathbb {E} (r|a^{\ast },x,\theta )=\max _{a&rsquo;}\mathbb {E} (r|a&rsquo;,x,\theta )\right]P(\theta |{\mathcal {D}})d\theta ,
$$</p><p>where $\mathbb {I}$ is the indicator function.</p><p>In practice, the rule is implemented by sampling. In each round, parameters $\theta^\ast$ are sampled from the posterior $P(\theta |{\mathcal {D}})$, and an action $a^{\ast }$ chosen that maximizes ${\mathbb {E}}[r|\theta ^{\ast },a^{\ast },x]$, <em>i.e.</em> the expected reward given the sampled parameters, the action, and the current context. Conceptually, this means that the player instantiates their beliefs randomly in each round according to the posterior distribution, and then acts optimally according to them. In most practical applications, it is computationally onerous to maintain and sample from a posterior distribution over models. As such, Thompson sampling is often used in conjunction with approximate sampling techniques.</p><h2 id=example x-intersect="currentHeading = '#example'">Example</h2><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>N_TRIALS <span style=font-weight:700>=</span> <span style=color:#099>2000</span>
</span></span><span style=display:flex><span>N_ARMS <span style=font-weight:700>=</span> <span style=color:#099>16</span>
</span></span><span style=display:flex><span>N_FEATURES <span style=font-weight:700>=</span> <span style=color:#099>5</span>
</span></span><span style=display:flex><span>BEST_ARMS <span style=font-weight:700>=</span> [<span style=color:#099>3</span>, <span style=color:#099>7</span>, <span style=color:#099>9</span>, <span style=color:#099>15</span>]
</span></span></code></pre></div><p>We now define a function to generate context vectors for all arms for each of the trial.
We need:</p><ul><li><code>n_trials</code>, number of trials ($N_T$)</li><li><code>n_arms</code>, number of arms per trial ($N_A$)</li><li><code>n_features</code>, number of feature per context vector ($N_f$)</li></ul><p>This function will return a matrix of size $N_{T} \times N_{A} \times N_{f}$</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=font-weight:700>def</span> <span style=color:#900;font-weight:700>make_design_matrix</span>(n_trials: <span style=color:#999>int</span>, 
</span></span><span style=display:flex><span>                       n_arms: <span style=color:#999>int</span>, 
</span></span><span style=display:flex><span>                       n_features: <span style=color:#999>int</span>) <span style=font-weight:700>-&gt;</span> np<span style=font-weight:700>.</span>ndarray:
</span></span><span style=display:flex><span>    available_arms <span style=font-weight:700>=</span> np<span style=font-weight:700>.</span>arange(n_arms)
</span></span><span style=display:flex><span>    X <span style=font-weight:700>=</span> np<span style=font-weight:700>.</span>array([[np<span style=font-weight:700>.</span>random<span style=font-weight:700>.</span>uniform(<span style=color:#099>0</span>, <span style=color:#099>1</span>, size <span style=font-weight:700>=</span> n_features) 
</span></span><span style=display:flex><span>                   <span style=font-weight:700>for</span> _ <span style=font-weight:700>in</span> np<span style=font-weight:700>.</span>arange(n_arms)] 
</span></span><span style=display:flex><span>                   <span style=font-weight:700>for</span> _ <span style=font-weight:700>in</span> np<span style=font-weight:700>.</span>arange(n_trials)])
</span></span><span style=display:flex><span>    <span style=font-weight:700>return</span> X
</span></span></code></pre></div><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>X <span style=font-weight:700>=</span> make_design_matrix(n_trials<span style=font-weight:700>=</span>N_TRIALS, 
</span></span><span style=display:flex><span>                       n_arms<span style=font-weight:700>=</span>N_ARMS, 
</span></span><span style=display:flex><span>                       n_features<span style=font-weight:700>=</span>N_FEATURES)
</span></span></code></pre></div><p>This will have the shape</p><p>![[Thompson sampling trials.excalidraw.svg]]</p><p>The following function will generate the true $\Theta = {\theta_1,\dots,\theta_n}$ for testing purposes.
We provide:</p><ul><li>$N_A$, number of arms (<code>n_arms</code>)</li><li>$N_f$, number of features for the context vector (<code>n_features</code>)</li><li><code>best_arms</code>, arms in which we should give some bias values (for good)</li><li><code>bias</code>, value to be added to the best arms</li></ul><p>A matrix of size $N_{A} \times N_{f}$, each value is a random value with $\mu = 0$ and standard deviation of $\frac{1}{4}$. However, for the best arms, we will add the bias.</p><p>![[Thompson sampling thetas.excalidraw.svg]]</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=font-weight:700>def</span> <span style=color:#900;font-weight:700>make_theta</span>(n_arms: <span style=color:#999>int</span>, 
</span></span><span style=display:flex><span>               n_features: <span style=color:#999>int</span>, best_arms, bias <span style=font-weight:700>=</span> <span style=color:#099>1</span>):
</span></span><span style=display:flex><span>    true_theta <span style=font-weight:700>=</span> np<span style=font-weight:700>.</span>array(
</span></span><span style=display:flex><span>        [np<span style=font-weight:700>.</span>random<span style=font-weight:700>.</span>normal(size<span style=font-weight:700>=</span>n_features, scale<span style=font-weight:700>=</span><span style=color:#099>1.0</span><span style=font-weight:700>/</span><span style=color:#099>4.0</span>) 
</span></span><span style=display:flex><span>        <span style=font-weight:700>for</span> _ <span style=font-weight:700>in</span> <span style=color:#999>range</span>(n_arms)])
</span></span><span style=display:flex><span>    true_theta[best_arms] <span style=font-weight:700>+=</span> bias
</span></span><span style=display:flex><span>    <span style=font-weight:700>return</span> true_theta
</span></span></code></pre></div><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>true_theta <span style=font-weight:700>=</span> make_theta(
</span></span><span style=display:flex><span>    n_arms<span style=font-weight:700>=</span>N_ARMS, 
</span></span><span style=display:flex><span>    n_features<span style=font-weight:700>=</span>N_FEATURES, 
</span></span><span style=display:flex><span>    best_arms<span style=font-weight:700>=</span>BEST_ARMS)
</span></span></code></pre></div><p><img src=https://ruivieira.dev/Thompson%20sampling_files/figure-gfm/cell-8-output-1.png alt loading=lazy></p><p>A function is also available to generate rewards. It creates rewards for each arm, given a context.</p><p>We provide:</p><ul><li>$a$, this is the arm index ($0\leq a \leq N_{A}-1$)</li><li><code>x</code>, is the context that we are observing for the arm index (arm)</li><li>$\theta$, is the theta (true or predicted) that are are using to estimate the reward for each arm (<code>theta</code>)</li><li><code>scale_noise</code>, we may need to add some random noise ($\mu=0$ and standard deviation as <code>scale_noise</code>)</li></ul><p>This will return the estimated score for the arm (with the arm index and the context observed corresponding to the given theta).</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=font-weight:700>def</span> <span style=color:#900;font-weight:700>generate_reward</span>(arm, x, theta, scale_noise <span style=font-weight:700>=</span> <span style=color:#099>1.0</span><span style=font-weight:700>/</span><span style=color:#099>10.0</span>):
</span></span><span style=display:flex><span>    signal <span style=font-weight:700>=</span> theta[arm]<span style=font-weight:700>.</span>dot(x)
</span></span><span style=display:flex><span>    noise <span style=font-weight:700>=</span> np<span style=font-weight:700>.</span>random<span style=font-weight:700>.</span>normal(scale<span style=font-weight:700>=</span>scale_noise)
</span></span><span style=display:flex><span>    <span style=font-weight:700>return</span> signal <span style=font-weight:700>+</span> noise
</span></span></code></pre></div><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>random_payoffs <span style=font-weight:700>=</span> np<span style=font-weight:700>.</span>array(
</span></span><span style=display:flex><span>    [generate_reward(
</span></span><span style=display:flex><span>        arm<span style=font-weight:700>=</span>np<span style=font-weight:700>.</span>random<span style=font-weight:700>.</span>choice(N_ARMS), 
</span></span><span style=display:flex><span>        x<span style=font-weight:700>=</span>X[t, np<span style=font-weight:700>.</span>random<span style=font-weight:700>.</span>choice(N_ARMS)], 
</span></span><span style=display:flex><span>        theta<span style=font-weight:700>=</span>true_theta) 
</span></span><span style=display:flex><span>        <span style=font-weight:700>for</span> t <span style=font-weight:700>in</span> <span style=color:#999>range</span>(N_TRIALS)])
</span></span></code></pre></div><p><img src=https://ruivieira.dev/Thompson%20sampling_files/figure-gfm/cell-11-output-1.png alt loading=lazy></p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#998;font-style:italic># Defining oracle (best payoffs based on the true_theta)</span>
</span></span><span style=display:flex><span>oracles <span style=font-weight:700>=</span> np<span style=font-weight:700>.</span>array(
</span></span><span style=display:flex><span>    [np<span style=font-weight:700>.</span>max(
</span></span><span style=display:flex><span>        [generate_reward(
</span></span><span style=display:flex><span>            arm<span style=font-weight:700>=</span>arm,
</span></span><span style=display:flex><span>            x<span style=font-weight:700>=</span>X[t, arm],
</span></span><span style=display:flex><span>            theta<span style=font-weight:700>=</span>true_theta) 
</span></span><span style=display:flex><span>        <span style=font-weight:700>for</span> arm <span style=font-weight:700>in</span> <span style=color:#999>range</span>(N_ARMS)]) 
</span></span><span style=display:flex><span>    <span style=font-weight:700>for</span> t <span style=font-weight:700>in</span> <span style=color:#999>range</span>(N_TRIALS)])
</span></span></code></pre></div><p><img src=https://ruivieira.dev/Thompson%20sampling_files/figure-gfm/cell-13-output-1.png alt loading=lazy></p><p>We also create a function to generate the cumulative regret over time.</p><p>We provide:</p><ul><li><code>payoffs</code>, an array of $T$ payoffs (for $T$ number of trials)</li><li><code>oracles</code>, an array of best values for $T$ trials (oracles)</li></ul><p>And we get an array of the cumulative sum over time (of size $T$).</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=font-weight:700>def</span> <span style=color:#900;font-weight:700>make_regret</span>(payoffs: np<span style=font-weight:700>.</span>ndarray, 
</span></span><span style=display:flex><span>                oracles: np<span style=font-weight:700>.</span>ndarray) <span style=font-weight:700>-&gt;</span> np<span style=font-weight:700>.</span>ndarray:
</span></span><span style=display:flex><span>    <span style=font-weight:700>return</span> np<span style=font-weight:700>.</span>cumsum(oracles <span style=font-weight:700>-</span> payoffs)
</span></span></code></pre></div><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>payoffs <span style=font-weight:700>=</span> [
</span></span><span style=display:flex><span>    [generate_reward(
</span></span><span style=display:flex><span>        arm<span style=font-weight:700>=</span>arm, 
</span></span><span style=display:flex><span>        x<span style=font-weight:700>=</span>X[t, arm], 
</span></span><span style=display:flex><span>        theta<span style=font-weight:700>=</span>true_theta) 
</span></span><span style=display:flex><span>        <span style=font-weight:700>for</span> arm <span style=font-weight:700>in</span> np<span style=font-weight:700>.</span>arange(N_ARMS)] 
</span></span><span style=display:flex><span>    <span style=font-weight:700>for</span> t <span style=font-weight:700>in</span> np<span style=font-weight:700>.</span>arange(N_TRIALS)]
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>ave_rewards <span style=font-weight:700>=</span> np<span style=font-weight:700>.</span>mean(payoffs, axis<span style=font-weight:700>=</span><span style=color:#099>0</span>)
</span></span></code></pre></div><h2 id=the-actual-sampling x-intersect="currentHeading = '#the-actual-sampling'">The actual sampling</h2><p>The method to perform the actual sampling is next.
We provide:</p><ul><li>$\delta$ (<code>delta</code>), with $0 &lt; \delta &lt; 1$.
With probability $1 - \delta$, linear thompson sampling satisfies the theoretical regret bound.</li><li>$R$, with $R \geq 0$.
Assume that the residual $ri(t) - bi(t)^T \hat{\mu}$ is R-sub-gaussian.
In this case, $R^2$ represents the variance for residuals of the linear model $bi(t)^T$.</li><li>$\epsilon$ (<code>epsilon</code>), with $0 &lt; \epsilon &lt; 1$
A parameter used by the Thompson Sampling algorithm. If the total trials $T$ is known, we can choose $\epsilon = \frac{1}{\ln{T}}$.</li></ul><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>delta<span style=font-weight:700>=</span><span style=color:#099>0.5</span>
</span></span><span style=display:flex><span>R <span style=font-weight:700>=</span> <span style=color:#099>0.01</span>
</span></span><span style=display:flex><span>epsilon<span style=font-weight:700>=</span><span style=color:#099>0.5</span>
</span></span></code></pre></div><p>We use <code>r_payoffs</code> to store the payoff for each trial (the payoff for the selected arm based on the <code>true_theta</code>).
As such, we initialise a zero array of size <code>n_trials</code>.</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>r_payoffs <span style=font-weight:700>=</span> np<span style=font-weight:700>.</span>zeros(N_TRIALS)
</span></span></code></pre></div><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>v <span style=font-weight:700>=</span> R <span style=font-weight:700>*</span> np<span style=font-weight:700>.</span>sqrt(<span style=color:#099>24</span> <span style=font-weight:700>/</span> epsilon <span style=font-weight:700>*</span> N_FEATURES <span style=font-weight:700>*</span> np<span style=font-weight:700>.</span>log(<span style=color:#099>1</span> <span style=font-weight:700>/</span> delta))
</span></span></code></pre></div><p>Model initialisation:</p><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>B <span style=font-weight:700>=</span> np<span style=font-weight:700>.</span>identity(N_FEATURES) 
</span></span><span style=display:flex><span>mu_hat <span style=font-weight:700>=</span> np<span style=font-weight:700>.</span>zeros(shape<span style=font-weight:700>=</span>(N_FEATURES, <span style=color:#099>1</span>))
</span></span><span style=display:flex><span>f <span style=font-weight:700>=</span> np<span style=font-weight:700>.</span>zeros(shape<span style=font-weight:700>=</span>(N_FEATURES,<span style=color:#099>1</span>))  
</span></span></code></pre></div><div class=highlight><pre tabindex=0 style=background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=font-weight:700>for</span> t <span style=font-weight:700>in</span> <span style=color:#999>range</span>(N_TRIALS):
</span></span><span style=display:flex><span>        context <span style=font-weight:700>=</span> X[t]
</span></span><span style=display:flex><span>        mu_tilde <span style=font-weight:700>=</span> np<span style=font-weight:700>.</span>random<span style=font-weight:700>.</span>multivariate_normal(mu_hat<span style=font-weight:700>.</span>flat, v<span style=font-weight:700>**</span><span style=color:#099>2</span> <span style=font-weight:700>*</span> np<span style=font-weight:700>.</span>linalg<span style=font-weight:700>.</span>inv(B))[<span style=font-weight:700>...</span>, np<span style=font-weight:700>.</span>newaxis]
</span></span><span style=display:flex><span>        score_array <span style=font-weight:700>=</span> context<span style=font-weight:700>.</span>dot(mu_tilde)
</span></span><span style=display:flex><span>        chosen_arm <span style=font-weight:700>=</span> np<span style=font-weight:700>.</span>argmax(score_array)
</span></span><span style=display:flex><span>        context_t <span style=font-weight:700>=</span> context[chosen_arm]
</span></span><span style=display:flex><span>        reward <span style=font-weight:700>=</span> generate_reward(arm<span style=font-weight:700>=</span>chosen_arm, x<span style=font-weight:700>=</span>context_t, theta<span style=font-weight:700>=</span>true_theta)
</span></span><span style=display:flex><span>        r_payoffs[t] <span style=font-weight:700>=</span> reward
</span></span><span style=display:flex><span>        context_t <span style=font-weight:700>=</span> np<span style=font-weight:700>.</span>reshape(context_t, (<span style=font-weight:700>-</span><span style=color:#099>1</span>, <span style=color:#099>1</span>))
</span></span><span style=display:flex><span>        B <span style=font-weight:700>+=</span> context_t<span style=font-weight:700>.</span>dot(context_t<span style=font-weight:700>.</span>T)
</span></span><span style=display:flex><span>        f <span style=font-weight:700>+=</span> reward<span style=font-weight:700>*</span>context_t
</span></span><span style=display:flex><span>        mu_hat <span style=font-weight:700>=</span> np<span style=font-weight:700>.</span>linalg<span style=font-weight:700>.</span>inv(B)<span style=font-weight:700>.</span>dot(f)
</span></span></code></pre></div><p><img src=https://ruivieira.dev/Thompson%20sampling_files/figure-gfm/cell-22-output-1.png alt loading=lazy></p></div></article><div id=footer-post-container><div id=footer-post><div id=nav-footer style=display:none><ul><li><a href=https://ruivieira.dev/>Home</a></li><li><a href=https://ruivieira.dev/blog/>Blog</a></li><li><a href=https://ruivieira.dev/draw/>Drawings</a></li><li><a href=https://ruivieira.dev/map/>All pages</a></li><li><a href=https://ruivieira.dev/search.html>Search</a></li></ul></div><div id=toc-footer style=display:none><nav id=TableOfContents><ul><li><a href=#nomenclature>Nomenclature</a></li><li><a href=#rationale>Rationale</a><ul><li><a href=#definition>Definition</a></li><li><a href=#implementation>Implementation</a></li></ul></li><li><a href=#example>Example</a></li><li><a href=#the-actual-sampling>The actual sampling</a></li></ul></nav></div><div id=share-footer style=display:none></div><div id=actions-footer><a id=menu-toggle class=icon href=# onclick='return $("#nav-footer").toggle(),!1' aria-label=Menu><i class="fas fa-bars fa-lg" aria-hidden=true></i> Menu</a>
<a id=toc-toggle class=icon href=# onclick='return $("#toc-footer").toggle(),!1' aria-label=TOC><i class="fas fa-list fa-lg" aria-hidden=true></i> TOC</a>
<a id=share-toggle class=icon href=# onclick='return $("#share-footer").toggle(),!1' aria-label=Share><i class="fas fa-share-alt fa-lg" aria-hidden=true></i> share</a>
<a id=top style=display:none class=icon href=# onclick='$("html, body").animate({scrollTop:0},"fast")' aria-label="Top of Page"><i class="fas fa-chevron-up fa-lg" aria-hidden=true></i> Top</a></div></div></div><footer id=footer><div class=footer-left>Copyright &copy; 2024 Rui Vieira</div><div class=footer-right><nav><ul><li><a href=https://ruivieira.dev/>Home</a></li><li><a href=https://ruivieira.dev/blog/>Blog</a></li><li><a href=https://ruivieira.dev/draw/>Drawings</a></li><li><a href=https://ruivieira.dev/map/>All pages</a></li><li><a href=https://ruivieira.dev/search.html>Search</a></li></ul></nav></div></footer></div></body><link rel=stylesheet href=https://ruivieira.dev/css/fa.min.css><script src=https://ruivieira.dev/js/jquery-3.6.0.min.js></script><script src=https://ruivieira.dev/js/mark.min.js></script><script src=https://ruivieira.dev/js/main.js></script><script>MathJax={tex:{inlineMath:[["$","$"],["\\(","\\)"]]},svg:{fontCache:"global"}}</script><script type=text/javascript id=MathJax-script async src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js></script></html>